{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "What_12layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjG64ra4aFu",
        "outputId": "1ec5d9d5-bace-4f16-e613-dbf4b4ae7a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8-7SARDZErK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwJv7Y8Rewez",
        "outputId": "038f3fef-2afe-4bb2-9aaa-67257cac4697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    \n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nDYhjJse6Qq",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aivGVg14e9iZ",
        "colab": {}
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIuiboIUfViV"
      },
      "source": [
        "# Load data from saved file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cog5VUzGgE5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f497c483-adf1-47c9-ec71-482d02a2b618"
      },
      "source": [
        "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
        "print(len(fore_idx))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000 10000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xX91RwMy-IP4",
        "colab": {}
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGz8Y88vIZPT",
        "colab": {}
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
        "\n",
        "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSO9SFE25Lrk",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obE1xeyRks1Q",
        "colab": {}
      },
      "source": [
        "batch = 256\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
        "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SadRzWBBZEsP",
        "colab": {}
      },
      "source": [
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IgGYMG_ZZEsT",
        "colab": {}
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)\n",
        "        \n",
        "        x = torch.cat((x1,x3),dim=1)\n",
        "        \n",
        "    \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thkdqW91Hpju",
        "colab": {}
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,1,3,pad=1)\n",
        "        self.pool = nn.MaxPool2d(3,stride=1,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1yVWgR4vFhe",
        "colab": {}
      },
      "source": [
        "class classy_inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classy_inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,64,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(64,16,16)\n",
        "        self.incept2 = inception_module(32,16,56)\n",
        "        \n",
        "        self.downsample1 = downsample_module(72,72)\n",
        "        \n",
        "        self.incept3 = inception_module(144,32,32)\n",
        "        self.incept4 = inception_module(64,48,32)\n",
        "        self.incept5 = inception_module(80,40,40)\n",
        "        self.incept6 = inception_module(80,32,48)\n",
        "        \n",
        "        self.downsample2 = downsample_module(80,64)\n",
        "        \n",
        "        self.incept7 = inception_module(144,128,64)\n",
        "        self.incept8 = inception_module(192,48,32)\n",
        "        \n",
        "        self.incept9 = inception_module(80,64,64)\n",
        "        self.incept10 = inception_module(128,64,32)\n",
        "\n",
        "        self.incept11 = inception_module(96,32,64)\n",
        "        self.incept12 = inception_module(96,32,48)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.pool = nn.AvgPool2d(7)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.linear = nn.Linear(4*4*80,3)\n",
        "    def forward(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        x = self.incept1.forward(x)\n",
        "        x = self.incept2.forward(x)\n",
        "        x = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        x = self.incept4.forward(x)\n",
        "        x = self.incept5.forward(x)\n",
        "        x = self.incept6.forward(x)\n",
        "        x = self.downsample2.forward(x) \n",
        "        x = self.incept7.forward(x)\n",
        "        x = self.incept8.forward(x)\n",
        "        \n",
        "        x = self.incept9.forward(x)\n",
        "        x = self.incept10.forward(x)\n",
        "        x = self.incept11.forward(x)\n",
        "        x = self.incept12.forward(x)\n",
        "        # print(x.shape)\n",
        "        x = self.pool(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1,4*4*80)\n",
        "        x = self.linear(x) \n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cOWrnzv1fVjD",
        "colab": {}
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFfAJZkcZEsY",
        "colab": {}
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = classy_inception_net().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 70\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        " \n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/checking_what_net/weights_12layers_inception/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mI-vqhB-fVjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d5cc7ba-ffe5-4f00-c612-94597e7f66ee"
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.097\n",
            "[1,    20] loss: 1.061\n",
            "[1,    30] loss: 1.103\n",
            "[1,    40] loss: 1.055\n",
            "[2,    10] loss: 1.065\n",
            "[2,    20] loss: 1.047\n",
            "[2,    30] loss: 1.045\n",
            "[2,    40] loss: 1.065\n",
            "[3,    10] loss: 1.074\n",
            "[3,    20] loss: 1.036\n",
            "[3,    30] loss: 1.026\n",
            "[3,    40] loss: 1.041\n",
            "[4,    10] loss: 1.014\n",
            "[4,    20] loss: 1.022\n",
            "[4,    30] loss: 1.000\n",
            "[4,    40] loss: 0.991\n",
            "[5,    10] loss: 1.039\n",
            "[5,    20] loss: 1.021\n",
            "[5,    30] loss: 0.997\n",
            "[5,    40] loss: 0.978\n",
            "[6,    10] loss: 1.004\n",
            "[6,    20] loss: 0.982\n",
            "[6,    30] loss: 0.989\n",
            "[6,    40] loss: 0.977\n",
            "[7,    10] loss: 0.977\n",
            "[7,    20] loss: 0.957\n",
            "[7,    30] loss: 0.970\n",
            "[7,    40] loss: 0.976\n",
            "[8,    10] loss: 0.945\n",
            "[8,    20] loss: 0.946\n",
            "[8,    30] loss: 0.940\n",
            "[8,    40] loss: 0.954\n",
            "[9,    10] loss: 0.933\n",
            "[9,    20] loss: 0.957\n",
            "[9,    30] loss: 0.945\n",
            "[9,    40] loss: 0.936\n",
            "[10,    10] loss: 0.928\n",
            "[10,    20] loss: 0.938\n",
            "[10,    30] loss: 0.913\n",
            "[10,    40] loss: 0.925\n",
            "[11,    10] loss: 0.902\n",
            "[11,    20] loss: 0.875\n",
            "[11,    30] loss: 0.877\n",
            "[11,    40] loss: 0.894\n",
            "[12,    10] loss: 0.871\n",
            "[12,    20] loss: 0.867\n",
            "[12,    30] loss: 0.880\n",
            "[12,    40] loss: 0.846\n",
            "[13,    10] loss: 0.872\n",
            "[13,    20] loss: 0.854\n",
            "[13,    30] loss: 0.823\n",
            "[13,    40] loss: 0.839\n",
            "[14,    10] loss: 0.860\n",
            "[14,    20] loss: 0.873\n",
            "[14,    30] loss: 0.896\n",
            "[14,    40] loss: 0.857\n",
            "[15,    10] loss: 0.820\n",
            "[15,    20] loss: 0.779\n",
            "[15,    30] loss: 0.799\n",
            "[15,    40] loss: 0.796\n",
            "[16,    10] loss: 0.751\n",
            "[16,    20] loss: 0.730\n",
            "[16,    30] loss: 0.735\n",
            "[16,    40] loss: 0.767\n",
            "[17,    10] loss: 0.820\n",
            "[17,    20] loss: 0.771\n",
            "[17,    30] loss: 0.780\n",
            "[17,    40] loss: 0.739\n",
            "[18,    10] loss: 0.696\n",
            "[18,    20] loss: 0.730\n",
            "[18,    30] loss: 0.697\n",
            "[18,    40] loss: 0.646\n",
            "[19,    10] loss: 0.656\n",
            "[19,    20] loss: 0.654\n",
            "[19,    30] loss: 0.616\n",
            "[19,    40] loss: 0.589\n",
            "[20,    10] loss: 0.605\n",
            "[20,    20] loss: 0.623\n",
            "[20,    30] loss: 0.579\n",
            "[20,    40] loss: 0.609\n",
            "[21,    10] loss: 0.610\n",
            "[21,    20] loss: 0.651\n",
            "[21,    30] loss: 0.614\n",
            "[21,    40] loss: 0.623\n",
            "[22,    10] loss: 0.609\n",
            "[22,    20] loss: 0.623\n",
            "[22,    30] loss: 0.570\n",
            "[22,    40] loss: 0.587\n",
            "[23,    10] loss: 0.604\n",
            "[23,    20] loss: 0.618\n",
            "[23,    30] loss: 0.551\n",
            "[23,    40] loss: 0.582\n",
            "[24,    10] loss: 0.680\n",
            "[24,    20] loss: 0.651\n",
            "[24,    30] loss: 0.616\n",
            "[24,    40] loss: 0.572\n",
            "[25,    10] loss: 0.534\n",
            "[25,    20] loss: 0.500\n",
            "[25,    30] loss: 0.460\n",
            "[25,    40] loss: 0.418\n",
            "[26,    10] loss: 0.451\n",
            "[26,    20] loss: 0.441\n",
            "[26,    30] loss: 0.403\n",
            "[26,    40] loss: 0.394\n",
            "[27,    10] loss: 0.348\n",
            "[27,    20] loss: 0.347\n",
            "[27,    30] loss: 0.335\n",
            "[27,    40] loss: 0.322\n",
            "[28,    10] loss: 0.340\n",
            "[28,    20] loss: 0.322\n",
            "[28,    30] loss: 0.289\n",
            "[28,    40] loss: 0.307\n",
            "[29,    10] loss: 0.473\n",
            "[29,    20] loss: 0.494\n",
            "[29,    30] loss: 0.414\n",
            "[29,    40] loss: 0.367\n",
            "[30,    10] loss: 0.410\n",
            "[30,    20] loss: 0.399\n",
            "[30,    30] loss: 0.353\n",
            "[30,    40] loss: 0.308\n",
            "[31,    10] loss: 0.293\n",
            "[31,    20] loss: 0.239\n",
            "[31,    30] loss: 0.197\n",
            "[31,    40] loss: 0.175\n",
            "[32,    10] loss: 0.142\n",
            "[32,    20] loss: 0.127\n",
            "[32,    30] loss: 0.105\n",
            "[32,    40] loss: 0.088\n",
            "[33,    10] loss: 0.060\n",
            "[33,    20] loss: 0.053\n",
            "[33,    30] loss: 0.043\n",
            "[33,    40] loss: 0.040\n",
            "[34,    10] loss: 0.026\n",
            "[34,    20] loss: 0.023\n",
            "[34,    30] loss: 0.020\n",
            "[34,    40] loss: 0.017\n",
            "[35,    10] loss: 0.011\n",
            "[35,    20] loss: 0.011\n",
            "[35,    30] loss: 0.010\n",
            "[35,    40] loss: 0.010\n",
            "[36,    10] loss: 0.008\n",
            "[36,    20] loss: 0.007\n",
            "[36,    30] loss: 0.007\n",
            "[36,    40] loss: 0.007\n",
            "[37,    10] loss: 0.006\n",
            "[37,    20] loss: 0.006\n",
            "[37,    30] loss: 0.005\n",
            "[37,    40] loss: 0.006\n",
            "[38,    10] loss: 0.005\n",
            "[38,    20] loss: 0.005\n",
            "[38,    30] loss: 0.004\n",
            "[38,    40] loss: 0.005\n",
            "[39,    10] loss: 0.004\n",
            "[39,    20] loss: 0.004\n",
            "[39,    30] loss: 0.004\n",
            "[39,    40] loss: 0.004\n",
            "[40,    10] loss: 0.003\n",
            "[40,    20] loss: 0.004\n",
            "[40,    30] loss: 0.004\n",
            "[40,    40] loss: 0.003\n",
            "[41,    10] loss: 0.003\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.003\n",
            "[41,    40] loss: 0.003\n",
            "[42,    10] loss: 0.003\n",
            "[42,    20] loss: 0.003\n",
            "[42,    30] loss: 0.003\n",
            "[42,    40] loss: 0.003\n",
            "[43,    10] loss: 0.003\n",
            "[43,    20] loss: 0.003\n",
            "[43,    30] loss: 0.003\n",
            "[43,    40] loss: 0.015\n",
            "[44,    10] loss: 0.124\n",
            "[44,    20] loss: 0.164\n",
            "[44,    30] loss: 0.147\n",
            "[44,    40] loss: 0.139\n",
            "[45,    10] loss: 0.355\n",
            "[45,    20] loss: 0.388\n",
            "[45,    30] loss: 0.375\n",
            "[45,    40] loss: 0.349\n",
            "[46,    10] loss: 0.579\n",
            "[46,    20] loss: 0.580\n",
            "[46,    30] loss: 0.514\n",
            "[46,    40] loss: 0.449\n",
            "[47,    10] loss: 0.569\n",
            "[47,    20] loss: 0.564\n",
            "[47,    30] loss: 0.443\n",
            "[47,    40] loss: 0.395\n",
            "[48,    10] loss: 0.375\n",
            "[48,    20] loss: 0.326\n",
            "[48,    30] loss: 0.258\n",
            "[48,    40] loss: 0.239\n",
            "[49,    10] loss: 0.348\n",
            "[49,    20] loss: 0.318\n",
            "[49,    30] loss: 0.241\n",
            "[49,    40] loss: 0.185\n",
            "[50,    10] loss: 0.165\n",
            "[50,    20] loss: 0.139\n",
            "[50,    30] loss: 0.104\n",
            "[50,    40] loss: 0.081\n",
            "[51,    10] loss: 0.051\n",
            "[51,    20] loss: 0.037\n",
            "[51,    30] loss: 0.033\n",
            "[51,    40] loss: 0.025\n",
            "[52,    10] loss: 0.014\n",
            "[52,    20] loss: 0.016\n",
            "[52,    30] loss: 0.013\n",
            "[52,    40] loss: 0.011\n",
            "[53,    10] loss: 0.007\n",
            "[53,    20] loss: 0.008\n",
            "[53,    30] loss: 0.007\n",
            "[53,    40] loss: 0.012\n",
            "[54,    10] loss: 0.011\n",
            "[54,    20] loss: 0.013\n",
            "[54,    30] loss: 0.010\n",
            "[54,    40] loss: 0.009\n",
            "[55,    10] loss: 0.006\n",
            "[55,    20] loss: 0.005\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.005\n",
            "[56,    10] loss: 0.004\n",
            "[56,    20] loss: 0.004\n",
            "[56,    30] loss: 0.004\n",
            "[56,    40] loss: 0.005\n",
            "[57,    10] loss: 0.004\n",
            "[57,    20] loss: 0.006\n",
            "[57,    30] loss: 0.004\n",
            "[57,    40] loss: 0.004\n",
            "[58,    10] loss: 0.003\n",
            "[58,    20] loss: 0.003\n",
            "[58,    30] loss: 0.003\n",
            "[58,    40] loss: 0.004\n",
            "[59,    10] loss: 0.003\n",
            "[59,    20] loss: 0.004\n",
            "[59,    30] loss: 0.003\n",
            "[59,    40] loss: 0.004\n",
            "[60,    10] loss: 0.003\n",
            "[60,    20] loss: 0.003\n",
            "[60,    30] loss: 0.003\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.002\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.008\n",
            "[62,    10] loss: 0.016\n",
            "[62,    20] loss: 0.020\n",
            "[62,    30] loss: 0.011\n",
            "[62,    40] loss: 0.008\n",
            "[63,    10] loss: 0.005\n",
            "[63,    20] loss: 0.004\n",
            "[63,    30] loss: 0.003\n",
            "[63,    40] loss: 0.003\n",
            "[64,    10] loss: 0.002\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.002\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.002\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.002\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.002\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.002\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.004\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 74 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 69 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.023\n",
            "[1,    20] loss: 0.908\n",
            "[1,    30] loss: 0.794\n",
            "[1,    40] loss: 0.770\n",
            "[2,    10] loss: 0.791\n",
            "[2,    20] loss: 0.771\n",
            "[2,    30] loss: 0.763\n",
            "[2,    40] loss: 0.802\n",
            "[3,    10] loss: 0.788\n",
            "[3,    20] loss: 0.757\n",
            "[3,    30] loss: 0.706\n",
            "[3,    40] loss: 0.695\n",
            "[4,    10] loss: 0.690\n",
            "[4,    20] loss: 0.653\n",
            "[4,    30] loss: 0.637\n",
            "[4,    40] loss: 0.618\n",
            "[5,    10] loss: 0.625\n",
            "[5,    20] loss: 0.592\n",
            "[5,    30] loss: 0.587\n",
            "[5,    40] loss: 0.613\n",
            "[6,    10] loss: 0.685\n",
            "[6,    20] loss: 0.585\n",
            "[6,    30] loss: 0.579\n",
            "[6,    40] loss: 0.596\n",
            "[7,    10] loss: 0.563\n",
            "[7,    20] loss: 0.587\n",
            "[7,    30] loss: 0.572\n",
            "[7,    40] loss: 0.515\n",
            "[8,    10] loss: 0.513\n",
            "[8,    20] loss: 0.516\n",
            "[8,    30] loss: 0.510\n",
            "[8,    40] loss: 0.486\n",
            "[9,    10] loss: 0.484\n",
            "[9,    20] loss: 0.471\n",
            "[9,    30] loss: 0.448\n",
            "[9,    40] loss: 0.457\n",
            "[10,    10] loss: 0.437\n",
            "[10,    20] loss: 0.466\n",
            "[10,    30] loss: 0.457\n",
            "[10,    40] loss: 0.421\n",
            "[11,    10] loss: 0.405\n",
            "[11,    20] loss: 0.390\n",
            "[11,    30] loss: 0.403\n",
            "[11,    40] loss: 0.418\n",
            "[12,    10] loss: 0.470\n",
            "[12,    20] loss: 0.454\n",
            "[12,    30] loss: 0.457\n",
            "[12,    40] loss: 0.427\n",
            "[13,    10] loss: 0.389\n",
            "[13,    20] loss: 0.369\n",
            "[13,    30] loss: 0.378\n",
            "[13,    40] loss: 0.378\n",
            "[14,    10] loss: 0.378\n",
            "[14,    20] loss: 0.369\n",
            "[14,    30] loss: 0.343\n",
            "[14,    40] loss: 0.338\n",
            "[15,    10] loss: 0.361\n",
            "[15,    20] loss: 0.349\n",
            "[15,    30] loss: 0.359\n",
            "[15,    40] loss: 0.308\n",
            "[16,    10] loss: 0.302\n",
            "[16,    20] loss: 0.289\n",
            "[16,    30] loss: 0.287\n",
            "[16,    40] loss: 0.293\n",
            "[17,    10] loss: 0.331\n",
            "[17,    20] loss: 0.341\n",
            "[17,    30] loss: 0.310\n",
            "[17,    40] loss: 0.286\n",
            "[18,    10] loss: 0.286\n",
            "[18,    20] loss: 0.288\n",
            "[18,    30] loss: 0.271\n",
            "[18,    40] loss: 0.255\n",
            "[19,    10] loss: 0.278\n",
            "[19,    20] loss: 0.249\n",
            "[19,    30] loss: 0.250\n",
            "[19,    40] loss: 0.208\n",
            "[20,    10] loss: 0.188\n",
            "[20,    20] loss: 0.185\n",
            "[20,    30] loss: 0.169\n",
            "[20,    40] loss: 0.172\n",
            "[21,    10] loss: 0.247\n",
            "[21,    20] loss: 0.225\n",
            "[21,    30] loss: 0.196\n",
            "[21,    40] loss: 0.175\n",
            "[22,    10] loss: 0.173\n",
            "[22,    20] loss: 0.167\n",
            "[22,    30] loss: 0.147\n",
            "[22,    40] loss: 0.126\n",
            "[23,    10] loss: 0.128\n",
            "[23,    20] loss: 0.106\n",
            "[23,    30] loss: 0.099\n",
            "[23,    40] loss: 0.107\n",
            "[24,    10] loss: 0.166\n",
            "[24,    20] loss: 0.184\n",
            "[24,    30] loss: 0.148\n",
            "[24,    40] loss: 0.130\n",
            "[25,    10] loss: 0.169\n",
            "[25,    20] loss: 0.134\n",
            "[25,    30] loss: 0.119\n",
            "[25,    40] loss: 0.099\n",
            "[26,    10] loss: 0.119\n",
            "[26,    20] loss: 0.125\n",
            "[26,    30] loss: 0.093\n",
            "[26,    40] loss: 0.074\n",
            "[27,    10] loss: 0.057\n",
            "[27,    20] loss: 0.053\n",
            "[27,    30] loss: 0.043\n",
            "[27,    40] loss: 0.036\n",
            "[28,    10] loss: 0.025\n",
            "[28,    20] loss: 0.021\n",
            "[28,    30] loss: 0.019\n",
            "[28,    40] loss: 0.015\n",
            "[29,    10] loss: 0.010\n",
            "[29,    20] loss: 0.009\n",
            "[29,    30] loss: 0.007\n",
            "[29,    40] loss: 0.007\n",
            "[30,    10] loss: 0.004\n",
            "[30,    20] loss: 0.004\n",
            "[30,    30] loss: 0.004\n",
            "[30,    40] loss: 0.006\n",
            "[31,    10] loss: 0.008\n",
            "[31,    20] loss: 0.010\n",
            "[31,    30] loss: 0.008\n",
            "[31,    40] loss: 0.006\n",
            "[32,    10] loss: 0.004\n",
            "[32,    20] loss: 0.004\n",
            "[32,    30] loss: 0.003\n",
            "[32,    40] loss: 0.004\n",
            "[33,    10] loss: 0.003\n",
            "[33,    20] loss: 0.003\n",
            "[33,    30] loss: 0.003\n",
            "[33,    40] loss: 0.003\n",
            "[34,    10] loss: 0.002\n",
            "[34,    20] loss: 0.002\n",
            "[34,    30] loss: 0.002\n",
            "[34,    40] loss: 0.003\n",
            "[35,    10] loss: 0.002\n",
            "[35,    20] loss: 0.003\n",
            "[35,    30] loss: 0.002\n",
            "[35,    40] loss: 0.002\n",
            "[36,    10] loss: 0.002\n",
            "[36,    20] loss: 0.002\n",
            "[36,    30] loss: 0.002\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.001\n",
            "[37,    40] loss: 0.002\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.002\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.002\n",
            "[39,    10] loss: 0.001\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.001\n",
            "[39,    40] loss: 0.002\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.003\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.003\n",
            "[42,    10] loss: 0.005\n",
            "[42,    20] loss: 0.004\n",
            "[42,    30] loss: 0.003\n",
            "[42,    40] loss: 0.002\n",
            "[43,    10] loss: 0.002\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.003\n",
            "[44,    10] loss: 0.015\n",
            "[44,    20] loss: 0.016\n",
            "[44,    30] loss: 0.011\n",
            "[44,    40] loss: 0.009\n",
            "[45,    10] loss: 0.005\n",
            "[45,    20] loss: 0.005\n",
            "[45,    30] loss: 0.003\n",
            "[45,    40] loss: 0.003\n",
            "[46,    10] loss: 0.002\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.002\n",
            "[47,    10] loss: 0.002\n",
            "[47,    20] loss: 0.002\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.003\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.004\n",
            "[50,    30] loss: 0.003\n",
            "[50,    40] loss: 0.003\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.001\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.001\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.001\n",
            "[57,    10] loss: 0.002\n",
            "[57,    20] loss: 0.003\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.001\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.001\n",
            "[58,    40] loss: 0.001\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.000\n",
            "[60,    30] loss: 0.000\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.000\n",
            "[61,    30] loss: 0.000\n",
            "[61,    40] loss: 0.001\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.000\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.000\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.000\n",
            "[65,    20] loss: 0.000\n",
            "[65,    30] loss: 0.000\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.000\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.000\n",
            "[66,    40] loss: 0.000\n",
            "[67,    10] loss: 0.000\n",
            "[67,    20] loss: 0.000\n",
            "[67,    30] loss: 0.000\n",
            "[67,    40] loss: 0.003\n",
            "[68,    10] loss: 0.015\n",
            "[68,    20] loss: 0.018\n",
            "[68,    30] loss: 0.011\n",
            "[68,    40] loss: 0.010\n",
            "[69,    10] loss: 0.007\n",
            "[69,    20] loss: 0.008\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.009\n",
            "[70,    10] loss: 0.078\n",
            "[70,    20] loss: 0.114\n",
            "[70,    30] loss: 0.104\n",
            "[70,    40] loss: 0.095\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 61 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 88 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 0.955\n",
            "[1,    20] loss: 0.690\n",
            "[1,    30] loss: 0.643\n",
            "[1,    40] loss: 0.599\n",
            "[2,    10] loss: 0.622\n",
            "[2,    20] loss: 0.572\n",
            "[2,    30] loss: 0.533\n",
            "[2,    40] loss: 0.582\n",
            "[3,    10] loss: 0.519\n",
            "[3,    20] loss: 0.499\n",
            "[3,    30] loss: 0.486\n",
            "[3,    40] loss: 0.480\n",
            "[4,    10] loss: 0.505\n",
            "[4,    20] loss: 0.443\n",
            "[4,    30] loss: 0.420\n",
            "[4,    40] loss: 0.429\n",
            "[5,    10] loss: 0.434\n",
            "[5,    20] loss: 0.405\n",
            "[5,    30] loss: 0.378\n",
            "[5,    40] loss: 0.348\n",
            "[6,    10] loss: 0.360\n",
            "[6,    20] loss: 0.305\n",
            "[6,    30] loss: 0.325\n",
            "[6,    40] loss: 0.303\n",
            "[7,    10] loss: 0.342\n",
            "[7,    20] loss: 0.310\n",
            "[7,    30] loss: 0.283\n",
            "[7,    40] loss: 0.313\n",
            "[8,    10] loss: 0.333\n",
            "[8,    20] loss: 0.320\n",
            "[8,    30] loss: 0.290\n",
            "[8,    40] loss: 0.262\n",
            "[9,    10] loss: 0.266\n",
            "[9,    20] loss: 0.246\n",
            "[9,    30] loss: 0.248\n",
            "[9,    40] loss: 0.231\n",
            "[10,    10] loss: 0.204\n",
            "[10,    20] loss: 0.222\n",
            "[10,    30] loss: 0.213\n",
            "[10,    40] loss: 0.198\n",
            "[11,    10] loss: 0.181\n",
            "[11,    20] loss: 0.178\n",
            "[11,    30] loss: 0.189\n",
            "[11,    40] loss: 0.180\n",
            "[12,    10] loss: 0.168\n",
            "[12,    20] loss: 0.169\n",
            "[12,    30] loss: 0.149\n",
            "[12,    40] loss: 0.157\n",
            "[13,    10] loss: 0.147\n",
            "[13,    20] loss: 0.139\n",
            "[13,    30] loss: 0.125\n",
            "[13,    40] loss: 0.140\n",
            "[14,    10] loss: 0.177\n",
            "[14,    20] loss: 0.198\n",
            "[14,    30] loss: 0.165\n",
            "[14,    40] loss: 0.184\n",
            "[15,    10] loss: 0.246\n",
            "[15,    20] loss: 0.190\n",
            "[15,    30] loss: 0.192\n",
            "[15,    40] loss: 0.175\n",
            "[16,    10] loss: 0.124\n",
            "[16,    20] loss: 0.107\n",
            "[16,    30] loss: 0.096\n",
            "[16,    40] loss: 0.116\n",
            "[17,    10] loss: 0.173\n",
            "[17,    20] loss: 0.171\n",
            "[17,    30] loss: 0.137\n",
            "[17,    40] loss: 0.141\n",
            "[18,    10] loss: 0.270\n",
            "[18,    20] loss: 0.214\n",
            "[18,    30] loss: 0.193\n",
            "[18,    40] loss: 0.163\n",
            "[19,    10] loss: 0.149\n",
            "[19,    20] loss: 0.120\n",
            "[19,    30] loss: 0.091\n",
            "[19,    40] loss: 0.096\n",
            "[20,    10] loss: 0.074\n",
            "[20,    20] loss: 0.073\n",
            "[20,    30] loss: 0.067\n",
            "[20,    40] loss: 0.076\n",
            "[21,    10] loss: 0.154\n",
            "[21,    20] loss: 0.130\n",
            "[21,    30] loss: 0.117\n",
            "[21,    40] loss: 0.106\n",
            "[22,    10] loss: 0.103\n",
            "[22,    20] loss: 0.069\n",
            "[22,    30] loss: 0.074\n",
            "[22,    40] loss: 0.059\n",
            "[23,    10] loss: 0.062\n",
            "[23,    20] loss: 0.060\n",
            "[23,    30] loss: 0.048\n",
            "[23,    40] loss: 0.045\n",
            "[24,    10] loss: 0.029\n",
            "[24,    20] loss: 0.025\n",
            "[24,    30] loss: 0.022\n",
            "[24,    40] loss: 0.055\n",
            "[25,    10] loss: 0.036\n",
            "[25,    20] loss: 0.032\n",
            "[25,    30] loss: 0.026\n",
            "[25,    40] loss: 0.023\n",
            "[26,    10] loss: 0.031\n",
            "[26,    20] loss: 0.027\n",
            "[26,    30] loss: 0.020\n",
            "[26,    40] loss: 0.064\n",
            "[27,    10] loss: 0.291\n",
            "[27,    20] loss: 0.333\n",
            "[27,    30] loss: 0.247\n",
            "[27,    40] loss: 0.232\n",
            "[28,    10] loss: 0.176\n",
            "[28,    20] loss: 0.152\n",
            "[28,    30] loss: 0.119\n",
            "[28,    40] loss: 0.127\n",
            "[29,    10] loss: 0.101\n",
            "[29,    20] loss: 0.072\n",
            "[29,    30] loss: 0.060\n",
            "[29,    40] loss: 0.052\n",
            "[30,    10] loss: 0.030\n",
            "[30,    20] loss: 0.027\n",
            "[30,    30] loss: 0.031\n",
            "[30,    40] loss: 0.024\n",
            "[31,    10] loss: 0.018\n",
            "[31,    20] loss: 0.016\n",
            "[31,    30] loss: 0.013\n",
            "[31,    40] loss: 0.015\n",
            "[32,    10] loss: 0.014\n",
            "[32,    20] loss: 0.011\n",
            "[32,    30] loss: 0.010\n",
            "[32,    40] loss: 0.028\n",
            "[33,    10] loss: 0.172\n",
            "[33,    20] loss: 0.163\n",
            "[33,    30] loss: 0.123\n",
            "[33,    40] loss: 0.116\n",
            "[34,    10] loss: 0.203\n",
            "[34,    20] loss: 0.153\n",
            "[34,    30] loss: 0.119\n",
            "[34,    40] loss: 0.079\n",
            "[35,    10] loss: 0.054\n",
            "[35,    20] loss: 0.045\n",
            "[35,    30] loss: 0.038\n",
            "[35,    40] loss: 0.031\n",
            "[36,    10] loss: 0.034\n",
            "[36,    20] loss: 0.027\n",
            "[36,    30] loss: 0.026\n",
            "[36,    40] loss: 0.018\n",
            "[37,    10] loss: 0.011\n",
            "[37,    20] loss: 0.011\n",
            "[37,    30] loss: 0.008\n",
            "[37,    40] loss: 0.009\n",
            "[38,    10] loss: 0.006\n",
            "[38,    20] loss: 0.007\n",
            "[38,    30] loss: 0.005\n",
            "[38,    40] loss: 0.005\n",
            "[39,    10] loss: 0.004\n",
            "[39,    20] loss: 0.003\n",
            "[39,    30] loss: 0.003\n",
            "[39,    40] loss: 0.003\n",
            "[40,    10] loss: 0.003\n",
            "[40,    20] loss: 0.003\n",
            "[40,    30] loss: 0.003\n",
            "[40,    40] loss: 0.003\n",
            "[41,    10] loss: 0.002\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.002\n",
            "[42,    10] loss: 0.002\n",
            "[42,    20] loss: 0.002\n",
            "[42,    30] loss: 0.002\n",
            "[42,    40] loss: 0.002\n",
            "[43,    10] loss: 0.002\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.005\n",
            "[44,    10] loss: 0.008\n",
            "[44,    20] loss: 0.009\n",
            "[44,    30] loss: 0.007\n",
            "[44,    40] loss: 0.005\n",
            "[45,    10] loss: 0.003\n",
            "[45,    20] loss: 0.002\n",
            "[45,    30] loss: 0.003\n",
            "[45,    40] loss: 0.002\n",
            "[46,    10] loss: 0.002\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.002\n",
            "[46,    40] loss: 0.002\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.002\n",
            "[50,    10] loss: 0.002\n",
            "[50,    20] loss: 0.001\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.001\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.001\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.002\n",
            "[53,    10] loss: 0.003\n",
            "[53,    20] loss: 0.003\n",
            "[53,    30] loss: 0.003\n",
            "[53,    40] loss: 0.002\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.002\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.003\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.005\n",
            "[56,    30] loss: 0.002\n",
            "[56,    40] loss: 0.002\n",
            "[57,    10] loss: 0.001\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.011\n",
            "[58,    10] loss: 0.050\n",
            "[58,    20] loss: 0.040\n",
            "[58,    30] loss: 0.038\n",
            "[58,    40] loss: 0.038\n",
            "[59,    10] loss: 0.109\n",
            "[59,    20] loss: 0.076\n",
            "[59,    30] loss: 0.057\n",
            "[59,    40] loss: 0.070\n",
            "[60,    10] loss: 0.203\n",
            "[60,    20] loss: 0.177\n",
            "[60,    30] loss: 0.129\n",
            "[60,    40] loss: 0.091\n",
            "[61,    10] loss: 0.066\n",
            "[61,    20] loss: 0.056\n",
            "[61,    30] loss: 0.033\n",
            "[61,    40] loss: 0.040\n",
            "[62,    10] loss: 0.056\n",
            "[62,    20] loss: 0.051\n",
            "[62,    30] loss: 0.039\n",
            "[62,    40] loss: 0.021\n",
            "[63,    10] loss: 0.012\n",
            "[63,    20] loss: 0.009\n",
            "[63,    30] loss: 0.009\n",
            "[63,    40] loss: 0.007\n",
            "[64,    10] loss: 0.004\n",
            "[64,    20] loss: 0.004\n",
            "[64,    30] loss: 0.003\n",
            "[64,    40] loss: 0.003\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.002\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.005\n",
            "[68,    10] loss: 0.015\n",
            "[68,    20] loss: 0.010\n",
            "[68,    30] loss: 0.007\n",
            "[68,    40] loss: 0.005\n",
            "[69,    10] loss: 0.003\n",
            "[69,    20] loss: 0.003\n",
            "[69,    30] loss: 0.002\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.002\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 50 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 93 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 0.928\n",
            "[1,    20] loss: 0.600\n",
            "[1,    30] loss: 0.524\n",
            "[1,    40] loss: 0.504\n",
            "[2,    10] loss: 0.489\n",
            "[2,    20] loss: 0.443\n",
            "[2,    30] loss: 0.438\n",
            "[2,    40] loss: 0.385\n",
            "[3,    10] loss: 0.414\n",
            "[3,    20] loss: 0.389\n",
            "[3,    30] loss: 0.334\n",
            "[3,    40] loss: 0.358\n",
            "[4,    10] loss: 0.351\n",
            "[4,    20] loss: 0.325\n",
            "[4,    30] loss: 0.288\n",
            "[4,    40] loss: 0.295\n",
            "[5,    10] loss: 0.270\n",
            "[5,    20] loss: 0.250\n",
            "[5,    30] loss: 0.289\n",
            "[5,    40] loss: 0.249\n",
            "[6,    10] loss: 0.233\n",
            "[6,    20] loss: 0.225\n",
            "[6,    30] loss: 0.205\n",
            "[6,    40] loss: 0.230\n",
            "[7,    10] loss: 0.217\n",
            "[7,    20] loss: 0.192\n",
            "[7,    30] loss: 0.190\n",
            "[7,    40] loss: 0.199\n",
            "[8,    10] loss: 0.171\n",
            "[8,    20] loss: 0.180\n",
            "[8,    30] loss: 0.177\n",
            "[8,    40] loss: 0.168\n",
            "[9,    10] loss: 0.150\n",
            "[9,    20] loss: 0.142\n",
            "[9,    30] loss: 0.141\n",
            "[9,    40] loss: 0.142\n",
            "[10,    10] loss: 0.125\n",
            "[10,    20] loss: 0.122\n",
            "[10,    30] loss: 0.122\n",
            "[10,    40] loss: 0.123\n",
            "[11,    10] loss: 0.116\n",
            "[11,    20] loss: 0.110\n",
            "[11,    30] loss: 0.099\n",
            "[11,    40] loss: 0.099\n",
            "[12,    10] loss: 0.083\n",
            "[12,    20] loss: 0.080\n",
            "[12,    30] loss: 0.070\n",
            "[12,    40] loss: 0.074\n",
            "[13,    10] loss: 0.046\n",
            "[13,    20] loss: 0.044\n",
            "[13,    30] loss: 0.041\n",
            "[13,    40] loss: 0.044\n",
            "[14,    10] loss: 0.029\n",
            "[14,    20] loss: 0.030\n",
            "[14,    30] loss: 0.028\n",
            "[14,    40] loss: 0.026\n",
            "[15,    10] loss: 0.022\n",
            "[15,    20] loss: 0.023\n",
            "[15,    30] loss: 0.023\n",
            "[15,    40] loss: 0.024\n",
            "[16,    10] loss: 0.030\n",
            "[16,    20] loss: 0.037\n",
            "[16,    30] loss: 0.030\n",
            "[16,    40] loss: 0.029\n",
            "[17,    10] loss: 0.024\n",
            "[17,    20] loss: 0.017\n",
            "[17,    30] loss: 0.019\n",
            "[17,    40] loss: 0.017\n",
            "[18,    10] loss: 0.016\n",
            "[18,    20] loss: 0.013\n",
            "[18,    30] loss: 0.014\n",
            "[18,    40] loss: 0.012\n",
            "[19,    10] loss: 0.009\n",
            "[19,    20] loss: 0.007\n",
            "[19,    30] loss: 0.006\n",
            "[19,    40] loss: 0.007\n",
            "[20,    10] loss: 0.004\n",
            "[20,    20] loss: 0.004\n",
            "[20,    30] loss: 0.003\n",
            "[20,    40] loss: 0.003\n",
            "[21,    10] loss: 0.003\n",
            "[21,    20] loss: 0.002\n",
            "[21,    30] loss: 0.002\n",
            "[21,    40] loss: 0.002\n",
            "[22,    10] loss: 0.002\n",
            "[22,    20] loss: 0.002\n",
            "[22,    30] loss: 0.002\n",
            "[22,    40] loss: 0.002\n",
            "[23,    10] loss: 0.002\n",
            "[23,    20] loss: 0.002\n",
            "[23,    30] loss: 0.002\n",
            "[23,    40] loss: 0.002\n",
            "[24,    10] loss: 0.005\n",
            "[24,    20] loss: 0.005\n",
            "[24,    30] loss: 0.005\n",
            "[24,    40] loss: 0.014\n",
            "[25,    10] loss: 0.120\n",
            "[25,    20] loss: 0.180\n",
            "[25,    30] loss: 0.170\n",
            "[25,    40] loss: 0.143\n",
            "[26,    10] loss: 0.116\n",
            "[26,    20] loss: 0.097\n",
            "[26,    30] loss: 0.082\n",
            "[26,    40] loss: 0.109\n",
            "[27,    10] loss: 0.174\n",
            "[27,    20] loss: 0.177\n",
            "[27,    30] loss: 0.148\n",
            "[27,    40] loss: 0.151\n",
            "[28,    10] loss: 0.172\n",
            "[28,    20] loss: 0.132\n",
            "[28,    30] loss: 0.107\n",
            "[28,    40] loss: 0.098\n",
            "[29,    10] loss: 0.081\n",
            "[29,    20] loss: 0.062\n",
            "[29,    30] loss: 0.059\n",
            "[29,    40] loss: 0.051\n",
            "[30,    10] loss: 0.029\n",
            "[30,    20] loss: 0.035\n",
            "[30,    30] loss: 0.027\n",
            "[30,    40] loss: 0.019\n",
            "[31,    10] loss: 0.014\n",
            "[31,    20] loss: 0.013\n",
            "[31,    30] loss: 0.011\n",
            "[31,    40] loss: 0.014\n",
            "[32,    10] loss: 0.014\n",
            "[32,    20] loss: 0.011\n",
            "[32,    30] loss: 0.010\n",
            "[32,    40] loss: 0.024\n",
            "[33,    10] loss: 0.121\n",
            "[33,    20] loss: 0.101\n",
            "[33,    30] loss: 0.097\n",
            "[33,    40] loss: 0.067\n",
            "[34,    10] loss: 0.052\n",
            "[34,    20] loss: 0.037\n",
            "[34,    30] loss: 0.034\n",
            "[34,    40] loss: 0.026\n",
            "[35,    10] loss: 0.017\n",
            "[35,    20] loss: 0.016\n",
            "[35,    30] loss: 0.016\n",
            "[35,    40] loss: 0.012\n",
            "[36,    10] loss: 0.006\n",
            "[36,    20] loss: 0.005\n",
            "[36,    30] loss: 0.006\n",
            "[36,    40] loss: 0.005\n",
            "[37,    10] loss: 0.004\n",
            "[37,    20] loss: 0.004\n",
            "[37,    30] loss: 0.004\n",
            "[37,    40] loss: 0.006\n",
            "[38,    10] loss: 0.004\n",
            "[38,    20] loss: 0.004\n",
            "[38,    30] loss: 0.004\n",
            "[38,    40] loss: 0.021\n",
            "[39,    10] loss: 0.081\n",
            "[39,    20] loss: 0.094\n",
            "[39,    30] loss: 0.072\n",
            "[39,    40] loss: 0.047\n",
            "[40,    10] loss: 0.029\n",
            "[40,    20] loss: 0.023\n",
            "[40,    30] loss: 0.017\n",
            "[40,    40] loss: 0.020\n",
            "[41,    10] loss: 0.043\n",
            "[41,    20] loss: 0.045\n",
            "[41,    30] loss: 0.034\n",
            "[41,    40] loss: 0.109\n",
            "[42,    10] loss: 0.370\n",
            "[42,    20] loss: 0.354\n",
            "[42,    30] loss: 0.269\n",
            "[42,    40] loss: 0.199\n",
            "[43,    10] loss: 0.148\n",
            "[43,    20] loss: 0.124\n",
            "[43,    30] loss: 0.095\n",
            "[43,    40] loss: 0.103\n",
            "[44,    10] loss: 0.069\n",
            "[44,    20] loss: 0.052\n",
            "[44,    30] loss: 0.055\n",
            "[44,    40] loss: 0.040\n",
            "[45,    10] loss: 0.024\n",
            "[45,    20] loss: 0.023\n",
            "[45,    30] loss: 0.022\n",
            "[45,    40] loss: 0.034\n",
            "[46,    10] loss: 0.053\n",
            "[46,    20] loss: 0.053\n",
            "[46,    30] loss: 0.041\n",
            "[46,    40] loss: 0.053\n",
            "[47,    10] loss: 0.104\n",
            "[47,    20] loss: 0.109\n",
            "[47,    30] loss: 0.066\n",
            "[47,    40] loss: 0.056\n",
            "[48,    10] loss: 0.033\n",
            "[48,    20] loss: 0.028\n",
            "[48,    30] loss: 0.018\n",
            "[48,    40] loss: 0.030\n",
            "[49,    10] loss: 0.033\n",
            "[49,    20] loss: 0.025\n",
            "[49,    30] loss: 0.019\n",
            "[49,    40] loss: 0.031\n",
            "[50,    10] loss: 0.089\n",
            "[50,    20] loss: 0.091\n",
            "[50,    30] loss: 0.061\n",
            "[50,    40] loss: 0.051\n",
            "[51,    10] loss: 0.054\n",
            "[51,    20] loss: 0.037\n",
            "[51,    30] loss: 0.030\n",
            "[51,    40] loss: 0.039\n",
            "[52,    10] loss: 0.063\n",
            "[52,    20] loss: 0.055\n",
            "[52,    30] loss: 0.048\n",
            "[52,    40] loss: 0.034\n",
            "[53,    10] loss: 0.017\n",
            "[53,    20] loss: 0.014\n",
            "[53,    30] loss: 0.013\n",
            "[53,    40] loss: 0.010\n",
            "[54,    10] loss: 0.006\n",
            "[54,    20] loss: 0.005\n",
            "[54,    30] loss: 0.004\n",
            "[54,    40] loss: 0.005\n",
            "[55,    10] loss: 0.004\n",
            "[55,    20] loss: 0.004\n",
            "[55,    30] loss: 0.004\n",
            "[55,    40] loss: 0.004\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.002\n",
            "[57,    10] loss: 0.002\n",
            "[57,    20] loss: 0.002\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.003\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.017\n",
            "[60,    10] loss: 0.123\n",
            "[60,    20] loss: 0.121\n",
            "[60,    30] loss: 0.091\n",
            "[60,    40] loss: 0.065\n",
            "[61,    10] loss: 0.044\n",
            "[61,    20] loss: 0.020\n",
            "[61,    30] loss: 0.018\n",
            "[61,    40] loss: 0.016\n",
            "[62,    10] loss: 0.008\n",
            "[62,    20] loss: 0.006\n",
            "[62,    30] loss: 0.006\n",
            "[62,    40] loss: 0.008\n",
            "[63,    10] loss: 0.005\n",
            "[63,    20] loss: 0.008\n",
            "[63,    30] loss: 0.008\n",
            "[63,    40] loss: 0.008\n",
            "[64,    10] loss: 0.016\n",
            "[64,    20] loss: 0.016\n",
            "[64,    30] loss: 0.010\n",
            "[64,    40] loss: 0.009\n",
            "[65,    10] loss: 0.005\n",
            "[65,    20] loss: 0.004\n",
            "[65,    30] loss: 0.004\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.003\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 48 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 79 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 0.930\n",
            "[1,    20] loss: 0.602\n",
            "[1,    30] loss: 0.482\n",
            "[1,    40] loss: 0.486\n",
            "[2,    10] loss: 0.467\n",
            "[2,    20] loss: 0.416\n",
            "[2,    30] loss: 0.438\n",
            "[2,    40] loss: 0.330\n",
            "[3,    10] loss: 0.337\n",
            "[3,    20] loss: 0.320\n",
            "[3,    30] loss: 0.302\n",
            "[3,    40] loss: 0.294\n",
            "[4,    10] loss: 0.266\n",
            "[4,    20] loss: 0.258\n",
            "[4,    30] loss: 0.250\n",
            "[4,    40] loss: 0.267\n",
            "[5,    10] loss: 0.302\n",
            "[5,    20] loss: 0.294\n",
            "[5,    30] loss: 0.258\n",
            "[5,    40] loss: 0.228\n",
            "[6,    10] loss: 0.221\n",
            "[6,    20] loss: 0.220\n",
            "[6,    30] loss: 0.187\n",
            "[6,    40] loss: 0.220\n",
            "[7,    10] loss: 0.216\n",
            "[7,    20] loss: 0.203\n",
            "[7,    30] loss: 0.177\n",
            "[7,    40] loss: 0.205\n",
            "[8,    10] loss: 0.205\n",
            "[8,    20] loss: 0.210\n",
            "[8,    30] loss: 0.172\n",
            "[8,    40] loss: 0.172\n",
            "[9,    10] loss: 0.150\n",
            "[9,    20] loss: 0.134\n",
            "[9,    30] loss: 0.135\n",
            "[9,    40] loss: 0.185\n",
            "[10,    10] loss: 0.154\n",
            "[10,    20] loss: 0.146\n",
            "[10,    30] loss: 0.120\n",
            "[10,    40] loss: 0.108\n",
            "[11,    10] loss: 0.095\n",
            "[11,    20] loss: 0.091\n",
            "[11,    30] loss: 0.081\n",
            "[11,    40] loss: 0.076\n",
            "[12,    10] loss: 0.066\n",
            "[12,    20] loss: 0.059\n",
            "[12,    30] loss: 0.070\n",
            "[12,    40] loss: 0.059\n",
            "[13,    10] loss: 0.049\n",
            "[13,    20] loss: 0.049\n",
            "[13,    30] loss: 0.054\n",
            "[13,    40] loss: 0.050\n",
            "[14,    10] loss: 0.053\n",
            "[14,    20] loss: 0.051\n",
            "[14,    30] loss: 0.048\n",
            "[14,    40] loss: 0.052\n",
            "[15,    10] loss: 0.133\n",
            "[15,    20] loss: 0.132\n",
            "[15,    30] loss: 0.119\n",
            "[15,    40] loss: 0.116\n",
            "[16,    10] loss: 0.127\n",
            "[16,    20] loss: 0.127\n",
            "[16,    30] loss: 0.100\n",
            "[16,    40] loss: 0.112\n",
            "[17,    10] loss: 0.111\n",
            "[17,    20] loss: 0.093\n",
            "[17,    30] loss: 0.080\n",
            "[17,    40] loss: 0.073\n",
            "[18,    10] loss: 0.043\n",
            "[18,    20] loss: 0.036\n",
            "[18,    30] loss: 0.030\n",
            "[18,    40] loss: 0.033\n",
            "[19,    10] loss: 0.035\n",
            "[19,    20] loss: 0.028\n",
            "[19,    30] loss: 0.024\n",
            "[19,    40] loss: 0.024\n",
            "[20,    10] loss: 0.032\n",
            "[20,    20] loss: 0.030\n",
            "[20,    30] loss: 0.025\n",
            "[20,    40] loss: 0.018\n",
            "[21,    10] loss: 0.011\n",
            "[21,    20] loss: 0.011\n",
            "[21,    30] loss: 0.009\n",
            "[21,    40] loss: 0.008\n",
            "[22,    10] loss: 0.005\n",
            "[22,    20] loss: 0.005\n",
            "[22,    30] loss: 0.004\n",
            "[22,    40] loss: 0.004\n",
            "[23,    10] loss: 0.003\n",
            "[23,    20] loss: 0.003\n",
            "[23,    30] loss: 0.003\n",
            "[23,    40] loss: 0.003\n",
            "[24,    10] loss: 0.002\n",
            "[24,    20] loss: 0.003\n",
            "[24,    30] loss: 0.003\n",
            "[24,    40] loss: 0.003\n",
            "[25,    10] loss: 0.002\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.002\n",
            "[25,    40] loss: 0.002\n",
            "[26,    10] loss: 0.002\n",
            "[26,    20] loss: 0.002\n",
            "[26,    30] loss: 0.002\n",
            "[26,    40] loss: 0.002\n",
            "[27,    10] loss: 0.002\n",
            "[27,    20] loss: 0.002\n",
            "[27,    30] loss: 0.002\n",
            "[27,    40] loss: 0.003\n",
            "[28,    10] loss: 0.009\n",
            "[28,    20] loss: 0.010\n",
            "[28,    30] loss: 0.008\n",
            "[28,    40] loss: 0.005\n",
            "[29,    10] loss: 0.004\n",
            "[29,    20] loss: 0.003\n",
            "[29,    30] loss: 0.003\n",
            "[29,    40] loss: 0.004\n",
            "[30,    10] loss: 0.002\n",
            "[30,    20] loss: 0.003\n",
            "[30,    30] loss: 0.002\n",
            "[30,    40] loss: 0.002\n",
            "[31,    10] loss: 0.002\n",
            "[31,    20] loss: 0.002\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.004\n",
            "[32,    10] loss: 0.022\n",
            "[32,    20] loss: 0.019\n",
            "[32,    30] loss: 0.017\n",
            "[32,    40] loss: 0.018\n",
            "[33,    10] loss: 0.013\n",
            "[33,    20] loss: 0.007\n",
            "[33,    30] loss: 0.006\n",
            "[33,    40] loss: 0.005\n",
            "[34,    10] loss: 0.003\n",
            "[34,    20] loss: 0.003\n",
            "[34,    30] loss: 0.002\n",
            "[34,    40] loss: 0.003\n",
            "[35,    10] loss: 0.003\n",
            "[35,    20] loss: 0.002\n",
            "[35,    30] loss: 0.002\n",
            "[35,    40] loss: 0.003\n",
            "[36,    10] loss: 0.004\n",
            "[36,    20] loss: 0.004\n",
            "[36,    30] loss: 0.003\n",
            "[36,    40] loss: 0.003\n",
            "[37,    10] loss: 0.002\n",
            "[37,    20] loss: 0.002\n",
            "[37,    30] loss: 0.002\n",
            "[37,    40] loss: 0.002\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.001\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.002\n",
            "[39,    10] loss: 0.002\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.002\n",
            "[39,    40] loss: 0.002\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.001\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.001\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.001\n",
            "[43,    10] loss: 0.000\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.000\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.000\n",
            "[44,    20] loss: 0.000\n",
            "[44,    30] loss: 0.000\n",
            "[44,    40] loss: 0.002\n",
            "[45,    10] loss: 0.005\n",
            "[45,    20] loss: 0.006\n",
            "[45,    30] loss: 0.004\n",
            "[45,    40] loss: 0.004\n",
            "[46,    10] loss: 0.002\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.002\n",
            "[46,    40] loss: 0.002\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.000\n",
            "[49,    30] loss: 0.000\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.000\n",
            "[50,    20] loss: 0.000\n",
            "[50,    30] loss: 0.000\n",
            "[50,    40] loss: 0.000\n",
            "[51,    10] loss: 0.000\n",
            "[51,    20] loss: 0.000\n",
            "[51,    30] loss: 0.000\n",
            "[51,    40] loss: 0.000\n",
            "[52,    10] loss: 0.000\n",
            "[52,    20] loss: 0.000\n",
            "[52,    30] loss: 0.000\n",
            "[52,    40] loss: 0.000\n",
            "[53,    10] loss: 0.000\n",
            "[53,    20] loss: 0.000\n",
            "[53,    30] loss: 0.000\n",
            "[53,    40] loss: 0.000\n",
            "[54,    10] loss: 0.000\n",
            "[54,    20] loss: 0.000\n",
            "[54,    30] loss: 0.000\n",
            "[54,    40] loss: 0.000\n",
            "[55,    10] loss: 0.000\n",
            "[55,    20] loss: 0.000\n",
            "[55,    30] loss: 0.000\n",
            "[55,    40] loss: 0.000\n",
            "[56,    10] loss: 0.000\n",
            "[56,    20] loss: 0.000\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.000\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.000\n",
            "[57,    30] loss: 0.000\n",
            "[57,    40] loss: 0.001\n",
            "[58,    10] loss: 0.001\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.000\n",
            "[60,    20] loss: 0.000\n",
            "[60,    30] loss: 0.000\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.001\n",
            "[62,    10] loss: 0.000\n",
            "[62,    20] loss: 0.000\n",
            "[62,    30] loss: 0.000\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.000\n",
            "[64,    10] loss: 0.000\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.000\n",
            "[64,    40] loss: 0.000\n",
            "[65,    10] loss: 0.000\n",
            "[65,    20] loss: 0.000\n",
            "[65,    30] loss: 0.000\n",
            "[65,    40] loss: 0.000\n",
            "[66,    10] loss: 0.000\n",
            "[66,    20] loss: 0.000\n",
            "[66,    30] loss: 0.000\n",
            "[66,    40] loss: 0.000\n",
            "[67,    10] loss: 0.000\n",
            "[67,    20] loss: 0.000\n",
            "[67,    30] loss: 0.000\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.000\n",
            "[68,    20] loss: 0.000\n",
            "[68,    30] loss: 0.000\n",
            "[68,    40] loss: 0.002\n",
            "[69,    10] loss: 0.008\n",
            "[69,    20] loss: 0.008\n",
            "[69,    30] loss: 0.003\n",
            "[69,    40] loss: 0.006\n",
            "[70,    10] loss: 0.017\n",
            "[70,    20] loss: 0.020\n",
            "[70,    30] loss: 0.015\n",
            "[70,    40] loss: 0.046\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 92 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 0.885\n",
            "[1,    20] loss: 0.536\n",
            "[1,    30] loss: 0.491\n",
            "[1,    40] loss: 0.478\n",
            "[2,    10] loss: 0.441\n",
            "[2,    20] loss: 0.403\n",
            "[2,    30] loss: 0.372\n",
            "[2,    40] loss: 0.355\n",
            "[3,    10] loss: 0.334\n",
            "[3,    20] loss: 0.298\n",
            "[3,    30] loss: 0.311\n",
            "[3,    40] loss: 0.282\n",
            "[4,    10] loss: 0.262\n",
            "[4,    20] loss: 0.241\n",
            "[4,    30] loss: 0.258\n",
            "[4,    40] loss: 0.223\n",
            "[5,    10] loss: 0.225\n",
            "[5,    20] loss: 0.213\n",
            "[5,    30] loss: 0.204\n",
            "[5,    40] loss: 0.193\n",
            "[6,    10] loss: 0.187\n",
            "[6,    20] loss: 0.181\n",
            "[6,    30] loss: 0.163\n",
            "[6,    40] loss: 0.159\n",
            "[7,    10] loss: 0.154\n",
            "[7,    20] loss: 0.145\n",
            "[7,    30] loss: 0.144\n",
            "[7,    40] loss: 0.152\n",
            "[8,    10] loss: 0.180\n",
            "[8,    20] loss: 0.160\n",
            "[8,    30] loss: 0.162\n",
            "[8,    40] loss: 0.134\n",
            "[9,    10] loss: 0.122\n",
            "[9,    20] loss: 0.112\n",
            "[9,    30] loss: 0.110\n",
            "[9,    40] loss: 0.094\n",
            "[10,    10] loss: 0.096\n",
            "[10,    20] loss: 0.099\n",
            "[10,    30] loss: 0.090\n",
            "[10,    40] loss: 0.069\n",
            "[11,    10] loss: 0.070\n",
            "[11,    20] loss: 0.063\n",
            "[11,    30] loss: 0.064\n",
            "[11,    40] loss: 0.101\n",
            "[12,    10] loss: 0.182\n",
            "[12,    20] loss: 0.174\n",
            "[12,    30] loss: 0.149\n",
            "[12,    40] loss: 0.180\n",
            "[13,    10] loss: 0.260\n",
            "[13,    20] loss: 0.182\n",
            "[13,    30] loss: 0.161\n",
            "[13,    40] loss: 0.121\n",
            "[14,    10] loss: 0.088\n",
            "[14,    20] loss: 0.085\n",
            "[14,    30] loss: 0.069\n",
            "[14,    40] loss: 0.075\n",
            "[15,    10] loss: 0.074\n",
            "[15,    20] loss: 0.073\n",
            "[15,    30] loss: 0.057\n",
            "[15,    40] loss: 0.081\n",
            "[16,    10] loss: 0.116\n",
            "[16,    20] loss: 0.120\n",
            "[16,    30] loss: 0.089\n",
            "[16,    40] loss: 0.078\n",
            "[17,    10] loss: 0.049\n",
            "[17,    20] loss: 0.045\n",
            "[17,    30] loss: 0.044\n",
            "[17,    40] loss: 0.045\n",
            "[18,    10] loss: 0.066\n",
            "[18,    20] loss: 0.064\n",
            "[18,    30] loss: 0.047\n",
            "[18,    40] loss: 0.081\n",
            "[19,    10] loss: 0.130\n",
            "[19,    20] loss: 0.114\n",
            "[19,    30] loss: 0.094\n",
            "[19,    40] loss: 0.073\n",
            "[20,    10] loss: 0.052\n",
            "[20,    20] loss: 0.039\n",
            "[20,    30] loss: 0.032\n",
            "[20,    40] loss: 0.042\n",
            "[21,    10] loss: 0.031\n",
            "[21,    20] loss: 0.026\n",
            "[21,    30] loss: 0.024\n",
            "[21,    40] loss: 0.019\n",
            "[22,    10] loss: 0.014\n",
            "[22,    20] loss: 0.011\n",
            "[22,    30] loss: 0.011\n",
            "[22,    40] loss: 0.013\n",
            "[23,    10] loss: 0.009\n",
            "[23,    20] loss: 0.008\n",
            "[23,    30] loss: 0.009\n",
            "[23,    40] loss: 0.008\n",
            "[24,    10] loss: 0.007\n",
            "[24,    20] loss: 0.006\n",
            "[24,    30] loss: 0.007\n",
            "[24,    40] loss: 0.006\n",
            "[25,    10] loss: 0.005\n",
            "[25,    20] loss: 0.006\n",
            "[25,    30] loss: 0.006\n",
            "[25,    40] loss: 0.005\n",
            "[26,    10] loss: 0.004\n",
            "[26,    20] loss: 0.003\n",
            "[26,    30] loss: 0.002\n",
            "[26,    40] loss: 0.004\n",
            "[27,    10] loss: 0.005\n",
            "[27,    20] loss: 0.004\n",
            "[27,    30] loss: 0.003\n",
            "[27,    40] loss: 0.006\n",
            "[28,    10] loss: 0.012\n",
            "[28,    20] loss: 0.010\n",
            "[28,    30] loss: 0.008\n",
            "[28,    40] loss: 0.007\n",
            "[29,    10] loss: 0.005\n",
            "[29,    20] loss: 0.004\n",
            "[29,    30] loss: 0.003\n",
            "[29,    40] loss: 0.003\n",
            "[30,    10] loss: 0.002\n",
            "[30,    20] loss: 0.002\n",
            "[30,    30] loss: 0.002\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.004\n",
            "[31,    20] loss: 0.003\n",
            "[31,    30] loss: 0.003\n",
            "[31,    40] loss: 0.002\n",
            "[32,    10] loss: 0.002\n",
            "[32,    20] loss: 0.002\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.001\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.003\n",
            "[37,    10] loss: 0.004\n",
            "[37,    20] loss: 0.003\n",
            "[37,    30] loss: 0.003\n",
            "[37,    40] loss: 0.103\n",
            "[38,    10] loss: 0.427\n",
            "[38,    20] loss: 0.360\n",
            "[38,    30] loss: 0.296\n",
            "[38,    40] loss: 0.273\n",
            "[39,    10] loss: 0.230\n",
            "[39,    20] loss: 0.186\n",
            "[39,    30] loss: 0.146\n",
            "[39,    40] loss: 0.142\n",
            "[40,    10] loss: 0.093\n",
            "[40,    20] loss: 0.091\n",
            "[40,    30] loss: 0.082\n",
            "[40,    40] loss: 0.088\n",
            "[41,    10] loss: 0.060\n",
            "[41,    20] loss: 0.054\n",
            "[41,    30] loss: 0.053\n",
            "[41,    40] loss: 0.049\n",
            "[42,    10] loss: 0.052\n",
            "[42,    20] loss: 0.058\n",
            "[42,    30] loss: 0.045\n",
            "[42,    40] loss: 0.038\n",
            "[43,    10] loss: 0.028\n",
            "[43,    20] loss: 0.028\n",
            "[43,    30] loss: 0.026\n",
            "[43,    40] loss: 0.027\n",
            "[44,    10] loss: 0.020\n",
            "[44,    20] loss: 0.024\n",
            "[44,    30] loss: 0.017\n",
            "[44,    40] loss: 0.014\n",
            "[45,    10] loss: 0.010\n",
            "[45,    20] loss: 0.010\n",
            "[45,    30] loss: 0.010\n",
            "[45,    40] loss: 0.008\n",
            "[46,    10] loss: 0.007\n",
            "[46,    20] loss: 0.006\n",
            "[46,    30] loss: 0.006\n",
            "[46,    40] loss: 0.005\n",
            "[47,    10] loss: 0.004\n",
            "[47,    20] loss: 0.005\n",
            "[47,    30] loss: 0.004\n",
            "[47,    40] loss: 0.003\n",
            "[48,    10] loss: 0.003\n",
            "[48,    20] loss: 0.003\n",
            "[48,    30] loss: 0.003\n",
            "[48,    40] loss: 0.005\n",
            "[49,    10] loss: 0.006\n",
            "[49,    20] loss: 0.004\n",
            "[49,    30] loss: 0.004\n",
            "[49,    40] loss: 0.003\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.003\n",
            "[50,    30] loss: 0.002\n",
            "[50,    40] loss: 0.002\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.013\n",
            "[52,    10] loss: 0.096\n",
            "[52,    20] loss: 0.116\n",
            "[52,    30] loss: 0.090\n",
            "[52,    40] loss: 0.066\n",
            "[53,    10] loss: 0.092\n",
            "[53,    20] loss: 0.066\n",
            "[53,    30] loss: 0.067\n",
            "[53,    40] loss: 0.061\n",
            "[54,    10] loss: 0.060\n",
            "[54,    20] loss: 0.043\n",
            "[54,    30] loss: 0.028\n",
            "[54,    40] loss: 0.027\n",
            "[55,    10] loss: 0.027\n",
            "[55,    20] loss: 0.031\n",
            "[55,    30] loss: 0.021\n",
            "[55,    40] loss: 0.014\n",
            "[56,    10] loss: 0.009\n",
            "[56,    20] loss: 0.006\n",
            "[56,    30] loss: 0.006\n",
            "[56,    40] loss: 0.008\n",
            "[57,    10] loss: 0.009\n",
            "[57,    20] loss: 0.012\n",
            "[57,    30] loss: 0.008\n",
            "[57,    40] loss: 0.006\n",
            "[58,    10] loss: 0.003\n",
            "[58,    20] loss: 0.004\n",
            "[58,    30] loss: 0.003\n",
            "[58,    40] loss: 0.004\n",
            "[59,    10] loss: 0.003\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.003\n",
            "[59,    40] loss: 0.005\n",
            "[60,    10] loss: 0.003\n",
            "[60,    20] loss: 0.004\n",
            "[60,    30] loss: 0.003\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.002\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.002\n",
            "[64,    10] loss: 0.002\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.014\n",
            "[65,    10] loss: 0.086\n",
            "[65,    20] loss: 0.052\n",
            "[65,    30] loss: 0.036\n",
            "[65,    40] loss: 0.036\n",
            "[66,    10] loss: 0.023\n",
            "[66,    20] loss: 0.014\n",
            "[66,    30] loss: 0.011\n",
            "[66,    40] loss: 0.036\n",
            "[67,    10] loss: 0.091\n",
            "[67,    20] loss: 0.079\n",
            "[67,    30] loss: 0.064\n",
            "[67,    40] loss: 0.039\n",
            "[68,    10] loss: 0.022\n",
            "[68,    20] loss: 0.017\n",
            "[68,    30] loss: 0.013\n",
            "[68,    40] loss: 0.014\n",
            "[69,    10] loss: 0.005\n",
            "[69,    20] loss: 0.004\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.006\n",
            "[70,    10] loss: 0.003\n",
            "[70,    20] loss: 0.003\n",
            "[70,    30] loss: 0.002\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 0.874\n",
            "[1,    20] loss: 0.547\n",
            "[1,    30] loss: 0.491\n",
            "[1,    40] loss: 0.436\n",
            "[2,    10] loss: 0.419\n",
            "[2,    20] loss: 0.373\n",
            "[2,    30] loss: 0.338\n",
            "[2,    40] loss: 0.312\n",
            "[3,    10] loss: 0.314\n",
            "[3,    20] loss: 0.305\n",
            "[3,    30] loss: 0.258\n",
            "[3,    40] loss: 0.265\n",
            "[4,    10] loss: 0.290\n",
            "[4,    20] loss: 0.235\n",
            "[4,    30] loss: 0.210\n",
            "[4,    40] loss: 0.180\n",
            "[5,    10] loss: 0.183\n",
            "[5,    20] loss: 0.159\n",
            "[5,    30] loss: 0.158\n",
            "[5,    40] loss: 0.192\n",
            "[6,    10] loss: 0.172\n",
            "[6,    20] loss: 0.170\n",
            "[6,    30] loss: 0.139\n",
            "[6,    40] loss: 0.145\n",
            "[7,    10] loss: 0.165\n",
            "[7,    20] loss: 0.160\n",
            "[7,    30] loss: 0.127\n",
            "[7,    40] loss: 0.135\n",
            "[8,    10] loss: 0.136\n",
            "[8,    20] loss: 0.100\n",
            "[8,    30] loss: 0.119\n",
            "[8,    40] loss: 0.110\n",
            "[9,    10] loss: 0.098\n",
            "[9,    20] loss: 0.110\n",
            "[9,    30] loss: 0.104\n",
            "[9,    40] loss: 0.103\n",
            "[10,    10] loss: 0.069\n",
            "[10,    20] loss: 0.074\n",
            "[10,    30] loss: 0.067\n",
            "[10,    40] loss: 0.075\n",
            "[11,    10] loss: 0.111\n",
            "[11,    20] loss: 0.100\n",
            "[11,    30] loss: 0.093\n",
            "[11,    40] loss: 0.078\n",
            "[12,    10] loss: 0.061\n",
            "[12,    20] loss: 0.058\n",
            "[12,    30] loss: 0.048\n",
            "[12,    40] loss: 0.050\n",
            "[13,    10] loss: 0.086\n",
            "[13,    20] loss: 0.077\n",
            "[13,    30] loss: 0.069\n",
            "[13,    40] loss: 0.056\n",
            "[14,    10] loss: 0.080\n",
            "[14,    20] loss: 0.077\n",
            "[14,    30] loss: 0.063\n",
            "[14,    40] loss: 0.046\n",
            "[15,    10] loss: 0.032\n",
            "[15,    20] loss: 0.027\n",
            "[15,    30] loss: 0.026\n",
            "[15,    40] loss: 0.025\n",
            "[16,    10] loss: 0.032\n",
            "[16,    20] loss: 0.025\n",
            "[16,    30] loss: 0.021\n",
            "[16,    40] loss: 0.022\n",
            "[17,    10] loss: 0.014\n",
            "[17,    20] loss: 0.010\n",
            "[17,    30] loss: 0.009\n",
            "[17,    40] loss: 0.010\n",
            "[18,    10] loss: 0.009\n",
            "[18,    20] loss: 0.007\n",
            "[18,    30] loss: 0.006\n",
            "[18,    40] loss: 0.005\n",
            "[19,    10] loss: 0.004\n",
            "[19,    20] loss: 0.004\n",
            "[19,    30] loss: 0.003\n",
            "[19,    40] loss: 0.003\n",
            "[20,    10] loss: 0.002\n",
            "[20,    20] loss: 0.002\n",
            "[20,    30] loss: 0.002\n",
            "[20,    40] loss: 0.004\n",
            "[21,    10] loss: 0.017\n",
            "[21,    20] loss: 0.010\n",
            "[21,    30] loss: 0.010\n",
            "[21,    40] loss: 0.011\n",
            "[22,    10] loss: 0.007\n",
            "[22,    20] loss: 0.006\n",
            "[22,    30] loss: 0.006\n",
            "[22,    40] loss: 0.004\n",
            "[23,    10] loss: 0.003\n",
            "[23,    20] loss: 0.002\n",
            "[23,    30] loss: 0.002\n",
            "[23,    40] loss: 0.002\n",
            "[24,    10] loss: 0.001\n",
            "[24,    20] loss: 0.001\n",
            "[24,    30] loss: 0.001\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.001\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.002\n",
            "[28,    10] loss: 0.009\n",
            "[28,    20] loss: 0.006\n",
            "[28,    30] loss: 0.004\n",
            "[28,    40] loss: 0.003\n",
            "[29,    10] loss: 0.002\n",
            "[29,    20] loss: 0.002\n",
            "[29,    30] loss: 0.002\n",
            "[29,    40] loss: 0.002\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.002\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.000\n",
            "[34,    40] loss: 0.000\n",
            "[35,    10] loss: 0.000\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.000\n",
            "[35,    40] loss: 0.064\n",
            "[36,    10] loss: 0.442\n",
            "[36,    20] loss: 0.427\n",
            "[36,    30] loss: 0.319\n",
            "[36,    40] loss: 0.267\n",
            "[37,    10] loss: 0.204\n",
            "[37,    20] loss: 0.170\n",
            "[37,    30] loss: 0.172\n",
            "[37,    40] loss: 0.156\n",
            "[38,    10] loss: 0.132\n",
            "[38,    20] loss: 0.126\n",
            "[38,    30] loss: 0.102\n",
            "[38,    40] loss: 0.095\n",
            "[39,    10] loss: 0.077\n",
            "[39,    20] loss: 0.060\n",
            "[39,    30] loss: 0.065\n",
            "[39,    40] loss: 0.077\n",
            "[40,    10] loss: 0.049\n",
            "[40,    20] loss: 0.049\n",
            "[40,    30] loss: 0.040\n",
            "[40,    40] loss: 0.046\n",
            "[41,    10] loss: 0.033\n",
            "[41,    20] loss: 0.032\n",
            "[41,    30] loss: 0.029\n",
            "[41,    40] loss: 0.045\n",
            "[42,    10] loss: 0.081\n",
            "[42,    20] loss: 0.088\n",
            "[42,    30] loss: 0.076\n",
            "[42,    40] loss: 0.055\n",
            "[43,    10] loss: 0.045\n",
            "[43,    20] loss: 0.033\n",
            "[43,    30] loss: 0.036\n",
            "[43,    40] loss: 0.033\n",
            "[44,    10] loss: 0.027\n",
            "[44,    20] loss: 0.028\n",
            "[44,    30] loss: 0.027\n",
            "[44,    40] loss: 0.022\n",
            "[45,    10] loss: 0.017\n",
            "[45,    20] loss: 0.014\n",
            "[45,    30] loss: 0.012\n",
            "[45,    40] loss: 0.026\n",
            "[46,    10] loss: 0.116\n",
            "[46,    20] loss: 0.087\n",
            "[46,    30] loss: 0.077\n",
            "[46,    40] loss: 0.094\n",
            "[47,    10] loss: 0.182\n",
            "[47,    20] loss: 0.116\n",
            "[47,    30] loss: 0.098\n",
            "[47,    40] loss: 0.055\n",
            "[48,    10] loss: 0.039\n",
            "[48,    20] loss: 0.037\n",
            "[48,    30] loss: 0.026\n",
            "[48,    40] loss: 0.028\n",
            "[49,    10] loss: 0.015\n",
            "[49,    20] loss: 0.015\n",
            "[49,    30] loss: 0.012\n",
            "[49,    40] loss: 0.012\n",
            "[50,    10] loss: 0.009\n",
            "[50,    20] loss: 0.008\n",
            "[50,    30] loss: 0.007\n",
            "[50,    40] loss: 0.008\n",
            "[51,    10] loss: 0.006\n",
            "[51,    20] loss: 0.007\n",
            "[51,    30] loss: 0.007\n",
            "[51,    40] loss: 0.007\n",
            "[52,    10] loss: 0.011\n",
            "[52,    20] loss: 0.010\n",
            "[52,    30] loss: 0.007\n",
            "[52,    40] loss: 0.008\n",
            "[53,    10] loss: 0.006\n",
            "[53,    20] loss: 0.006\n",
            "[53,    30] loss: 0.004\n",
            "[53,    40] loss: 0.005\n",
            "[54,    10] loss: 0.006\n",
            "[54,    20] loss: 0.010\n",
            "[54,    30] loss: 0.006\n",
            "[54,    40] loss: 0.006\n",
            "[55,    10] loss: 0.004\n",
            "[55,    20] loss: 0.003\n",
            "[55,    30] loss: 0.003\n",
            "[55,    40] loss: 0.004\n",
            "[56,    10] loss: 0.002\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.004\n",
            "[57,    10] loss: 0.016\n",
            "[57,    20] loss: 0.012\n",
            "[57,    30] loss: 0.010\n",
            "[57,    40] loss: 0.009\n",
            "[58,    10] loss: 0.026\n",
            "[58,    20] loss: 0.014\n",
            "[58,    30] loss: 0.014\n",
            "[58,    40] loss: 0.045\n",
            "[59,    10] loss: 0.162\n",
            "[59,    20] loss: 0.115\n",
            "[59,    30] loss: 0.092\n",
            "[59,    40] loss: 0.068\n",
            "[60,    10] loss: 0.078\n",
            "[60,    20] loss: 0.082\n",
            "[60,    30] loss: 0.059\n",
            "[60,    40] loss: 0.035\n",
            "[61,    10] loss: 0.024\n",
            "[61,    20] loss: 0.017\n",
            "[61,    30] loss: 0.016\n",
            "[61,    40] loss: 0.031\n",
            "[62,    10] loss: 0.058\n",
            "[62,    20] loss: 0.068\n",
            "[62,    30] loss: 0.051\n",
            "[62,    40] loss: 0.031\n",
            "[63,    10] loss: 0.017\n",
            "[63,    20] loss: 0.010\n",
            "[63,    30] loss: 0.013\n",
            "[63,    40] loss: 0.011\n",
            "[64,    10] loss: 0.006\n",
            "[64,    20] loss: 0.005\n",
            "[64,    30] loss: 0.006\n",
            "[64,    40] loss: 0.016\n",
            "[65,    10] loss: 0.057\n",
            "[65,    20] loss: 0.067\n",
            "[65,    30] loss: 0.032\n",
            "[65,    40] loss: 0.043\n",
            "[66,    10] loss: 0.059\n",
            "[66,    20] loss: 0.053\n",
            "[66,    30] loss: 0.041\n",
            "[66,    40] loss: 0.036\n",
            "[67,    10] loss: 0.015\n",
            "[67,    20] loss: 0.011\n",
            "[67,    30] loss: 0.011\n",
            "[67,    40] loss: 0.009\n",
            "[68,    10] loss: 0.005\n",
            "[68,    20] loss: 0.004\n",
            "[68,    30] loss: 0.004\n",
            "[68,    40] loss: 0.004\n",
            "[69,    10] loss: 0.003\n",
            "[69,    20] loss: 0.002\n",
            "[69,    30] loss: 0.002\n",
            "[69,    40] loss: 0.003\n",
            "[70,    10] loss: 0.002\n",
            "[70,    20] loss: 0.003\n",
            "[70,    30] loss: 0.002\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 0.900\n",
            "[1,    20] loss: 0.561\n",
            "[1,    30] loss: 0.479\n",
            "[1,    40] loss: 0.453\n",
            "[2,    10] loss: 0.436\n",
            "[2,    20] loss: 0.385\n",
            "[2,    30] loss: 0.353\n",
            "[2,    40] loss: 0.324\n",
            "[3,    10] loss: 0.339\n",
            "[3,    20] loss: 0.297\n",
            "[3,    30] loss: 0.280\n",
            "[3,    40] loss: 0.273\n",
            "[4,    10] loss: 0.227\n",
            "[4,    20] loss: 0.233\n",
            "[4,    30] loss: 0.207\n",
            "[4,    40] loss: 0.222\n",
            "[5,    10] loss: 0.215\n",
            "[5,    20] loss: 0.182\n",
            "[5,    30] loss: 0.180\n",
            "[5,    40] loss: 0.144\n",
            "[6,    10] loss: 0.134\n",
            "[6,    20] loss: 0.138\n",
            "[6,    30] loss: 0.134\n",
            "[6,    40] loss: 0.170\n",
            "[7,    10] loss: 0.164\n",
            "[7,    20] loss: 0.157\n",
            "[7,    30] loss: 0.144\n",
            "[7,    40] loss: 0.130\n",
            "[8,    10] loss: 0.138\n",
            "[8,    20] loss: 0.123\n",
            "[8,    30] loss: 0.121\n",
            "[8,    40] loss: 0.146\n",
            "[9,    10] loss: 0.171\n",
            "[9,    20] loss: 0.172\n",
            "[9,    30] loss: 0.158\n",
            "[9,    40] loss: 0.158\n",
            "[10,    10] loss: 0.124\n",
            "[10,    20] loss: 0.104\n",
            "[10,    30] loss: 0.093\n",
            "[10,    40] loss: 0.093\n",
            "[11,    10] loss: 0.110\n",
            "[11,    20] loss: 0.095\n",
            "[11,    30] loss: 0.070\n",
            "[11,    40] loss: 0.060\n",
            "[12,    10] loss: 0.046\n",
            "[12,    20] loss: 0.047\n",
            "[12,    30] loss: 0.040\n",
            "[12,    40] loss: 0.043\n",
            "[13,    10] loss: 0.031\n",
            "[13,    20] loss: 0.028\n",
            "[13,    30] loss: 0.031\n",
            "[13,    40] loss: 0.023\n",
            "[14,    10] loss: 0.022\n",
            "[14,    20] loss: 0.019\n",
            "[14,    30] loss: 0.018\n",
            "[14,    40] loss: 0.017\n",
            "[15,    10] loss: 0.015\n",
            "[15,    20] loss: 0.014\n",
            "[15,    30] loss: 0.015\n",
            "[15,    40] loss: 0.029\n",
            "[16,    10] loss: 0.101\n",
            "[16,    20] loss: 0.093\n",
            "[16,    30] loss: 0.096\n",
            "[16,    40] loss: 0.066\n",
            "[17,    10] loss: 0.076\n",
            "[17,    20] loss: 0.063\n",
            "[17,    30] loss: 0.050\n",
            "[17,    40] loss: 0.056\n",
            "[18,    10] loss: 0.061\n",
            "[18,    20] loss: 0.036\n",
            "[18,    30] loss: 0.032\n",
            "[18,    40] loss: 0.021\n",
            "[19,    10] loss: 0.012\n",
            "[19,    20] loss: 0.015\n",
            "[19,    30] loss: 0.013\n",
            "[19,    40] loss: 0.011\n",
            "[20,    10] loss: 0.007\n",
            "[20,    20] loss: 0.006\n",
            "[20,    30] loss: 0.006\n",
            "[20,    40] loss: 0.008\n",
            "[21,    10] loss: 0.016\n",
            "[21,    20] loss: 0.016\n",
            "[21,    30] loss: 0.011\n",
            "[21,    40] loss: 0.010\n",
            "[22,    10] loss: 0.007\n",
            "[22,    20] loss: 0.005\n",
            "[22,    30] loss: 0.004\n",
            "[22,    40] loss: 0.007\n",
            "[23,    10] loss: 0.005\n",
            "[23,    20] loss: 0.006\n",
            "[23,    30] loss: 0.005\n",
            "[23,    40] loss: 0.004\n",
            "[24,    10] loss: 0.002\n",
            "[24,    20] loss: 0.002\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.002\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.004\n",
            "[26,    10] loss: 0.005\n",
            "[26,    20] loss: 0.004\n",
            "[26,    30] loss: 0.003\n",
            "[26,    40] loss: 0.002\n",
            "[27,    10] loss: 0.002\n",
            "[27,    20] loss: 0.002\n",
            "[27,    30] loss: 0.002\n",
            "[27,    40] loss: 0.002\n",
            "[28,    10] loss: 0.002\n",
            "[28,    20] loss: 0.002\n",
            "[28,    30] loss: 0.002\n",
            "[28,    40] loss: 0.003\n",
            "[29,    10] loss: 0.017\n",
            "[29,    20] loss: 0.009\n",
            "[29,    30] loss: 0.007\n",
            "[29,    40] loss: 0.007\n",
            "[30,    10] loss: 0.006\n",
            "[30,    20] loss: 0.007\n",
            "[30,    30] loss: 0.003\n",
            "[30,    40] loss: 0.007\n",
            "[31,    10] loss: 0.032\n",
            "[31,    20] loss: 0.044\n",
            "[31,    30] loss: 0.032\n",
            "[31,    40] loss: 0.040\n",
            "[32,    10] loss: 0.132\n",
            "[32,    20] loss: 0.126\n",
            "[32,    30] loss: 0.098\n",
            "[32,    40] loss: 0.073\n",
            "[33,    10] loss: 0.059\n",
            "[33,    20] loss: 0.043\n",
            "[33,    30] loss: 0.030\n",
            "[33,    40] loss: 0.029\n",
            "[34,    10] loss: 0.015\n",
            "[34,    20] loss: 0.010\n",
            "[34,    30] loss: 0.009\n",
            "[34,    40] loss: 0.013\n",
            "[35,    10] loss: 0.016\n",
            "[35,    20] loss: 0.019\n",
            "[35,    30] loss: 0.011\n",
            "[35,    40] loss: 0.025\n",
            "[36,    10] loss: 0.088\n",
            "[36,    20] loss: 0.083\n",
            "[36,    30] loss: 0.058\n",
            "[36,    40] loss: 0.053\n",
            "[37,    10] loss: 0.042\n",
            "[37,    20] loss: 0.030\n",
            "[37,    30] loss: 0.025\n",
            "[37,    40] loss: 0.017\n",
            "[38,    10] loss: 0.008\n",
            "[38,    20] loss: 0.006\n",
            "[38,    30] loss: 0.004\n",
            "[38,    40] loss: 0.005\n",
            "[39,    10] loss: 0.003\n",
            "[39,    20] loss: 0.004\n",
            "[39,    30] loss: 0.003\n",
            "[39,    40] loss: 0.002\n",
            "[40,    10] loss: 0.002\n",
            "[40,    20] loss: 0.002\n",
            "[40,    30] loss: 0.002\n",
            "[40,    40] loss: 0.006\n",
            "[41,    10] loss: 0.034\n",
            "[41,    20] loss: 0.034\n",
            "[41,    30] loss: 0.021\n",
            "[41,    40] loss: 0.017\n",
            "[42,    10] loss: 0.011\n",
            "[42,    20] loss: 0.006\n",
            "[42,    30] loss: 0.006\n",
            "[42,    40] loss: 0.007\n",
            "[43,    10] loss: 0.017\n",
            "[43,    20] loss: 0.015\n",
            "[43,    30] loss: 0.009\n",
            "[43,    40] loss: 0.007\n",
            "[44,    10] loss: 0.008\n",
            "[44,    20] loss: 0.006\n",
            "[44,    30] loss: 0.004\n",
            "[44,    40] loss: 0.006\n",
            "[45,    10] loss: 0.004\n",
            "[45,    20] loss: 0.003\n",
            "[45,    30] loss: 0.004\n",
            "[45,    40] loss: 0.003\n",
            "[46,    10] loss: 0.002\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.011\n",
            "[47,    10] loss: 0.039\n",
            "[47,    20] loss: 0.032\n",
            "[47,    30] loss: 0.022\n",
            "[47,    40] loss: 0.036\n",
            "[48,    10] loss: 0.068\n",
            "[48,    20] loss: 0.047\n",
            "[48,    30] loss: 0.040\n",
            "[48,    40] loss: 0.022\n",
            "[49,    10] loss: 0.019\n",
            "[49,    20] loss: 0.014\n",
            "[49,    30] loss: 0.010\n",
            "[49,    40] loss: 0.006\n",
            "[50,    10] loss: 0.005\n",
            "[50,    20] loss: 0.004\n",
            "[50,    30] loss: 0.003\n",
            "[50,    40] loss: 0.003\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.004\n",
            "[53,    10] loss: 0.014\n",
            "[53,    20] loss: 0.012\n",
            "[53,    30] loss: 0.008\n",
            "[53,    40] loss: 0.007\n",
            "[54,    10] loss: 0.003\n",
            "[54,    20] loss: 0.002\n",
            "[54,    30] loss: 0.002\n",
            "[54,    40] loss: 0.011\n",
            "[55,    10] loss: 0.057\n",
            "[55,    20] loss: 0.051\n",
            "[55,    30] loss: 0.039\n",
            "[55,    40] loss: 0.026\n",
            "[56,    10] loss: 0.027\n",
            "[56,    20] loss: 0.026\n",
            "[56,    30] loss: 0.016\n",
            "[56,    40] loss: 0.016\n",
            "[57,    10] loss: 0.033\n",
            "[57,    20] loss: 0.033\n",
            "[57,    30] loss: 0.024\n",
            "[57,    40] loss: 0.018\n",
            "[58,    10] loss: 0.013\n",
            "[58,    20] loss: 0.008\n",
            "[58,    30] loss: 0.006\n",
            "[58,    40] loss: 0.007\n",
            "[59,    10] loss: 0.004\n",
            "[59,    20] loss: 0.004\n",
            "[59,    30] loss: 0.003\n",
            "[59,    40] loss: 0.002\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.010\n",
            "[65,    10] loss: 0.047\n",
            "[65,    20] loss: 0.048\n",
            "[65,    30] loss: 0.031\n",
            "[65,    40] loss: 0.023\n",
            "[66,    10] loss: 0.013\n",
            "[66,    20] loss: 0.008\n",
            "[66,    30] loss: 0.007\n",
            "[66,    40] loss: 0.005\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.002\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 87 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 0.866\n",
            "[1,    20] loss: 0.552\n",
            "[1,    30] loss: 0.511\n",
            "[1,    40] loss: 0.436\n",
            "[2,    10] loss: 0.410\n",
            "[2,    20] loss: 0.384\n",
            "[2,    30] loss: 0.366\n",
            "[2,    40] loss: 0.365\n",
            "[3,    10] loss: 0.303\n",
            "[3,    20] loss: 0.290\n",
            "[3,    30] loss: 0.285\n",
            "[3,    40] loss: 0.269\n",
            "[4,    10] loss: 0.241\n",
            "[4,    20] loss: 0.231\n",
            "[4,    30] loss: 0.205\n",
            "[4,    40] loss: 0.206\n",
            "[5,    10] loss: 0.218\n",
            "[5,    20] loss: 0.237\n",
            "[5,    30] loss: 0.200\n",
            "[5,    40] loss: 0.196\n",
            "[6,    10] loss: 0.184\n",
            "[6,    20] loss: 0.175\n",
            "[6,    30] loss: 0.160\n",
            "[6,    40] loss: 0.136\n",
            "[7,    10] loss: 0.112\n",
            "[7,    20] loss: 0.126\n",
            "[7,    30] loss: 0.115\n",
            "[7,    40] loss: 0.120\n",
            "[8,    10] loss: 0.174\n",
            "[8,    20] loss: 0.169\n",
            "[8,    30] loss: 0.145\n",
            "[8,    40] loss: 0.138\n",
            "[9,    10] loss: 0.142\n",
            "[9,    20] loss: 0.118\n",
            "[9,    30] loss: 0.111\n",
            "[9,    40] loss: 0.104\n",
            "[10,    10] loss: 0.095\n",
            "[10,    20] loss: 0.093\n",
            "[10,    30] loss: 0.093\n",
            "[10,    40] loss: 0.068\n",
            "[11,    10] loss: 0.076\n",
            "[11,    20] loss: 0.075\n",
            "[11,    30] loss: 0.074\n",
            "[11,    40] loss: 0.082\n",
            "[12,    10] loss: 0.168\n",
            "[12,    20] loss: 0.173\n",
            "[12,    30] loss: 0.116\n",
            "[12,    40] loss: 0.117\n",
            "[13,    10] loss: 0.083\n",
            "[13,    20] loss: 0.073\n",
            "[13,    30] loss: 0.070\n",
            "[13,    40] loss: 0.083\n",
            "[14,    10] loss: 0.159\n",
            "[14,    20] loss: 0.132\n",
            "[14,    30] loss: 0.123\n",
            "[14,    40] loss: 0.092\n",
            "[15,    10] loss: 0.077\n",
            "[15,    20] loss: 0.065\n",
            "[15,    30] loss: 0.052\n",
            "[15,    40] loss: 0.079\n",
            "[16,    10] loss: 0.201\n",
            "[16,    20] loss: 0.170\n",
            "[16,    30] loss: 0.144\n",
            "[16,    40] loss: 0.121\n",
            "[17,    10] loss: 0.075\n",
            "[17,    20] loss: 0.069\n",
            "[17,    30] loss: 0.056\n",
            "[17,    40] loss: 0.042\n",
            "[18,    10] loss: 0.030\n",
            "[18,    20] loss: 0.028\n",
            "[18,    30] loss: 0.030\n",
            "[18,    40] loss: 0.026\n",
            "[19,    10] loss: 0.019\n",
            "[19,    20] loss: 0.020\n",
            "[19,    30] loss: 0.019\n",
            "[19,    40] loss: 0.021\n",
            "[20,    10] loss: 0.038\n",
            "[20,    20] loss: 0.041\n",
            "[20,    30] loss: 0.037\n",
            "[20,    40] loss: 0.098\n",
            "[21,    10] loss: 0.217\n",
            "[21,    20] loss: 0.195\n",
            "[21,    30] loss: 0.167\n",
            "[21,    40] loss: 0.140\n",
            "[22,    10] loss: 0.142\n",
            "[22,    20] loss: 0.107\n",
            "[22,    30] loss: 0.094\n",
            "[22,    40] loss: 0.064\n",
            "[23,    10] loss: 0.045\n",
            "[23,    20] loss: 0.033\n",
            "[23,    30] loss: 0.034\n",
            "[23,    40] loss: 0.029\n",
            "[24,    10] loss: 0.018\n",
            "[24,    20] loss: 0.017\n",
            "[24,    30] loss: 0.017\n",
            "[24,    40] loss: 0.015\n",
            "[25,    10] loss: 0.010\n",
            "[25,    20] loss: 0.009\n",
            "[25,    30] loss: 0.007\n",
            "[25,    40] loss: 0.008\n",
            "[26,    10] loss: 0.006\n",
            "[26,    20] loss: 0.006\n",
            "[26,    30] loss: 0.005\n",
            "[26,    40] loss: 0.005\n",
            "[27,    10] loss: 0.006\n",
            "[27,    20] loss: 0.005\n",
            "[27,    30] loss: 0.005\n",
            "[27,    40] loss: 0.006\n",
            "[28,    10] loss: 0.006\n",
            "[28,    20] loss: 0.007\n",
            "[28,    30] loss: 0.005\n",
            "[28,    40] loss: 0.006\n",
            "[29,    10] loss: 0.004\n",
            "[29,    20] loss: 0.006\n",
            "[29,    30] loss: 0.004\n",
            "[29,    40] loss: 0.008\n",
            "[30,    10] loss: 0.042\n",
            "[30,    20] loss: 0.029\n",
            "[30,    30] loss: 0.024\n",
            "[30,    40] loss: 0.031\n",
            "[31,    10] loss: 0.060\n",
            "[31,    20] loss: 0.043\n",
            "[31,    30] loss: 0.035\n",
            "[31,    40] loss: 0.039\n",
            "[32,    10] loss: 0.036\n",
            "[32,    20] loss: 0.037\n",
            "[32,    30] loss: 0.027\n",
            "[32,    40] loss: 0.021\n",
            "[33,    10] loss: 0.031\n",
            "[33,    20] loss: 0.027\n",
            "[33,    30] loss: 0.020\n",
            "[33,    40] loss: 0.016\n",
            "[34,    10] loss: 0.012\n",
            "[34,    20] loss: 0.007\n",
            "[34,    30] loss: 0.007\n",
            "[34,    40] loss: 0.007\n",
            "[35,    10] loss: 0.013\n",
            "[35,    20] loss: 0.010\n",
            "[35,    30] loss: 0.007\n",
            "[35,    40] loss: 0.008\n",
            "[36,    10] loss: 0.007\n",
            "[36,    20] loss: 0.005\n",
            "[36,    30] loss: 0.004\n",
            "[36,    40] loss: 0.004\n",
            "[37,    10] loss: 0.002\n",
            "[37,    20] loss: 0.002\n",
            "[37,    30] loss: 0.002\n",
            "[37,    40] loss: 0.004\n",
            "[38,    10] loss: 0.007\n",
            "[38,    20] loss: 0.007\n",
            "[38,    30] loss: 0.005\n",
            "[38,    40] loss: 0.004\n",
            "[39,    10] loss: 0.003\n",
            "[39,    20] loss: 0.002\n",
            "[39,    30] loss: 0.002\n",
            "[39,    40] loss: 0.003\n",
            "[40,    10] loss: 0.003\n",
            "[40,    20] loss: 0.003\n",
            "[40,    30] loss: 0.003\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.001\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.001\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.001\n",
            "[45,    10] loss: 0.001\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.000\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.013\n",
            "[50,    10] loss: 0.051\n",
            "[50,    20] loss: 0.057\n",
            "[50,    30] loss: 0.034\n",
            "[50,    40] loss: 0.059\n",
            "[51,    10] loss: 0.105\n",
            "[51,    20] loss: 0.103\n",
            "[51,    30] loss: 0.073\n",
            "[51,    40] loss: 0.102\n",
            "[52,    10] loss: 0.214\n",
            "[52,    20] loss: 0.163\n",
            "[52,    30] loss: 0.115\n",
            "[52,    40] loss: 0.065\n",
            "[53,    10] loss: 0.044\n",
            "[53,    20] loss: 0.039\n",
            "[53,    30] loss: 0.029\n",
            "[53,    40] loss: 0.018\n",
            "[54,    10] loss: 0.013\n",
            "[54,    20] loss: 0.016\n",
            "[54,    30] loss: 0.011\n",
            "[54,    40] loss: 0.012\n",
            "[55,    10] loss: 0.006\n",
            "[55,    20] loss: 0.006\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.008\n",
            "[56,    10] loss: 0.008\n",
            "[56,    20] loss: 0.006\n",
            "[56,    30] loss: 0.006\n",
            "[56,    40] loss: 0.007\n",
            "[57,    10] loss: 0.007\n",
            "[57,    20] loss: 0.007\n",
            "[57,    30] loss: 0.006\n",
            "[57,    40] loss: 0.005\n",
            "[58,    10] loss: 0.004\n",
            "[58,    20] loss: 0.003\n",
            "[58,    30] loss: 0.003\n",
            "[58,    40] loss: 0.003\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.020\n",
            "[60,    10] loss: 0.087\n",
            "[60,    20] loss: 0.077\n",
            "[60,    30] loss: 0.058\n",
            "[60,    40] loss: 0.053\n",
            "[61,    10] loss: 0.053\n",
            "[61,    20] loss: 0.039\n",
            "[61,    30] loss: 0.021\n",
            "[61,    40] loss: 0.025\n",
            "[62,    10] loss: 0.033\n",
            "[62,    20] loss: 0.052\n",
            "[62,    30] loss: 0.023\n",
            "[62,    40] loss: 0.040\n",
            "[63,    10] loss: 0.054\n",
            "[63,    20] loss: 0.038\n",
            "[63,    30] loss: 0.018\n",
            "[63,    40] loss: 0.016\n",
            "[64,    10] loss: 0.009\n",
            "[64,    20] loss: 0.008\n",
            "[64,    30] loss: 0.007\n",
            "[64,    40] loss: 0.006\n",
            "[65,    10] loss: 0.003\n",
            "[65,    20] loss: 0.003\n",
            "[65,    30] loss: 0.003\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.010\n",
            "[68,    10] loss: 0.033\n",
            "[68,    20] loss: 0.033\n",
            "[68,    30] loss: 0.022\n",
            "[68,    40] loss: 0.016\n",
            "[69,    10] loss: 0.007\n",
            "[69,    20] loss: 0.005\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.009\n",
            "[70,    10] loss: 0.021\n",
            "[70,    20] loss: 0.016\n",
            "[70,    30] loss: 0.014\n",
            "[70,    40] loss: 0.008\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AbZaQekCfVjN",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ouBomi5DfVjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "cfe68c7f-f0be-4975-ace9-684f80f29f51"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend()\n",
        "fig.savefig(\"Figure.pdf\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd3xUZfb/38+U9E4SSIOQQAIhjd6b\nKAgWlFVE14Jr26+6rrsugmVdV1fFld8WXV3sYsGGLhaaIlKk9xoICYQkhIQkpNeZzPP74yYQIGWS\nzGRSnvfrdV/J3PvMvecOZM49zznn8wgpJQqFQqHovugcbYBCoVAoHItyBAqFQtHNUY5AoVAoujnK\nESgUCkU3RzkChUKh6OYYHG1Aa/D395fh4eGONkOhUCg6Fbt3786TUgZcur9TOoLw8HB27drlaDMU\nCoWiUyGEONXQfjU1pFAoFN0c5QgUCoWim6McgUKhUHRzOmWOQKFQdE1MJhOZmZlUVlY62pROjYuL\nC6GhoRiNRqvGK0egUCg6DJmZmXh6ehIeHo4QwtHmdEqklOTn55OZmUnfvn2teo+aGlIoFB2GyspK\nevTooZxAGxBC0KNHjxZFVcoRKBSKDoVyAm2npZ9ht3IEh04X8frPKSjpbYVCobhAt3IE3+w7zStr\njrFw9VHlDBQKRbM8++yzLFq0qMkxy5cv58iRIza9blpaGkuXLm30+NVXX42Pjw/XXnutTa7XrRzB\nE9MHcvuo3ry54QTPf5+knIFCoWgzjnAE8+bN46OPPrLZ9bqVI9DpBM/PjOXuseG8t/kkz3xzGItF\nOQOFQnGBF154gaioKMaNG8exY8fO73/77bcZPnw4CQkJ/OpXv6K8vJwtW7bw7bffMm/ePBITE0lN\nTW1wHMCXX35JbGwsCQkJTJgwAYCamhrmzZvH8OHDiY+P58033wRgwYIFbNq0icTERP75z39eZuOU\nKVPw9PS02T13u/JRIQTPXBuDk17HmxtPYLZY+NsNceh1KkGlUHQk/vrdYY5kFdv0nDHBXvzlukGN\nHt+9ezefffYZ+/btw2w2M2TIEIYOHQrArFmzuO+++wB4+umneffdd/nd737H9ddfz7XXXstNN90E\ngI+PT4PjnnvuOdasWUNISAiFhYUAvPvuu3h7e7Nz506qqqoYO3YsU6dOZeHChSxatIjvv//epvff\nGN3OEYDmDBZMH4BRr+M/P6ew7cQ57hsfwawhIbgY9Y42T6FQOIhNmzZx44034ubmBsD1119//tih\nQ4d4+umnKSwspLS0lGnTpjV4jsbGjR07lrlz5zJ79mxmzZoFwA8//MCBAwdYtmwZAEVFRRw/fhwn\nJyd73uZldEtHAJoz+NO0aGJDvHljfQpP/u8g//gxmd+MC+f2UX3wcrGuI0+hUNiHpp7cHcHcuXNZ\nvnw5CQkJfPDBB6xfv75F4xYvXsz27dtZsWIFQ4cOZffu3Ugpee211y5zKo2d2150qxxBQ1wd24tv\nHhrL0ntHMjDIk7+vPsbsxVupNlscbZpCoWhnJkyYwPLly6moqKCkpITvvvvu/LGSkhKCgoIwmUx8\n8skn5/d7enpSUlLS7LjU1FRGjhzJc889R0BAABkZGUybNo3//ve/mEwmAJKTkykrK7vsnPam2zsC\n0KKDMf38+eiekbzx6yEczS7hrY2pjjZLoVC0M0OGDOGWW24hISGB6dOnM3z48PPHnn/+eUaOHMnY\nsWMZMGDA+f1z5szhlVdeYfDgwaSmpjY6bt68ecTFxREbG8uYMWNISEjg3nvvJSYmhiFDhhAbG8sD\nDzyA2WwmPj4evV5PQkJCg8ni8ePHc/PNN/PTTz8RGhrKmjVr2nTfojOWUA4bNkzac2Gahz7Zw49J\nOaz+/XgiAjzsdh2FQnExSUlJDBw40NFmdAka+iyFELullMMuHasiggb4y/UxOBt0PPH1QdVroFAo\nujzKETRAoKcLT84YyPaT5/hiV4ajzVEoFAq7ohxBI9wyLIwRff14YUUSuSVVjjZHoVAo7IZdHYEQ\n4j0hxFkhxKFGjgshxKtCiBQhxAEhxBB72tMSdDrBS7PiqDRZeOp/B1l9KJsPt6bxypqjzF92gLVH\nchxtokKhUNgEe/cRfAD8B/iwkePTgf6120jgv7U/OwSRAR48fEU//vFjMj/UfvHrdQI3Jz1f7M7g\nqRkDuWdcXyWbq1AoOjV2dQRSyo1CiPAmhswEPpRaRnabEMJHCBEkpTxjT7tawkOT+zEs3BcvFyM9\nvVzo4e5EdY2FP36xj7+tSCKzoII/XxujJCoUCkWnxdE5ghCgfjY2s3bfZQgh7hdC7BJC7MrNzW0X\n40CLAMZE+hMb4k2ApzM6ncDFqOc/tw7hvvF9+WBLGr/9eDcV1TXtZpNCoWgfOqIM9b59+xg9ejSD\nBg0iPj6ezz//vM3Xc7QjsBop5VtSymFSymEBAQGONgedTvDUNTE8e10Ma5NymPPWVjILyh1tlkKh\naGfa2xG4ubnx4YcfcvjwYVavXs2jjz56XsSutTjaEZwGwuq9Dq3d12mYO7Yvb94+lNTcMmb8exNr\nDmc72iSFQtEGOroMdVRUFP379wcgODiYwMBA2jpL4mjRuW+Bh4UQn6EliYs6Un7AWqYO6sWKRzx5\neOleHvhoN3PHhPPEjAE4G5SSqULRalYtgOyDtj1nrziYvrDRw51NhnrHjh1UV1cTGRnZpo/Fro5A\nCPEpMAnwF0JkAn8BjABSysXASmAGkAKUA3fb0x570qeHO8v+bzQvrzrGe5tPsjPtHItvH0qYn5uj\nTVMoFFbSmWSoz5w5wx133MGSJUvQ6do2uWPvqqFbmzkugYfsaUN74mzQ88x1MYyJ7MEfv9jHnLe2\n8dn9o5QzUChaQxNP7o6gI8lQFxcXc8011/DCCy8watSoNt+bo3MEXZIrY3qy9L5RlFaZmfPWNjLO\nqSSyQtEZ6Awy1NXV1dx4443ceeed56ej2opyBHYiNsSbT+4dqZyBQtGJ6Awy1F988QUbN27kgw8+\nIDExkcTERPbt29em+1Yy1Hbm0Okifv3OdjycDXx870j6+rs72iSFosOiZKhtR0tkqB1dNdTlqYsM\nfv3OdiYvWo+Pm5E+PdwJ7+FG/0AP7hgVjrebWhZToVA4DuUI2oHYEG++eWgsPx7JIS2/jFP55ew+\nVcC3+7P4ZHs6i25OYGw/f0ebqVAouinKEbQT4f7u3Dch4qJ9BzOL+P3ne/n1O9u5Z1xf5k2LxsWo\neg8UCkX7opLFDiQu1JsVvxvPnaP78O4vJ5n5n80cz2m/BasVCoUClCNwOK5Oep6bGcsHdw8nv6yK\n+z/aTaVJCdgpFIr2QzmCDsKk6ED+eUsiJ/PK+O/6VEebo1AouhHKEXQgxvcP4PqEYP67PpXU3FJH\nm6NQdHs6ogz1qVOnGDJkCImJiQwaNIjFixe3+XrKEXQwnr52IM5GHX9efojO2OOhUHQ32tsRBAUF\nsXXrVvbt28f27dtZuHAhWVlZbbqecgQdjEBPF+ZfPYAtqfks39epFLkVii5BR5ehdnJywtnZGYCq\nqiosFkub71mVj3ZAbhvRm2W7M/nb90lMjg7Ex615JUKFoqvx8o6XOXruqE3POcBvAPNHzG/0eGeR\noc7IyOCaa64hJSWFV155heDg4DZ9Lioi6IDodIIXb4yjsMLEy6tt+4egUCgap74MtZeX12Uy1OPH\njycuLo5PPvmEw4cPN3iOxsbVyVC//fbb1NRolYE//PADH374IYmJiYwcOZL8/HyOHz/erJ1hYWEc\nOHCAlJQUlixZQk5OTpvuW0UEHZSYYC/uGdeXtzaewN/DmT9eFYUQwtFmKRTtRlNP7o6gI8lQ1xEc\nHExsbCybNm1qkxJp94sIOlECdt60aOYMD+O1dSk8+vk+qsyqv0ChsCedQYY6MzOTiooKAAoKCvjl\nl1+Ijo5u0313r4hg4ytw6Gv4vy3QCZ6ujXodL82KI8zPjVfWHCOrsIK37hiGr3vjOYNKUw2rD2Uz\nIy4IJ0P38/MKRVuoL0MdGBjYoAx1QEAAI0eOPP9FPWfOHO677z5effVVli1b1ui4efPmcfz4caSU\nTJkyhYSEBOLj40lLS2PIkCFIKQkICGD58uUXyVDPnTuXP/zhD+ftSEpK4rHHHkMIgZSSP/3pT8TF\nxbXpvruXDPWOt2Hln+CRveAX0fz4DsR3+7N47Mv9hPi48u5dw4gI8LhsTKWphgc+2s2G5FyevyGW\nO0b1cYClCkXrUTLUtqMlMtTd65Gx70Tt54kNjrWjFVyXEMzSe0dSVGFi5n82s/bIxcmhKnMND36y\nhw3JuXi5GFh18IyDLFUoFJ2N7uUI/PuDZxCc7HyOAGBYuB/fPjyWPv5u3PvhLv7xYzIWi6TabOGh\nT/ay7uhZXrwxjrvGhLPtRD75pVWONlmhUHQCupcjEEKLCk5uBBs0YTiCUF83lv12DL8aEsqrPx3n\n3g938fDSPaxNyuH5mYO4bWRvpscGYZHww5G2lZQpFIruQfdyBAARE6E8H842XAPcGXAx6ll0czzP\nzxzExuRcfjiSw1+ui+GO0eEADAzyJLyHGyvV9JBCobCC7lU1BBfyBCc3Qq+2ZdodiRCCO0aHkxDm\nQ3ZRJVMH9bro2Iy4IN7ceIKCsuomq4wUCoWi+0UE3iHQo1+nTBg3RHyoz0VOoI4ZcUHUWCQ/HMl2\ngFUKhaIz0f0cAWhRwanNUGNytCV2Y1CwF2F+rqw8qByBQtFaOqIMdR3FxcWEhoby8MMPt/l63dMR\nREyE6lI4vcfRltiNuumhzSl5FJV3XYenUDgaRzmCP//5z+dVTNtK93QE4eMB0WnLSK1lRmwQZjU9\npFC0iI4uQw2aSmpOTg5Tp061yT3bPVkshLga+DegB96RUi685HhvYAngUztmgZRypV2NcvODoHgt\nTzDxcbteypHEh3oT4uPKqkPZ3DwszNHmKBQtIvvFF6lKsq36rvPAAfR68slGj3cGGWqLxcJjjz3G\nxx9/zNq1a23yudg1IhBC6IHXgelADHCrECLmkmFPA19IKQcDc4A37GnTefpOhMwdUF3eLpdzBNr0\nUC82Hc+luFJNDykUzdEZZKjfeOMNZsyYQWhoqI3u2v4RwQggRUp5AkAI8RkwE6g/oSYBr9rfvYG2\nrblmLRETYcurkL4V+k1pl0s6gulxQby96SRrj+Qwa4jt/uMoFPamqSd3R9BRZKi3bt3Kpk2beOON\nNygtLaW6uhoPDw8WLlzY6Huaw945ghAgo97rzNp99XkWuF0IkQmsBH7X0ImEEPcLIXYJIXbl5ua2\n3bLeo0Fn7PJ5gsFhPgR7u7B8X/v4V4WiM9MZZKg/+eQT0tPTSUtLY9GiRdx5551tcgLQMZLFtwIf\nSClDgRnAR0KIy+ySUr4lpRwmpRwWEBDQ9qs6uUPYiC7TT9AYQghuHdGbjcm5JOc0/B9LoVBo1Jeh\nnj59eoMy1GPHjmXAgAHn98+ZM4dXXnmFwYMHk5qa2ui4efPmERcXR2xsLGPGjCEhIYF7772XmJgY\nhgwZQmxsLA888ABms/kiGeqGksW2xq4y1EKI0cCzUsppta+fAJBSvlRvzGHgaillRu3rE8AoKeXZ\nxs7bahnqS1n/Mqx/CR4/oSWQuygFZdWMWbiOa+KDWHRzgqPNUSgaRclQ246OJEO9E+gvhOgrhHBC\nSwZ/e8mYdGBKrZEDARfABnM/VhAxEZBw/Id2uZyj8HV3YvawUL7Zd5qc4soGx5wtrlQroCkU3RS7\nOgIppRl4GFgDJKFVBx0WQjwnhKhLxz8G3CeE2A98CsyV7bVaTuhwCBgIG/7epbuMAe4dH0GNRfL+\n5rTLjh3JKmbCKz/z6k/NL5qtUCi6HnbPEUgpV0opo6SUkVLKF2r3PSOl/Lb29yNSyrFSygQpZaKU\nsv0ez3V6mPIMnEuFvR+322UdQZifG9Pjgvhk+ylKq8zn95dUmnho6R4qTRZ+VLLVCkW3pCMkix1L\n9HQIGwXrF3bpngKA+8dHUFJp5rMd6QBIKZn/1QHSz5UzI64XyTmlZBVWONhKhULR3ihHIARc+SyU\nZsP2xY62xq4khPkwsq8f7/1yElONhSVb0lh5MJt506L5w5VRAKw/1j7pGYVC0XFQjgCgz2iIuhp+\n+ReUn3O0NXblgYkRZBVV8tLKo7ywMokrBwZy//gI+gV6EOLjyobkRou1FApFF0U5gjqmPANVxfCL\n/Wt2HcmkqED6BXrw3uaT9PRy4f/dnIhOJxBCMDE6gM0p+VSbO+cyngqFremoMtR6vZ7ExEQSExMv\nksFoLcoR1NFzEMTfAjvegqLTjrbGbuh0gkev7I+Pm5E3fj0Ebzfj+WMTowIorTKz+1SBAy1UKDoX\njnAErq6u7Nu3j3379vHtt5dW5Lcc5QjqM/lJkBbY+HdHW2JXro0PZvfTVxEf6nPR/rH9/DHqBevV\n9JCiG9MZZKhtTfdbs7gpfPtA7K/g8HK45h9aeWkXRa8Tl+3zcDYwrI8fG47l8sR01d2pcCybvkgm\nL6PUpuf0D/Ng/OyoRo93BhlqgMrKSoYNG4bBYGDBggXccMMNbfpcul1EUG5qpkS035VQWQhZe9vH\noA7GxOgAjmaXkF3UcAeyQtGV6Qwy1ACnTp1i165dLF26lEcffZTU1NQ23Xe3igj+vvPvfJ/6PRvn\nbGx8UMRkQEDKTxB6mSRHl2dSdAALVx1lQ/JZbhne29HmKLoxTT25O4KOIkMNEBKiiThHREQwadIk\n9u7dS2RkZKvvrVtFBEHuQRRUFZBfkd/4IPceEJwIqT+1n2EdiOienvTycmFDsuonUHQ/OoMMdUFB\nAVVVVQDk5eWxefNmYmIuXe+rZXSriCDSW/OYqYWp9HDt0cTAKVoZaUUhuPo0Pq4LIoRgYlQAKw+d\nwVxjwaDvVs8Kim5OfRnqwMDABmWoAwICGDly5Pkv6jlz5nDffffx6quvsmzZskbHzZs3j+PHjyOl\nZMqUKSQkJBAfH09aWhpDhgxBSklAQADLly+/SIZ67ty5/OEPfzhvR1JSEg888AA6nQ6LxcKCBQva\n7AjsKkNtL1orQ51TlsOVy67kiRFPcNvA2xofeGoLvD8dZn8EMW2v0e1srDp4hv/7ZA9fPDCaEX27\nrjy3ouOhZKhth81lqIUQLwkhvIQQBiHEGiFEjhCiiW/SjkmgWyCeRk9OFJ1oemDocHDy7LbTQ2P7\n+6PXCdYfU2WkCkV3wNq4f7qUshi4Fm1N4QHAfLtZZSeEEET6RJJSmNL0QL1RW6sgZR10woiprXi5\nGBna21fpDikU3QRrHUFdLmEG8KWUsgBt0flOR6RPJKmFqTQ7JRY5GYrSIb9tZVmdlatienLkTDEp\nZ21bx61QKDoe1jqCVUKIQ8BI4EchhD9QZT+z7EekTySFVYXkVzZROQRawhi67fTQzMHB6HWCr/Zk\nOtoUhUJhZ6xyBFLKecAVwFAppQmoAGbZ0zB7EemjVQ6dKGwmT+DXF/witH6CbkigpwuTogL4ek8m\nNZZOGfwpFAorsTZZPAuokFKahRALgPeBALtaZif6+fQDaD5PAFpUkLYJzJ0y+GkzNw0NJae4ik3H\nVa5AoejKWDs19KyUskQIMQYtT/AJ0ClXcQlwDcDT6ElqoRVz/5FXgKkcMrbb37AOyJSBPfF1M/Ll\nbjU9pOiedFQZ6vT0dKZOncrAgQOJiYkhLS2tTdez1hHU1P68FnhTSvkN4NymKzsIqyuHAPqOB52h\n204PORl0zEwM4cfDORSVmxxtjkLRIXGEI7jzzjuZN28eSUlJ7Nixg8DAwDZdz1pHcEYI8TowB1gp\nhHBqwXs7HJE+kaQWWVE55OyprWfcTRPGoE0PVddY+HZ/112jQaGoT0eXoT5y5Ahms5mrrroKAA8P\nD9zc3Np0z9ZKTMxGmxJ6TUpZIIQIBha06coOpJ9PP746/hX5lfn4u/o3M/gK+Ok5KM4Cr+D2MbAD\nMSjYiwG9PFm2O5M7Roc72hxFN+LnD97i7KlmijpaSGCfCCbPvb/R451Bhjo5ORkfHx9mzZrFyZMn\nufLKK1m4cCF6fetl862tGioFDgOThBC/BXyllKtafVUHE+ETAWBdniDmBhB6bT3jbogQgpuHhbE/\ns4jknIZFsBSKrkJnkKE2m81s2rSJRYsWsXPnTk6cOMEHH3zQpvu2KiIQQjwMPAgsr931hRDidSnl\nG226ejuT+8YbFH+/gn7L3ge0yqGRQSObflOPSBhyB+x6D0Y/CL7h9je0g3FDYjAvrUxi2e5Mnpyh\ndGAU7UNTT+6OoKPIUIeGhpKYmEhEhPZAe8MNN7Bt2zbuueeeVt+btfP89wMjpJRPSimfRGss+22r\nr+oghE5H9YkT9BCeeDp5Nt9LUMfE+dpqZT+/ZF8DOyg9PJy5YkAgX+85jbnG9gvb5xRXql4FRYeg\nM8hQDx8+nMLCQnJztbLudevWtVl91FpHIIDqeq9Ntfs6FZVeweT7DsB8Jpt+Pv2sqxwCLTcw8gE4\n8DnkNBwOdnVuGhpKXmkVS3ekNzpGStni6qIDmYWMWbiO5XtVMlrheOrLUE+fPr1BGeqxY8cyYMCA\n8/vnzJnDK6+8wuDBg0lNTW103Lx584iLiyM2NpYxY8aQkJDAvffeS0xMDEOGDCE2NpYHHngAs9l8\nkQz1pclivV7PokWLmDJlCnFxcUgpz+ckWotVMtRCiMeBW4GvanfdCHwqpWy6wNZOtFaGev1rm0g6\nUMrtd3ny/3RrWZu+lk23bEIIK3xa+Tn4dyL0GQ23fd4Kqzs35hoLv1myi03Hc/nn7ERuGBxy0fFq\ns4Wnlx/ky92Z/OuWRGYmhjRypgtYLJKbFm9hT3ohc8eE8+z1g+xlvqKToGSobYfNZaillH8HHgDK\na7ffOsoJtAXPYB8semfK0rWIoKiqqHnNoTrc/GDc7yF5NZzaal9DOyAGvY43bx/KyL5+PPblflYe\nPHP+WGF5NXe+t50vdmUS4uPKn77cz5aUvGbPuXzfafakF2LUC1JzlbidQuEomnQEtWsQeAkhvICj\nwDu127Hafc0ihLhaCHFMCJFSK0/R0JjZQogjQojDQojGuyjaiHdvrVS0OCP/vOaQVZVDdYz8LXj0\nhLXPavLUpgqt2WzNU7DiMbDYfv68I+HqpOfdu4aTGObDI5/uZe2RHE7mlXHjG1vYc6qQf8xOYMXv\nxtPX350HPtpN0pniRs9VWmXmpVVHSQjzYUZcEKlK5VShcBjNVQ0dRpObrps7qZtHErW/N7m6uRBC\nD7wOXAVkAjuFEN9KKY/UG9MfeAIYW9uj0LYWuSbw8NeaLkpySi7SHGq2cqgOJ3eY+Lj2pf/eNDiz\nH8yVWnmprNG0iQbMsJf5HQJ3ZwPv3z2cO97ZzoOf7MHVSY9OwCf3jWR4uLaa2ZLfjGDWG1uY+/4O\nvn5wLCE+rped57V1x8ktqeKtO4ayOSWPb/ZlUVZlxt25W62eqlB0CJqMCKSUYVLK3rU/636ve33e\nCQghBjRyihFAipTyhJSyGvgMmHnJmPuA12vXOEBKabdlsbKzkylzOkRpQRX+rv54OXm1LCIAGHIX\nBA+G8nwYejf8ehnMTwOf3to6x91gIRsvFyNLfjOC6F6eBHo6s/yhseedAECQtysf3D2C8uoa7npv\nB+fKqi96/8m8Mt775SQ3DQ1lcG9f+gV6AHAit6xd70OhUGjY6vFrKTCkgf0hQEa915lopaf1iQIQ\nQmwG9GgCd6svPZEQ4n60MlZ6924yEGmU3dvXUtMrl5Js7/OaQy12BHoj3L/+8v1jHoGVf4L0rdBn\nTKvs60z4uDmx/KGxAOh1lyfbo3t58vadw7jz3R2MfuknpgwM5PqEYCZFB/L890dwNuh5/OpoACID\nNEeQkltCXKh3+92EQqEAbKcX1JZSUgPQH5iEVpn0thDC59JBUsq3pJTDpJTDAgJap4AdFLaXIcNW\nUiR1yJqa8+Jz1lRONUvir8HNX4sKugl6nWjQCdQxKqIH/3toDHOGh7H9xDl++/Eehj7/I+uOnuWR\nKf0I9HQBoE8Pd/Q6QepZFREoFI7AVo6gsW/S00BYvdehtfvqkwl8K6U0SSlPAslojsHmuFT1xGAw\nUe5lwXz2LP18+lFcXWx95VBTOLlpyeTjP0D2obafr4swKNibv86MZfuTU/jwNyOYHhfEtEE9mTum\n7/kxTgYdffzc1LKYig5HR5Sh/vnnn0lMTDy/ubi4sHz58gbHWou9FUR3Av2FEH1rFUvnAN9eMmY5\nWjRA7RKYUYBtlaZqcXPWROOkXyWmM2fOVw5Z3VjWHMPvAaM7bP63bc7XhTDodUyICmDRzQm8eccw\nnAwX/9eLDPRQJaSKTkl7O4LJkyezb98+9u3bx7p163Bzc2Pq1Kltup6tHEFNQzullGbgYWANkAR8\nIaU8LIR4TghRp+a0BsgXQhwBfgbmSSlt8Ih+OX5+WqBh8C6jOjOLaN9oBIK9Z/fa5gJufjDsbjj0\nFRScss05uwmRAR6k5ZfZRcJCoWgJHV2Guj7Lli1j+vTp7SNDLYSIb2B3EZAhpbRIKYc3cBwAKeVK\nYOUl+56p97sE/li72ZWAkEFkZILRo4jS9BxCXXyJC4hjY8ZG/i/h/2xzkVEPwvY3YevrMOPvtjln\nN6BfoAemGkn6uXIiapPHiu5N4XepVGfZNm/kFOyOz3WRjR7vDDLU9fnss8/44x/b/tVpbUTwLrAb\n+BD4CNgFfAMcF0JMabMV7YSHfxhSCoxupRRlFQAwIWQCh/IPkVfRfCesVXiHQPwtsOdDKLPRObsB\ndSWkKk+gcCSdQYa6jjNnznDw4MHLlEtbg7Xlo2nAPVLKAwBCiDjgz8CTwDIgsc2WtAMGLzfMVW7o\n3UopzdPCtYlhE/nPvv+wKXMTN/a/0TYXGvsI7PsYdrwFk5+0zTm7OBEB7gCk5JbSttlORVehqSd3\nR9BRZKjr+OKLL7jxxhsxGo1tvDPrI4KBdU4AQEp5EIiRUtooy9o+CIOOmiovjK6llBZrKpnRvtEE\nugWyMXOj7S4UEA1R02HnO1BdbrvzdmG8XIz09HJWJaQKh9IZZKjr+PTTT7n11lttct/WOoKjQojX\nhBBja7dXa/c5A2abWNJeVNpV9eUAACAASURBVHljcC6jrEKTTRZCMDF0IluytlBdU938+61lzO+0\n7uP9n9runF2cyAAPUlTlkMKBdAYZatCqijIyMpg4caJN7ttaGWo34HfAuNpdm4HXgErAQ0pZZBNr\nrKS1MtQAqz6+HUOv7Zxd/Gtuefv3GHx92ZCxgYfXPcybV73JmGAbdQVLCW9fAZVF8PBObWEbRZM8\n880h/rfnNAeenWqdNLiiy6FkqG2HPWSoy6WUL0spr6vdFkopy6SUNe3tBNqKE4HodBbK/Wown9Gk\nlEcEjcBZ72zb6SEhtFzBuVQ41mmXd25XIgM8KKkyc7akytGmKBTdCqscgRBilBBiVa1UdHLdZm/j\n7IG7i7ZgisWnHFNWFgCuBldG9BrBhowNtpGbqGPAdeDTB7a8ZrtzdmFU5ZBjqDLXcMX/W88XOzOa\nH6zoklibI3gfeAO4Ehhfb+t0+Ppp8tNGn1KqTmed3z8xdCKZpZmcLD5pu4vpDTD6IcjYBhk7bHfe\nLkqdI1Adxu1LWl45J3LL+Ot3h8kqrHC0OQoHYK0jKJZSfielzJJS5tRtdrXMTgQEaskbJ/cSStIv\n1PlPCNU6/TZm2HB6CDQxOhcfFRVYQaCnMx7OBhURtDMn87RKrbLqGp755rBto2JFp8BaR7BOCPGS\nEGK4ECK+brOrZXbCwz8ES40eo3sZJTkXVtAK8ggiyjeKDZkbbHtBZw9NgyjpOzhnFwmlLoMQQmkO\nOYA6R/DIlP6sTcph9aFsB1ukaG+sdQTjard/oK049jrwH3sZZU8MPi6YqjwwuJVScq7yomMTQiew\n9+xeiqpsnP8ecb+2jsHW12173i5IZIC7igjamZN5pQR4OvPIFf0YFOzFM98epqjC5GizFO2ItVVD\n4xvYJtjbOHsgnPVYqjwxuJRRWnqxVt7E0InUyBq2ZG2x7UU9e0HCrbB7CeS3cCGcbka/QA9yiqso\nrlRfRO1FWl45fXu4Y9DrWDgrnvzSKl5efdTRZnUIOqIMNcDjjz/OoEGDGDhwII888kibp/OaW7z+\n1tqfjzS0tenKDkIIAVU+GF3KKLMYsFRcSI7F+cfh4+zDzxk/2/7Ck58EgzOsUZITTVG3WplatrL9\nOJFXRl9/TeIjLtSb34zty9Lt6ew4ec7BlnUO2tsRbNmyhc2bN3PgwAEOHTrEzp072bChbVPazUUE\nvrU/AxrZOiU6sy9OzhWUuRgxnbkwH6rX6ZkWPo0fT/1IVmlWE2doBZ69YMI8SF4Nx9fa9txdCFVC\n2r6UVJrIK62ib63WE8Afp0YR6uvKiyuTHGiZ4+joMtRCCCorK6murqaqqgqTyUTPnj3bdM9Nis5J\nKd+o/fnnNl2lg+FMIAhJpX81pqwsnCMurJZ1b9y9fH38a9468BbPjnnWthce9X+wZwmsXgB9t4DB\nybbn7wL09nPDqBcqYdxOpNWKL4b3uOAI3JwMXD2oFx9vP3VehsURrFq1iuxs2yaue/XqxfTp0xs9\n3hlkqEePHs3kyZMJCgpCSsnDDz/c5m5saxvK/IUQjwsh3hBCvFW3tenKDsTdRVs9U/qWYsq6eOXM\nXu69uDnqZpanLCej2MYNNgZnmPYS5B/XlEkVl2HU6+jTQyWM24sTedrnHFEvIgAI9XWl0mQhv8yG\n+ludgM4gQ52SkkJSUhKZmZmcPn2adevWsWnTpjbdt7Uy1N8A24BfaGQ1ss6Ej1ckZYDRq5zKzDOX\nHb837l6+Ov4Viw8s5oVxL9j24lHToN+VsOFliJ8NHoG2PX8XYGCQF5tT8qg01eBiVBpN9iQtrxwh\ntEisPqG+2uvMggr8PZwdYVqTT+6OoKPIUP/vf/9j1KhReHho06jTp09n69atjB/f+h5fa8tH3aWU\nj0kpl0opP6/bWn1VB+MfqHUXO7mXUFK7QE19AtwCmBM9h+9PfM+JIhvX/guhRQWmcvjpOdueu4sw\ne1go58qq+f7A5U5aYVtO5pUS7O16mcMN9XMFILOge8modwYZ6t69e7NhwwbMZjMmk4kNGza0z9QQ\nsEoI0WXWC/H074XZbMToUUZJI9Upv4n7Dc56ZxbvW2x7AwKiYORvYe/HsL/T+lO7Ma6fP/0CPXh/\n80nV5WpnTuaVXTYtBBDiU+cIupfkRGeQob7pppuIjIwkLi6OhIQEEhISuO6669p039bKUBcA3kA5\nUA0ItOWG/dp09VbSFhlqgJriKlatH4epzI0enyUy4Zt/Nzju33v+zTsH3+Gr678iyjeq1ddrkKoS\nWHoLnNoM4/4AVzwDOmv9ctfn422neHr5Ib787WiGhzvkv1mXR0pJwl9/YGZiCM/fEHvZ8cTnfuDa\n+CD+dkNcu9mkZKhth81lqAF/wIjmDAJqX3fe8lEPJ62pzLWc0iqBNDe8ts7cQXPxMHrwxr43bG+E\nsyfcsRyGzoVf/gmf3aY5h6aQEjYugtxOKfzaImYNCcHLxcAHm9McbUqX5VxZNcWV5vM9BJcS6uva\n7SKC7kpzDWX9a38d1MjWKRG6ek1lTm6Yc3MbHOft7M0dMXfwU/pPJBfY4cvX4ATX/gumvwLHf4B3\nroKCtMbHH1sF656HX/5he1s6GG5OBuaM6M3qw9lKEdNO1GkM9W1gaggg1MdNOYJuQnMRwYLan683\nsHVKraE69GY/jE5VlLuL8+sSNMRtA27DWe/M50ftNJcvBIy8H27/Ckqy4PM7wNJAYZaUsP4l7fej\nK8Hc9cv67hzdByklH2075WhTuiTnHUGPxiOC0wUVKk/TDWjSEUgp76n92WW0hupwQivbrParoCql\ncf0fHxcfrg6/mu9OfEdJdTNTN20hcjJc8w/IPgB7P7r8+LFV2rGYG6CqCE7aWCW1AxLq68bUmF58\nuiOdSlPLqpZNNRZue3sb64+dtZN1nZ+TeWUYdIJQX9cGj4f6ulJhquFcN+sl6I5YnZ0UQgwQQswS\nQtxWt9nTMHvj7hyq/eJTRnED3Xv1mTNgDhXmCr5L/a7JcW0m9lfQe4xWVlpRr6y1Lhrw7Qs3vAHO\nXnBkuX1t6SDMHRtOYbmJ5XtPNz+4HsdzStmSms+7v9hwoaEuxsm8Mnr7uWHQN/w1UL+XQNG1sbaz\n+GngLWAxMB34F3CTHe2yOz4emqyEwbOEkt37qc7MbHRsrH8ssT1i+fzY5/YNk4WA6S9rTmD9wgv7\nk1dr0cCEeeDkDtHT4egKqOn6Cp0j+/oxMMiLD7akteizP3RakxLfnJJHXqlaA7khTtYTm2uIC70E\nyhF0dayNCG4BJgNnpJR3AAlA4/+DOgG+fhFICU4eZVS6+FH0zTdNjr9lwC2cKDrBzuyd9jUsKB6G\n3g073oacIxdHA/G3aGNiZmrOIq1tbeWdASEEd48J52h2CXvSC61+36GsIgw6gUXCyoOqMe1SLBZJ\nWn7TjuBCL0H3aiqrT0eVoZ4/fz6xsbHExsby+edtz19a6wgqpJQ1gFkI4QlkA33afHUH4h0YgKna\nFSf3ckgcTdHyb5p84rw6/Gq8nb357Nhn9jfuiqe18tLV87XcwJn9WjSgr1UEibwCnDzgSNPOq6sw\nLbYXep3g56PWz/cfOl3EkN6+RPf05Lv9NlaS7QJkF1dSabIQ3oQj8HQx4u1qVBFBM7S3I1ixYgV7\n9uxh3759bN++nUWLFlFcXNzgWGux1hHsFUL4AO8Bu4AdtVuzCCGuFkIcE0KkCCEWNDHuV0IIKYS4\nrNnBHhh9XTFVeWB0K8ccPwZTRgYVu3c3Ot7F4MKN/W5kXfo6zpbbOQHp5qc5g5Mb4ZuHLo4GAIyu\nmmZR0vdQ03APRFfC29XIkN4+rE+27nM311g4cqaY2BBvrksIYmdaAadVCepFpNVWDEU04Qigrpeg\ne0UEHV2G+siRI0yYMAGDwYC7uzvx8fGsXr26TffcrOic0DRon5VSFgKvCyHWAF5Syj1WvFePVmp6\nFZAJ7BRCfCulPHLJOE/g98D2VtxDq9B7OWGp8sLomke+rgc+bm4ULl+O27DG/dDsqNksObyEZcnL\neDDxQfsaOPRu2P0B5ByCqX+7EA3UETMTDn0F6Vugb6cu4LKKSdGBvLLmGGdLKgn0dGly7Im8MipN\nFmJDvBjax5dFPySz4kAW90+IbCdrOz4nmukhqCPU19VhiwQlJz9PSalt10Tw9BhIVFTjqvqdQYY6\nISGBv/71rzz22GOUl5fz888/ExMT06bPpdmIQGrzJT/We51ijROoZQSQIqU8IaWsBj4DZjYw7nng\nZaCygWN2QeekhypvjC5lZJ04jedVUylZtfqiFcsuJcwrjLEhY1mWvAyTxc6JWr0BbnwTxv7+4mig\njn5XgsEVjnxrXzs6CBOjtEb2jcl5zY6tSxTHhnjTp4c7CaHefKumhy4iLa8MF6OOns041VBframs\nu/QSdAYZ6qlTpzJjxgzGjBnDrbfeyujRo9Hr26bSa60M9T4hxGAp5d4Wnj8EqC/qnwmMrD9ACDEE\nCJNSrhBCzGvsREKI+4H7QVPfswV6kx96g5nqyix0M2Zi+WY5JWt/wvu6axt9z5zoOTy87mF+Tv+Z\nqeF21uHrFattDeHkDv2vgqRvYfrfu7xO0aBgLwI8nVl/7Cw3DQ1tcuyh08W4GHXnpz2uSwjmbyuS\nOJFbSkTtUpjdnZN5ZYT3cEena3rRmfq9BD3aWY66qSd3R9BRZKgBnnrqKZ566ikAbrvtNqKi2qaF\n1pzERJ2jGIw2rXNMCLFHCLFXCGFtVNDU+XXAP4DHmhsrpXxLSjlMSjksIMA2MkdOaMu76VxTOeca\nhjE4mKLlTdfnjwsZR7B7MF+nfG0TG9pEzEwozYGMdptRcxhCCCZGBbDpeB7mGkuTYw+dLiImyOt8\nffy18cEIAd/tV9VDdTRXOlpHd+sl6Awy1DU1NeTn5wNw4MABDhw4wNSpbXsobe4xsi4hfD0QDcwA\nbkbrIbjZivOfBsLqvQ6t3VeHJxALrBdCpAGjgG/bK2Hs7hQCgMEjgzOpRXjfMJOyrVsx5eQ0+h69\nTs+MiBlsy9pGfkV+e5jZOFHTQO/cbaqHJkYFUFRhYn9m42WkFovkcFYRsSHe5/f18nZheLgf3+4/\n3W2mOJrCXGMh/Vy5lY6ge/USdAYZapPJxPjx44mJieH+++/n448/xmCwdnKnEaSUjW7A3qaON7eh\nTT2dAPoCTsB+YFAT49cDw5o779ChQ6UtOPDlWvnj2gi55B8T5PuP/yCr0tLkkegBMvfNt5p83/Fz\nx2XsB7Hy4yMf28SONvHpbVK+GCrlqa2OtsTuFJRVyb4LvpeL1hxtdEzq2RLZZ/738vMd6Rft/2hr\nmuwz/3t5+HSRvc3s8JzMLZV95n8vv9iZ3uzYoopq2Wf+93Lx+pR2sEzKI0eOtMt1ugMNfZbALtnA\nd2pzEUGAEOKPjW1WOBkz8DCwBkgCvpBSHhZCPCeEuL7pd9sff//elJX54hVUTvHZZKo8e+I6dChF\nX3/d5JNjP99+RPtGs/Lkyna0thGmvQjuAfDhTK3noAvj4+bE4N6+rD/WsFoswKEsrZ66fkQAML22\nF+G7AyppfF5szoqIwEv1EnQLmnMEesADbQqnoa1ZpJQrpZRRUspIKeULtfuekVJeVu4ipZwkpWz9\nijMtxKenH5UFobj3KMQiU8k6XojPzTdRnZZG+fam2yRmRMzgQO4B2y9w31J8+8A9P0BgDHz2a9jz\noWPtsTOTogI4eLqI3JKGZSMOnS7CSa+jf8+Lk8I9PJwZ28+fNYey28NMKo6dQzaTy3AUdY6gqWay\n+nTHXoLuRnOO4IyU8jkp5V8b2trFQjui93LGrWAgOr0Flx7JnD5+Dq+rr0bn7U3hF023bc/oOwOB\nYMXJFe1kbRO4+8Nd30HEJPj2d7DxFU2aogsyKVpTjd2Y3HBUcOh0EQOCPDE2IKQ2tLcvJ/PLqKhu\nmZJpSzGdLSf//cNUHGy+1NURZBVW4GLU0cPdyarx7b1ATVPRuMI6WvoZNucImq4t6+QYA10JKR6K\nlAK3PqVkHEpG5+KCzw03UPzjWsx5jf8h93LvxdCeQ1lxYkXH+I/r7AG3fqb1HKz7G7w3DdJ+cbRV\nNmdQsBf+Hk6sb8ARSCk5dLqIQcHeDbwTonp6ICWknC21q42WUk222ZzXMadTsosrCfJ2ResVbZ72\n7CVwcXEhPz+/Y/xNdVKklOTn5+Pi0nSPSH2aSzVPaZtJHRth1NM7IobM0h54BpWRvuco5cXV+Nwy\nm3NLllD49f/wv/++Rt8/I2IGz219jqRzScT0aFtnn00wOMENi6HPGFj/MnxwjaZLNOUZCB7saOts\ngk4nmBAVwLqjZ6mxSPT16uAzzlVQXGkmLqQRR9BLm808llNCXGjDY2yBpVyT/TAXdEzV0+yiSnp6\nWd8TEOLTfr0EoaGhZGZmktvIqoEK63BxcSE0tOl+m/o06QiklOfabFEHxy3WH7E3Eo+wnVjkCc6k\nFBI5JAK3ESMo/OILetx7D6KRZq2pfaby4vYXWXFiRcdwBKA1lg2dq0UGO9+FTf8P3poEExfA5Ccc\nbZ1NmBQdyNd7TrMvo5ChfXzP7z+UVddR7NXg+/r4ueGk13E8x44LDAGWilpHcK7dGuVbxJmiSkb0\n9bN6fP0SUns7AqPRSN++fe16DcXldO12VCtwGeCHb0E8Op0F59AzZCRpPQS+c27BlJlJ2eYtjb7X\n29mb8SHjWXVyFTUNLS/pSIyuMOZh+P1+GHidts5xSfskSu3NhP7+6ASsukRe+tBpTXo6qmfDdQwG\nvY7IQA+OtZMjqClo3BHkZZxi6Z//RGWZfaepLsVikZwtqaSXt/XTBt2tqaw70u0dgc7FQITvZKQU\nuIdVkXZgPwCeV16J3s+Pgs+blp2eETGD3IpcdubYeZ2C1uLiBVc9BxYzbPuvo62xCT5uTlwTH8w7\nv5zk/c0XViA7lFVMVE9PXIyN665E9fTgeI6dcwS1U0M1RVWNVg5lHUviTPJRck6k2NWWS8kvq8ZU\nI+nlZb0jCPFV6xJ0dbq9IwDwju2DqbgXXj2LKDhzlKpyE8LJCZ9fzaL05/VNdhpPCp2Eu9GdlSc6\nQE9BY/hFaGsd73oPKoscbY1NWHRzPNMG9eSv3x3hv+tTzyeKG5sWqiOqpyenCysoqbSfaKClovbc\nEmoKG84TVJRo/Q4FZ9q3ryGnWItSWhIReLsa8XIxqIigC6McAeAy0A+XggF4eOZT45LJmVTty9Jn\n9myoqaFw2bLG32twYUrvKaw8uZLX9r7GucoOmlYZ9yhUFWvOoAvgbNDzn9uGcH1CMC+vPspTyw9x\nrqz6skayS6mbNjpux8qhuogAwNzI9NAFR9CytZjbypmiWkfQgogAtOkhtaZD10U5AkDv4USIYSw6\nncQlrJQTe7Vw3SksDPdx4yj8chnS3PgCMI8MfoRxIeN468BbTFs2jYU7FnKmtIMJnAUlaBVEW98A\n0yVfTiXZ8MG1sO9Tx9jWSox6Hf+8JZGbh4aydHs6QKOlo3VE1zqC5Gz75QksFWZ0nlqNfk0jlUOO\ncgTZRdqXeVALIgJQTWVdHeUIaukZMQFp0eEVUkHqnl3n65h9b52DOTubkh9+aPy97j351+R/8c3M\nb5gaPpXPj37OjK9nsGDTAg7nN6xZ7hDG/QHKzsL+el/4pWdhyXXa+ser5kNZx2yCagy9TvDyr+K5\ne2w4vf3ciAlqemoo1NcVV6OeZDvmCSwVZoxB7qBrvHKozhEUZrfv1FB2cSUGnWhx9U93W5egu6Ec\nQS3usaFQHIaXXy5FxalkJhUA4DF5Mk4REeS9+VazfwQRPhG8MO4FVs5ayZwBc/g5/WfmfD+Hu1bd\nxdpTax1fWRQ+HkKGwuZ/g6VG+9Jfcj0UZcK1/4LqUvj5Bcfa2Ap0OsFfrhvEhnmTcHVqeoEOnU7Q\nv6cHyXasHLKUm9C7G9F7Ozc7NVSYk01NE9GmrTlTVEmgp/NF/RfWEOrrSnl1DQXldl6QSeEQlCOo\nxeDjjG/1EDw8z1HjUcTOFdqTvNDp6HH/fVQdO0ZpM4tF1BHkEcT8EfNZe/Na5g2bR055Dn9Y/wd+\nu/a3jn2iEgLGPgoFJ2H3+5oTKEiD2z6HYXfD8Hu15TGzDznOxjZgbads/0BPOzsCMzo3AwZfF2qa\niAh0ej3SYqHobOPFCM1RnZ5OZXKy1eOzi1pWOlpHXS9Bam77lrsq2gflCOrRs+cEhJC49zaTcXgz\nuRnal4X3NddgDAkhf/GbLfoi93Ty5M5Bd7LixhU8mPAg285sY+/Zli7yZmMGXAs9+sOKxyA/BW79\n9MKax5MWgIs3rHmiy2oVAUT38uBsSRWF5dU2P7eskciqGnSuBvS+Lo12F1eWlBAYHgG0LU+Q8+JL\nZM173OrxdfISLSWxtw/erkYeX3aAc2W2/9wUjkU5gnoExIxH1hjwDDyHyZLE3h/TABBGIz3uvYeK\n/fsp397y1cD0Oj13DboLb2dvPjziYHVQnU7rMHb2gjlLIXLyhWNufjDpSTi5EY514HLYNtK/LmFs\nhzyBpVKb5tG5GjD4uWApqUaaLp4SrDGbqSovIzhqINC2PIEpK4vqtDSrHlCklLXyEi2PCAI9XXj3\nrmFkFVbwmw92Ul7dftNZCvujHEE9nHv64lLWHz+/05jc9SRv2UVJbWjvPWsWhoAA8ha/2apzuxnd\nmB01m3Xp60gvTrel2S0n9lcwPw36X3n5sWG/gYABsOYpMHdMrZy2Ulc5ZI8OY0vtHLrOzYjeT/vC\nvTQqqCzVrusbHIqLh2ebIgJzbi6yqgqzFdo8JVVmyqtrWlwxVMewcD9evXUwBzIL+d3Svc0uGaro\nPChHcAlBvjNwcyvGvbczpqoD7F+nrTegc3bG7+67Kd+2jYp9+1p17lsH3Ipep+fjpI9taXLr0DWS\nVNUbYNoLWh5h++L2takFZJZkUljZ+JKVTRHk7YKns8EumkN18hLCzYDBV6vMuTRhXFGs9am4enrh\n2yu41Y7AUl1NTYFW1GDKzGx2fHZRy5vJLmXaoF48NzOWn46e5an/HVJVRF0E5QguIXTwbKRFj09o\nOmbLKQ6tT6aq9inP95bZ6Ly9SVn8OZVlLa+eCHALYEbfGSxPWU5RVQfu8O13JfSfBusXQvZBR1vT\nIA/+9CAv73y5Ve8VQqscOmaHXoK6ZjKdq5Yshss1h+oqhlw9PfENCuZcK7uLa+pFAaaM5hdIOmMD\nRwBw+6g+/O6Kfny+K4N3Np1s/g2KDo9yBJfg7OWPe9VwAgNOgrcfVSUHOLwpi+oKM4d3FrBj1DNs\n5Cp++m/rtIXujLmTCnMFXyZ/aWPLbcx1/wYXH/j0VijtWJLAUkoySjI4lNf66qaonlrlkK2faGXF\nBUeg83QCg8B87uKpoQuOwAvfoBBK8/MwVbVcqbT+dFB1RvMRQU4ru4ob4o9XRZEQ5sOqQx2scVLR\nKpQjaIDw/rdhdKqixwADyMPsWnmC9xdsZuNnyTgH+NGj8ChpKRWUNqEu2RjRftGMChrFp0mfYqrp\nwDXZXkFw61Kt1+Dz2ztUvqCwqhCzxUx6SToV5tbJHkT19KSg3EReqW0rYOrnCIROYPBxaSAi0CIR\nV08vfINDACjMbvkXquns2Qu/tyAiaE2y+FKEEMQGe5FytlRND3UBlCNogJ79piJN7rj3TMZkLkXW\nnKLf0EBuWjCM2U+PYvRYV5CSvZ9sa9X57xp0F2crzrI6bbWNLbcxwYPhhjcgYxt892iHKSnNrdCe\nhC3SQmphaqvOcV5zyMZ5grocgc5FW+pD7+t8WXdxXUTg4umFT69goHUlpOaz2ufgFBFBtTU5guIK\n/D2ccDLY5s++X6AHxZVmcks7zkOConUoR9AAOp0RL6cr8OuRgV9YFD37ZDDlzoH0DNfkC8Ie+DUB\nFSc4tq8Yc3nLo4KxwWOJ9I5kyeElHf9pKnaWtqjN/qWw5TVHWwNAbvmFKZHkAuubqeoT1Utb3N7W\nlUOWcjPCWY/Qa81tBr+GIoJinFxdMRiN+AbVOYKW5wnMubmg1+MaH48pvflKtNY2kzVGv0DtM7T3\n0p8K+6McQSNEJ96NTmfBNaKAE7t3XhS665yciJ8eRZXBg4OvL2/xuYUQ3DnoTo4VHGNdxjpbmm0f\nJs7XZKx/fAbSW95HYWvOll+YEjl27lirzhHg4YyPm9HmvQSWCq2ruA69rwuWcjOWqgt19xUlxbh6\nag8VTi6uePj6tTIiOIshIACn8D6Yc3OxVDQ9TXamqNIm+YE66hxBam6Zzc6pcAzKETSCl088lqpA\nXAIP4u3Rix/fef2ip/foWybiRilH9pZcNFdrLddFXkc/n368tP0lykwd/A9Jp9OmiJy9NAkKB5NX\noQnjDfAb0OqIQAhxPmFsSzRHYDz/2lDXS1AvYVxRUoyLxwVxPN+gkNZFBLWOwBgaBoDpdNPOJKfY\nthFBLy8XPJwNpKqIoNOjHEEjCCHw978OL+9ceifEk35wH4c3/HThuE4QO7k3hV6RpP695fX2Rp2R\nZ8c8y9nys7y2t2NMuTSJkzvEXAdJ34HJsbr0Z8vP4uXkRZx/HMcKjrV6ei2qVnzOltNzlnITOtcL\nEUFDJaQVxcW4el3qCFoREeTmYggMxClMW6S8uomEcaVJE4xrjbxEYwghiAxwV1NDXQDlCJogetBd\nSAml7tvoHR3Hhg/foayw4Pzx2Gti0AkLR4/VUL6n5RpCCQEJzBkwh6VJSzmY2zHr9S8ibjZUl8Cx\nVQ41I7cilwDXAKJ8oyipLiGnvHWibdE9PSmpNJNdbLtF5i0V5oscgb6uqaxewriy9MLUEIBPUDAV\nJcVUlLYsOjGfPYshMABjWG1E0EQJabYNK4bqExnooRxBF0A5giZwcwuhxtQPp54HGTX8JkxVlax7\n78LTv6uHE/2GBJLdaxSZz7+EpbzlC3c8MvgRAtwCeHbrs5gsHbicFCB8HHgGwYEvHGpGbkUuAW4B\nRPtFA63PE9RVDh21h7Py0AAAIABJREFUYWPZpTkCnbsR4aS7OCIoudgR+AbVlpC2YHrIUl1NTWEh\nhoAA9L6+6NzcqM5sPCKoc3atlZdojMgAD7KLK+269KfC/ihH0Awhobfg4lrKMae7GHy7M1mZa0je\n8cv543FT+lCjdya92IeM+x/AUtay+X4PJw+eGvkUyQXJLDm8xNbm2xadHuJugpQfodxxS3LmlucS\n6BZIP59+QOsrhwaFeCME7M9onVTFpUgpNQlq1ws5AiGEpkJaGxGYTSaqKyoucQS1lUMtEJ+r6yo2\nBgYihMAYFmZVRGDLHAGohHFXwe6OQAhxtRDimBAiRQixoIHjfxRCHBFCHBBC/CSE6GNvm1rCoEF3\nk595C3nngrA4H6bftRmk5d9DZvrXAPTs64V/mAdZibPJS8ok/b77qSlt2R/FFb2v4MreV7J4/2LH\nC9LVUlpdSlpR2uUH4maDxQyH/3f5MVMFpP5s134DKSW5Fbn4u/rj6eRJiEcIxwpaFxF4OBuI7unJ\n3nQbOYLqGrDIiyIC0PIEdUtWVtaTl6jDp2cvhNC1KE9QV6BgCAwEwBgWiqmJiKC1axU3hyoh7RrY\n1REIIfTA68B0IAa4VQgRc8mwvcAwKWU8sAz4uz1tailCCCZPe5Rjx8aRu+dRwgL+QnWJgaNH/0JN\nTSVCCEZcF0F5tYHtw59mV2U8Sff9kZqSlk03PDHyCYw6I/M3zqfc5Pi1YZ/b+hy3rbjt8umqXnGa\nOunBSyQypIRvfwcf3WDXHEJdV3Ggm/YFGOUb1eqIAGBwbx/2ZRRisbTdedXXGaqPwc8Fc0ElUsqL\n5CXq0BuMeAf2pCDLekdQ10xmCAgAwCk0jOqMzEYT3znFlXi6GHB3NjR4vLX08XPDqBfKEXRy7B0R\njABSpJQnpJTVwGfAzPoDpJQ/Synrvvm2AaF2tqnFBPQMZHBIDEdKMnHVj8eDmxGGcg7tXARA33h/\n7vjbaBKu6kNe0DA2eN3MN498Sk5SttXXCHQL5MVxL3Lk3BHmb5yP2WIbvXcpJatOrmJ3zm6r33Om\n9Aw/nPqBElMJh/MuWXNZCIi7GdK3QmG96GXPh5pzEHrY+bZNbG+Iuh4Cf1d/QHMEp4pPUWluXcJ3\ncJgvRRUmTua3fWrDUtGwI9D7uiCrarCUmxt0BKAljFtSQlqnM3Q+IugdhqysbFSO+kxRhc3zAwAG\nvY7wHu5q5bJOjr0dQQhQP17NrN3XGPcADT5OCiHuF0LsEkLsyrVCe93WXHHjVJwxsnrFKsbOfILK\nfB+y8z+holSbK3f3dmbsr/px18LxxA7UcdYQyrJ/H+F/r+zi5P5cpBVPnJN7T2bBiAWsz1zPwh0L\n21zWWF1TzbNbn+XxjY/z4vYXrX7fp8c+RaJde1fOrssHxN2s/ayLCnIOw6rHIWISTJgHqesgL6VN\ntjdGXQ9BXUQQ7RfdJqmJwb19AGwyPXQ+Irh0ashPqxyqKai8SGeoPr5BwRRkZ1n9b24++//ZO/Pw\nqMq7/X/O7JNkMpkkk0z2jSwkYQkkgMjiAqLihiKi4tK6Vm1tq+3Ptlq1b1+12tZqa90XFNyoiCio\ngAICCUsIkJCdLGRPJttk9u2c3x8DgZAFRaz1Lfd1ealnnudsmXm+z3e77y5QKJAbDACojlYOjUI1\n0THgPu0VQ0cxLirkTC/BDxz/McliQRCWAfnAUyN9LknSS5Ik5UuSlG884g7/OxESqees2Mm02ruo\nqaghI+vXKLQetn8yVCZQq1Mx9+fnsfgyBWn1a+g91M7658tY+fBODu09eePZtVnX8qOcH/Fe9Xu8\ndvC1U77fHmcPt264ldW1q8k0ZFLTVzO4iI4Fh9fBBzUfMC9xHuPCxrGnYwSWVUMSJMwIVA+5bfD+\nTQGJyytfDmgfyxRQfOr3PhaOegRGbeA7kGHIAE49YZxmDEGnVrCvqe/kg08C0XmMcO54yA1HBWpc\nxzyCUP2QMYaYOLwu55Dy5LHg6+pCERmJIAv8hJXxAUd6NPK5ju/II4CAITjc68DjOyNU80PFd20I\nWoGE4/4//sixIRAEYR7wO+AySZL+YxmsZiycjUEMZsPnG0jKXITgicen3UbDvuG0C5GXXsCM2+cw\n46sHyFftRaGSsfG1cnrbTx6C+PnUn3NR8kX8reRvrKtf943vs6q3iqXrllLZU8lTc57i0ZmPArCr\n/eT0EJ/Uf8KAZ4Absm+gwFTAvq59I5e1TrwazFWw8mrorYOrXoGQKNCZYPxlsH8FeE5/ruMo4Zwx\nKGAIEnQJaBXaU04Yy2QCkxPDTqtHIIyQIwDw97pxWgM6FJoQ3ZAxR0tIv27C+Ggz2VEo4+JAEEak\no/b5RcxW92lPFB/FuKgQ/KJE42kIr53B94Pv2hDsAdIFQUgRBEEFLAXWHj9AEIQ84EUCRuCbczX8\nG6GJD2VWZB4Wl5WiwiJy836PSudj58aHcTuG/wjCFi/GdP99hG54jemeTSjVcra+ffJOWJkg44+z\n/sjU6Kk8uOPBEY2Bz+Oh4qsvefuh+/nwT48OHq/rr+PGT29EkiSWX7ScC1MuJCs8C71az872sdlS\nRUlkReUKciJymGScRIGpAKfPOTxPAJBzZWDn31QY4CJKmYPb3UlV9e/x598ILsvwhPJpgNlhJlQV\niloeCLfIBBnpYenfLmGcEEZVx8C31uEdLUcg0ygQtIpBj0ClDUKuGDrG8A1ZSI82kw1eQ6VCYTKN\n6BGYbW5ECUynsav4eKQZz1QO/dDxnRoCSZJ8wD3A50Al8L4kSeWCIPxBEITLjgx7CggBVgmCsF8Q\nhLWjnO4/AtnnTSbZb+Srr74CYQIaVQb6jEa2vDmylnHELT8m4vbbca1aQW5oI221/VQVnZx7XiVX\n8ex5zzLZOJkHtj0w2GMwYO7iq7ff4KW7bubT5/5K9+FG6kv24Dgif/jn4j+jkCl4e+HbZEcECrTk\nMjnTTNMoaisa0wgVthXSYGlgWfYyBEEgPzofYOTwUFA4TP1RgIxuzq8A6OxaT2vrSsxaG0TlBJLG\np7mU1Ow0D+YHjiIjPIPq3lOnmshLNCBKUNry7VTjRKcPFAKCcvjPShGuwdftHEYvcRS6yEjkSiW9\nrSfXFYBjPEPHQxUfPyId9dHS0e8qNJRqDAbOGIIfMr7zHIEkSeslScqQJClNkqT/PXLs95IkrT3y\n3/MkSYqWJGnykX8uG/uM3y802RHM0U9G4Rf48K1VjBv3S9ShXlpbV7Pj/ZUjLkbGX/ycsKsXE/ru\nExjD/ez44BBO68kFUUJVobww/wXmJ83nz8V/5skdj/PWA/dSvHY1cVk5LH7wj1z5m0cAaKuupLCt\nkO2t27lj4h3DFssZMTPodHTSONA46vVWVKzAqDWyIGkBAAaNgXRD+siGAGDhn2HJ8kH9Y5u1AgBz\n90aYdmtA5rJ590mf85vA7DAPVgwdRYYhgwHPwClTTUxOCCSM93/LxjLpSDOZIAjDPlOnheGut+Cz\nuIYligFkMjkx6Zk0l59cdU30ePBbLCijhv6NA01lww1J53dEL3EUQSoFcWHaM4bgB4z/mGTxDwWC\nTCD5rmnMjc2n3WrmwFv9BGuySDzbwc7Vb7PjvRXDjIEgCJgeeoigqVNI3fwXPE4fn63cyUsvvURP\nT8+Y11PL1Tw15ymuy7qOL4pW47JZWfjL/8fl9/+OpAmTMY3LRK5U0lx5kL8U/4W4kDiuzbp22HnO\nij0LYNTwUF1/HTvadrA0aylK+bFkZ0F0AfvN+7+WmprVVglAT89WxJxFAbbS01xKetQj6P9gNfai\nIgAyDQGqiVMNDxmCVSRHBH3rhPGJhHPHI7ggGkSJMHvEiIYAIHniFLoa606aMB7sITjBEKgS4vF1\ndSG6hpbSftceAQTyBGcMwQ8XZwzBKUAWpOSs2xeQZkpml70S5e5zEFQWJl2WwK4P32P7O8cEZ0S/\nn5pdO/jw2V9SMcuAXuUirm87Za3baWtrY/fuk++Y5TI5D0x7gEuVsxEFiae73xiUaFQolZjS0ik/\nsIOavhp+PuXnqOSqYedI0CUQFxJHUVsRfX272bnrIny+Y01vKytXoparWZyxeMi8wTxBzwh5guMg\nim7s9lpCgjPx+230OQ7C5OugfA3Yxkj9iCLseRX6Tx4SESURs9NMnCeE9ocfpuXen+Pt7CLdkA6c\nuiGAQHiopKn/W5XsnsgzdDyUxiBUyaGYxES0IaMYgklTAGgq2z/mdXxHu4pPCA0pExKB4XTUHQMu\n1AoZYSdUM51OjIsKob7bdloa887g348zhuAUIQgCl193JQq1kr1ODZr+VJRhe5k/+SZqPt3K5uUv\nU/TBO7x8z49Z948/EpS1EWXap/h/fT0d+nZEmRe1FMqBAwfwek++2xYEgXCzgDY+mqLu3dy58U6s\nnsBCHpWegbO1i8n6CSxIXjDqOWbEzGBPxx7aO9Zgt9fQ1xfwDjrtnaytW8slqZcQrgkfMmdq9FRg\nlDzBcbDZa5EkH4mJtyCTaTF3b4KCW0H0QvHro08sXw3rfgnLLx00GD2vvEL7Q78fNvRoV3H6rlbw\n+ZBcLtp//xAhyhBig2NPmXwOAv0EZqubNsupM5GeyDx6IoILTATL9Bjk0SN+HpWcilYXSuOBkjGv\nc2Iz2VEM0lGfoFZ2VJlspJDV6cK4qBBcXpHW/u+XovybQJKkk25w/ltwxhB8C4SGhnLhhRfS5u7B\nrliIT9NLfcQGbCk5bG9oZ9uOvYTFJTLz9iSUQSKSJKPW8xrtpigyqlrQ9CXicrnYu+vASa/lcTnp\nrD/EpKlz+dOcP1FqLuXWDbfS7+qnQtuKTBL4UeRVY/7Yz4o9C5vXRmfPFgD6+wOL+/MHnscv+bl1\nwq3D5pw0T3AER/MDev0UIsJn0d29CSliHGReHJC4tI3QBOj3webHICwJrB2wcjG4rVjWr6d/1Spc\nFRVDhpsdZpAkor8oQztlClH3349961dYVn9IRvi3pJpICDRmjRQeEqWvVx8vOnzDegiOhzJTh0d0\nE26PHPFzQSYjaWIejaX7xvRMfCfwDA2efxQ66o7TrEw2EgY5h35AHcafNnzK0k+WcsB88t/f/3Wc\nMQTfEpMnTyYtLY3Ckn76+kxokgoJi9aShglnqI6+SDc27yYS4m9EcF5MkL6LnPG9nJudQH7xO8j9\nar78bDuHy8fOFbRVV4LMg9/0dyaoLDxz3jMc6jvEzZ/dzDvW9UiAtmPsBPR003Qi5BKiJ5BU7evf\nRX1/PR8e+pClmUuJ143M7vF18gRWWwVyeQhabRJG43zc7g6s1oMw/w/gc8KWETqbS98N9CBc+Dgs\neRM6DiK9fR2eunoAel4d2pRmdprJbAFlSxdhixdjWHY9Qfn5dD7+OBPEGBoHGk9Z7S0rRodaIRvW\nT7C1eStT35rKFWuu4LfbfsvKypXs69qHX/QPO4foHD1HAOB2O2iylaPt1Q6Wmp6IpIl5OCz9mA83\njHoeX1cXKJXIw8KGHB+Njrq1/7trJjuKcUdKSE/WYez1WrDbT60L/HTj04YAicHu9tNb0PBDxBlD\n8C0hCAKLFy/m2muv5awZT6JUuph9sZ/Ft1zLJf484uO24HZrqavLpbwhEUufkTDDJgz330ra7HTS\nq8vxKPpZ83wRxZ82jroTbK4oIyzVjl/qo6XlLebEz+H5ec/TZm/DIfMQFh9Ha3XFiHOPIkwTxpyI\nwC4yKupirNYKniv5M1qFltsn3j7qvKN5goM9o1e0WK0VhIRkIQgyIiLOBWSB8FBkOuT/OCBx2VV5\nbILPA1v+BLF5Aa8h4wK4/Dm85TuQ3G4UJhMDn302pBzS7DBz/gERgoMIvXABgkxGzOOPIYkiM5fv\nRxT9bG3eOuY7GA1KuYyJ8fphHsHO9p3IZXLidfEUtRfxxO4nuPHTG3m5bGgSXPKJSB5xTEPgtA5Q\nby1FEAUc+0fOmyRPzAPgcOnoQkc+s3lIV/FRjERHbXf7aO13Dtb6f1cwBKsID1aNmTCWJImyg/ew\nt+QapK/pZX1XsHqs7GjbAUBJ19ihuP8GnDEEpwFarZbMzEzi4s7GaFxAU9OrCCYfoQtq0Qb34O9e\nxK5dBxiw2gjyLAbBS9nBB4l54nFmzJqGIIoo1NXs+qieHR+MzNHTXFFG9MTAjt9mq8Rqq2JazDRW\nXrySf877J8nZk2mrqUL0D9+pHo9JISosfoHIqEWASLN5Cz/O/TEGjWHUOSfLE0iSiM1WhU43HgCV\nKpywsHy6uzcFBsx9AFQ62PDQsUn73gJLE5z7YIDIDmDytbhTbgIg+oI4kAn0vnFMo6G3p4WzKiV0\nCy9GFhQUuFZCAtG/uh/5njKuqAhhw+ENYz7/WMhLNHCwbWAIVUJVbxWZ4Zn84/x/sHnJZr64+gsy\nDZkUtRUNmTvYTDZKshgChqDP0wnhcux7RiYkDAmPIDIhacw8wYnNZMfjRDrq2iMLc4ZJN+L404lx\nxrErh3p6t9LXV4jX24fdXvud389Y2NK8Ba/oJdOQyf6u/SN6eP9NOGMITjPSUn+J3++guuYRWnyv\no/fOIK98PtfOXcR1113HuZf/hJ6yGAbsW+np2Uzyz35GSlgYA0ozsfZSSr9sxmIeSs3gdbnoba9A\nbTATH3cDgqCgoyOgB5BuSOes2LOIz8rG63LScmgbRTvnMWA9tnuv3tnOro/r6W23EiZ2UOOSUePy\nI0oCE4LVLBu/bMxnMmgMZBgyRjUETudh/H473X4t579/Pk0DTRgj52GzVeF0NkNwBMz9VUDQ5tAX\nAd2Cr54K8BWNO3/IudzKQBNcsO0T9NPG0f/BB/j6Art0zZd7UPsg4uprhswJu+Yags6awdUb7JTU\nbTvl8FBeQhgen0hle4APSJIkqnurGR8+fnBMVFAUM+NmUtpdOli5BV/fEAAoJ+jxttnxtI68aCZN\nmkJrVTle98iJa5+5a1gPwVGcSEdd3RG4Zta/wRCkRYVwyGwb0asVRR+HDj2BUhkoRrBYvrm06+nE\nhsYNRAdFc1POTdi8Nmr7v1/D9H3jjCE4zQgOHkeMaRFdXesQBIGcs/+EKkFHyGY7SboY1EHBxMff\ngrNXTUXF7/D5rMy45BLcGg0aayGCz8OOF3cMOWdrTSX61D4QICHhZiIizqGjYy2SdGwXE5eVA8Dh\npn/icDTQ1PQqAH6vyNZ3ayhe18jqZz4AvxV3x0Se3/kmhz0CBWFhBCmDTvpcBaYC9neNnCewHkkU\nr2kqocvZxasHXyUych5AIDwEMO12MCTDhgdhzytgbYfzjvMGjsBTV4ciOhr5pEsJ132F5HTS9847\nACRsqaYtRo0mN2fIHEEmI+qX96F0+Zh60HXK4aG8xKEJ41ZbK1avdVASc/BdRBfgE31Dkoyi4wjh\nnHb0ZPFRQxA8xQQK2aheQfLEPPw+Hy0VI4fifF3mYaWjR6FMiEdyufB3BwgGqzqsaJVyEgwn/xt/\nW6RHhdDv8NI5MJwurL3jA+z2WjIz/4BSGY7F8v2FY46GheYnzR/0dks6/7vDQ2cMwXeAlJR7Uaki\nGZf2ANqQeMKXjUdQyTC/VIp1WytT5l9Be1EyHq+ZyqrfkpaWRmhoKOaF80n2lNPQLFB1///gtwV2\njM0VpRgyLOhCJhIUlIzJdAUeTxe9vYWD19RFRBKerMMtK0Gh0NHV9RkeTzfNlb14XX7Ou3E8OfMD\ni4Op4lrO2rwMmyMJlbcNv//k5HAF0QW4/K4RKyystgoQFGzsKCdCE8HaurUMSGqCg9MxmzcGBinU\nMO9R6KqAjQ9DyhxImT3sXO66OtRpaXDlS2gmTCUk1kPf8tdxlOzD2DRA9cy4ESujNLk5qNLTmX9Q\nfsrhIZNeQ4xew57DAUNwtBw1y5A1ZFxeVB5yQU5xxzGK7tF4ho6HcyBgCIIiwwiaEIljXxeiZ3hI\nIm58DgqlisYR8gSi243fYhlWMXQU6pSUwLXKAkakptNKRnQIMtk3Kx31dtpxNw18ozkzUiMA2H5o\nKMut3++gvv5v6EPziDJeiF4/BcvA97fwHg0LLUheQGxILKZg0399nuCMIfgOoNXGMevsQuLjrwdA\noVdjvHUCypgQLOvqsTxfTX7SLbTvjqKraz1t7SvJy8ujvrWVvEevQymXONAYQsPlV+AoKaHj8Ha0\n4W5iY68CIDLiPBSKUDo61gy5btz0ASQ/TMh9Hkny0Nb2L+r2daHSKsiYFo02opygoDTkV9qxaMwY\nKi9Bknxfy02fFjMNnVLHy2UvD3P9rdZyrISgUgTx4vwXQYI3yt/AGDkPi6UYr/dIJU725YFwkOQP\n5AZOgCSKuOvrUY1LA6UWlr5DxLQQ/BYbrff+FK8CzLNPFLgLQBAEwhYtIqXFS/2Br045PHRuvJy9\nlQ3Y3T6q+qqQCTLGGcYNGROiCmF8+PghobLRtAiOh9M6gDo4GJlcTvBZMUhuP7btw0nmlCo1ceNz\nRkwYD/YQGEc2BEH5+cgNBixrAt+N6g4rmd8wLGTf20nn3/fR/Vo5kv/rJ3XHx+gw6tRsrRlaKny4\n6VU8ni7Gpf8GQRDQ66fgcDTg8fx7da9tO1qxbGgcDAtNNE4EAoa9pLPkW+t//JBxxhB8RwiodB6D\n0hSM8dYJRN42AXmYGmNbFNP6f4u9KZyamj+SkRFYQIrL9pK3MI3uyEn0a+Oou+lm/OoDIMmIjl4I\ngFyuJirqYrrMn+PzBRY8p7MZub6GnsowBHcihrAZtLS+TcOBLlImRiLIvPT17yE8fCbXTr+KvKt1\n4MpFkgQ62wo5GTSouKH7bPbV72Jry7HQiyRJDFgPUmWzc8W4K8gMz2Rh6kI+qPkApW4akuSnu3vz\n0ZcCi18LlIomTh92DV97O5LDgTrtyMIbHIH2V6vRGEV85h52Z8rQG0fXNdJfdimSXMbM/acWHvL7\n/YS07mKacIgNFR1U9VSREpqCVjGctbPAVEBZd9lgnuDreAQum3WQXkKdGIomOwLr1hb8tuFlv8mT\nptDT0sRA99BF9Ri9xMihIUGlQn/ZpVg3b6arpYNum4dM08idzCdC8ov0r62jb1UNcp0KyeXD3fj1\nvQJBEJibYWRbrRn/kQ5jt9tMU9NLGI0LCNMHwjD60EBl1MDA2B3UpxOSJDGwpQXrl8201zUyP2k+\nMkGG5PEwNWoqZqeZFtvIoj7/DThjCP7N0KSFYfzJJCJuziFEG864ul/hGZBRXXkv+fnjKS4uptG+\nF7VORuvcu3Fk5hKW1o9WykWpPFbZE2NahCg6MZs/B6Dx8PMIMjmd+yNoraogLn4ZbncrCv0+0qYY\nsVj2I4pODGEzKVz+Oof+8RpnXxGHx5JEY81WnCMsRsdj5wfv4thRyczWeJ7a8xQef2C8x9OFz9tH\ns0cYTDrfMuEW3H43HzaXoFab6DIfJzqnjwt4BiPAXReoL1ePSxs8JkSkEXn3vYDE53nCoA7BSFBE\nRqKbO5e55QIb6z8b83lGQmNjIx6Xk3CZk092VlLVV0WuJhXLunXDdov5pny8opdScylwJEcggKAZ\n2yM4nmdIf2EyktfPwBdNw8YOlpGWDfUKRmsmOx76K68Cr5fm9wNeQWb0yT0Cv82D+ZUybIVthMyK\nI/pnU0Au4Kr8Zrv2uRlG+h1eSlsCXmBDwzOIoodxab8aHBMaOgFBUPxb8wTedjviEaLH6zovZkHy\nAhx791KdX0DekQa///g8gb0HXrsIDp984/ZNccYQfA8QBAFtVjjhSzIJ9cZgrL8DvzSAWnqRc8+Z\nS3lFOfbogxg7+9FPn4Uy2I/2kx781mPcQHr9VLSaRNo7PsTpbKW9fTWxMdegkIXTWl2BMXIe+MMJ\nT/+KhOxwevsKARnN+yyUbvoMv99PXfFGYhLORqGrZd0/i/G6Ry6hMzc1UvzxamRyBSktWlr6m1hZ\nuRKAHktgVxdlmEpiaIDrJkWfwvyk+bxb/R6GyAvo6dn6tcIA7kMBQ6BKTR1yXLf0LoSfxlGVIGBU\nh4809dh7WbSIMKuIZds3Dw8dPHgQlUoFgoClrYoOewdzCgdou+9+HLuHVkxNiZqCTJANhodEpw9B\no0AYIxbvHBhqCJRRQQQXmLDv6sDXPZSaISIhiRBDOI0Hvrkh0GRmoMnJQfr0Y4CThoYkr5+ufx7A\n02wj/JpMwi5JRaZVoE4Lw1X1zQzBrHGRyATYWmPGbq+nrf194uKuIygokLtoq6mkeO3HhARn0f9v\nNASu6kDeZ09iDdPsuWTak7F+8SWSx0PYtjJCVaH/+XmChq0B/Y8RuMS+Lc4Ygu8R2uwIQs6OxdSV\nT7D5CmTBh7G2P8L82WlY7X1sD95LV8wWBK8W9ZZeWu+7D+lIn4AgCJhMV9DXV0RN7R8ASE6+g7is\nbFqrykGS018/m6DoMry+Vvp6d6BWpLH1jbeImFSAK6eAsuLdGKOmIpP7sFpL2fVR/bB7lESRjS//\nA3VQMBf+5F58dicL/Pm8WPoi3c5u9jYFdp0Ls+4YMu+2ibdh89rYY5cjST46u06utOY+dAh5ZCQK\nw/Cehu4pFwMQZR5bC1k3dy6SXsesA55vFB7y+XxUVlaSlZVFfFIqKfJekCDmQCCG3//eu0PGh6hC\nyA7PHtR0Hotw7igCHsFQicrQeUkICgHL541DjguCQNLEKRwuLcFpO7YB8JnNI3YVnwj9lYsIbq4n\nz91JZMjYC4erth9/r4uIazMJygsYGEmS0IwLxdftxGv++kpzhmAVkxLC2Fpjpr7haWQyNSnJdw9+\nvuP9lWx/903MNW4GBg4git9ODOjrwlXdiyxGy5PBL+NUe7B+fhjHroBin3XTpsE8wX806reAWg8x\nk0/7qc8Ygu8Z+otSUMaHkFiziBD/xagj2nAJjzA7uxBDdAOu6P1oO6cScdtj2L/aRtdTfx6cazJd\nAUh0d28iNmYxGk0scZnZ9LW3UVtcR3fVTARBxuGml7EMlNK6z0Z4bDyy+BR8ooTNlEx1YaCEMSGv\nnYodbbgdQ8tbhhvDAAAgAElEQVRDS7/4nPaaKubecAuZZ89BF2FkYlskbr+bp/c+TWtPERZRzdSY\nWUPmZYVnMTtuNq/VbCAoOGNYYnskuOsOBSqGRoA5IhkAY8XHY55DUKkIv+xy8msltlZ8ctJrHkVd\nXR0ul4vc3Fxm5OcRjERyXySK6kZkej0DGzfh6x5aDZNvyqfUXIrL5wrwDI2RH4CAIdDohu7O5ToV\nIbPjcZZ1D6vSmXLxZXhdbja9/NxgaCogSBN5UgI5/cKF+GQKLm/fd9KxzvIeBI0cTWbA25JEkeY7\n78T8zO8ATik81NdfSlfXehISfoxKFQi9eN0uWqvKMaWl01nlQhRddDTtOMnZvj1Epw9P0wBt0X3Y\nsOM/W4encQCvGRQmE55Ddcz0JdM40EiPc2yql+8V9VsgeRbIx/6enQrOGILvGYJCRsS1WQiSQHLN\nLWQ3vkZkzWI8mhZSs75ALvdT3RmNrdVAyLU30/vGG/QfqQgJCkpCr5+CIChISroTCHDVCIKMdX+7\nH2f3V6jECbS2vg34cXToWfCzX9PY2EhOTg5yhZzC8ia02nHoYg7hdfup2H5MPc3W18u2t98gIWci\n2XPOQyaTk3vuPDorq1gWu5i1dWsxCDZCQsaPuNjcPvF2+tx9NIomBgb243CMzp8jSRKeQ3WjGoIu\nZ2ARjmzaBe2lY77TsKuuQukHYcP2rx0eKi8vR6PRkJqaSmZmJn6ZSE5XIJwR88jD4PXSv/rDIXMK\nTAWDeYKTMY963S58HveIWgS6OXHIQpT0f1SN33Fs9x2VnMrMJddTs3M7ldsCCXefuQvlcRVDnlYb\n7vrhgjpCqJ5dcblMrtmJ6Bk9/yP5JVyVPWiywhEUgeWg9/U3sG/9Cnd5MbIQCec3MASiy8XcDCOL\nxn2CJOhJSjxGZNhSWY7f6+XsJcs455oAu+yW9/9AW03laKc7LXAd6gMR1rKR6KBo0s+dgqCRUGdf\ngfGX9wEwoTzwPdnX9f02uo2K3gboPwyp53wnpz9jCP4DoIjQYlicjrfZitQA46bfx5QpH2OvmUNr\nRQaNdi0dmGkVZ6MomEXHo3/AXd+A6PJhavgxcbX34lhtxfpVC3oiWPrwk6iDpyCT9VGxPvAjFn0C\n5173B9q6exBFkVmzZrFgziz8MjmtzVoc7lLiMkMo3dyM/0jJ4JblL+Pzeph3692DC33uOfMBmNIV\nS6zWQKRCIi36nBGfa3LUZC5KuYh/HNqNBLSP4RX4OjsR7XbU6eNG/NzsNKNXhaJWBMGukWVBj0KT\nlYU4LolZpV6+bPpyzLEAXq+XqqoqsrOzUSgUKJVK+vR9aBWx2AyR6C68kKAZM+h/7z0k8Vg5ZV5U\nXiBP0LkHyeEdZB51u92sXbuW3t5jC6jzSH5nRHUytQLdnBi8rS7aHxn6bAWXXUlcVg5fvPY8lq7O\nI6L1gYS56PDS/dpBzK+VD8sxtPY7+TQ+H7XDhu3LzaM+u7vRgujwoc0J9AA4y8vp+tvfCDn/fBTR\n0fg7y/Actgw2zI2F3pUrqTlrJlGd68iNrKLKdgUKxTEPqPFACQqlirjsXFJyz0WpMBIc7eT9P/yW\n5vKxjfu3gau6D59K5CPX59wx6Q7kSgUCNcjCEnDGKFFPyCW48CBqufrflifw+3x0NzV+/QkNR8Kc\nqed8B3dzxhD8xyBogpGwReOIuCGb4LwoIuKSuezO17nuzo8xGqMo0hzC6HJTlnwTLp2Jtt89Secz\nJVASQoR6Lp5WG5b1DZhfKEVc2U2EZg4L7n6Sy+9+CcEXRZB6MskTpnLw4EHCw8MxmUwUnDuPSNcA\nnZ1hiKKDzNl2bH1u6kvM1OzaQXXRNqYvWkJ47LGSzVBjFMkT86jZtpWnpv8MAH3ohFGf67FZjzEn\n+bIArcXhNxHFkevSBxPFo4WGHGaMQVEwaSmUrQJ794jjjsK05DrSOmDN58+eVF2ttrYWj8dDTk6g\nY9nlc1GjrECSK9iePhlRAsPSa/C2tmLfcSyUoVPpGB8+nuKO4iEewd69eykpKWH9+vWDY492FY+k\nVwzg7yvDb2lGEidiKzxGpy2Tybno7l8C8Olzf8XTZR7sIbBsPIzo8CLIBPrWHBpS2VTVYWVfVAZS\npJH+D1eP+uyu8h5QCGgywhEdDtruux9FeDgxf/wfDEuvwVG8DkRw1Yytmua32ej++z8QnQ7qqx7D\n6TOwoix/iFBN44ES4sbnoFSpEQQBgyGfiDQ5emM0nzzz5EmV2U4FkiThqO5mt7aMgtgCFqcHhJdc\nJZ/QH/MZFda7cF4divvgQWbKM/5teYId769g+a9/ivnrGoP6LaCLCZA4fgc4Ywj+gxAyPQZtdsSQ\nYwqFgoULF2ITHZSqD5PoEumc8mvkCUvx9Q9gvGMSkTflEPPrAmJ+N52IG7PxymVMDpKTPD4CU1o6\ns8/9lOlnv47VaqWxsZHc3FwEQUAQBGbPX4CzLpCAPtzzPqFRGrauXMknT/+JqOQ0Ci5bPOw+J5x3\nAbaebsT2gEuvCxm5yQtAIVPwv7P+F5luJippgL8X/XxEfn9PXSAJrB43ukdg1Bph2h3gdwfYTMeA\n/tJLkVRK5n7ayrtV74w59uDBgwQHB5OcnAwEZDsjuzrQOhy0REexs74H3XnnIY+MpO/d94bMLTAV\nUNZVNmgI/H4/O3fuRKVScejQIWpqAov6oCEYRaZy4OOP8FS/hegw07+2DVf1MW9CHxXNeT+6k9aq\ncg6pBBRRUXjabNh3thM8Iwb9xcm4D/XjKDnGaFrdMYAoyAi94grs27bj7RzOdipJEs6KHjTpBmRq\nOZ2PP47n8GFi//QnFAYDYUuWINlaAc9Jw0O9byzH39+P5g9X4olzoy+OpcMiUnmE62ig20xva/Ng\nWSwEtCvcnjYu/OlteJxO1j3z5ElJE78pvO02sPopCank0ZmPIggCvt5e3DXVWFMDRr0zYjueBJFz\nG4Ko6q3C4f36yfFTgb2/j32ffQySxJ61H5x8gihC/daAN/AdiQudMQQ/ACQnJzNx4kTK5E1oFA5y\nNHI6fbChx8vuj4qx97uxW9w4fSLeqCAOukVCZAKunYF4v1IZhkKho6KiAkmSyM3NHTx39pxz0Q54\nsfYkIZdvxBa8gv72L0mYMI1rHn0ChXI4d05a/nS0oXo6WnagUkWiVo9eygggE2TcMuM5/Ciw9HzO\nQzsewisO3aW7D9UhNxhQhI9cHmp2mgM9BFFZkHpuQN5yjJ2+wmAg+he/oKBW4sBbz2JxW0Yc53a7\nqampITs7G7k80ARY1VvFlDqJpJYWgoP8fLi7HkGlIuyqq7Bt3oy34xhHUH50PgqfDKRAV/HBgwcZ\nGBjgyiuvJCIigs8//xy/33/MEIwgU+nt7MJeWIj+8gtRhtfit7TR/WYFzspjicvsOeeRkpBCjSmc\nA93tdL5TiqBVoJ+fRPC0GFRJoVjW1Q82p1V32og3aIlafCWIIv2rVg2/bpsdf78bbU4EA59voH/V\nv4i49VaCZwSa/RQREYRefCHe1n24qnpH7TL29fXR+/rrhMyfR1dSCSq3gYgVNcxrLh7sMj7aJX1U\njhNArw8YBXlIN/NuvYvmijIKV7094jVOFXsLA4t9/tlziAsJeLaO3bvxxoo4g1uJ6rkauUeH5Q45\nqeWd+CX/EBoVySd+o6opAFH003igBL9v5IqoPWv/hd/jJS1/BlU7tjJgHkPKFaDzIDh7v7OwEJwx\nBD8YXHDBBShVSnaFNaC/NJXUX0wizNNEaQW88cAO3vh/O1j+m0LeerCIpn4PvvgQrFuah3yJy8vL\niYqKIuq4GnSlWsOkeRfS+LEWn1tD4oTtiGlTCDJcgkozvKMWQK5Qkn3+BOShhwgNyf9a969UhhAb\nfTEzdArW1X3EXZvuYsBzrEpmkGNoBIiSSLejO+ARAEy/E6xtUPHRmNcMv/FGmJTN0k/tvLH5LyOO\nqampwefzDTGOlb2VTK0TyNLrkQlQU1VBt81N2NVXgyTRv+pfg2PzovPQiwGuf0GjoLCwEKPRSGZm\nJhdccAE9PT3s3r17kGdopNDQwMdrQRQJu/xyIm+7EUfh0wiClZ4VlTjKAiEwX0cH6V/tIlKupL/B\nhWD2s7v5E9597AH2b1xH2KI0RLcfy7pAQr66Y4Askw5VcjIh886n+4UXsBcNpc52lncHmuCUfbQ/\n9BCa3FyMP71nyBjDsmX4WvYiuf2jdhn3vPIKosMBt+Zgt9eSPvlhgqdO457SNRzcWQZA475igrRB\nDNz/a+qvWITocKDTZSOTqbBY9pIz93xyz53Prg/fo2Ff8YjX+abotHdireykPaSHK/OuHjxu37UL\n5xw5gqAkdfrdmA7eijfcjSO9Gr1DYHnFcna178Lj9dC9vJzOv+7F02Id40pDUb5mA843m2n4ny/p\nXl5O35pDDGxuxtftxNbbw4ENn5I95zzO+9HtCILA3nUnqair3xL4d8rcU3gLXw9nDMEPBCEhIZx3\n3nk0OzvZ660lLFbHJQ9fQMHBv5Ft3cbZCyI5d1kW5904ngW35RK3bDyCUkb/kdixxWKhqalpMA5+\nPCYvuATJp6BzRzpqtZOkSdsobdpGb8fIX36/34UyfhN+jwxn09czBAAxpiuQSy7+mHcNxR3F3Lj+\nRlqsAcpk96FDAY6hEdDn6sMn+Y51FadfAJEZ8NlvoK9x1OsJcjlpTz2NWpIT/ewHNFqOjfX7nTQe\nfpHWtp+QllZLXNwx42iuOoCpVyRx1iz04ZEkCt3c8sYefFHRBM+eRf+qVUhHdnuhqlByj4TGDg+0\n0dnZycyZMxEEgYyMDNLS0ti6dSuWvj4QBDTBQwViJEmif80atHl5qJKTUSUlEXr+bGwb/xelSUvv\nykrMr5TR/sjfUfr8LPnjM8xIvhwxQkbkuRmIfpEvX3uBzWteI2R2LI59Xdiqeqk32wcbyWIffxx1\nSjItP7t3sHsbAmWjymglLXfegiwoiLi/PY2gGtpzoJ0wASlGwT5ZHS//6w2am4eqn3k7u+hbsZLQ\nyy+hyf4uISHZRJsWEvvUkwgqJRd/+BzNf3+Ohp07MLR2IPZbcFdVYX7mWWQyNTrdhMEO4/N+fCfG\nxGTWP/fXIdQa3d3dfPTRR1gsI3t1I0GSJJ7c9gSZ9iRME1OQCceWOvueIhzTRYzG+QQnJhA17nwM\njRfimOPnl/I0drfv5tYNt/LSs0/gru1HlEuBHIx4ci4ir9uNv3AAtTyI3r427K09OA6YGfi8EfPL\nZez54ANE0c+Mq5YSGhlF1sw5lH75+aDHOCLqt0BkJoTGfO3n/6Y4Ywh+QCgoKCA7O5stW7bw6quv\n0h8UxPhH7yW2+hM0j9xIdNV6sqYZGTc1CmWYBv2FKbjrLDj2dVFeHhDpPn7nexS6iEiuf+xpljzw\nBunpvybS2ER4SiEr33oX3wju7aG6J3B5GnDWn03Ru2so+tc7o7rBx8NgOBuVKpJEoY0X579Il7OL\n69dfT1nVV4gDA8c4hk5Aqy3Q1BUVdGSxlsngmhXg98CKqwKt96NAlZhI2H33Mqle5LNn70MUvbS2\nvkNR0fnU1T2J1+sgNm4nu/dcQk/PVkRJJHRvgJs+ZO45TJuaR6Rgo7+9kZ++vY/Qq5fg6+pi4LPP\nB68xM2wGAKtK1hAUHMSECYHkuSAILFiwIBB+amtHExyCTD6Ug8pVXoHnUB36y4/RbkTcdhuipQeZ\nvBj9RSm4G3uQGS4idNGfsO+0Izl8mK6dyOzrbuL6x/7KjCuv4eDmDXxRvBx5uJreD2uJEQUyjlBL\nyHU64p9/AUGlovnOn+Dr7cXb7cTX6cC+YzWCUknSG6+jih8qVerxeNixYwerx2ewV9XIgNPKqlWr\ncBxX4tr9wvNIoohvWRIuVzNpqb9EEGQoTSZ89z9ImqWVltdfxisTyFp6PWmffUrY0mvoffNNnPv3\no9fnMWAtRxTdKFVqLvnFb/B7vaz6n9/SVlOJKIqsWbOGffv28eqrr2I2j6B9PQLWN6zHXtuNHDlR\nE5IHj3s7uxjQNSCqvcTGBLyE0AuSMTYvQdUfTXh8Jesu/Asvxt/EXI2fPROfZf3Eh+jxfknfEe9m\nLNS8vxmdwkhZUifV+nI+qv47mlvjMd41Cf+AG8UBPznnzCMs2gRA/mVX4XO72b9hlIZLnztAKZF6\nztd67lOF/JFHHvlOL/Bd4KWXXnrk9ttHl1b8vwpBEMjOziYqKoqysjJ27dqFMjmZ3J/+FG/TYfpW\nrMS6ZQua3FyUUVEoY0Nw1/bhPNDNdlspITods2bMwF5YiHXzZlTx8YNKX8FhBpRqDfrQPAasB9Go\nd9LSEUrRhia8nToUShkhBjU9vZuprf0jCQk/ZurshxnoNrPvs4+pL9lDbEYWwWGjK50JggyPp5v2\n9n+REJrEFRPuZ+PhTRzY+i9ml/nYMiuUxhAnHr+HxoFGPqj9gL/t/RsvlgZKKm/JveWYVxAcCYln\nBUpJG7dD7mKQj6wFEDoxj9qtHxPbXssh07uYuz8mJDiTrs5LObB/HLNm3YDDsZuWluWY+/cStLWJ\nUHkU8Xffi8lkoq6uDqO7lc0dCrrDEyhoPYhl9WrUmRmoU1JIskfTUt5IldhGaWgprlAXEyInIBNk\nBAcH43A4qG44TKggkb9g4ZB763npZdxVVcQ+/hgytRoAhdGIs7QU26ZNhN+wkK4/3Y3CGIpMn4G3\n2UbwdBMh02MGvxOJuZPQRUay7/OPccjtxPgSuVpSEWX3ozFokBvUKPShBE2dQt+KFTiK9yJo0/G2\nefHUf0Tiqy8O0lcfRUVFBW+99RZVVVUkp6Qwo7iV8cFncVBqorOzk9zcXLwtLbT/9nfoly7isOkj\nQoIzSEv79WCpsTE7g99VQ1V0NGEeMxc+8HtUGi1B+flYPv4Y27Zt6K66hK7uT9CHTiYoKAWtLpS4\nzGxqdu2gZP1aGrvMNHR0MWvWLNra2igpKSE5OZnQUaqvIOBB3vPFPVw/cCnxfhNhl40bpP6wbf6S\nVt2nyE2RZI5/FEGQIVPJkSuVyHbH0J/wBebuj/AJxdiNpWh0ZkK1FhymPXT43qO9bTVW20GCg9JR\nqYZ+1z12J5b36timrqLa2UZQfBJSZysNJXuYdNnFHN6/jzgxlcRzphIUH5gbrA+jo66GQ7uLmLxg\nIXLFCb0oTTuhZDnMvv+0VAw9+uij7Y888shLJx4/Ywh+YBAEgaioKCZPnszAwAC7d++mvL4e9fnn\nY5ozG/fGjfS9sRzrxo24q6tRxgXT0+pjp6uS7H4dqre+wrq5BEdxGT0vPY3fOoBm/HhkWu3g+cPD\nZ9HRuYZIYy2Ssoumwz1Ub1BRUVSOQ/0AojsW8/6f0FbrJDFnGuNnTaC68CtKPg1UQsSMyxy28z2K\nMP1UnK5mmlveQOFt54aCJxEL9xF3sJP/ndLEJx1fsLp2NWvr1lJqLiUuJI5F6Yv4xZRfkBN5QlhL\nHw/GTCh6LqBzkH1FwFsY4Z3JJ4fSlvopYp8DRej1SOIyioqqmT9/PpMnX0Bc3FLk8mA6OlajnezF\nPc2Lw9OAgJfc3LOpKK8lRd7LqgY50YsuI6Otit7lb6KMiUGmjWVr3W5sajdMgLdr3mZ763byovII\n14Rjio5iV2EhHrWWqdNnoDySgJc8Htp/81uC58wm7PKhRHzKmBj6Vr7NwLr1CJJI4guPo5mbxG5r\nBeHTEgjVD10Io1PSiE0fz+4vVlNtr6BJbiDDH4JjVweuih6QgTYnBXVaCr1vLEfypYPPSdxjt6BO\nG8rt1NDQwLvvvktkZCRXX301s+fMQXm4BmWnFrVcxwFLLY6PPkK1/E1Eux0enEm35Uuys/9MkDZh\n8DxymUB2QTblG9bglGuJn30hUaEaZCoV6tQU+t58E3VIHNbELtrb30ejjkGnyybUGEXuuRcw0N9P\nWbsZjehl3tw5FMycRUVFBXv27CEmJobw8HB6enqorKykqKiIyspKEhISeHLfk1SaK/hl900EpYUT\nNOkYUWHn+y9gzqsmIfFHhEecfex9x+lwF1oIqklH5yggsvMyxi94lPTx/w91xOW8WLKd2W0LkGnA\n4tsZ+H1EnItKday4ofa1L2mzWylTNJORkUFj42HiJkyma98uzI31lJZuItU4GVmTj6Cp0chUgd+I\nLjyS/Rs/QR5Rit4Yi1odfeyPUfJmwBhc8lckuZq6EjNh0doxOa3GwhlD8H8MKpWK7OxsYmNjMZvN\n7N+/n5L2dqxz56LOycHh99FzoBRz4WYOBTnoDtUwsyeSoIh0FMYclHFTUaXNxLZxNT2vPodod6Aw\nmZCHhaFQBBFumInH24xCuZeohAMYMoowJO8DeT+OQw9h7wuit81Oze5OPC4dC+64Gqe1h/2ff0LF\nti/R6kKJTEga1nEskykxGhegVOppaV3BQO8X5LSnIdZ389PnC7k09VJmxMzg0rRLeXDGgyzJXEKB\nqYDo4OiRX4QxE7ThsPOfAdWz1HOGeQZWazkHa+9BkEIIfULEs7mTL0U/0YlRLLrsyiOltArCwvLZ\n9WktURvaCZ46kX5nMZ2dH9HZ9Q65udnU13hJ17p4oUZB0EUXk2ttxbJ8OQ5DKl/5DlOQn889F/6U\nVH0q6xvW83bl22jkGrpWb6W3thJPmJHaQ4cYP348KpUK2+bNWFZ/SNT996E+Urp6FMrYWOyFRXgb\nGzE9+ijyCRNY+c7bVNRVcaD0ADqdjpiYoTHjsGgTaVMK2Fa4C9FcSGV3IarIIEL8YXhK+7Fub0Om\ni0WdlYPoTyDkrFh0J+g7dHd389Zbb6HX67n55puJjAzQQ6gzx2H74i0ixVj6lQoqtH5ivX7iblhM\nneo19Po8UlN+NuzPE4SXitVvcjg8h2crJWakRhCj16JKSsLb3IzlnQ9IX/Z37Mpmmptfx+ezYjDM\nRKlSU3q4me7eXsJ62znwyRpUSgUXXLWE+vp6du3aRXFxMdu3b6empga73U5HRwfF+/bQ3nmYJ/p+\ngdaqIPT8BJSm4MH7OfTlg7hTXGRnP4VSeYz3SZAJKE0heEpAORBB6EQdurwJAf0ETRhqfQzb99cx\ns/Ja4mcuwez8hPaO1YPGwNVtpWt9A5tUZSQkJ7Js2TIkSWJfaSkpuZNoLvoKQSFj5l034S7uxd/v\nJmhC4N0GGUKxiC8j6Yrp6PgQrSaBkJAjqnhf/AFCY3FP+DGbXq9kz7oGQiO1GBNOTXp0NEMg/BDF\nGPLz86Xi4tNTWfB/BX19fZSWlrJ//376+oY35sTqdNx6113ItFokn4irqpf+j+vxW9zgb8K28Vkk\n1wDysDC0kyahzZuMdsoUFNlJbNv5PBbLRvRhXYzP+iNxcQHNYEmSqNjexvZ/HUImwIyrUhGFBg58\n8gHmhjqMSSnMuf5HQ0oGj0d/fzFlB3+K19GNrsZI2vV/IUxfgEx2Clwqmx6F7X+FoEiYeQ8U3Apq\nHQ5HA8V7lyCTqcmf+j62Tjcvv/IqSoeTKUUbKV6YhCY6BrnNhbw9A39/AYIkI2NGDHkLElHqGmlq\nfo3Ozo+Ry+Mp2ZtNN1NYZzaQqBhgga0KW3AwbrnE3cuuJzI94L53O7t5tPBRbBv3k9ugZ/JVV5I8\ndS7vvvsuOp2OG2+8EdtDD+HYt5/0LZsRTgwJAO76Buw7i9AuWsSKFStob2/nkksuoaysjIaGBqZO\nncpFF12E4oS5Zz/xJdMiYWlENxXbNtPb2kxkUDyTUs4nwheD4An85qN/MQVl9LFF0m6388orr+B2\nu7ntttswjED+J4kS5s8PsaJoNX65xCWXCLRZXiJ/6gfo9cPJ0Gp3FbL2r49x3n2P8vOtA/TYPDx3\n/RTOTouAAQv1l1yKMjqahJXLqW9+muaWNzAYZhIcdC/vvPMJc+fOZeb0aWx58xUObt5IRHwi5912\nD/uqa/H5fCQnJ5OUlERERAT1+6tZ9dEHuCUfZ6nHc/bCcwiaaBzcjLibmykqOgdtSArTL9404teo\n78MD9K96A1fJZxhuuAHN3T+jwyWSGa3jicLHOf+zTCKDjUT9JJp9ZTcCMGncC7Qt7+ALWyUWtZO7\n7rmbsLAwRFFk1apVVFZWMj4ilPRx45hy0WUMfNHEwMbDhF+XhSo7iNKyO+nrK6Rtl5HYSSrQtJKc\nfDepMT9CeDIVc+7v+Wz/DFzOZnIW1DF19q2oNaPTsY8FQRD2SpI0rMLjOzcEgiBcCDwDyIFXJEl6\n4oTP1cCbwFSgB7hGkqTGsc55xhCMDkmS6Orqwu124/P58Pv9+Hw+YmNj0euHMl+KHj/WL5uxbmtB\nkIM8xIG/rxlPYzmeulJEWyeCRoW2IJ99mZkcsPaj0QSTlJREUlISycnJiKJIRWk15bvLGRD7EAUR\nDSpiNAb83S30tRwkWB+KITYOQ0wc4TFx6KNNaEP1BIXqkWu8lC9fiCvXhyT3o1SGYzTOJ9wwE40m\nHo0mDpUqAkEYua5BEkUsXZ00V9bQV11Kqmsrpr5NNKmyqI09B0XCRmRykYyMVzGZJrFixQqam5tZ\nOqMAy5P/Q1BTD26VnoqsG+gLH09kdylStByLfAI+j0jKpEgmz09Eod9Pbe3DuFwttLen098Xg1zu\nRSWXMMgUGPrs6HcexBU6Ffm8JUTPP5eWwo3seud1alMcHJjgZFnOMmI9sVRsqkAjVzB7zRqSLr+C\n6Af+36h/T4fDwVtvvUVnZydLliwhKysLv9/P5s2b2b59O7GxscydO5fu7m46Ojpoa++gw9xDUFgk\nMzNNJA3sxlvxIY29kdRaIvB6IC0mn6RxE1FNNmAwxRBmikWh0fDmm2/S2trKzTffTEJCwqj3BNBY\nWMXbX77F1GmrkSzJpPY+hCkjHlW8DplWgaCWI1PJ2fL2qzTs2cOyh5+mt8fFC59W02110ymHYJOG\n/L79hNXsIdjlRjCYcEyXE5XyKYLkx2aPJjpxIemJ84gwTKbxwH42vvh37JY+pl1xFeMKzsZrdeKr\nsyHUe4KAWicAAA6aSURBVFF0yzDL+tgT3Uxbn5nc3FxmzZqF0WhELpfT9NGfqNW9RIbh1yTk3THq\ns4luN5WPPIbsw/dp0Mfy9OSr0cXHcNnkKJpb3uVHhxcjCSLukFpaJj+DJPqxlf2IvR47F6emkr9k\nCTKNBggk3F977TV6e3u5/vrrSUhIQJAEup7fj7vPTOv0Z3AoajD1xLHjCw82i4HEOe0YMi0YfIkI\nhaFU6XPQJxWjDgtUfOXk/A1T9KX/v727j5Hjvus4/v7OzM7u3t6D786PtR07piGRmyaOi6KkVBRa\nBZIWVUIUJVElIhQptBRIJVSaCKniSULwB9BAhQhQQFC1iNKWEFVJUycgRKPETmO7dlw3bmI3Tm3f\n+eF8t88z8/vyx/wuXp/vHCfxZcfd70ta3cxvx+vP7s3Nd+fp97vo72cpfSkEkg/T9X3gNuAosBO4\nW1Vf6FnmN4AbVPXjInIX8EuqeufFXtcKweWVTDeZfeww3VfmyGZ7OyhTkDbu7DGS6Zd5uXyWH41G\nTK2oUo/P3zBPuGHW6TgkQ8yGM5yITtGVlFADahojTtEswWUdNO0iaUKQdiHpQNIhKjnGNjUZ3TjN\n8LppgujcHabqQlw2jLoIFFBBHWiaIeoIJMjH5A0gISNFUYQ4biPi2Lv3Nhr1ScS3v8cdZWPjRdIk\n5fTcag4FHwQZYtvqPVx/PcRr1tPNauw/tIp9B8bpJiFhqKxc22Zi63/hVnwLkaX/brReonuqRr0J\nbq6KzI3STuaI3SyTpYRoPOTslhqlcptGt8JsOkwjGaWVTeCkQhgIUQRRIASnO6StgHjdOJWhMrEL\nCdURa0ZjxvHDowHO+cNvodIOYTZtsSZKcd1hhIBMMhpRg8g5httKqZFBuwkiKAICOjxGMjTCmm6d\nVbUyQ2NjVGo1wrhMWCoTxWWCKCJN66RumsxN0Q6/Szx2kOef+zD1xgQr3QhXZ6upaYUKJcpaokIJ\nUUFFceSPGanzUjjFD4OTZOIINSAT59+CMDnUpLrme1w1ehrGfpTnc0J+04M7/+ZaFdAA0QCXhLRm\nAtr1Ci23lrlsnDhuU63MUSvPUBs+g4jj0JFPU4prlOMy5TjGRWXSICYJY1pE/PcPzrDn1To3n36R\n+3d/lUrz/Muoo9XXk67dSmfyHXTXpjRv/luIG6TtmKHDCdGJEmltC662Co1H6JbG2H+qReoSKpWM\nsRUhIyMBY6WdaPkkq/f8OuWT17O7dpAz2Qzjp4Wh9c9T2b6b+e8/rr0Gad9E4G7hJ7e/nw1brnqD\nf+W5fhWCW4HfV9Vf8PMPAqjqn/Qs87hf5mkRiYDjwCq9SDArBMvHdTLSky3S6SbJdCufPtkinWqg\nyblfSYM2x4IZUMfq48eJXv422Yl9ed8uQ6uZnbiWmc3XMTNRJi1laODIxJGS0ZGEJl10iY2pSEZ1\naJZKuUG53KBcqVMuNxFRRByCIqJ5TfAbAtV8oxYAQxpTo8yQVhg7chut2TWcCRqcljpDGnNjtvmC\n/1NVcWSoOhwKqn6zpT19+OT/J9U5wnLG0bNH2De9i04nJYgclckm1ckW1ck2lYkO8bAjqiYXFI20\nNUqnMUknC4krs5SHzlIqdd7w70pV/PsXVAPyLWa+wcwjz28xfW7UNy3xuTvA+WVV88V6skugRJXz\n7y6e3j/OK09vIB2dIF2ximyJmxAXClzEeDrOhmwl690ETbpMB2eZCWaZDedYkYywrbOFarWDrnqR\n7uiRPIMKEIAGOHXMpQ1msxYdzQjiNuHoFPHoGeLauU740m5EqzlCqzPGiamfYObMOy4p42tUAf95\nzH92wbmLIcrlOqvGj1CrKLXhKaqjUwTR63fSl6YlDrzwfmZm/BVgCuJ/Z4IwOnac2ug0p0+vp9k8\nN/7ElqzMr/7Rg2/sPXhLFYLL37H1+dYDvXegHAUWDlb72jKqmorIWWASOK9XMRG5D7gP4Kqr3lw1\nNK8vKIfE64eJ119445NrJGjX5V0NZMrmJENKGeJaZHN34OpzuEYDTTNwmV8uRVVRhbQL3XqAyyrI\nytW0g4C5+hzNVgOnDnX5cqoQqBATUZISpXaIzDmyUEkko0tKR7uEwxHDG0YojZeRKKRSqbBp0yYi\nQtJTLeYOn6I5Okel3mK42WFdq4umGbNBhoQREoaEUUh1OEQ7XbIupJ0MTZM8u8sf6hyoyzeS6sgS\nOMtJmpNtrp643hcKJQqUUqhEbUc8LWwZ2UTWKNFImjSSOmkaId2VkFVxKjgnBLN5UXNBnUZ4gg5d\nsgySLCD134LDOEPDLkQdCDuvFah8I60IjkhAyAjIC2VcGSUIwrwICZTikPJQRLlWIosSZltnSJMW\nSbtD2k7Iug7nAEdeBFXIN3753heqqAtIuyMknXHS7gq6rRW4dJjhdwWo5utFpikahGgQ4HD5wDPq\n8OUJUQgJqAZl4nJAkzleoo7DkaoSAMNaoxs6dlYOAUJwpkrlzLuJNSB2ISUVIhUSMiQQgiCgJiGh\nQOl4l5K0CWSWNDwGWiMKRylpSCVRKhmsU0eiSicv/Tgcqg40/yLQW/Yd4guBr7G+GEQSUA7KlIMq\nUbKKoH4NjoBZHKdxSGUOCVoEQZdAOgRBB1yAdqq4dkw3jWm5MqWgxqTA/P5SKF0i6VIiQ6dW405M\nMqHKpCaIKqLKxvcu3cnjm7XcheCyUdWHgYch3yPoc5yBIyKES450Nc7iV/D3T2ltjYm1NS4+sKUx\nBpb/zuJXgd6zTht826LL+ENDY+QnjY0xxrwNlrsQ7ASuEZGrRSQG7gIeWbDMI8A9fvqjwJMXOz9g\njDHm8lrWQ0P+mP9vAo+TXz76BVXdLyJ/COxS1UeAfwD+RUQOAafJi4Uxxpi3ybKfI1DVbwDfWND2\n2Z7pNvArC/+dMcaYt4f1PmqMMQPOCoExxgw4KwTGGDPgrBAYY8yAuyJ7HxWRaeDIm/znK1lw13LB\nWd7ld6VltrzL68c57yZVvaDr0iuyELwVIrJrsb42isryLr8rLbPlXV6DmNcODRljzICzQmCMMQNu\nEAvBBcO0FZzlXX5XWmbLu7wGLu/AnSMwxhhzvkHcIzDGGNPDCoExxgy4gSoEInK7iBwUkUMi8kC/\n8ywkIl8QkSkR2dfTNiEiT4jIi/7neD8z9hKRjSLylIi8ICL7ReR+317IzCJSEZFnRWSPz/sHvv1q\nEXnGrxf/5rtMLwwRCUXkeRF51M8XNq+IHBaR74rIbhHZ5dsKuT4AiMgKEfmKiHxPRA6IyK1FzSsi\n1/rPdf4xKyKfuhx5B6YQiEgIfB64A9gK3C0iW/ub6gL/BNy+oO0BYIeqXgPs8PNFkQK/o6pbgVuA\nT/rPtKiZO8AHVPVGYBtwu4jcAvwp8Beq+k7gDHBvHzMu5n7gQM980fP+nKpu67m2vajrA8DngMdU\n9TrgRvLPuZB5VfWg/1y3Ae8BmsDXuBx58/Fkf/wfwK3A4z3zDwIP9jvXIjk3A/t65g8C6/z0OuBg\nvzNeJPt/ArddCZmBIeA75GNonwSixdaTfj/IR/XbAXwAeJR8+Pki5z0MrFzQVsj1gXw0xJfxF80U\nPe+CjD8P/N/lyjswewTAeuCVnvmjvq3o1qjqMT99HFjTzzBLEZHNwE3AMxQ4sz/MshuYAp4AfgDM\nqGrqFynaevGXwO+SDysPMEmx8yrwTRF5TkTu821FXR+uBqaBf/SH3v5eRGoUN2+vu4Av+em3nHeQ\nCsEVT/OSX7jrfUVkGPgP4FOqOtv7XNEyq2qm+a71BuBm4Lo+R1qSiPwiMKWqz/U7yxvwPlXdTn4I\n9pMi8jO9TxZsfYiA7cDfqOpNQIMFh1UKlhcAf07oI8C/L3zuzeYdpELwKrCxZ36Dbyu6EyKyDsD/\nnOpznvOISIm8CHxRVb/qmwudGUBVZ4CnyA+trBCR+dH6irRe/DTwERE5DHyZ/PDQ5yhuXlT1Vf9z\nivz49c0Ud304ChxV1Wf8/FfIC0NR8867A/iOqp7w82857yAVgp3ANf6Ki5h81+qRPme6FI8A9/jp\ne8iPwxeCiAj5mNMHVPXPe54qZGYRWSUiK/x0lfx8xgHygvBRv1hh8qrqg6q6QVU3k6+vT6rqxyho\nXhGpicjI/DT5cex9FHR9UNXjwCsicq1v+iDwAgXN2+Nuzh0WgsuRt98nPd7mEywfAr5Pflz49/qd\nZ5F8XwKOAQn5t5V7yY8J7wBeBL4FTPQ7Z0/e95Hvhu4FdvvHh4qaGbgBeN7n3Qd81rdvAZ4FDpHv\nbpf7nXWR7D8LPFrkvD7XHv/YP/83VtT1wWfbBuzy68TXgfGC560Bp4Cxnra3nNe6mDDGmAE3SIeG\njDHGLMIKgTHGDDgrBMYYM+CsEBhjzICzQmCMMQPOCoExPUQkW9DD42XrcExENvf2LGtMUUSvv4gx\nA6WleRcUxgwM2yMw5hL4fvb/zPe1/6yIvNO3bxaRJ0Vkr4jsEJGrfPsaEfmaH/tgj4i8179UKCJ/\n58dD+Ka/wxkR+W0/rsNeEflyn96mGVBWCIw5X3XBoaE7e547q6rvBv6avFdQgL8C/llVbwC+CDzk\n2x8C/kfzsQ+2k99pC3AN8HlVfRcwA/yyb38AuMm/zseX680Zsxi7s9iYHiJSV9XhRdoPkw9q85Lv\naO+4qk6KyEnyvuAT335MVVeKyDSwQVU7Pa+xGXhC8wFEEJHPACVV/WMReQyok3dz8HVVrS/zWzXm\nNbZHYMyl0yWm34hOz3TGufN0HyYfQW87sLOnd1Fjlp0VAmMu3Z09P5/2098m7xkU4GPA//rpHcAn\n4LXBcMaWelERCYCNqvoU8BnykbMu2CsxZrnYtw5jzlf1I5jNe0xV5y8hHReRveTf6u/2bb9FPsLV\np8lHu/o1334/8LCI3Ev+zf8T5D3LLiYE/tUXCwEe0ny8BGPeFnaOwJhL4M8R/JSqnux3FmMuNzs0\nZIwxA872CIwxZsDZHoExxgw4KwTGGDPgrBAYY8yAs0JgjDEDzgqBMcYMuP8HpzPK5mpoWcUAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlY1RknNfVjb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}