{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Focus_net_Dirichlet_1e5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjG64ra4aFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "91d95464-c920-4861-ce5b-b1d62ac310cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8-7SARDZErK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwJv7Y8Rewez",
        "outputId": "21957a91-da93-4905-94bb-8d4d2989e2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    \n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:04, 39474663.15it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nDYhjJse6Qq",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aivGVg14e9iZ",
        "colab": {}
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h6Jy35SSfBS9",
        "colab": {}
      },
      "source": [
        "# dict = {\"mosaic_list_of_images\": mosaic_list_of_images, \"mosaic_label\": mosaic_label , \"fore_idx\":fore_idx}\n",
        "# f = open(\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl\",\"wb\")\n",
        "# pickle.dump(dict,f)\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIuiboIUfViV"
      },
      "source": [
        "# Load data from saved file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbYf6zuBfViX",
        "colab": {}
      },
      "source": [
        "# with open('/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl', 'rb') as f:\n",
        "#     data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwZjDB5lfVib",
        "colab": {}
      },
      "source": [
        "# mosaic_list_of_images = data[\"mosaic_list_of_images\"]\n",
        "# mosaic_label = data[\"mosaic_label\"]\n",
        "# fore_idx = data[\"fore_idx\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cog5VUzGgE5L",
        "outputId": "56211335-4189-4679-b46e-9aebda87eee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
        "print(len(fore_idx))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000 10000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xX91RwMy-IP4",
        "colab": {}
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUNla6YuZwMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset_dirichlet(mosaic_dataset,labels,foreground_index,a_value):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  alpha = 1e5\n",
        "  dirichlet_sampled_weight = np.random.dirichlet([a_value*alpha, alpha, alpha, alpha, alpha, alpha, alpha, alpha, alpha],size =10000)\n",
        "\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    c = 1\n",
        "    for j in range(9):\n",
        "       \n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dirichlet_sampled_weight[i][0]\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*dirichlet_sampled_weight[i][c]\n",
        "        c+=1\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGz8Y88vIZPT",
        "colab": {}
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 16/7)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 32/5)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 10)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 16)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 28)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 64)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
        "\n",
        "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSO9SFE25Lrk",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obE1xeyRks1Q",
        "colab": {}
      },
      "source": [
        "batch = 256\n",
        "\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
        "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SadRzWBBZEsP",
        "colab": {}
      },
      "source": [
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IgGYMG_ZZEsT",
        "colab": {}
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "        #self.conv1 = nn.Conv2d(3,self.f0,1)\n",
        "        #self.conv3 = nn.Conv2d(3,self.f1,3,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)\n",
        "        \n",
        "        x = torch.cat((x1,x3),dim=1)\n",
        "        \n",
        "    \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thkdqW91Hpju",
        "colab": {}
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,2,3,pad=0)\n",
        "        self.pool = nn.MaxPool2d(3,stride=2,padding=0)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x,x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1yVWgR4vFhe",
        "colab": {}
      },
      "source": [
        "class inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,96,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(96,32,32)\n",
        "        self.incept2 = inception_module(64,32,48)\n",
        "        \n",
        "        self.downsample1 = downsample_module(80,80)\n",
        "        \n",
        "        self.incept3 = inception_module(160,112,48)\n",
        "        self.incept4 = inception_module(160,96,64)\n",
        "        self.incept5 = inception_module(160,80,80)\n",
        "        self.incept6 = inception_module(160,48,96)\n",
        "        \n",
        "        self.downsample2 = downsample_module(144,96)\n",
        "        \n",
        "        self.incept7 = inception_module(240,176,60)\n",
        "        self.incept8 = inception_module(236,176,60)\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(5)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.linear = nn.Linear(236,3)\n",
        "    def forward(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        #act1 = x\n",
        "        \n",
        "        x = self.incept1.forward(x)\n",
        "        #act2 = x\n",
        "        \n",
        "        x = self.incept2.forward(x)\n",
        "        #act3 = x\n",
        "        \n",
        "        x,act4 = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        #act5 = x\n",
        "        \n",
        "        x = self.incept4.forward(x)\n",
        "        #act6 = x\n",
        "        \n",
        "        x = self.incept5.forward(x)\n",
        "        #act7 = x\n",
        "        \n",
        "        x = self.incept6.forward(x)\n",
        "        #act8 = x\n",
        "        \n",
        "        x,act9 = self.downsample2.forward(x)\n",
        "        \n",
        "        x = self.incept7.forward(x)\n",
        "        #act10 = x\n",
        "        x = self.incept8.forward(x)\n",
        "        #act11 = x\n",
        "        #print(x.shape)\n",
        "        x = self.pool(x)\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1,1*1*236)\n",
        "        x = self.linear(x) \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cOWrnzv1fVjD",
        "colab": {}
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFfAJZkcZEsY",
        "colab": {}
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = inception_net().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 70\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "    #     if (epoch%5 == 0):\n",
        "    #         _,actis= inc(inputs)\n",
        "    #         acti.append(actis)\n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/focus_weights/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mI-vqhB-fVjJ",
        "outputId": "2de929ed-706f-46e1-a02d-abba4f7a1847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.085\n",
            "[1,    20] loss: 1.081\n",
            "[1,    30] loss: 1.067\n",
            "[1,    40] loss: 1.047\n",
            "[2,    10] loss: 1.057\n",
            "[2,    20] loss: 1.049\n",
            "[2,    30] loss: 1.055\n",
            "[2,    40] loss: 1.053\n",
            "[3,    10] loss: 1.050\n",
            "[3,    20] loss: 1.034\n",
            "[3,    30] loss: 1.029\n",
            "[3,    40] loss: 1.026\n",
            "[4,    10] loss: 1.007\n",
            "[4,    20] loss: 0.997\n",
            "[4,    30] loss: 1.018\n",
            "[4,    40] loss: 0.987\n",
            "[5,    10] loss: 0.974\n",
            "[5,    20] loss: 0.973\n",
            "[5,    30] loss: 0.965\n",
            "[5,    40] loss: 1.001\n",
            "[6,    10] loss: 0.960\n",
            "[6,    20] loss: 0.956\n",
            "[6,    30] loss: 0.979\n",
            "[6,    40] loss: 0.983\n",
            "[7,    10] loss: 0.929\n",
            "[7,    20] loss: 0.948\n",
            "[7,    30] loss: 0.938\n",
            "[7,    40] loss: 0.923\n",
            "[8,    10] loss: 0.888\n",
            "[8,    20] loss: 0.849\n",
            "[8,    30] loss: 0.842\n",
            "[8,    40] loss: 0.851\n",
            "[9,    10] loss: 0.845\n",
            "[9,    20] loss: 0.909\n",
            "[9,    30] loss: 0.885\n",
            "[9,    40] loss: 0.865\n",
            "[10,    10] loss: 0.800\n",
            "[10,    20] loss: 0.783\n",
            "[10,    30] loss: 0.756\n",
            "[10,    40] loss: 0.763\n",
            "[11,    10] loss: 0.714\n",
            "[11,    20] loss: 0.729\n",
            "[11,    30] loss: 0.740\n",
            "[11,    40] loss: 0.741\n",
            "[12,    10] loss: 0.670\n",
            "[12,    20] loss: 0.686\n",
            "[12,    30] loss: 0.669\n",
            "[12,    40] loss: 0.700\n",
            "[13,    10] loss: 0.571\n",
            "[13,    20] loss: 0.569\n",
            "[13,    30] loss: 0.556\n",
            "[13,    40] loss: 0.516\n",
            "[14,    10] loss: 0.389\n",
            "[14,    20] loss: 0.409\n",
            "[14,    30] loss: 0.405\n",
            "[14,    40] loss: 0.409\n",
            "[15,    10] loss: 0.286\n",
            "[15,    20] loss: 0.264\n",
            "[15,    30] loss: 0.283\n",
            "[15,    40] loss: 0.300\n",
            "[16,    10] loss: 0.414\n",
            "[16,    20] loss: 0.471\n",
            "[16,    30] loss: 0.461\n",
            "[16,    40] loss: 0.436\n",
            "[17,    10] loss: 0.321\n",
            "[17,    20] loss: 0.309\n",
            "[17,    30] loss: 0.283\n",
            "[17,    40] loss: 0.288\n",
            "[18,    10] loss: 0.252\n",
            "[18,    20] loss: 0.275\n",
            "[18,    30] loss: 0.267\n",
            "[18,    40] loss: 0.233\n",
            "[19,    10] loss: 0.203\n",
            "[19,    20] loss: 0.222\n",
            "[19,    30] loss: 0.205\n",
            "[19,    40] loss: 0.249\n",
            "[20,    10] loss: 0.315\n",
            "[20,    20] loss: 0.299\n",
            "[20,    30] loss: 0.279\n",
            "[20,    40] loss: 0.245\n",
            "[21,    10] loss: 0.173\n",
            "[21,    20] loss: 0.185\n",
            "[21,    30] loss: 0.147\n",
            "[21,    40] loss: 0.165\n",
            "[22,    10] loss: 0.267\n",
            "[22,    20] loss: 0.277\n",
            "[22,    30] loss: 0.221\n",
            "[22,    40] loss: 0.227\n",
            "[23,    10] loss: 0.299\n",
            "[23,    20] loss: 0.348\n",
            "[23,    30] loss: 0.307\n",
            "[23,    40] loss: 0.228\n",
            "[24,    10] loss: 0.257\n",
            "[24,    20] loss: 0.263\n",
            "[24,    30] loss: 0.218\n",
            "[24,    40] loss: 0.185\n",
            "[25,    10] loss: 0.109\n",
            "[25,    20] loss: 0.105\n",
            "[25,    30] loss: 0.090\n",
            "[25,    40] loss: 0.070\n",
            "[26,    10] loss: 0.030\n",
            "[26,    20] loss: 0.034\n",
            "[26,    30] loss: 0.020\n",
            "[26,    40] loss: 0.042\n",
            "[27,    10] loss: 0.061\n",
            "[27,    20] loss: 0.098\n",
            "[27,    30] loss: 0.079\n",
            "[27,    40] loss: 0.053\n",
            "[28,    10] loss: 0.026\n",
            "[28,    20] loss: 0.021\n",
            "[28,    30] loss: 0.018\n",
            "[28,    40] loss: 0.016\n",
            "[29,    10] loss: 0.011\n",
            "[29,    20] loss: 0.019\n",
            "[29,    30] loss: 0.015\n",
            "[29,    40] loss: 0.013\n",
            "[30,    10] loss: 0.004\n",
            "[30,    20] loss: 0.004\n",
            "[30,    30] loss: 0.004\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.002\n",
            "[31,    20] loss: 0.002\n",
            "[31,    30] loss: 0.002\n",
            "[31,    40] loss: 0.007\n",
            "[32,    10] loss: 0.007\n",
            "[32,    20] loss: 0.012\n",
            "[32,    30] loss: 0.012\n",
            "[32,    40] loss: 0.033\n",
            "[33,    10] loss: 0.074\n",
            "[33,    20] loss: 0.084\n",
            "[33,    30] loss: 0.056\n",
            "[33,    40] loss: 0.057\n",
            "[34,    10] loss: 0.044\n",
            "[34,    20] loss: 0.050\n",
            "[34,    30] loss: 0.046\n",
            "[34,    40] loss: 0.032\n",
            "[35,    10] loss: 0.017\n",
            "[35,    20] loss: 0.014\n",
            "[35,    30] loss: 0.013\n",
            "[35,    40] loss: 0.009\n",
            "[36,    10] loss: 0.007\n",
            "[36,    20] loss: 0.007\n",
            "[36,    30] loss: 0.006\n",
            "[36,    40] loss: 0.003\n",
            "[37,    10] loss: 0.002\n",
            "[37,    20] loss: 0.002\n",
            "[37,    30] loss: 0.002\n",
            "[37,    40] loss: 0.004\n",
            "[38,    10] loss: 0.003\n",
            "[38,    20] loss: 0.006\n",
            "[38,    30] loss: 0.004\n",
            "[38,    40] loss: 0.004\n",
            "[39,    10] loss: 0.003\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.002\n",
            "[39,    40] loss: 0.002\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.030\n",
            "[41,    10] loss: 0.137\n",
            "[41,    20] loss: 0.202\n",
            "[41,    30] loss: 0.178\n",
            "[41,    40] loss: 0.206\n",
            "[42,    10] loss: 0.289\n",
            "[42,    20] loss: 0.305\n",
            "[42,    30] loss: 0.276\n",
            "[42,    40] loss: 0.256\n",
            "[43,    10] loss: 0.234\n",
            "[43,    20] loss: 0.211\n",
            "[43,    30] loss: 0.179\n",
            "[43,    40] loss: 0.140\n",
            "[44,    10] loss: 0.065\n",
            "[44,    20] loss: 0.051\n",
            "[44,    30] loss: 0.034\n",
            "[44,    40] loss: 0.046\n",
            "[45,    10] loss: 0.061\n",
            "[45,    20] loss: 0.060\n",
            "[45,    30] loss: 0.052\n",
            "[45,    40] loss: 0.049\n",
            "[46,    10] loss: 0.038\n",
            "[46,    20] loss: 0.040\n",
            "[46,    30] loss: 0.036\n",
            "[46,    40] loss: 0.035\n",
            "[47,    10] loss: 0.022\n",
            "[47,    20] loss: 0.025\n",
            "[47,    30] loss: 0.015\n",
            "[47,    40] loss: 0.014\n",
            "[48,    10] loss: 0.006\n",
            "[48,    20] loss: 0.004\n",
            "[48,    30] loss: 0.004\n",
            "[48,    40] loss: 0.006\n",
            "[49,    10] loss: 0.003\n",
            "[49,    20] loss: 0.003\n",
            "[49,    30] loss: 0.003\n",
            "[49,    40] loss: 0.006\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.004\n",
            "[50,    30] loss: 0.003\n",
            "[50,    40] loss: 0.004\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.002\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.002\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.003\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.002\n",
            "[55,    30] loss: 0.002\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.001\n",
            "[56,    30] loss: 0.001\n",
            "[56,    40] loss: 0.001\n",
            "[57,    10] loss: 0.001\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.001\n",
            "[58,    10] loss: 0.001\n",
            "[58,    20] loss: 0.000\n",
            "[58,    30] loss: 0.000\n",
            "[58,    40] loss: 0.006\n",
            "[59,    10] loss: 0.005\n",
            "[59,    20] loss: 0.011\n",
            "[59,    30] loss: 0.009\n",
            "[59,    40] loss: 0.005\n",
            "[60,    10] loss: 0.007\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.001\n",
            "[62,    10] loss: 0.000\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.000\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.000\n",
            "[64,    10] loss: 0.000\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.000\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.000\n",
            "[65,    20] loss: 0.000\n",
            "[65,    30] loss: 0.000\n",
            "[65,    40] loss: 0.000\n",
            "[66,    10] loss: 0.000\n",
            "[66,    20] loss: 0.000\n",
            "[66,    30] loss: 0.000\n",
            "[66,    40] loss: 0.000\n",
            "[67,    10] loss: 0.000\n",
            "[67,    20] loss: 0.000\n",
            "[67,    30] loss: 0.000\n",
            "[67,    40] loss: 0.000\n",
            "[68,    10] loss: 0.000\n",
            "[68,    20] loss: 0.000\n",
            "[68,    30] loss: 0.000\n",
            "[68,    40] loss: 0.002\n",
            "[69,    10] loss: 0.000\n",
            "[69,    20] loss: 0.000\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.000\n",
            "[70,    20] loss: 0.000\n",
            "[70,    30] loss: 0.000\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 79 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 73 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 68 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.028\n",
            "[1,    20] loss: 0.895\n",
            "[1,    30] loss: 0.840\n",
            "[1,    40] loss: 0.811\n",
            "[2,    10] loss: 0.837\n",
            "[2,    20] loss: 0.784\n",
            "[2,    30] loss: 0.746\n",
            "[2,    40] loss: 0.763\n",
            "[3,    10] loss: 0.748\n",
            "[3,    20] loss: 0.704\n",
            "[3,    30] loss: 0.721\n",
            "[3,    40] loss: 0.734\n",
            "[4,    10] loss: 0.706\n",
            "[4,    20] loss: 0.684\n",
            "[4,    30] loss: 0.643\n",
            "[4,    40] loss: 0.660\n",
            "[5,    10] loss: 0.625\n",
            "[5,    20] loss: 0.612\n",
            "[5,    30] loss: 0.618\n",
            "[5,    40] loss: 0.604\n",
            "[6,    10] loss: 0.582\n",
            "[6,    20] loss: 0.545\n",
            "[6,    30] loss: 0.546\n",
            "[6,    40] loss: 0.536\n",
            "[7,    10] loss: 0.509\n",
            "[7,    20] loss: 0.489\n",
            "[7,    30] loss: 0.471\n",
            "[7,    40] loss: 0.482\n",
            "[8,    10] loss: 0.458\n",
            "[8,    20] loss: 0.473\n",
            "[8,    30] loss: 0.416\n",
            "[8,    40] loss: 0.465\n",
            "[9,    10] loss: 0.392\n",
            "[9,    20] loss: 0.416\n",
            "[9,    30] loss: 0.397\n",
            "[9,    40] loss: 0.366\n",
            "[10,    10] loss: 0.314\n",
            "[10,    20] loss: 0.312\n",
            "[10,    30] loss: 0.306\n",
            "[10,    40] loss: 0.338\n",
            "[11,    10] loss: 0.355\n",
            "[11,    20] loss: 0.347\n",
            "[11,    30] loss: 0.341\n",
            "[11,    40] loss: 0.346\n",
            "[12,    10] loss: 0.325\n",
            "[12,    20] loss: 0.293\n",
            "[12,    30] loss: 0.263\n",
            "[12,    40] loss: 0.242\n",
            "[13,    10] loss: 0.201\n",
            "[13,    20] loss: 0.174\n",
            "[13,    30] loss: 0.157\n",
            "[13,    40] loss: 0.123\n",
            "[14,    10] loss: 0.112\n",
            "[14,    20] loss: 0.090\n",
            "[14,    30] loss: 0.071\n",
            "[14,    40] loss: 0.080\n",
            "[15,    10] loss: 0.107\n",
            "[15,    20] loss: 0.121\n",
            "[15,    30] loss: 0.114\n",
            "[15,    40] loss: 0.114\n",
            "[16,    10] loss: 0.213\n",
            "[16,    20] loss: 0.228\n",
            "[16,    30] loss: 0.174\n",
            "[16,    40] loss: 0.176\n",
            "[17,    10] loss: 0.156\n",
            "[17,    20] loss: 0.181\n",
            "[17,    30] loss: 0.162\n",
            "[17,    40] loss: 0.153\n",
            "[18,    10] loss: 0.167\n",
            "[18,    20] loss: 0.155\n",
            "[18,    30] loss: 0.134\n",
            "[18,    40] loss: 0.105\n",
            "[19,    10] loss: 0.113\n",
            "[19,    20] loss: 0.095\n",
            "[19,    30] loss: 0.075\n",
            "[19,    40] loss: 0.063\n",
            "[20,    10] loss: 0.072\n",
            "[20,    20] loss: 0.056\n",
            "[20,    30] loss: 0.053\n",
            "[20,    40] loss: 0.060\n",
            "[21,    10] loss: 0.076\n",
            "[21,    20] loss: 0.066\n",
            "[21,    30] loss: 0.071\n",
            "[21,    40] loss: 0.048\n",
            "[22,    10] loss: 0.029\n",
            "[22,    20] loss: 0.020\n",
            "[22,    30] loss: 0.020\n",
            "[22,    40] loss: 0.028\n",
            "[23,    10] loss: 0.084\n",
            "[23,    20] loss: 0.079\n",
            "[23,    30] loss: 0.060\n",
            "[23,    40] loss: 0.053\n",
            "[24,    10] loss: 0.022\n",
            "[24,    20] loss: 0.020\n",
            "[24,    30] loss: 0.015\n",
            "[24,    40] loss: 0.014\n",
            "[25,    10] loss: 0.008\n",
            "[25,    20] loss: 0.007\n",
            "[25,    30] loss: 0.006\n",
            "[25,    40] loss: 0.005\n",
            "[26,    10] loss: 0.002\n",
            "[26,    20] loss: 0.002\n",
            "[26,    30] loss: 0.002\n",
            "[26,    40] loss: 0.003\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.001\n",
            "[28,    10] loss: 0.001\n",
            "[28,    20] loss: 0.001\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.002\n",
            "[29,    10] loss: 0.002\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.001\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.001\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.002\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.000\n",
            "[33,    40] loss: 0.020\n",
            "[34,    10] loss: 0.094\n",
            "[34,    20] loss: 0.102\n",
            "[34,    30] loss: 0.099\n",
            "[34,    40] loss: 0.082\n",
            "[35,    10] loss: 0.065\n",
            "[35,    20] loss: 0.060\n",
            "[35,    30] loss: 0.045\n",
            "[35,    40] loss: 0.040\n",
            "[36,    10] loss: 0.031\n",
            "[36,    20] loss: 0.035\n",
            "[36,    30] loss: 0.028\n",
            "[36,    40] loss: 0.026\n",
            "[37,    10] loss: 0.016\n",
            "[37,    20] loss: 0.008\n",
            "[37,    30] loss: 0.006\n",
            "[37,    40] loss: 0.007\n",
            "[38,    10] loss: 0.003\n",
            "[38,    20] loss: 0.003\n",
            "[38,    30] loss: 0.002\n",
            "[38,    40] loss: 0.004\n",
            "[39,    10] loss: 0.003\n",
            "[39,    20] loss: 0.005\n",
            "[39,    30] loss: 0.003\n",
            "[39,    40] loss: 0.003\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.002\n",
            "[40,    30] loss: 0.002\n",
            "[40,    40] loss: 0.008\n",
            "[41,    10] loss: 0.016\n",
            "[41,    20] loss: 0.018\n",
            "[41,    30] loss: 0.021\n",
            "[41,    40] loss: 0.033\n",
            "[42,    10] loss: 0.100\n",
            "[42,    20] loss: 0.125\n",
            "[42,    30] loss: 0.078\n",
            "[42,    40] loss: 0.111\n",
            "[43,    10] loss: 0.295\n",
            "[43,    20] loss: 0.321\n",
            "[43,    30] loss: 0.214\n",
            "[43,    40] loss: 0.168\n",
            "[44,    10] loss: 0.096\n",
            "[44,    20] loss: 0.093\n",
            "[44,    30] loss: 0.061\n",
            "[44,    40] loss: 0.045\n",
            "[45,    10] loss: 0.019\n",
            "[45,    20] loss: 0.013\n",
            "[45,    30] loss: 0.010\n",
            "[45,    40] loss: 0.011\n",
            "[46,    10] loss: 0.005\n",
            "[46,    20] loss: 0.004\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.003\n",
            "[47,    10] loss: 0.002\n",
            "[47,    20] loss: 0.002\n",
            "[47,    30] loss: 0.002\n",
            "[47,    40] loss: 0.003\n",
            "[48,    10] loss: 0.002\n",
            "[48,    20] loss: 0.003\n",
            "[48,    30] loss: 0.002\n",
            "[48,    40] loss: 0.004\n",
            "[49,    10] loss: 0.002\n",
            "[49,    20] loss: 0.002\n",
            "[49,    30] loss: 0.002\n",
            "[49,    40] loss: 0.006\n",
            "[50,    10] loss: 0.015\n",
            "[50,    20] loss: 0.011\n",
            "[50,    30] loss: 0.009\n",
            "[50,    40] loss: 0.007\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.002\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.002\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.007\n",
            "[56,    10] loss: 0.019\n",
            "[56,    20] loss: 0.023\n",
            "[56,    30] loss: 0.011\n",
            "[56,    40] loss: 0.013\n",
            "[57,    10] loss: 0.007\n",
            "[57,    20] loss: 0.022\n",
            "[57,    30] loss: 0.012\n",
            "[57,    40] loss: 0.007\n",
            "[58,    10] loss: 0.003\n",
            "[58,    20] loss: 0.005\n",
            "[58,    30] loss: 0.004\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.003\n",
            "[60,    10] loss: 0.002\n",
            "[60,    20] loss: 0.003\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.017\n",
            "[62,    10] loss: 0.029\n",
            "[62,    20] loss: 0.035\n",
            "[62,    30] loss: 0.015\n",
            "[62,    40] loss: 0.026\n",
            "[63,    10] loss: 0.041\n",
            "[63,    20] loss: 0.042\n",
            "[63,    30] loss: 0.028\n",
            "[63,    40] loss: 0.035\n",
            "[64,    10] loss: 0.072\n",
            "[64,    20] loss: 0.093\n",
            "[64,    30] loss: 0.073\n",
            "[64,    40] loss: 0.054\n",
            "[65,    10] loss: 0.037\n",
            "[65,    20] loss: 0.029\n",
            "[65,    30] loss: 0.020\n",
            "[65,    40] loss: 0.020\n",
            "[66,    10] loss: 0.008\n",
            "[66,    20] loss: 0.007\n",
            "[66,    30] loss: 0.006\n",
            "[66,    40] loss: 0.004\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.000\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 73 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 88 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 0.953\n",
            "[1,    20] loss: 0.758\n",
            "[1,    30] loss: 0.688\n",
            "[1,    40] loss: 0.698\n",
            "[2,    10] loss: 0.603\n",
            "[2,    20] loss: 0.600\n",
            "[2,    30] loss: 0.559\n",
            "[2,    40] loss: 0.559\n",
            "[3,    10] loss: 0.526\n",
            "[3,    20] loss: 0.497\n",
            "[3,    30] loss: 0.486\n",
            "[3,    40] loss: 0.455\n",
            "[4,    10] loss: 0.405\n",
            "[4,    20] loss: 0.412\n",
            "[4,    30] loss: 0.447\n",
            "[4,    40] loss: 0.392\n",
            "[5,    10] loss: 0.375\n",
            "[5,    20] loss: 0.354\n",
            "[5,    30] loss: 0.343\n",
            "[5,    40] loss: 0.338\n",
            "[6,    10] loss: 0.314\n",
            "[6,    20] loss: 0.277\n",
            "[6,    30] loss: 0.292\n",
            "[6,    40] loss: 0.281\n",
            "[7,    10] loss: 0.244\n",
            "[7,    20] loss: 0.237\n",
            "[7,    30] loss: 0.233\n",
            "[7,    40] loss: 0.226\n",
            "[8,    10] loss: 0.192\n",
            "[8,    20] loss: 0.167\n",
            "[8,    30] loss: 0.161\n",
            "[8,    40] loss: 0.150\n",
            "[9,    10] loss: 0.118\n",
            "[9,    20] loss: 0.101\n",
            "[9,    30] loss: 0.109\n",
            "[9,    40] loss: 0.106\n",
            "[10,    10] loss: 0.079\n",
            "[10,    20] loss: 0.066\n",
            "[10,    30] loss: 0.064\n",
            "[10,    40] loss: 0.070\n",
            "[11,    10] loss: 0.164\n",
            "[11,    20] loss: 0.168\n",
            "[11,    30] loss: 0.188\n",
            "[11,    40] loss: 0.181\n",
            "[12,    10] loss: 0.232\n",
            "[12,    20] loss: 0.215\n",
            "[12,    30] loss: 0.165\n",
            "[12,    40] loss: 0.123\n",
            "[13,    10] loss: 0.133\n",
            "[13,    20] loss: 0.090\n",
            "[13,    30] loss: 0.081\n",
            "[13,    40] loss: 0.082\n",
            "[14,    10] loss: 0.091\n",
            "[14,    20] loss: 0.074\n",
            "[14,    30] loss: 0.082\n",
            "[14,    40] loss: 0.087\n",
            "[15,    10] loss: 0.154\n",
            "[15,    20] loss: 0.140\n",
            "[15,    30] loss: 0.111\n",
            "[15,    40] loss: 0.097\n",
            "[16,    10] loss: 0.064\n",
            "[16,    20] loss: 0.056\n",
            "[16,    30] loss: 0.044\n",
            "[16,    40] loss: 0.047\n",
            "[17,    10] loss: 0.123\n",
            "[17,    20] loss: 0.097\n",
            "[17,    30] loss: 0.080\n",
            "[17,    40] loss: 0.107\n",
            "[18,    10] loss: 0.100\n",
            "[18,    20] loss: 0.141\n",
            "[18,    30] loss: 0.097\n",
            "[18,    40] loss: 0.066\n",
            "[19,    10] loss: 0.033\n",
            "[19,    20] loss: 0.025\n",
            "[19,    30] loss: 0.022\n",
            "[19,    40] loss: 0.038\n",
            "[20,    10] loss: 0.129\n",
            "[20,    20] loss: 0.069\n",
            "[20,    30] loss: 0.061\n",
            "[20,    40] loss: 0.055\n",
            "[21,    10] loss: 0.039\n",
            "[21,    20] loss: 0.019\n",
            "[21,    30] loss: 0.016\n",
            "[21,    40] loss: 0.017\n",
            "[22,    10] loss: 0.015\n",
            "[22,    20] loss: 0.015\n",
            "[22,    30] loss: 0.012\n",
            "[22,    40] loss: 0.010\n",
            "[23,    10] loss: 0.008\n",
            "[23,    20] loss: 0.008\n",
            "[23,    30] loss: 0.004\n",
            "[23,    40] loss: 0.007\n",
            "[24,    10] loss: 0.011\n",
            "[24,    20] loss: 0.006\n",
            "[24,    30] loss: 0.007\n",
            "[24,    40] loss: 0.005\n",
            "[25,    10] loss: 0.003\n",
            "[25,    20] loss: 0.004\n",
            "[25,    30] loss: 0.003\n",
            "[25,    40] loss: 0.002\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.001\n",
            "[28,    10] loss: 0.001\n",
            "[28,    20] loss: 0.001\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.001\n",
            "[29,    10] loss: 0.001\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.002\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.001\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.000\n",
            "[33,    20] loss: 0.000\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.000\n",
            "[34,    40] loss: 0.002\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.004\n",
            "[35,    30] loss: 0.002\n",
            "[35,    40] loss: 0.002\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.002\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.001\n",
            "[37,    40] loss: 0.001\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.001\n",
            "[38,    30] loss: 0.000\n",
            "[38,    40] loss: 0.004\n",
            "[39,    10] loss: 0.012\n",
            "[39,    20] loss: 0.005\n",
            "[39,    30] loss: 0.003\n",
            "[39,    40] loss: 0.003\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.001\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.011\n",
            "[42,    10] loss: 0.050\n",
            "[42,    20] loss: 0.035\n",
            "[42,    30] loss: 0.027\n",
            "[42,    40] loss: 0.018\n",
            "[43,    10] loss: 0.014\n",
            "[43,    20] loss: 0.011\n",
            "[43,    30] loss: 0.009\n",
            "[43,    40] loss: 0.005\n",
            "[44,    10] loss: 0.004\n",
            "[44,    20] loss: 0.003\n",
            "[44,    30] loss: 0.003\n",
            "[44,    40] loss: 0.002\n",
            "[45,    10] loss: 0.001\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.000\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.000\n",
            "[48,    10] loss: 0.000\n",
            "[48,    20] loss: 0.000\n",
            "[48,    30] loss: 0.000\n",
            "[48,    40] loss: 0.002\n",
            "[49,    10] loss: 0.002\n",
            "[49,    20] loss: 0.003\n",
            "[49,    30] loss: 0.003\n",
            "[49,    40] loss: 0.003\n",
            "[50,    10] loss: 0.009\n",
            "[50,    20] loss: 0.011\n",
            "[50,    30] loss: 0.006\n",
            "[50,    40] loss: 0.009\n",
            "[51,    10] loss: 0.005\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.038\n",
            "[52,    10] loss: 0.220\n",
            "[52,    20] loss: 0.222\n",
            "[52,    30] loss: 0.131\n",
            "[52,    40] loss: 0.124\n",
            "[53,    10] loss: 0.085\n",
            "[53,    20] loss: 0.083\n",
            "[53,    30] loss: 0.069\n",
            "[53,    40] loss: 0.064\n",
            "[54,    10] loss: 0.023\n",
            "[54,    20] loss: 0.024\n",
            "[54,    30] loss: 0.015\n",
            "[54,    40] loss: 0.012\n",
            "[55,    10] loss: 0.006\n",
            "[55,    20] loss: 0.004\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.003\n",
            "[56,    10] loss: 0.002\n",
            "[56,    20] loss: 0.002\n",
            "[56,    30] loss: 0.002\n",
            "[56,    40] loss: 0.002\n",
            "[57,    10] loss: 0.001\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.001\n",
            "[58,    10] loss: 0.001\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.001\n",
            "[58,    40] loss: 0.001\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.001\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.007\n",
            "[62,    10] loss: 0.016\n",
            "[62,    20] loss: 0.010\n",
            "[62,    30] loss: 0.007\n",
            "[62,    40] loss: 0.004\n",
            "[63,    10] loss: 0.003\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.004\n",
            "[64,    10] loss: 0.005\n",
            "[64,    20] loss: 0.004\n",
            "[64,    30] loss: 0.002\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.003\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.012\n",
            "[67,    10] loss: 0.035\n",
            "[67,    20] loss: 0.024\n",
            "[67,    30] loss: 0.020\n",
            "[67,    40] loss: 0.018\n",
            "[68,    10] loss: 0.006\n",
            "[68,    20] loss: 0.005\n",
            "[68,    30] loss: 0.003\n",
            "[68,    40] loss: 0.014\n",
            "[69,    10] loss: 0.080\n",
            "[69,    20] loss: 0.072\n",
            "[69,    30] loss: 0.034\n",
            "[69,    40] loss: 0.031\n",
            "[70,    10] loss: 0.012\n",
            "[70,    20] loss: 0.010\n",
            "[70,    30] loss: 0.008\n",
            "[70,    40] loss: 0.006\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 52 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 92 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 0.900\n",
            "[1,    20] loss: 0.669\n",
            "[1,    30] loss: 0.601\n",
            "[1,    40] loss: 0.549\n",
            "[2,    10] loss: 0.517\n",
            "[2,    20] loss: 0.519\n",
            "[2,    30] loss: 0.491\n",
            "[2,    40] loss: 0.430\n",
            "[3,    10] loss: 0.378\n",
            "[3,    20] loss: 0.398\n",
            "[3,    30] loss: 0.381\n",
            "[3,    40] loss: 0.359\n",
            "[4,    10] loss: 0.368\n",
            "[4,    20] loss: 0.331\n",
            "[4,    30] loss: 0.308\n",
            "[4,    40] loss: 0.290\n",
            "[5,    10] loss: 0.274\n",
            "[5,    20] loss: 0.272\n",
            "[5,    30] loss: 0.239\n",
            "[5,    40] loss: 0.238\n",
            "[6,    10] loss: 0.218\n",
            "[6,    20] loss: 0.237\n",
            "[6,    30] loss: 0.222\n",
            "[6,    40] loss: 0.223\n",
            "[7,    10] loss: 0.238\n",
            "[7,    20] loss: 0.237\n",
            "[7,    30] loss: 0.186\n",
            "[7,    40] loss: 0.168\n",
            "[8,    10] loss: 0.143\n",
            "[8,    20] loss: 0.145\n",
            "[8,    30] loss: 0.117\n",
            "[8,    40] loss: 0.112\n",
            "[9,    10] loss: 0.081\n",
            "[9,    20] loss: 0.090\n",
            "[9,    30] loss: 0.081\n",
            "[9,    40] loss: 0.096\n",
            "[10,    10] loss: 0.152\n",
            "[10,    20] loss: 0.146\n",
            "[10,    30] loss: 0.131\n",
            "[10,    40] loss: 0.131\n",
            "[11,    10] loss: 0.114\n",
            "[11,    20] loss: 0.091\n",
            "[11,    30] loss: 0.077\n",
            "[11,    40] loss: 0.059\n",
            "[12,    10] loss: 0.043\n",
            "[12,    20] loss: 0.035\n",
            "[12,    30] loss: 0.029\n",
            "[12,    40] loss: 0.033\n",
            "[13,    10] loss: 0.030\n",
            "[13,    20] loss: 0.029\n",
            "[13,    30] loss: 0.024\n",
            "[13,    40] loss: 0.026\n",
            "[14,    10] loss: 0.046\n",
            "[14,    20] loss: 0.040\n",
            "[14,    30] loss: 0.039\n",
            "[14,    40] loss: 0.035\n",
            "[15,    10] loss: 0.104\n",
            "[15,    20] loss: 0.098\n",
            "[15,    30] loss: 0.081\n",
            "[15,    40] loss: 0.094\n",
            "[16,    10] loss: 0.210\n",
            "[16,    20] loss: 0.161\n",
            "[16,    30] loss: 0.135\n",
            "[16,    40] loss: 0.099\n",
            "[17,    10] loss: 0.069\n",
            "[17,    20] loss: 0.053\n",
            "[17,    30] loss: 0.043\n",
            "[17,    40] loss: 0.046\n",
            "[18,    10] loss: 0.021\n",
            "[18,    20] loss: 0.020\n",
            "[18,    30] loss: 0.017\n",
            "[18,    40] loss: 0.017\n",
            "[19,    10] loss: 0.010\n",
            "[19,    20] loss: 0.008\n",
            "[19,    30] loss: 0.005\n",
            "[19,    40] loss: 0.005\n",
            "[20,    10] loss: 0.003\n",
            "[20,    20] loss: 0.003\n",
            "[20,    30] loss: 0.002\n",
            "[20,    40] loss: 0.002\n",
            "[21,    10] loss: 0.002\n",
            "[21,    20] loss: 0.001\n",
            "[21,    30] loss: 0.001\n",
            "[21,    40] loss: 0.005\n",
            "[22,    10] loss: 0.009\n",
            "[22,    20] loss: 0.010\n",
            "[22,    30] loss: 0.007\n",
            "[22,    40] loss: 0.026\n",
            "[23,    10] loss: 0.168\n",
            "[23,    20] loss: 0.168\n",
            "[23,    30] loss: 0.119\n",
            "[23,    40] loss: 0.125\n",
            "[24,    10] loss: 0.058\n",
            "[24,    20] loss: 0.054\n",
            "[24,    30] loss: 0.035\n",
            "[24,    40] loss: 0.032\n",
            "[25,    10] loss: 0.020\n",
            "[25,    20] loss: 0.017\n",
            "[25,    30] loss: 0.016\n",
            "[25,    40] loss: 0.009\n",
            "[26,    10] loss: 0.008\n",
            "[26,    20] loss: 0.005\n",
            "[26,    30] loss: 0.004\n",
            "[26,    40] loss: 0.010\n",
            "[27,    10] loss: 0.022\n",
            "[27,    20] loss: 0.014\n",
            "[27,    30] loss: 0.013\n",
            "[27,    40] loss: 0.010\n",
            "[28,    10] loss: 0.005\n",
            "[28,    20] loss: 0.004\n",
            "[28,    30] loss: 0.003\n",
            "[28,    40] loss: 0.002\n",
            "[29,    10] loss: 0.002\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.004\n",
            "[30,    10] loss: 0.005\n",
            "[30,    20] loss: 0.003\n",
            "[30,    30] loss: 0.006\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.002\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.002\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.004\n",
            "[35,    10] loss: 0.021\n",
            "[35,    20] loss: 0.024\n",
            "[35,    30] loss: 0.014\n",
            "[35,    40] loss: 0.050\n",
            "[36,    10] loss: 0.360\n",
            "[36,    20] loss: 0.239\n",
            "[36,    30] loss: 0.202\n",
            "[36,    40] loss: 0.167\n",
            "[37,    10] loss: 0.156\n",
            "[37,    20] loss: 0.154\n",
            "[37,    30] loss: 0.086\n",
            "[37,    40] loss: 0.109\n",
            "[38,    10] loss: 0.106\n",
            "[38,    20] loss: 0.081\n",
            "[38,    30] loss: 0.060\n",
            "[38,    40] loss: 0.041\n",
            "[39,    10] loss: 0.022\n",
            "[39,    20] loss: 0.014\n",
            "[39,    30] loss: 0.014\n",
            "[39,    40] loss: 0.011\n",
            "[40,    10] loss: 0.006\n",
            "[40,    20] loss: 0.006\n",
            "[40,    30] loss: 0.004\n",
            "[40,    40] loss: 0.005\n",
            "[41,    10] loss: 0.003\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.003\n",
            "[41,    40] loss: 0.004\n",
            "[42,    10] loss: 0.003\n",
            "[42,    20] loss: 0.004\n",
            "[42,    30] loss: 0.003\n",
            "[42,    40] loss: 0.003\n",
            "[43,    10] loss: 0.002\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.007\n",
            "[44,    10] loss: 0.024\n",
            "[44,    20] loss: 0.026\n",
            "[44,    30] loss: 0.018\n",
            "[44,    40] loss: 0.021\n",
            "[45,    10] loss: 0.040\n",
            "[45,    20] loss: 0.017\n",
            "[45,    30] loss: 0.014\n",
            "[45,    40] loss: 0.010\n",
            "[46,    10] loss: 0.004\n",
            "[46,    20] loss: 0.004\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.007\n",
            "[47,    10] loss: 0.009\n",
            "[47,    20] loss: 0.007\n",
            "[47,    30] loss: 0.007\n",
            "[47,    40] loss: 0.004\n",
            "[48,    10] loss: 0.002\n",
            "[48,    20] loss: 0.002\n",
            "[48,    30] loss: 0.002\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.003\n",
            "[50,    10] loss: 0.002\n",
            "[50,    20] loss: 0.005\n",
            "[50,    30] loss: 0.002\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.001\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.002\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.031\n",
            "[54,    10] loss: 0.059\n",
            "[54,    20] loss: 0.044\n",
            "[54,    30] loss: 0.025\n",
            "[54,    40] loss: 0.091\n",
            "[55,    10] loss: 0.264\n",
            "[55,    20] loss: 0.212\n",
            "[55,    30] loss: 0.140\n",
            "[55,    40] loss: 0.093\n",
            "[56,    10] loss: 0.075\n",
            "[56,    20] loss: 0.059\n",
            "[56,    30] loss: 0.046\n",
            "[56,    40] loss: 0.040\n",
            "[57,    10] loss: 0.018\n",
            "[57,    20] loss: 0.021\n",
            "[57,    30] loss: 0.018\n",
            "[57,    40] loss: 0.015\n",
            "[58,    10] loss: 0.011\n",
            "[58,    20] loss: 0.007\n",
            "[58,    30] loss: 0.007\n",
            "[58,    40] loss: 0.005\n",
            "[59,    10] loss: 0.003\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.003\n",
            "[59,    40] loss: 0.003\n",
            "[60,    10] loss: 0.002\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.003\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.030\n",
            "[62,    10] loss: 0.063\n",
            "[62,    20] loss: 0.061\n",
            "[62,    30] loss: 0.058\n",
            "[62,    40] loss: 0.048\n",
            "[63,    10] loss: 0.040\n",
            "[63,    20] loss: 0.052\n",
            "[63,    30] loss: 0.025\n",
            "[63,    40] loss: 0.021\n",
            "[64,    10] loss: 0.008\n",
            "[64,    20] loss: 0.005\n",
            "[64,    30] loss: 0.004\n",
            "[64,    40] loss: 0.008\n",
            "[65,    10] loss: 0.012\n",
            "[65,    20] loss: 0.010\n",
            "[65,    30] loss: 0.006\n",
            "[65,    40] loss: 0.004\n",
            "[66,    10] loss: 0.003\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.030\n",
            "[67,    10] loss: 0.095\n",
            "[67,    20] loss: 0.061\n",
            "[67,    30] loss: 0.039\n",
            "[67,    40] loss: 0.041\n",
            "[68,    10] loss: 0.027\n",
            "[68,    20] loss: 0.026\n",
            "[68,    30] loss: 0.017\n",
            "[68,    40] loss: 0.015\n",
            "[69,    10] loss: 0.013\n",
            "[69,    20] loss: 0.012\n",
            "[69,    30] loss: 0.005\n",
            "[69,    40] loss: 0.004\n",
            "[70,    10] loss: 0.003\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.002\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 48 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 80 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 94 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 0.904\n",
            "[1,    20] loss: 0.641\n",
            "[1,    30] loss: 0.563\n",
            "[1,    40] loss: 0.503\n",
            "[2,    10] loss: 0.471\n",
            "[2,    20] loss: 0.414\n",
            "[2,    30] loss: 0.415\n",
            "[2,    40] loss: 0.423\n",
            "[3,    10] loss: 0.428\n",
            "[3,    20] loss: 0.356\n",
            "[3,    30] loss: 0.362\n",
            "[3,    40] loss: 0.323\n",
            "[4,    10] loss: 0.351\n",
            "[4,    20] loss: 0.319\n",
            "[4,    30] loss: 0.270\n",
            "[4,    40] loss: 0.299\n",
            "[5,    10] loss: 0.309\n",
            "[5,    20] loss: 0.256\n",
            "[5,    30] loss: 0.227\n",
            "[5,    40] loss: 0.250\n",
            "[6,    10] loss: 0.193\n",
            "[6,    20] loss: 0.188\n",
            "[6,    30] loss: 0.167\n",
            "[6,    40] loss: 0.157\n",
            "[7,    10] loss: 0.161\n",
            "[7,    20] loss: 0.156\n",
            "[7,    30] loss: 0.133\n",
            "[7,    40] loss: 0.337\n",
            "[8,    10] loss: 0.388\n",
            "[8,    20] loss: 0.326\n",
            "[8,    30] loss: 0.267\n",
            "[8,    40] loss: 0.242\n",
            "[9,    10] loss: 0.200\n",
            "[9,    20] loss: 0.185\n",
            "[9,    30] loss: 0.157\n",
            "[9,    40] loss: 0.157\n",
            "[10,    10] loss: 0.125\n",
            "[10,    20] loss: 0.119\n",
            "[10,    30] loss: 0.108\n",
            "[10,    40] loss: 0.083\n",
            "[11,    10] loss: 0.067\n",
            "[11,    20] loss: 0.058\n",
            "[11,    30] loss: 0.052\n",
            "[11,    40] loss: 0.072\n",
            "[12,    10] loss: 0.074\n",
            "[12,    20] loss: 0.080\n",
            "[12,    30] loss: 0.064\n",
            "[12,    40] loss: 0.059\n",
            "[13,    10] loss: 0.049\n",
            "[13,    20] loss: 0.050\n",
            "[13,    30] loss: 0.040\n",
            "[13,    40] loss: 0.038\n",
            "[14,    10] loss: 0.060\n",
            "[14,    20] loss: 0.056\n",
            "[14,    30] loss: 0.044\n",
            "[14,    40] loss: 0.029\n",
            "[15,    10] loss: 0.018\n",
            "[15,    20] loss: 0.014\n",
            "[15,    30] loss: 0.013\n",
            "[15,    40] loss: 0.013\n",
            "[16,    10] loss: 0.011\n",
            "[16,    20] loss: 0.008\n",
            "[16,    30] loss: 0.005\n",
            "[16,    40] loss: 0.006\n",
            "[17,    10] loss: 0.004\n",
            "[17,    20] loss: 0.004\n",
            "[17,    30] loss: 0.003\n",
            "[17,    40] loss: 0.021\n",
            "[18,    10] loss: 0.085\n",
            "[18,    20] loss: 0.082\n",
            "[18,    30] loss: 0.064\n",
            "[18,    40] loss: 0.101\n",
            "[19,    10] loss: 0.157\n",
            "[19,    20] loss: 0.099\n",
            "[19,    30] loss: 0.087\n",
            "[19,    40] loss: 0.088\n",
            "[20,    10] loss: 0.071\n",
            "[20,    20] loss: 0.059\n",
            "[20,    30] loss: 0.037\n",
            "[20,    40] loss: 0.040\n",
            "[21,    10] loss: 0.016\n",
            "[21,    20] loss: 0.020\n",
            "[21,    30] loss: 0.014\n",
            "[21,    40] loss: 0.019\n",
            "[22,    10] loss: 0.034\n",
            "[22,    20] loss: 0.024\n",
            "[22,    30] loss: 0.017\n",
            "[22,    40] loss: 0.074\n",
            "[23,    10] loss: 0.180\n",
            "[23,    20] loss: 0.201\n",
            "[23,    30] loss: 0.126\n",
            "[23,    40] loss: 0.094\n",
            "[24,    10] loss: 0.050\n",
            "[24,    20] loss: 0.043\n",
            "[24,    30] loss: 0.027\n",
            "[24,    40] loss: 0.032\n",
            "[25,    10] loss: 0.029\n",
            "[25,    20] loss: 0.024\n",
            "[25,    30] loss: 0.015\n",
            "[25,    40] loss: 0.012\n",
            "[26,    10] loss: 0.006\n",
            "[26,    20] loss: 0.007\n",
            "[26,    30] loss: 0.004\n",
            "[26,    40] loss: 0.005\n",
            "[27,    10] loss: 0.003\n",
            "[27,    20] loss: 0.003\n",
            "[27,    30] loss: 0.003\n",
            "[27,    40] loss: 0.005\n",
            "[28,    10] loss: 0.005\n",
            "[28,    20] loss: 0.004\n",
            "[28,    30] loss: 0.004\n",
            "[28,    40] loss: 0.015\n",
            "[29,    10] loss: 0.050\n",
            "[29,    20] loss: 0.041\n",
            "[29,    30] loss: 0.036\n",
            "[29,    40] loss: 0.021\n",
            "[30,    10] loss: 0.011\n",
            "[30,    20] loss: 0.009\n",
            "[30,    30] loss: 0.007\n",
            "[30,    40] loss: 0.007\n",
            "[31,    10] loss: 0.004\n",
            "[31,    20] loss: 0.003\n",
            "[31,    30] loss: 0.003\n",
            "[31,    40] loss: 0.047\n",
            "[32,    10] loss: 0.162\n",
            "[32,    20] loss: 0.137\n",
            "[32,    30] loss: 0.083\n",
            "[32,    40] loss: 0.059\n",
            "[33,    10] loss: 0.035\n",
            "[33,    20] loss: 0.020\n",
            "[33,    30] loss: 0.017\n",
            "[33,    40] loss: 0.021\n",
            "[34,    10] loss: 0.013\n",
            "[34,    20] loss: 0.011\n",
            "[34,    30] loss: 0.005\n",
            "[34,    40] loss: 0.007\n",
            "[35,    10] loss: 0.004\n",
            "[35,    20] loss: 0.003\n",
            "[35,    30] loss: 0.003\n",
            "[35,    40] loss: 0.002\n",
            "[36,    10] loss: 0.002\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.002\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.002\n",
            "[37,    30] loss: 0.002\n",
            "[37,    40] loss: 0.002\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.001\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.001\n",
            "[39,    10] loss: 0.001\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.001\n",
            "[39,    40] loss: 0.002\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.002\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.001\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.015\n",
            "[42,    10] loss: 0.027\n",
            "[42,    20] loss: 0.020\n",
            "[42,    30] loss: 0.014\n",
            "[42,    40] loss: 0.017\n",
            "[43,    10] loss: 0.027\n",
            "[43,    20] loss: 0.014\n",
            "[43,    30] loss: 0.010\n",
            "[43,    40] loss: 0.010\n",
            "[44,    10] loss: 0.005\n",
            "[44,    20] loss: 0.003\n",
            "[44,    30] loss: 0.002\n",
            "[44,    40] loss: 0.003\n",
            "[45,    10] loss: 0.002\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.002\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.007\n",
            "[49,    10] loss: 0.033\n",
            "[49,    20] loss: 0.020\n",
            "[49,    30] loss: 0.018\n",
            "[49,    40] loss: 0.016\n",
            "[50,    10] loss: 0.022\n",
            "[50,    20] loss: 0.015\n",
            "[50,    30] loss: 0.009\n",
            "[50,    40] loss: 0.008\n",
            "[51,    10] loss: 0.009\n",
            "[51,    20] loss: 0.006\n",
            "[51,    30] loss: 0.003\n",
            "[51,    40] loss: 0.003\n",
            "[52,    10] loss: 0.002\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.005\n",
            "[53,    10] loss: 0.015\n",
            "[53,    20] loss: 0.008\n",
            "[53,    30] loss: 0.006\n",
            "[53,    40] loss: 0.032\n",
            "[54,    10] loss: 0.066\n",
            "[54,    20] loss: 0.050\n",
            "[54,    30] loss: 0.031\n",
            "[54,    40] loss: 0.020\n",
            "[55,    10] loss: 0.008\n",
            "[55,    20] loss: 0.007\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.010\n",
            "[56,    10] loss: 0.030\n",
            "[56,    20] loss: 0.018\n",
            "[56,    30] loss: 0.012\n",
            "[56,    40] loss: 0.010\n",
            "[57,    10] loss: 0.012\n",
            "[57,    20] loss: 0.012\n",
            "[57,    30] loss: 0.012\n",
            "[57,    40] loss: 0.016\n",
            "[58,    10] loss: 0.048\n",
            "[58,    20] loss: 0.032\n",
            "[58,    30] loss: 0.016\n",
            "[58,    40] loss: 0.053\n",
            "[59,    10] loss: 0.108\n",
            "[59,    20] loss: 0.086\n",
            "[59,    30] loss: 0.071\n",
            "[59,    40] loss: 0.043\n",
            "[60,    10] loss: 0.025\n",
            "[60,    20] loss: 0.018\n",
            "[60,    30] loss: 0.013\n",
            "[60,    40] loss: 0.013\n",
            "[61,    10] loss: 0.009\n",
            "[61,    20] loss: 0.006\n",
            "[61,    30] loss: 0.005\n",
            "[61,    40] loss: 0.005\n",
            "[62,    10] loss: 0.004\n",
            "[62,    20] loss: 0.005\n",
            "[62,    30] loss: 0.003\n",
            "[62,    40] loss: 0.004\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.005\n",
            "[64,    10] loss: 0.003\n",
            "[64,    20] loss: 0.006\n",
            "[64,    30] loss: 0.003\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.000\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.000\n",
            "[70,    30] loss: 0.000\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 75 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 0.901\n",
            "[1,    20] loss: 0.629\n",
            "[1,    30] loss: 0.504\n",
            "[1,    40] loss: 0.477\n",
            "[2,    10] loss: 0.518\n",
            "[2,    20] loss: 0.455\n",
            "[2,    30] loss: 0.418\n",
            "[2,    40] loss: 0.338\n",
            "[3,    10] loss: 0.360\n",
            "[3,    20] loss: 0.309\n",
            "[3,    30] loss: 0.291\n",
            "[3,    40] loss: 0.281\n",
            "[4,    10] loss: 0.251\n",
            "[4,    20] loss: 0.243\n",
            "[4,    30] loss: 0.235\n",
            "[4,    40] loss: 0.211\n",
            "[5,    10] loss: 0.228\n",
            "[5,    20] loss: 0.209\n",
            "[5,    30] loss: 0.189\n",
            "[5,    40] loss: 0.159\n",
            "[6,    10] loss: 0.147\n",
            "[6,    20] loss: 0.123\n",
            "[6,    30] loss: 0.125\n",
            "[6,    40] loss: 0.106\n",
            "[7,    10] loss: 0.116\n",
            "[7,    20] loss: 0.104\n",
            "[7,    30] loss: 0.085\n",
            "[7,    40] loss: 0.104\n",
            "[8,    10] loss: 0.245\n",
            "[8,    20] loss: 0.240\n",
            "[8,    30] loss: 0.179\n",
            "[8,    40] loss: 0.145\n",
            "[9,    10] loss: 0.106\n",
            "[9,    20] loss: 0.089\n",
            "[9,    30] loss: 0.086\n",
            "[9,    40] loss: 0.088\n",
            "[10,    10] loss: 0.085\n",
            "[10,    20] loss: 0.058\n",
            "[10,    30] loss: 0.052\n",
            "[10,    40] loss: 0.040\n",
            "[11,    10] loss: 0.035\n",
            "[11,    20] loss: 0.036\n",
            "[11,    30] loss: 0.026\n",
            "[11,    40] loss: 0.018\n",
            "[12,    10] loss: 0.015\n",
            "[12,    20] loss: 0.011\n",
            "[12,    30] loss: 0.009\n",
            "[12,    40] loss: 0.016\n",
            "[13,    10] loss: 0.029\n",
            "[13,    20] loss: 0.023\n",
            "[13,    30] loss: 0.020\n",
            "[13,    40] loss: 0.019\n",
            "[14,    10] loss: 0.032\n",
            "[14,    20] loss: 0.025\n",
            "[14,    30] loss: 0.021\n",
            "[14,    40] loss: 0.020\n",
            "[15,    10] loss: 0.011\n",
            "[15,    20] loss: 0.010\n",
            "[15,    30] loss: 0.006\n",
            "[15,    40] loss: 0.013\n",
            "[16,    10] loss: 0.097\n",
            "[16,    20] loss: 0.071\n",
            "[16,    30] loss: 0.041\n",
            "[16,    40] loss: 0.038\n",
            "[17,    10] loss: 0.038\n",
            "[17,    20] loss: 0.026\n",
            "[17,    30] loss: 0.018\n",
            "[17,    40] loss: 0.042\n",
            "[18,    10] loss: 0.150\n",
            "[18,    20] loss: 0.114\n",
            "[18,    30] loss: 0.089\n",
            "[18,    40] loss: 0.077\n",
            "[19,    10] loss: 0.060\n",
            "[19,    20] loss: 0.038\n",
            "[19,    30] loss: 0.030\n",
            "[19,    40] loss: 0.025\n",
            "[20,    10] loss: 0.015\n",
            "[20,    20] loss: 0.011\n",
            "[20,    30] loss: 0.014\n",
            "[20,    40] loss: 0.014\n",
            "[21,    10] loss: 0.017\n",
            "[21,    20] loss: 0.010\n",
            "[21,    30] loss: 0.007\n",
            "[21,    40] loss: 0.007\n",
            "[22,    10] loss: 0.003\n",
            "[22,    20] loss: 0.003\n",
            "[22,    30] loss: 0.002\n",
            "[22,    40] loss: 0.005\n",
            "[23,    10] loss: 0.013\n",
            "[23,    20] loss: 0.011\n",
            "[23,    30] loss: 0.008\n",
            "[23,    40] loss: 0.005\n",
            "[24,    10] loss: 0.004\n",
            "[24,    20] loss: 0.003\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.001\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.001\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.005\n",
            "[27,    10] loss: 0.023\n",
            "[27,    20] loss: 0.022\n",
            "[27,    30] loss: 0.012\n",
            "[27,    40] loss: 0.012\n",
            "[28,    10] loss: 0.006\n",
            "[28,    20] loss: 0.004\n",
            "[28,    30] loss: 0.004\n",
            "[28,    40] loss: 0.011\n",
            "[29,    10] loss: 0.043\n",
            "[29,    20] loss: 0.034\n",
            "[29,    30] loss: 0.040\n",
            "[29,    40] loss: 0.043\n",
            "[30,    10] loss: 0.087\n",
            "[30,    20] loss: 0.068\n",
            "[30,    30] loss: 0.061\n",
            "[30,    40] loss: 0.058\n",
            "[31,    10] loss: 0.116\n",
            "[31,    20] loss: 0.079\n",
            "[31,    30] loss: 0.061\n",
            "[31,    40] loss: 0.055\n",
            "[32,    10] loss: 0.078\n",
            "[32,    20] loss: 0.066\n",
            "[32,    30] loss: 0.041\n",
            "[32,    40] loss: 0.033\n",
            "[33,    10] loss: 0.019\n",
            "[33,    20] loss: 0.013\n",
            "[33,    30] loss: 0.013\n",
            "[33,    40] loss: 0.011\n",
            "[34,    10] loss: 0.005\n",
            "[34,    20] loss: 0.008\n",
            "[34,    30] loss: 0.005\n",
            "[34,    40] loss: 0.006\n",
            "[35,    10] loss: 0.005\n",
            "[35,    20] loss: 0.004\n",
            "[35,    30] loss: 0.003\n",
            "[35,    40] loss: 0.003\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.002\n",
            "[36,    40] loss: 0.014\n",
            "[37,    10] loss: 0.057\n",
            "[37,    20] loss: 0.033\n",
            "[37,    30] loss: 0.014\n",
            "[37,    40] loss: 0.013\n",
            "[38,    10] loss: 0.005\n",
            "[38,    20] loss: 0.007\n",
            "[38,    30] loss: 0.003\n",
            "[38,    40] loss: 0.010\n",
            "[39,    10] loss: 0.028\n",
            "[39,    20] loss: 0.024\n",
            "[39,    30] loss: 0.019\n",
            "[39,    40] loss: 0.008\n",
            "[40,    10] loss: 0.004\n",
            "[40,    20] loss: 0.005\n",
            "[40,    30] loss: 0.002\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.001\n",
            "[41,    30] loss: 0.001\n",
            "[41,    40] loss: 0.005\n",
            "[42,    10] loss: 0.010\n",
            "[42,    20] loss: 0.004\n",
            "[42,    30] loss: 0.004\n",
            "[42,    40] loss: 0.003\n",
            "[43,    10] loss: 0.003\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.014\n",
            "[45,    10] loss: 0.029\n",
            "[45,    20] loss: 0.015\n",
            "[45,    30] loss: 0.008\n",
            "[45,    40] loss: 0.011\n",
            "[46,    10] loss: 0.027\n",
            "[46,    20] loss: 0.034\n",
            "[46,    30] loss: 0.018\n",
            "[46,    40] loss: 0.012\n",
            "[47,    10] loss: 0.015\n",
            "[47,    20] loss: 0.016\n",
            "[47,    30] loss: 0.016\n",
            "[47,    40] loss: 0.011\n",
            "[48,    10] loss: 0.006\n",
            "[48,    20] loss: 0.006\n",
            "[48,    30] loss: 0.003\n",
            "[48,    40] loss: 0.003\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.001\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.000\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.000\n",
            "[51,    40] loss: 0.000\n",
            "[52,    10] loss: 0.000\n",
            "[52,    20] loss: 0.000\n",
            "[52,    30] loss: 0.000\n",
            "[52,    40] loss: 0.000\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.000\n",
            "[53,    30] loss: 0.000\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.000\n",
            "[54,    20] loss: 0.000\n",
            "[54,    30] loss: 0.000\n",
            "[54,    40] loss: 0.000\n",
            "[55,    10] loss: 0.000\n",
            "[55,    20] loss: 0.000\n",
            "[55,    30] loss: 0.000\n",
            "[55,    40] loss: 0.002\n",
            "[56,    10] loss: 0.000\n",
            "[56,    20] loss: 0.001\n",
            "[56,    30] loss: 0.001\n",
            "[56,    40] loss: 0.001\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.000\n",
            "[57,    30] loss: 0.000\n",
            "[57,    40] loss: 0.010\n",
            "[58,    10] loss: 0.022\n",
            "[58,    20] loss: 0.021\n",
            "[58,    30] loss: 0.008\n",
            "[58,    40] loss: 0.033\n",
            "[59,    10] loss: 0.077\n",
            "[59,    20] loss: 0.062\n",
            "[59,    30] loss: 0.059\n",
            "[59,    40] loss: 0.031\n",
            "[60,    10] loss: 0.060\n",
            "[60,    20] loss: 0.062\n",
            "[60,    30] loss: 0.024\n",
            "[60,    40] loss: 0.022\n",
            "[61,    10] loss: 0.006\n",
            "[61,    20] loss: 0.006\n",
            "[61,    30] loss: 0.005\n",
            "[61,    40] loss: 0.005\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.003\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.003\n",
            "[63,    40] loss: 0.002\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.011\n",
            "[65,    10] loss: 0.055\n",
            "[65,    20] loss: 0.050\n",
            "[65,    30] loss: 0.026\n",
            "[65,    40] loss: 0.019\n",
            "[66,    10] loss: 0.010\n",
            "[66,    20] loss: 0.009\n",
            "[66,    30] loss: 0.007\n",
            "[66,    40] loss: 0.008\n",
            "[67,    10] loss: 0.017\n",
            "[67,    20] loss: 0.011\n",
            "[67,    30] loss: 0.009\n",
            "[67,    40] loss: 0.005\n",
            "[68,    10] loss: 0.002\n",
            "[68,    20] loss: 0.002\n",
            "[68,    30] loss: 0.002\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.007\n",
            "[70,    10] loss: 0.029\n",
            "[70,    20] loss: 0.014\n",
            "[70,    30] loss: 0.014\n",
            "[70,    40] loss: 0.007\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 0.889\n",
            "[1,    20] loss: 0.605\n",
            "[1,    30] loss: 0.518\n",
            "[1,    40] loss: 0.459\n",
            "[2,    10] loss: 0.474\n",
            "[2,    20] loss: 0.436\n",
            "[2,    30] loss: 0.379\n",
            "[2,    40] loss: 0.355\n",
            "[3,    10] loss: 0.334\n",
            "[3,    20] loss: 0.327\n",
            "[3,    30] loss: 0.314\n",
            "[3,    40] loss: 0.290\n",
            "[4,    10] loss: 0.227\n",
            "[4,    20] loss: 0.252\n",
            "[4,    30] loss: 0.241\n",
            "[4,    40] loss: 0.222\n",
            "[5,    10] loss: 0.289\n",
            "[5,    20] loss: 0.195\n",
            "[5,    30] loss: 0.204\n",
            "[5,    40] loss: 0.201\n",
            "[6,    10] loss: 0.297\n",
            "[6,    20] loss: 0.232\n",
            "[6,    30] loss: 0.196\n",
            "[6,    40] loss: 0.177\n",
            "[7,    10] loss: 0.154\n",
            "[7,    20] loss: 0.130\n",
            "[7,    30] loss: 0.131\n",
            "[7,    40] loss: 0.141\n",
            "[8,    10] loss: 0.146\n",
            "[8,    20] loss: 0.150\n",
            "[8,    30] loss: 0.121\n",
            "[8,    40] loss: 0.097\n",
            "[9,    10] loss: 0.066\n",
            "[9,    20] loss: 0.068\n",
            "[9,    30] loss: 0.054\n",
            "[9,    40] loss: 0.051\n",
            "[10,    10] loss: 0.030\n",
            "[10,    20] loss: 0.031\n",
            "[10,    30] loss: 0.033\n",
            "[10,    40] loss: 0.031\n",
            "[11,    10] loss: 0.017\n",
            "[11,    20] loss: 0.017\n",
            "[11,    30] loss: 0.018\n",
            "[11,    40] loss: 0.024\n",
            "[12,    10] loss: 0.068\n",
            "[12,    20] loss: 0.068\n",
            "[12,    30] loss: 0.053\n",
            "[12,    40] loss: 0.059\n",
            "[13,    10] loss: 0.032\n",
            "[13,    20] loss: 0.022\n",
            "[13,    30] loss: 0.020\n",
            "[13,    40] loss: 0.015\n",
            "[14,    10] loss: 0.009\n",
            "[14,    20] loss: 0.007\n",
            "[14,    30] loss: 0.007\n",
            "[14,    40] loss: 0.009\n",
            "[15,    10] loss: 0.018\n",
            "[15,    20] loss: 0.021\n",
            "[15,    30] loss: 0.015\n",
            "[15,    40] loss: 0.012\n",
            "[16,    10] loss: 0.010\n",
            "[16,    20] loss: 0.008\n",
            "[16,    30] loss: 0.007\n",
            "[16,    40] loss: 0.011\n",
            "[17,    10] loss: 0.029\n",
            "[17,    20] loss: 0.032\n",
            "[17,    30] loss: 0.019\n",
            "[17,    40] loss: 0.029\n",
            "[18,    10] loss: 0.110\n",
            "[18,    20] loss: 0.119\n",
            "[18,    30] loss: 0.082\n",
            "[18,    40] loss: 0.071\n",
            "[19,    10] loss: 0.107\n",
            "[19,    20] loss: 0.085\n",
            "[19,    30] loss: 0.043\n",
            "[19,    40] loss: 0.041\n",
            "[20,    10] loss: 0.023\n",
            "[20,    20] loss: 0.018\n",
            "[20,    30] loss: 0.015\n",
            "[20,    40] loss: 0.017\n",
            "[21,    10] loss: 0.040\n",
            "[21,    20] loss: 0.025\n",
            "[21,    30] loss: 0.017\n",
            "[21,    40] loss: 0.044\n",
            "[22,    10] loss: 0.102\n",
            "[22,    20] loss: 0.061\n",
            "[22,    30] loss: 0.035\n",
            "[22,    40] loss: 0.035\n",
            "[23,    10] loss: 0.020\n",
            "[23,    20] loss: 0.017\n",
            "[23,    30] loss: 0.011\n",
            "[23,    40] loss: 0.015\n",
            "[24,    10] loss: 0.022\n",
            "[24,    20] loss: 0.014\n",
            "[24,    30] loss: 0.011\n",
            "[24,    40] loss: 0.008\n",
            "[25,    10] loss: 0.005\n",
            "[25,    20] loss: 0.004\n",
            "[25,    30] loss: 0.003\n",
            "[25,    40] loss: 0.004\n",
            "[26,    10] loss: 0.002\n",
            "[26,    20] loss: 0.003\n",
            "[26,    30] loss: 0.002\n",
            "[26,    40] loss: 0.002\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.002\n",
            "[28,    10] loss: 0.001\n",
            "[28,    20] loss: 0.002\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.106\n",
            "[29,    10] loss: 0.429\n",
            "[29,    20] loss: 0.291\n",
            "[29,    30] loss: 0.216\n",
            "[29,    40] loss: 0.176\n",
            "[30,    10] loss: 0.130\n",
            "[30,    20] loss: 0.115\n",
            "[30,    30] loss: 0.082\n",
            "[30,    40] loss: 0.059\n",
            "[31,    10] loss: 0.042\n",
            "[31,    20] loss: 0.029\n",
            "[31,    30] loss: 0.028\n",
            "[31,    40] loss: 0.026\n",
            "[32,    10] loss: 0.017\n",
            "[32,    20] loss: 0.016\n",
            "[32,    30] loss: 0.013\n",
            "[32,    40] loss: 0.012\n",
            "[33,    10] loss: 0.007\n",
            "[33,    20] loss: 0.006\n",
            "[33,    30] loss: 0.006\n",
            "[33,    40] loss: 0.009\n",
            "[34,    10] loss: 0.017\n",
            "[34,    20] loss: 0.013\n",
            "[34,    30] loss: 0.010\n",
            "[34,    40] loss: 0.047\n",
            "[35,    10] loss: 0.199\n",
            "[35,    20] loss: 0.142\n",
            "[35,    30] loss: 0.099\n",
            "[35,    40] loss: 0.113\n",
            "[36,    10] loss: 0.141\n",
            "[36,    20] loss: 0.095\n",
            "[36,    30] loss: 0.075\n",
            "[36,    40] loss: 0.049\n",
            "[37,    10] loss: 0.029\n",
            "[37,    20] loss: 0.027\n",
            "[37,    30] loss: 0.024\n",
            "[37,    40] loss: 0.019\n",
            "[38,    10] loss: 0.024\n",
            "[38,    20] loss: 0.015\n",
            "[38,    30] loss: 0.011\n",
            "[38,    40] loss: 0.008\n",
            "[39,    10] loss: 0.005\n",
            "[39,    20] loss: 0.005\n",
            "[39,    30] loss: 0.005\n",
            "[39,    40] loss: 0.004\n",
            "[40,    10] loss: 0.003\n",
            "[40,    20] loss: 0.003\n",
            "[40,    30] loss: 0.003\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.002\n",
            "[41,    20] loss: 0.002\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.006\n",
            "[42,    10] loss: 0.003\n",
            "[42,    20] loss: 0.005\n",
            "[42,    30] loss: 0.002\n",
            "[42,    40] loss: 0.003\n",
            "[43,    10] loss: 0.002\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.008\n",
            "[45,    10] loss: 0.023\n",
            "[45,    20] loss: 0.012\n",
            "[45,    30] loss: 0.008\n",
            "[45,    40] loss: 0.012\n",
            "[46,    10] loss: 0.034\n",
            "[46,    20] loss: 0.021\n",
            "[46,    30] loss: 0.014\n",
            "[46,    40] loss: 0.009\n",
            "[47,    10] loss: 0.004\n",
            "[47,    20] loss: 0.004\n",
            "[47,    30] loss: 0.003\n",
            "[47,    40] loss: 0.003\n",
            "[48,    10] loss: 0.002\n",
            "[48,    20] loss: 0.002\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.001\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.001\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.001\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.004\n",
            "[52,    10] loss: 0.008\n",
            "[52,    20] loss: 0.008\n",
            "[52,    30] loss: 0.002\n",
            "[52,    40] loss: 0.004\n",
            "[53,    10] loss: 0.002\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.001\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.000\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.000\n",
            "[57,    30] loss: 0.000\n",
            "[57,    40] loss: 0.022\n",
            "[58,    10] loss: 0.163\n",
            "[58,    20] loss: 0.157\n",
            "[58,    30] loss: 0.081\n",
            "[58,    40] loss: 0.109\n",
            "[59,    10] loss: 0.099\n",
            "[59,    20] loss: 0.058\n",
            "[59,    30] loss: 0.045\n",
            "[59,    40] loss: 0.028\n",
            "[60,    10] loss: 0.015\n",
            "[60,    20] loss: 0.016\n",
            "[60,    30] loss: 0.008\n",
            "[60,    40] loss: 0.011\n",
            "[61,    10] loss: 0.006\n",
            "[61,    20] loss: 0.007\n",
            "[61,    30] loss: 0.004\n",
            "[61,    40] loss: 0.004\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.003\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.007\n",
            "[63,    10] loss: 0.007\n",
            "[63,    20] loss: 0.006\n",
            "[63,    30] loss: 0.005\n",
            "[63,    40] loss: 0.004\n",
            "[64,    10] loss: 0.002\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.003\n",
            "[65,    10] loss: 0.003\n",
            "[65,    20] loss: 0.003\n",
            "[65,    30] loss: 0.003\n",
            "[65,    40] loss: 0.015\n",
            "[66,    10] loss: 0.059\n",
            "[66,    20] loss: 0.046\n",
            "[66,    30] loss: 0.023\n",
            "[66,    40] loss: 0.048\n",
            "[67,    10] loss: 0.124\n",
            "[67,    20] loss: 0.064\n",
            "[67,    30] loss: 0.035\n",
            "[67,    40] loss: 0.041\n",
            "[68,    10] loss: 0.017\n",
            "[68,    20] loss: 0.014\n",
            "[68,    30] loss: 0.008\n",
            "[68,    40] loss: 0.007\n",
            "[69,    10] loss: 0.004\n",
            "[69,    20] loss: 0.003\n",
            "[69,    30] loss: 0.003\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.002\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 0.884\n",
            "[1,    20] loss: 0.604\n",
            "[1,    30] loss: 0.522\n",
            "[1,    40] loss: 0.485\n",
            "[2,    10] loss: 0.478\n",
            "[2,    20] loss: 0.394\n",
            "[2,    30] loss: 0.365\n",
            "[2,    40] loss: 0.336\n",
            "[3,    10] loss: 0.337\n",
            "[3,    20] loss: 0.319\n",
            "[3,    30] loss: 0.274\n",
            "[3,    40] loss: 0.271\n",
            "[4,    10] loss: 0.261\n",
            "[4,    20] loss: 0.237\n",
            "[4,    30] loss: 0.221\n",
            "[4,    40] loss: 0.215\n",
            "[5,    10] loss: 0.212\n",
            "[5,    20] loss: 0.200\n",
            "[5,    30] loss: 0.197\n",
            "[5,    40] loss: 0.188\n",
            "[6,    10] loss: 0.168\n",
            "[6,    20] loss: 0.145\n",
            "[6,    30] loss: 0.121\n",
            "[6,    40] loss: 0.146\n",
            "[7,    10] loss: 0.152\n",
            "[7,    20] loss: 0.114\n",
            "[7,    30] loss: 0.106\n",
            "[7,    40] loss: 0.112\n",
            "[8,    10] loss: 0.116\n",
            "[8,    20] loss: 0.129\n",
            "[8,    30] loss: 0.095\n",
            "[8,    40] loss: 0.083\n",
            "[9,    10] loss: 0.066\n",
            "[9,    20] loss: 0.054\n",
            "[9,    30] loss: 0.045\n",
            "[9,    40] loss: 0.070\n",
            "[10,    10] loss: 0.252\n",
            "[10,    20] loss: 0.249\n",
            "[10,    30] loss: 0.171\n",
            "[10,    40] loss: 0.192\n",
            "[11,    10] loss: 0.183\n",
            "[11,    20] loss: 0.135\n",
            "[11,    30] loss: 0.109\n",
            "[11,    40] loss: 0.094\n",
            "[12,    10] loss: 0.086\n",
            "[12,    20] loss: 0.064\n",
            "[12,    30] loss: 0.056\n",
            "[12,    40] loss: 0.047\n",
            "[13,    10] loss: 0.048\n",
            "[13,    20] loss: 0.038\n",
            "[13,    30] loss: 0.031\n",
            "[13,    40] loss: 0.027\n",
            "[14,    10] loss: 0.021\n",
            "[14,    20] loss: 0.023\n",
            "[14,    30] loss: 0.019\n",
            "[14,    40] loss: 0.021\n",
            "[15,    10] loss: 0.023\n",
            "[15,    20] loss: 0.013\n",
            "[15,    30] loss: 0.011\n",
            "[15,    40] loss: 0.010\n",
            "[16,    10] loss: 0.006\n",
            "[16,    20] loss: 0.005\n",
            "[16,    30] loss: 0.005\n",
            "[16,    40] loss: 0.005\n",
            "[17,    10] loss: 0.005\n",
            "[17,    20] loss: 0.006\n",
            "[17,    30] loss: 0.003\n",
            "[17,    40] loss: 0.044\n",
            "[18,    10] loss: 0.160\n",
            "[18,    20] loss: 0.126\n",
            "[18,    30] loss: 0.098\n",
            "[18,    40] loss: 0.077\n",
            "[19,    10] loss: 0.046\n",
            "[19,    20] loss: 0.037\n",
            "[19,    30] loss: 0.028\n",
            "[19,    40] loss: 0.025\n",
            "[20,    10] loss: 0.011\n",
            "[20,    20] loss: 0.011\n",
            "[20,    30] loss: 0.008\n",
            "[20,    40] loss: 0.008\n",
            "[21,    10] loss: 0.005\n",
            "[21,    20] loss: 0.005\n",
            "[21,    30] loss: 0.005\n",
            "[21,    40] loss: 0.003\n",
            "[22,    10] loss: 0.002\n",
            "[22,    20] loss: 0.002\n",
            "[22,    30] loss: 0.002\n",
            "[22,    40] loss: 0.004\n",
            "[23,    10] loss: 0.003\n",
            "[23,    20] loss: 0.003\n",
            "[23,    30] loss: 0.002\n",
            "[23,    40] loss: 0.002\n",
            "[24,    10] loss: 0.001\n",
            "[24,    20] loss: 0.002\n",
            "[24,    30] loss: 0.001\n",
            "[24,    40] loss: 0.001\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.001\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.003\n",
            "[26,    10] loss: 0.004\n",
            "[26,    20] loss: 0.002\n",
            "[26,    30] loss: 0.003\n",
            "[26,    40] loss: 0.002\n",
            "[27,    10] loss: 0.002\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.003\n",
            "[28,    10] loss: 0.003\n",
            "[28,    20] loss: 0.002\n",
            "[28,    30] loss: 0.002\n",
            "[28,    40] loss: 0.002\n",
            "[29,    10] loss: 0.001\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.002\n",
            "[30,    10] loss: 0.005\n",
            "[30,    20] loss: 0.003\n",
            "[30,    30] loss: 0.003\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.003\n",
            "[31,    20] loss: 0.003\n",
            "[31,    30] loss: 0.002\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.008\n",
            "[36,    10] loss: 0.038\n",
            "[36,    20] loss: 0.019\n",
            "[36,    30] loss: 0.019\n",
            "[36,    40] loss: 0.011\n",
            "[37,    10] loss: 0.006\n",
            "[37,    20] loss: 0.006\n",
            "[37,    30] loss: 0.004\n",
            "[37,    40] loss: 0.003\n",
            "[38,    10] loss: 0.002\n",
            "[38,    20] loss: 0.002\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.006\n",
            "[39,    10] loss: 0.021\n",
            "[39,    20] loss: 0.012\n",
            "[39,    30] loss: 0.011\n",
            "[39,    40] loss: 0.021\n",
            "[40,    10] loss: 0.067\n",
            "[40,    20] loss: 0.037\n",
            "[40,    30] loss: 0.037\n",
            "[40,    40] loss: 0.030\n",
            "[41,    10] loss: 0.012\n",
            "[41,    20] loss: 0.008\n",
            "[41,    30] loss: 0.008\n",
            "[41,    40] loss: 0.006\n",
            "[42,    10] loss: 0.002\n",
            "[42,    20] loss: 0.002\n",
            "[42,    30] loss: 0.002\n",
            "[42,    40] loss: 0.002\n",
            "[43,    10] loss: 0.002\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.001\n",
            "[45,    10] loss: 0.001\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.000\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.000\n",
            "[48,    10] loss: 0.000\n",
            "[48,    20] loss: 0.000\n",
            "[48,    30] loss: 0.000\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.000\n",
            "[49,    20] loss: 0.000\n",
            "[49,    30] loss: 0.000\n",
            "[49,    40] loss: 0.000\n",
            "[50,    10] loss: 0.000\n",
            "[50,    20] loss: 0.000\n",
            "[50,    30] loss: 0.000\n",
            "[50,    40] loss: 0.001\n",
            "[51,    10] loss: 0.000\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.000\n",
            "[52,    10] loss: 0.000\n",
            "[52,    20] loss: 0.000\n",
            "[52,    30] loss: 0.000\n",
            "[52,    40] loss: 0.000\n",
            "[53,    10] loss: 0.000\n",
            "[53,    20] loss: 0.000\n",
            "[53,    30] loss: 0.000\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.000\n",
            "[54,    20] loss: 0.000\n",
            "[54,    30] loss: 0.000\n",
            "[54,    40] loss: 0.017\n",
            "[55,    10] loss: 0.198\n",
            "[55,    20] loss: 0.107\n",
            "[55,    30] loss: 0.079\n",
            "[55,    40] loss: 0.071\n",
            "[56,    10] loss: 0.177\n",
            "[56,    20] loss: 0.087\n",
            "[56,    30] loss: 0.068\n",
            "[56,    40] loss: 0.059\n",
            "[57,    10] loss: 0.039\n",
            "[57,    20] loss: 0.027\n",
            "[57,    30] loss: 0.021\n",
            "[57,    40] loss: 0.018\n",
            "[58,    10] loss: 0.008\n",
            "[58,    20] loss: 0.004\n",
            "[58,    30] loss: 0.008\n",
            "[58,    40] loss: 0.005\n",
            "[59,    10] loss: 0.003\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.003\n",
            "[59,    40] loss: 0.010\n",
            "[60,    10] loss: 0.041\n",
            "[60,    20] loss: 0.039\n",
            "[60,    30] loss: 0.020\n",
            "[60,    40] loss: 0.078\n",
            "[61,    10] loss: 0.160\n",
            "[61,    20] loss: 0.121\n",
            "[61,    30] loss: 0.072\n",
            "[61,    40] loss: 0.066\n",
            "[62,    10] loss: 0.074\n",
            "[62,    20] loss: 0.054\n",
            "[62,    30] loss: 0.042\n",
            "[62,    40] loss: 0.019\n",
            "[63,    10] loss: 0.012\n",
            "[63,    20] loss: 0.009\n",
            "[63,    30] loss: 0.007\n",
            "[63,    40] loss: 0.018\n",
            "[64,    10] loss: 0.072\n",
            "[64,    20] loss: 0.047\n",
            "[64,    30] loss: 0.025\n",
            "[64,    40] loss: 0.027\n",
            "[65,    10] loss: 0.022\n",
            "[65,    20] loss: 0.016\n",
            "[65,    30] loss: 0.015\n",
            "[65,    40] loss: 0.007\n",
            "[66,    10] loss: 0.004\n",
            "[66,    20] loss: 0.003\n",
            "[66,    30] loss: 0.006\n",
            "[66,    40] loss: 0.021\n",
            "[67,    10] loss: 0.044\n",
            "[67,    20] loss: 0.045\n",
            "[67,    30] loss: 0.015\n",
            "[67,    40] loss: 0.017\n",
            "[68,    10] loss: 0.008\n",
            "[68,    20] loss: 0.007\n",
            "[68,    30] loss: 0.005\n",
            "[68,    40] loss: 0.005\n",
            "[69,    10] loss: 0.002\n",
            "[69,    20] loss: 0.002\n",
            "[69,    30] loss: 0.002\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 0.913\n",
            "[1,    20] loss: 0.628\n",
            "[1,    30] loss: 0.499\n",
            "[1,    40] loss: 0.467\n",
            "[2,    10] loss: 0.467\n",
            "[2,    20] loss: 0.385\n",
            "[2,    30] loss: 0.394\n",
            "[2,    40] loss: 0.379\n",
            "[3,    10] loss: 0.360\n",
            "[3,    20] loss: 0.306\n",
            "[3,    30] loss: 0.303\n",
            "[3,    40] loss: 0.252\n",
            "[4,    10] loss: 0.226\n",
            "[4,    20] loss: 0.213\n",
            "[4,    30] loss: 0.210\n",
            "[4,    40] loss: 0.271\n",
            "[5,    10] loss: 0.246\n",
            "[5,    20] loss: 0.237\n",
            "[5,    30] loss: 0.218\n",
            "[5,    40] loss: 0.190\n",
            "[6,    10] loss: 0.170\n",
            "[6,    20] loss: 0.151\n",
            "[6,    30] loss: 0.148\n",
            "[6,    40] loss: 0.192\n",
            "[7,    10] loss: 0.343\n",
            "[7,    20] loss: 0.274\n",
            "[7,    30] loss: 0.230\n",
            "[7,    40] loss: 0.240\n",
            "[8,    10] loss: 0.213\n",
            "[8,    20] loss: 0.167\n",
            "[8,    30] loss: 0.155\n",
            "[8,    40] loss: 0.176\n",
            "[9,    10] loss: 0.153\n",
            "[9,    20] loss: 0.139\n",
            "[9,    30] loss: 0.112\n",
            "[9,    40] loss: 0.108\n",
            "[10,    10] loss: 0.087\n",
            "[10,    20] loss: 0.079\n",
            "[10,    30] loss: 0.068\n",
            "[10,    40] loss: 0.062\n",
            "[11,    10] loss: 0.039\n",
            "[11,    20] loss: 0.035\n",
            "[11,    30] loss: 0.026\n",
            "[11,    40] loss: 0.026\n",
            "[12,    10] loss: 0.018\n",
            "[12,    20] loss: 0.012\n",
            "[12,    30] loss: 0.015\n",
            "[12,    40] loss: 0.012\n",
            "[13,    10] loss: 0.008\n",
            "[13,    20] loss: 0.008\n",
            "[13,    30] loss: 0.007\n",
            "[13,    40] loss: 0.007\n",
            "[14,    10] loss: 0.005\n",
            "[14,    20] loss: 0.005\n",
            "[14,    30] loss: 0.004\n",
            "[14,    40] loss: 0.005\n",
            "[15,    10] loss: 0.003\n",
            "[15,    20] loss: 0.003\n",
            "[15,    30] loss: 0.003\n",
            "[15,    40] loss: 0.015\n",
            "[16,    10] loss: 0.081\n",
            "[16,    20] loss: 0.056\n",
            "[16,    30] loss: 0.050\n",
            "[16,    40] loss: 0.054\n",
            "[17,    10] loss: 0.104\n",
            "[17,    20] loss: 0.074\n",
            "[17,    30] loss: 0.058\n",
            "[17,    40] loss: 0.047\n",
            "[18,    10] loss: 0.058\n",
            "[18,    20] loss: 0.055\n",
            "[18,    30] loss: 0.031\n",
            "[18,    40] loss: 0.033\n",
            "[19,    10] loss: 0.042\n",
            "[19,    20] loss: 0.039\n",
            "[19,    30] loss: 0.029\n",
            "[19,    40] loss: 0.035\n",
            "[20,    10] loss: 0.081\n",
            "[20,    20] loss: 0.065\n",
            "[20,    30] loss: 0.040\n",
            "[20,    40] loss: 0.037\n",
            "[21,    10] loss: 0.088\n",
            "[21,    20] loss: 0.048\n",
            "[21,    30] loss: 0.038\n",
            "[21,    40] loss: 0.038\n",
            "[22,    10] loss: 0.057\n",
            "[22,    20] loss: 0.045\n",
            "[22,    30] loss: 0.028\n",
            "[22,    40] loss: 0.025\n",
            "[23,    10] loss: 0.012\n",
            "[23,    20] loss: 0.011\n",
            "[23,    30] loss: 0.009\n",
            "[23,    40] loss: 0.009\n",
            "[24,    10] loss: 0.004\n",
            "[24,    20] loss: 0.003\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.003\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.002\n",
            "[25,    30] loss: 0.002\n",
            "[25,    40] loss: 0.007\n",
            "[26,    10] loss: 0.016\n",
            "[26,    20] loss: 0.009\n",
            "[26,    30] loss: 0.008\n",
            "[26,    40] loss: 0.006\n",
            "[27,    10] loss: 0.009\n",
            "[27,    20] loss: 0.005\n",
            "[27,    30] loss: 0.004\n",
            "[27,    40] loss: 0.017\n",
            "[28,    10] loss: 0.061\n",
            "[28,    20] loss: 0.055\n",
            "[28,    30] loss: 0.032\n",
            "[28,    40] loss: 0.032\n",
            "[29,    10] loss: 0.054\n",
            "[29,    20] loss: 0.033\n",
            "[29,    30] loss: 0.019\n",
            "[29,    40] loss: 0.013\n",
            "[30,    10] loss: 0.007\n",
            "[30,    20] loss: 0.005\n",
            "[30,    30] loss: 0.004\n",
            "[30,    40] loss: 0.005\n",
            "[31,    10] loss: 0.002\n",
            "[31,    20] loss: 0.002\n",
            "[31,    30] loss: 0.002\n",
            "[31,    40] loss: 0.002\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.004\n",
            "[33,    10] loss: 0.009\n",
            "[33,    20] loss: 0.004\n",
            "[33,    30] loss: 0.004\n",
            "[33,    40] loss: 0.006\n",
            "[34,    10] loss: 0.010\n",
            "[34,    20] loss: 0.006\n",
            "[34,    30] loss: 0.004\n",
            "[34,    40] loss: 0.005\n",
            "[35,    10] loss: 0.008\n",
            "[35,    20] loss: 0.003\n",
            "[35,    30] loss: 0.004\n",
            "[35,    40] loss: 0.007\n",
            "[36,    10] loss: 0.018\n",
            "[36,    20] loss: 0.011\n",
            "[36,    30] loss: 0.008\n",
            "[36,    40] loss: 0.005\n",
            "[37,    10] loss: 0.004\n",
            "[37,    20] loss: 0.002\n",
            "[37,    30] loss: 0.002\n",
            "[37,    40] loss: 0.001\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.001\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.001\n",
            "[39,    10] loss: 0.001\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.000\n",
            "[39,    40] loss: 0.001\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.002\n",
            "[41,    10] loss: 0.001\n",
            "[41,    20] loss: 0.002\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.091\n",
            "[42,    10] loss: 0.317\n",
            "[42,    20] loss: 0.292\n",
            "[42,    30] loss: 0.168\n",
            "[42,    40] loss: 0.157\n",
            "[43,    10] loss: 0.151\n",
            "[43,    20] loss: 0.087\n",
            "[43,    30] loss: 0.080\n",
            "[43,    40] loss: 0.048\n",
            "[44,    10] loss: 0.028\n",
            "[44,    20] loss: 0.019\n",
            "[44,    30] loss: 0.022\n",
            "[44,    40] loss: 0.034\n",
            "[45,    10] loss: 0.060\n",
            "[45,    20] loss: 0.048\n",
            "[45,    30] loss: 0.033\n",
            "[45,    40] loss: 0.034\n",
            "[46,    10] loss: 0.029\n",
            "[46,    20] loss: 0.023\n",
            "[46,    30] loss: 0.018\n",
            "[46,    40] loss: 0.009\n",
            "[47,    10] loss: 0.008\n",
            "[47,    20] loss: 0.004\n",
            "[47,    30] loss: 0.005\n",
            "[47,    40] loss: 0.022\n",
            "[48,    10] loss: 0.028\n",
            "[48,    20] loss: 0.020\n",
            "[48,    30] loss: 0.015\n",
            "[48,    40] loss: 0.009\n",
            "[49,    10] loss: 0.005\n",
            "[49,    20] loss: 0.005\n",
            "[49,    30] loss: 0.003\n",
            "[49,    40] loss: 0.004\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.002\n",
            "[50,    30] loss: 0.002\n",
            "[50,    40] loss: 0.006\n",
            "[51,    10] loss: 0.010\n",
            "[51,    20] loss: 0.009\n",
            "[51,    30] loss: 0.007\n",
            "[51,    40] loss: 0.009\n",
            "[52,    10] loss: 0.021\n",
            "[52,    20] loss: 0.011\n",
            "[52,    30] loss: 0.006\n",
            "[52,    40] loss: 0.007\n",
            "[53,    10] loss: 0.003\n",
            "[53,    20] loss: 0.004\n",
            "[53,    30] loss: 0.003\n",
            "[53,    40] loss: 0.002\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.002\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.001\n",
            "[56,    40] loss: 0.001\n",
            "[57,    10] loss: 0.001\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.007\n",
            "[58,    10] loss: 0.010\n",
            "[58,    20] loss: 0.017\n",
            "[58,    30] loss: 0.007\n",
            "[58,    40] loss: 0.022\n",
            "[59,    10] loss: 0.055\n",
            "[59,    20] loss: 0.059\n",
            "[59,    30] loss: 0.047\n",
            "[59,    40] loss: 0.031\n",
            "[60,    10] loss: 0.019\n",
            "[60,    20] loss: 0.009\n",
            "[60,    30] loss: 0.006\n",
            "[60,    40] loss: 0.005\n",
            "[61,    10] loss: 0.003\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.033\n",
            "[65,    10] loss: 0.114\n",
            "[65,    20] loss: 0.084\n",
            "[65,    30] loss: 0.058\n",
            "[65,    40] loss: 0.068\n",
            "[66,    10] loss: 0.077\n",
            "[66,    20] loss: 0.062\n",
            "[66,    30] loss: 0.042\n",
            "[66,    40] loss: 0.028\n",
            "[67,    10] loss: 0.021\n",
            "[67,    20] loss: 0.016\n",
            "[67,    30] loss: 0.008\n",
            "[67,    40] loss: 0.008\n",
            "[68,    10] loss: 0.004\n",
            "[68,    20] loss: 0.002\n",
            "[68,    30] loss: 0.002\n",
            "[68,    40] loss: 0.006\n",
            "[69,    10] loss: 0.005\n",
            "[69,    20] loss: 0.004\n",
            "[69,    30] loss: 0.003\n",
            "[69,    40] loss: 0.024\n",
            "[70,    10] loss: 0.054\n",
            "[70,    20] loss: 0.048\n",
            "[70,    30] loss: 0.023\n",
            "[70,    40] loss: 0.051\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AbZaQekCfVjN",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ouBomi5DfVjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3228c371-0da4-4d3f-9c87-deb3ae0148b4"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend()\n",
        "fig.savefig(\"Figure.pdf\")\n",
        "fig.savefig(\"Figure.png\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3P8AT9XOmovyzwoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlY1RknNfVjb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uMVKmRRhfVjd",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}