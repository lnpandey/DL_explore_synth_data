{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Focus_net_Dirichlet_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjG64ra4aFu",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8-7SARDZErK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwJv7Y8Rewez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8ab34cff-e9bf-4965-f08e-0da0420faabe"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    \n",
        "  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 169828352/170498071 [00:11<00:00, 16719711.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:30, 16719711.41it/s]                               "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nDYhjJse6Qq",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aivGVg14e9iZ",
        "colab": {}
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h6Jy35SSfBS9",
        "colab": {}
      },
      "source": [
        "# dict = {\"mosaic_list_of_images\": mosaic_list_of_images, \"mosaic_label\": mosaic_label , \"fore_idx\":fore_idx}\n",
        "# f = open(\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl\",\"wb\")\n",
        "# pickle.dump(dict,f)\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIuiboIUfViV"
      },
      "source": [
        "# Load data from saved file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbYf6zuBfViX",
        "colab": {}
      },
      "source": [
        "# with open('/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl', 'rb') as f:\n",
        "#     data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwZjDB5lfVib",
        "colab": {}
      },
      "source": [
        "# mosaic_list_of_images = data[\"mosaic_list_of_images\"]\n",
        "# mosaic_label = data[\"mosaic_label\"]\n",
        "# fore_idx = data[\"fore_idx\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cog5VUzGgE5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9eb3b5dc-6dd8-48e1-fd6e-b4c27f6df25f"
      },
      "source": [
        "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
        "print(len(fore_idx))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000 10000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xX91RwMy-IP4",
        "colab": {}
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUNla6YuZwMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset_dirichlet(mosaic_dataset,labels,foreground_index,a_value):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  alpha = 0.5\n",
        "  dirichlet_sampled_weight = np.random.dirichlet([a_value*alpha, alpha, alpha, alpha, alpha, alpha, alpha, alpha, alpha],size =10000)\n",
        "\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    c = 1\n",
        "    for j in range(9):\n",
        "       \n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dirichlet_sampled_weight[i][0]\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*dirichlet_sampled_weight[i][c]\n",
        "        c+=1\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGz8Y88vIZPT",
        "colab": {}
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 16/7)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 32/5)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 10)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 16)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 28)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset_dirichlet(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 64)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
        "\n",
        "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSO9SFE25Lrk",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obE1xeyRks1Q",
        "colab": {}
      },
      "source": [
        "batch = 256\n",
        "\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
        "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SadRzWBBZEsP",
        "colab": {}
      },
      "source": [
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IgGYMG_ZZEsT",
        "colab": {}
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "        #self.conv1 = nn.Conv2d(3,self.f0,1)\n",
        "        #self.conv3 = nn.Conv2d(3,self.f1,3,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)\n",
        "        \n",
        "        x = torch.cat((x1,x3),dim=1)\n",
        "        \n",
        "    \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thkdqW91Hpju",
        "colab": {}
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,2,3,pad=0)\n",
        "        self.pool = nn.MaxPool2d(3,stride=2,padding=0)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x,x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1yVWgR4vFhe",
        "colab": {}
      },
      "source": [
        "class inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,96,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(96,32,32)\n",
        "        self.incept2 = inception_module(64,32,48)\n",
        "        \n",
        "        self.downsample1 = downsample_module(80,80)\n",
        "        \n",
        "        self.incept3 = inception_module(160,112,48)\n",
        "        self.incept4 = inception_module(160,96,64)\n",
        "        self.incept5 = inception_module(160,80,80)\n",
        "        self.incept6 = inception_module(160,48,96)\n",
        "        \n",
        "        self.downsample2 = downsample_module(144,96)\n",
        "        \n",
        "        self.incept7 = inception_module(240,176,60)\n",
        "        self.incept8 = inception_module(236,176,60)\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(5)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.linear = nn.Linear(236,3)\n",
        "    def forward(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        #act1 = x\n",
        "        \n",
        "        x = self.incept1.forward(x)\n",
        "        #act2 = x\n",
        "        \n",
        "        x = self.incept2.forward(x)\n",
        "        #act3 = x\n",
        "        \n",
        "        x,act4 = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        #act5 = x\n",
        "        \n",
        "        x = self.incept4.forward(x)\n",
        "        #act6 = x\n",
        "        \n",
        "        x = self.incept5.forward(x)\n",
        "        #act7 = x\n",
        "        \n",
        "        x = self.incept6.forward(x)\n",
        "        #act8 = x\n",
        "        \n",
        "        x,act9 = self.downsample2.forward(x)\n",
        "        \n",
        "        x = self.incept7.forward(x)\n",
        "        #act10 = x\n",
        "        x = self.incept8.forward(x)\n",
        "        #act11 = x\n",
        "        #print(x.shape)\n",
        "        x = self.pool(x)\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1,1*1*236)\n",
        "        x = self.linear(x) \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cOWrnzv1fVjD",
        "colab": {}
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFfAJZkcZEsY",
        "colab": {}
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = inception_net().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 70\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "    #     if (epoch%5 == 0):\n",
        "    #         _,actis= inc(inputs)\n",
        "    #         acti.append(actis)\n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/focus_weights/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mI-vqhB-fVjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "984ef3ab-f682-4d59-9f4e-bf3b27efa16c"
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.111\n",
            "[1,    20] loss: 1.106\n",
            "[1,    30] loss: 1.088\n",
            "[1,    40] loss: 1.104\n",
            "[2,    10] loss: 1.084\n",
            "[2,    20] loss: 1.084\n",
            "[2,    30] loss: 1.078\n",
            "[2,    40] loss: 1.073\n",
            "[3,    10] loss: 1.077\n",
            "[3,    20] loss: 1.077\n",
            "[3,    30] loss: 1.071\n",
            "[3,    40] loss: 1.063\n",
            "[4,    10] loss: 1.057\n",
            "[4,    20] loss: 1.049\n",
            "[4,    30] loss: 1.050\n",
            "[4,    40] loss: 1.062\n",
            "[5,    10] loss: 1.040\n",
            "[5,    20] loss: 1.039\n",
            "[5,    30] loss: 1.033\n",
            "[5,    40] loss: 1.028\n",
            "[6,    10] loss: 1.003\n",
            "[6,    20] loss: 1.009\n",
            "[6,    30] loss: 1.008\n",
            "[6,    40] loss: 1.045\n",
            "[7,    10] loss: 0.995\n",
            "[7,    20] loss: 1.003\n",
            "[7,    30] loss: 0.980\n",
            "[7,    40] loss: 0.967\n",
            "[8,    10] loss: 0.950\n",
            "[8,    20] loss: 0.959\n",
            "[8,    30] loss: 0.957\n",
            "[8,    40] loss: 0.964\n",
            "[9,    10] loss: 0.926\n",
            "[9,    20] loss: 0.941\n",
            "[9,    30] loss: 0.965\n",
            "[9,    40] loss: 0.957\n",
            "[10,    10] loss: 0.852\n",
            "[10,    20] loss: 0.862\n",
            "[10,    30] loss: 0.833\n",
            "[10,    40] loss: 0.869\n",
            "[11,    10] loss: 0.814\n",
            "[11,    20] loss: 0.836\n",
            "[11,    30] loss: 0.850\n",
            "[11,    40] loss: 0.883\n",
            "[12,    10] loss: 0.745\n",
            "[12,    20] loss: 0.752\n",
            "[12,    30] loss: 0.783\n",
            "[12,    40] loss: 0.766\n",
            "[13,    10] loss: 0.700\n",
            "[13,    20] loss: 0.773\n",
            "[13,    30] loss: 0.737\n",
            "[13,    40] loss: 0.724\n",
            "[14,    10] loss: 0.605\n",
            "[14,    20] loss: 0.599\n",
            "[14,    30] loss: 0.586\n",
            "[14,    40] loss: 0.626\n",
            "[15,    10] loss: 0.569\n",
            "[15,    20] loss: 0.591\n",
            "[15,    30] loss: 0.561\n",
            "[15,    40] loss: 0.533\n",
            "[16,    10] loss: 0.486\n",
            "[16,    20] loss: 0.509\n",
            "[16,    30] loss: 0.506\n",
            "[16,    40] loss: 0.514\n",
            "[17,    10] loss: 0.419\n",
            "[17,    20] loss: 0.484\n",
            "[17,    30] loss: 0.435\n",
            "[17,    40] loss: 0.458\n",
            "[18,    10] loss: 0.712\n",
            "[18,    20] loss: 0.707\n",
            "[18,    30] loss: 0.664\n",
            "[18,    40] loss: 0.647\n",
            "[19,    10] loss: 0.446\n",
            "[19,    20] loss: 0.404\n",
            "[19,    30] loss: 0.337\n",
            "[19,    40] loss: 0.312\n",
            "[20,    10] loss: 0.276\n",
            "[20,    20] loss: 0.277\n",
            "[20,    30] loss: 0.246\n",
            "[20,    40] loss: 0.260\n",
            "[21,    10] loss: 0.244\n",
            "[21,    20] loss: 0.255\n",
            "[21,    30] loss: 0.241\n",
            "[21,    40] loss: 0.230\n",
            "[22,    10] loss: 0.210\n",
            "[22,    20] loss: 0.256\n",
            "[22,    30] loss: 0.227\n",
            "[22,    40] loss: 0.209\n",
            "[23,    10] loss: 0.191\n",
            "[23,    20] loss: 0.253\n",
            "[23,    30] loss: 0.209\n",
            "[23,    40] loss: 0.168\n",
            "[24,    10] loss: 0.106\n",
            "[24,    20] loss: 0.093\n",
            "[24,    30] loss: 0.083\n",
            "[24,    40] loss: 0.107\n",
            "[25,    10] loss: 0.297\n",
            "[25,    20] loss: 0.348\n",
            "[25,    30] loss: 0.275\n",
            "[25,    40] loss: 0.329\n",
            "[26,    10] loss: 0.266\n",
            "[26,    20] loss: 0.297\n",
            "[26,    30] loss: 0.278\n",
            "[26,    40] loss: 0.227\n",
            "[27,    10] loss: 0.140\n",
            "[27,    20] loss: 0.115\n",
            "[27,    30] loss: 0.085\n",
            "[27,    40] loss: 0.081\n",
            "[28,    10] loss: 0.039\n",
            "[28,    20] loss: 0.032\n",
            "[28,    30] loss: 0.027\n",
            "[28,    40] loss: 0.165\n",
            "[29,    10] loss: 0.414\n",
            "[29,    20] loss: 0.514\n",
            "[29,    30] loss: 0.438\n",
            "[29,    40] loss: 0.398\n",
            "[30,    10] loss: 0.294\n",
            "[30,    20] loss: 0.261\n",
            "[30,    30] loss: 0.200\n",
            "[30,    40] loss: 0.181\n",
            "[31,    10] loss: 0.164\n",
            "[31,    20] loss: 0.165\n",
            "[31,    30] loss: 0.121\n",
            "[31,    40] loss: 0.102\n",
            "[32,    10] loss: 0.106\n",
            "[32,    20] loss: 0.099\n",
            "[32,    30] loss: 0.090\n",
            "[32,    40] loss: 0.068\n",
            "[33,    10] loss: 0.037\n",
            "[33,    20] loss: 0.042\n",
            "[33,    30] loss: 0.032\n",
            "[33,    40] loss: 0.064\n",
            "[34,    10] loss: 0.060\n",
            "[34,    20] loss: 0.072\n",
            "[34,    30] loss: 0.046\n",
            "[34,    40] loss: 0.038\n",
            "[35,    10] loss: 0.016\n",
            "[35,    20] loss: 0.015\n",
            "[35,    30] loss: 0.011\n",
            "[35,    40] loss: 0.016\n",
            "[36,    10] loss: 0.011\n",
            "[36,    20] loss: 0.014\n",
            "[36,    30] loss: 0.012\n",
            "[36,    40] loss: 0.031\n",
            "[37,    10] loss: 0.089\n",
            "[37,    20] loss: 0.145\n",
            "[37,    30] loss: 0.095\n",
            "[37,    40] loss: 0.083\n",
            "[38,    10] loss: 0.084\n",
            "[38,    20] loss: 0.081\n",
            "[38,    30] loss: 0.060\n",
            "[38,    40] loss: 0.052\n",
            "[39,    10] loss: 0.031\n",
            "[39,    20] loss: 0.029\n",
            "[39,    30] loss: 0.026\n",
            "[39,    40] loss: 0.026\n",
            "[40,    10] loss: 0.021\n",
            "[40,    20] loss: 0.017\n",
            "[40,    30] loss: 0.017\n",
            "[40,    40] loss: 0.015\n",
            "[41,    10] loss: 0.015\n",
            "[41,    20] loss: 0.017\n",
            "[41,    30] loss: 0.018\n",
            "[41,    40] loss: 0.015\n",
            "[42,    10] loss: 0.010\n",
            "[42,    20] loss: 0.010\n",
            "[42,    30] loss: 0.009\n",
            "[42,    40] loss: 0.022\n",
            "[43,    10] loss: 0.046\n",
            "[43,    20] loss: 0.087\n",
            "[43,    30] loss: 0.055\n",
            "[43,    40] loss: 0.064\n",
            "[44,    10] loss: 0.051\n",
            "[44,    20] loss: 0.077\n",
            "[44,    30] loss: 0.050\n",
            "[44,    40] loss: 0.042\n",
            "[45,    10] loss: 0.021\n",
            "[45,    20] loss: 0.017\n",
            "[45,    30] loss: 0.016\n",
            "[45,    40] loss: 0.009\n",
            "[46,    10] loss: 0.005\n",
            "[46,    20] loss: 0.004\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.004\n",
            "[47,    10] loss: 0.002\n",
            "[47,    20] loss: 0.002\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.026\n",
            "[48,    10] loss: 0.089\n",
            "[48,    20] loss: 0.154\n",
            "[48,    30] loss: 0.096\n",
            "[48,    40] loss: 0.082\n",
            "[49,    10] loss: 0.040\n",
            "[49,    20] loss: 0.041\n",
            "[49,    30] loss: 0.029\n",
            "[49,    40] loss: 0.036\n",
            "[50,    10] loss: 0.012\n",
            "[50,    20] loss: 0.015\n",
            "[50,    30] loss: 0.011\n",
            "[50,    40] loss: 0.022\n",
            "[51,    10] loss: 0.037\n",
            "[51,    20] loss: 0.078\n",
            "[51,    30] loss: 0.061\n",
            "[51,    40] loss: 0.071\n",
            "[52,    10] loss: 0.099\n",
            "[52,    20] loss: 0.125\n",
            "[52,    30] loss: 0.082\n",
            "[52,    40] loss: 0.070\n",
            "[53,    10] loss: 0.030\n",
            "[53,    20] loss: 0.025\n",
            "[53,    30] loss: 0.029\n",
            "[53,    40] loss: 0.025\n",
            "[54,    10] loss: 0.027\n",
            "[54,    20] loss: 0.022\n",
            "[54,    30] loss: 0.023\n",
            "[54,    40] loss: 0.016\n",
            "[55,    10] loss: 0.010\n",
            "[55,    20] loss: 0.008\n",
            "[55,    30] loss: 0.004\n",
            "[55,    40] loss: 0.012\n",
            "[56,    10] loss: 0.046\n",
            "[56,    20] loss: 0.056\n",
            "[56,    30] loss: 0.048\n",
            "[56,    40] loss: 0.038\n",
            "[57,    10] loss: 0.047\n",
            "[57,    20] loss: 0.065\n",
            "[57,    30] loss: 0.051\n",
            "[57,    40] loss: 0.055\n",
            "[58,    10] loss: 0.084\n",
            "[58,    20] loss: 0.107\n",
            "[58,    30] loss: 0.086\n",
            "[58,    40] loss: 0.077\n",
            "[59,    10] loss: 0.100\n",
            "[59,    20] loss: 0.143\n",
            "[59,    30] loss: 0.107\n",
            "[59,    40] loss: 0.094\n",
            "[60,    10] loss: 0.055\n",
            "[60,    20] loss: 0.051\n",
            "[60,    30] loss: 0.031\n",
            "[60,    40] loss: 0.028\n",
            "[61,    10] loss: 0.013\n",
            "[61,    20] loss: 0.012\n",
            "[61,    30] loss: 0.008\n",
            "[61,    40] loss: 0.006\n",
            "[62,    10] loss: 0.003\n",
            "[62,    20] loss: 0.003\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.033\n",
            "[63,    10] loss: 0.049\n",
            "[63,    20] loss: 0.119\n",
            "[63,    30] loss: 0.077\n",
            "[63,    40] loss: 0.062\n",
            "[64,    10] loss: 0.026\n",
            "[64,    20] loss: 0.023\n",
            "[64,    30] loss: 0.018\n",
            "[64,    40] loss: 0.019\n",
            "[65,    10] loss: 0.005\n",
            "[65,    20] loss: 0.005\n",
            "[65,    30] loss: 0.004\n",
            "[65,    40] loss: 0.037\n",
            "[66,    10] loss: 0.135\n",
            "[66,    20] loss: 0.242\n",
            "[66,    30] loss: 0.243\n",
            "[66,    40] loss: 0.162\n",
            "[67,    10] loss: 0.106\n",
            "[67,    20] loss: 0.074\n",
            "[67,    30] loss: 0.100\n",
            "[67,    40] loss: 0.083\n",
            "[68,    10] loss: 0.100\n",
            "[68,    20] loss: 0.114\n",
            "[68,    30] loss: 0.102\n",
            "[68,    40] loss: 0.094\n",
            "[69,    10] loss: 0.073\n",
            "[69,    20] loss: 0.083\n",
            "[69,    30] loss: 0.064\n",
            "[69,    40] loss: 0.050\n",
            "[70,    10] loss: 0.028\n",
            "[70,    20] loss: 0.032\n",
            "[70,    30] loss: 0.028\n",
            "[70,    40] loss: 0.040\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 55 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 60 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 61 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 61 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 61 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 59 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.110\n",
            "[1,    20] loss: 1.065\n",
            "[1,    30] loss: 1.041\n",
            "[1,    40] loss: 1.030\n",
            "[2,    10] loss: 1.030\n",
            "[2,    20] loss: 1.026\n",
            "[2,    30] loss: 1.007\n",
            "[2,    40] loss: 1.016\n",
            "[3,    10] loss: 1.001\n",
            "[3,    20] loss: 0.998\n",
            "[3,    30] loss: 0.988\n",
            "[3,    40] loss: 0.997\n",
            "[4,    10] loss: 0.981\n",
            "[4,    20] loss: 0.964\n",
            "[4,    30] loss: 0.978\n",
            "[4,    40] loss: 0.975\n",
            "[5,    10] loss: 0.952\n",
            "[5,    20] loss: 0.941\n",
            "[5,    30] loss: 0.930\n",
            "[5,    40] loss: 0.959\n",
            "[6,    10] loss: 0.944\n",
            "[6,    20] loss: 0.934\n",
            "[6,    30] loss: 0.922\n",
            "[6,    40] loss: 0.951\n",
            "[7,    10] loss: 0.898\n",
            "[7,    20] loss: 0.920\n",
            "[7,    30] loss: 0.885\n",
            "[7,    40] loss: 0.864\n",
            "[8,    10] loss: 0.834\n",
            "[8,    20] loss: 0.886\n",
            "[8,    30] loss: 0.841\n",
            "[8,    40] loss: 0.888\n",
            "[9,    10] loss: 0.813\n",
            "[9,    20] loss: 0.843\n",
            "[9,    30] loss: 0.844\n",
            "[9,    40] loss: 0.828\n",
            "[10,    10] loss: 0.765\n",
            "[10,    20] loss: 0.767\n",
            "[10,    30] loss: 0.725\n",
            "[10,    40] loss: 0.791\n",
            "[11,    10] loss: 0.764\n",
            "[11,    20] loss: 0.764\n",
            "[11,    30] loss: 0.762\n",
            "[11,    40] loss: 0.692\n",
            "[12,    10] loss: 0.713\n",
            "[12,    20] loss: 0.725\n",
            "[12,    30] loss: 0.714\n",
            "[12,    40] loss: 0.636\n",
            "[13,    10] loss: 0.574\n",
            "[13,    20] loss: 0.561\n",
            "[13,    30] loss: 0.545\n",
            "[13,    40] loss: 0.548\n",
            "[14,    10] loss: 0.528\n",
            "[14,    20] loss: 0.538\n",
            "[14,    30] loss: 0.561\n",
            "[14,    40] loss: 0.569\n",
            "[15,    10] loss: 0.597\n",
            "[15,    20] loss: 0.596\n",
            "[15,    30] loss: 0.556\n",
            "[15,    40] loss: 0.522\n",
            "[16,    10] loss: 0.471\n",
            "[16,    20] loss: 0.444\n",
            "[16,    30] loss: 0.381\n",
            "[16,    40] loss: 0.337\n",
            "[17,    10] loss: 0.279\n",
            "[17,    20] loss: 0.237\n",
            "[17,    30] loss: 0.247\n",
            "[17,    40] loss: 0.261\n",
            "[18,    10] loss: 0.343\n",
            "[18,    20] loss: 0.451\n",
            "[18,    30] loss: 0.399\n",
            "[18,    40] loss: 0.407\n",
            "[19,    10] loss: 0.479\n",
            "[19,    20] loss: 0.430\n",
            "[19,    30] loss: 0.388\n",
            "[19,    40] loss: 0.367\n",
            "[20,    10] loss: 0.308\n",
            "[20,    20] loss: 0.301\n",
            "[20,    30] loss: 0.256\n",
            "[20,    40] loss: 0.232\n",
            "[21,    10] loss: 0.286\n",
            "[21,    20] loss: 0.285\n",
            "[21,    30] loss: 0.271\n",
            "[21,    40] loss: 0.243\n",
            "[22,    10] loss: 0.267\n",
            "[22,    20] loss: 0.260\n",
            "[22,    30] loss: 0.202\n",
            "[22,    40] loss: 0.162\n",
            "[23,    10] loss: 0.099\n",
            "[23,    20] loss: 0.086\n",
            "[23,    30] loss: 0.082\n",
            "[23,    40] loss: 0.063\n",
            "[24,    10] loss: 0.043\n",
            "[24,    20] loss: 0.046\n",
            "[24,    30] loss: 0.030\n",
            "[24,    40] loss: 0.041\n",
            "[25,    10] loss: 0.048\n",
            "[25,    20] loss: 0.064\n",
            "[25,    30] loss: 0.052\n",
            "[25,    40] loss: 0.049\n",
            "[26,    10] loss: 0.029\n",
            "[26,    20] loss: 0.016\n",
            "[26,    30] loss: 0.015\n",
            "[26,    40] loss: 0.013\n",
            "[27,    10] loss: 0.005\n",
            "[27,    20] loss: 0.005\n",
            "[27,    30] loss: 0.004\n",
            "[27,    40] loss: 0.004\n",
            "[28,    10] loss: 0.002\n",
            "[28,    20] loss: 0.002\n",
            "[28,    30] loss: 0.002\n",
            "[28,    40] loss: 0.002\n",
            "[29,    10] loss: 0.002\n",
            "[29,    20] loss: 0.001\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.001\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.001\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.001\n",
            "[37,    40] loss: 0.004\n",
            "[38,    10] loss: 0.005\n",
            "[38,    20] loss: 0.013\n",
            "[38,    30] loss: 0.005\n",
            "[38,    40] loss: 0.003\n",
            "[39,    10] loss: 0.002\n",
            "[39,    20] loss: 0.001\n",
            "[39,    30] loss: 0.001\n",
            "[39,    40] loss: 0.001\n",
            "[40,    10] loss: 0.001\n",
            "[40,    20] loss: 0.001\n",
            "[40,    30] loss: 0.001\n",
            "[40,    40] loss: 0.003\n",
            "[41,    10] loss: 0.002\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.004\n",
            "[42,    10] loss: 0.002\n",
            "[42,    20] loss: 0.006\n",
            "[42,    30] loss: 0.006\n",
            "[42,    40] loss: 0.006\n",
            "[43,    10] loss: 0.004\n",
            "[43,    20] loss: 0.004\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.005\n",
            "[44,    10] loss: 0.007\n",
            "[44,    20] loss: 0.010\n",
            "[44,    30] loss: 0.007\n",
            "[44,    40] loss: 0.005\n",
            "[45,    10] loss: 0.002\n",
            "[45,    20] loss: 0.003\n",
            "[45,    30] loss: 0.002\n",
            "[45,    40] loss: 0.004\n",
            "[46,    10] loss: 0.011\n",
            "[46,    20] loss: 0.023\n",
            "[46,    30] loss: 0.012\n",
            "[46,    40] loss: 0.014\n",
            "[47,    10] loss: 0.008\n",
            "[47,    20] loss: 0.011\n",
            "[47,    30] loss: 0.008\n",
            "[47,    40] loss: 0.006\n",
            "[48,    10] loss: 0.003\n",
            "[48,    20] loss: 0.004\n",
            "[48,    30] loss: 0.002\n",
            "[48,    40] loss: 0.004\n",
            "[49,    10] loss: 0.005\n",
            "[49,    20] loss: 0.006\n",
            "[49,    30] loss: 0.005\n",
            "[49,    40] loss: 0.010\n",
            "[50,    10] loss: 0.005\n",
            "[50,    20] loss: 0.004\n",
            "[50,    30] loss: 0.004\n",
            "[50,    40] loss: 0.005\n",
            "[51,    10] loss: 0.001\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.001\n",
            "[51,    40] loss: 0.007\n",
            "[52,    10] loss: 0.010\n",
            "[52,    20] loss: 0.018\n",
            "[52,    30] loss: 0.022\n",
            "[52,    40] loss: 0.028\n",
            "[53,    10] loss: 0.132\n",
            "[53,    20] loss: 0.143\n",
            "[53,    30] loss: 0.216\n",
            "[53,    40] loss: 0.202\n",
            "[54,    10] loss: 0.303\n",
            "[54,    20] loss: 0.354\n",
            "[54,    30] loss: 0.341\n",
            "[54,    40] loss: 0.286\n",
            "[55,    10] loss: 0.166\n",
            "[55,    20] loss: 0.142\n",
            "[55,    30] loss: 0.125\n",
            "[55,    40] loss: 0.146\n",
            "[56,    10] loss: 0.246\n",
            "[56,    20] loss: 0.276\n",
            "[56,    30] loss: 0.210\n",
            "[56,    40] loss: 0.186\n",
            "[57,    10] loss: 0.157\n",
            "[57,    20] loss: 0.124\n",
            "[57,    30] loss: 0.108\n",
            "[57,    40] loss: 0.088\n",
            "[58,    10] loss: 0.037\n",
            "[58,    20] loss: 0.036\n",
            "[58,    30] loss: 0.025\n",
            "[58,    40] loss: 0.033\n",
            "[59,    10] loss: 0.076\n",
            "[59,    20] loss: 0.083\n",
            "[59,    30] loss: 0.067\n",
            "[59,    40] loss: 0.039\n",
            "[60,    10] loss: 0.014\n",
            "[60,    20] loss: 0.015\n",
            "[60,    30] loss: 0.012\n",
            "[60,    40] loss: 0.016\n",
            "[61,    10] loss: 0.021\n",
            "[61,    20] loss: 0.025\n",
            "[61,    30] loss: 0.013\n",
            "[61,    40] loss: 0.014\n",
            "[62,    10] loss: 0.006\n",
            "[62,    20] loss: 0.007\n",
            "[62,    30] loss: 0.004\n",
            "[62,    40] loss: 0.004\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.008\n",
            "[64,    10] loss: 0.009\n",
            "[64,    20] loss: 0.015\n",
            "[64,    30] loss: 0.010\n",
            "[64,    40] loss: 0.008\n",
            "[65,    10] loss: 0.004\n",
            "[65,    20] loss: 0.003\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.002\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.005\n",
            "[68,    10] loss: 0.002\n",
            "[68,    20] loss: 0.004\n",
            "[68,    30] loss: 0.003\n",
            "[68,    40] loss: 0.011\n",
            "[69,    10] loss: 0.040\n",
            "[69,    20] loss: 0.033\n",
            "[69,    30] loss: 0.024\n",
            "[69,    40] loss: 0.032\n",
            "[70,    10] loss: 0.055\n",
            "[70,    20] loss: 0.076\n",
            "[70,    30] loss: 0.053\n",
            "[70,    40] loss: 0.038\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 61 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 73 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 76 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 78 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 78 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 78 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 74 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 1.054\n",
            "[1,    20] loss: 0.964\n",
            "[1,    30] loss: 0.948\n",
            "[1,    40] loss: 0.926\n",
            "[2,    10] loss: 0.889\n",
            "[2,    20] loss: 0.886\n",
            "[2,    30] loss: 0.872\n",
            "[2,    40] loss: 0.874\n",
            "[3,    10] loss: 0.847\n",
            "[3,    20] loss: 0.818\n",
            "[3,    30] loss: 0.818\n",
            "[3,    40] loss: 0.808\n",
            "[4,    10] loss: 0.780\n",
            "[4,    20] loss: 0.782\n",
            "[4,    30] loss: 0.814\n",
            "[4,    40] loss: 0.772\n",
            "[5,    10] loss: 0.737\n",
            "[5,    20] loss: 0.736\n",
            "[5,    30] loss: 0.765\n",
            "[5,    40] loss: 0.738\n",
            "[6,    10] loss: 0.689\n",
            "[6,    20] loss: 0.678\n",
            "[6,    30] loss: 0.691\n",
            "[6,    40] loss: 0.656\n",
            "[7,    10] loss: 0.680\n",
            "[7,    20] loss: 0.645\n",
            "[7,    30] loss: 0.631\n",
            "[7,    40] loss: 0.647\n",
            "[8,    10] loss: 0.642\n",
            "[8,    20] loss: 0.619\n",
            "[8,    30] loss: 0.586\n",
            "[8,    40] loss: 0.566\n",
            "[9,    10] loss: 0.557\n",
            "[9,    20] loss: 0.586\n",
            "[9,    30] loss: 0.553\n",
            "[9,    40] loss: 0.565\n",
            "[10,    10] loss: 0.472\n",
            "[10,    20] loss: 0.433\n",
            "[10,    30] loss: 0.428\n",
            "[10,    40] loss: 0.436\n",
            "[11,    10] loss: 0.414\n",
            "[11,    20] loss: 0.460\n",
            "[11,    30] loss: 0.460\n",
            "[11,    40] loss: 0.438\n",
            "[12,    10] loss: 0.329\n",
            "[12,    20] loss: 0.306\n",
            "[12,    30] loss: 0.304\n",
            "[12,    40] loss: 0.352\n",
            "[13,    10] loss: 0.475\n",
            "[13,    20] loss: 0.490\n",
            "[13,    30] loss: 0.470\n",
            "[13,    40] loss: 0.429\n",
            "[14,    10] loss: 0.313\n",
            "[14,    20] loss: 0.307\n",
            "[14,    30] loss: 0.299\n",
            "[14,    40] loss: 0.271\n",
            "[15,    10] loss: 0.179\n",
            "[15,    20] loss: 0.193\n",
            "[15,    30] loss: 0.166\n",
            "[15,    40] loss: 0.157\n",
            "[16,    10] loss: 0.122\n",
            "[16,    20] loss: 0.117\n",
            "[16,    30] loss: 0.118\n",
            "[16,    40] loss: 0.117\n",
            "[17,    10] loss: 0.239\n",
            "[17,    20] loss: 0.285\n",
            "[17,    30] loss: 0.256\n",
            "[17,    40] loss: 0.247\n",
            "[18,    10] loss: 0.165\n",
            "[18,    20] loss: 0.174\n",
            "[18,    30] loss: 0.123\n",
            "[18,    40] loss: 0.136\n",
            "[19,    10] loss: 0.139\n",
            "[19,    20] loss: 0.171\n",
            "[19,    30] loss: 0.143\n",
            "[19,    40] loss: 0.123\n",
            "[20,    10] loss: 0.232\n",
            "[20,    20] loss: 0.288\n",
            "[20,    30] loss: 0.223\n",
            "[20,    40] loss: 0.195\n",
            "[21,    10] loss: 0.211\n",
            "[21,    20] loss: 0.175\n",
            "[21,    30] loss: 0.190\n",
            "[21,    40] loss: 0.154\n",
            "[22,    10] loss: 0.115\n",
            "[22,    20] loss: 0.102\n",
            "[22,    30] loss: 0.072\n",
            "[22,    40] loss: 0.063\n",
            "[23,    10] loss: 0.026\n",
            "[23,    20] loss: 0.022\n",
            "[23,    30] loss: 0.017\n",
            "[23,    40] loss: 0.019\n",
            "[24,    10] loss: 0.024\n",
            "[24,    20] loss: 0.020\n",
            "[24,    30] loss: 0.013\n",
            "[24,    40] loss: 0.015\n",
            "[25,    10] loss: 0.018\n",
            "[25,    20] loss: 0.011\n",
            "[25,    30] loss: 0.009\n",
            "[25,    40] loss: 0.008\n",
            "[26,    10] loss: 0.004\n",
            "[26,    20] loss: 0.003\n",
            "[26,    30] loss: 0.003\n",
            "[26,    40] loss: 0.008\n",
            "[27,    10] loss: 0.012\n",
            "[27,    20] loss: 0.021\n",
            "[27,    30] loss: 0.014\n",
            "[27,    40] loss: 0.015\n",
            "[28,    10] loss: 0.011\n",
            "[28,    20] loss: 0.010\n",
            "[28,    30] loss: 0.009\n",
            "[28,    40] loss: 0.007\n",
            "[29,    10] loss: 0.003\n",
            "[29,    20] loss: 0.003\n",
            "[29,    30] loss: 0.002\n",
            "[29,    40] loss: 0.003\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.006\n",
            "[31,    20] loss: 0.006\n",
            "[31,    30] loss: 0.005\n",
            "[31,    40] loss: 0.004\n",
            "[32,    10] loss: 0.004\n",
            "[32,    20] loss: 0.006\n",
            "[32,    30] loss: 0.004\n",
            "[32,    40] loss: 0.005\n",
            "[33,    10] loss: 0.002\n",
            "[33,    20] loss: 0.003\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.002\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.001\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.000\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.010\n",
            "[37,    10] loss: 0.042\n",
            "[37,    20] loss: 0.055\n",
            "[37,    30] loss: 0.051\n",
            "[37,    40] loss: 0.183\n",
            "[38,    10] loss: 0.559\n",
            "[38,    20] loss: 0.582\n",
            "[38,    30] loss: 0.547\n",
            "[38,    40] loss: 0.447\n",
            "[39,    10] loss: 0.258\n",
            "[39,    20] loss: 0.220\n",
            "[39,    30] loss: 0.180\n",
            "[39,    40] loss: 0.131\n",
            "[40,    10] loss: 0.048\n",
            "[40,    20] loss: 0.041\n",
            "[40,    30] loss: 0.029\n",
            "[40,    40] loss: 0.024\n",
            "[41,    10] loss: 0.011\n",
            "[41,    20] loss: 0.012\n",
            "[41,    30] loss: 0.008\n",
            "[41,    40] loss: 0.010\n",
            "[42,    10] loss: 0.004\n",
            "[42,    20] loss: 0.005\n",
            "[42,    30] loss: 0.004\n",
            "[42,    40] loss: 0.003\n",
            "[43,    10] loss: 0.003\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.003\n",
            "[44,    10] loss: 0.002\n",
            "[44,    20] loss: 0.003\n",
            "[44,    30] loss: 0.002\n",
            "[44,    40] loss: 0.002\n",
            "[45,    10] loss: 0.002\n",
            "[45,    20] loss: 0.002\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.003\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.002\n",
            "[46,    40] loss: 0.015\n",
            "[47,    10] loss: 0.045\n",
            "[47,    20] loss: 0.046\n",
            "[47,    30] loss: 0.043\n",
            "[47,    40] loss: 0.061\n",
            "[48,    10] loss: 0.108\n",
            "[48,    20] loss: 0.127\n",
            "[48,    30] loss: 0.120\n",
            "[48,    40] loss: 0.088\n",
            "[49,    10] loss: 0.067\n",
            "[49,    20] loss: 0.068\n",
            "[49,    30] loss: 0.055\n",
            "[49,    40] loss: 0.057\n",
            "[50,    10] loss: 0.023\n",
            "[50,    20] loss: 0.019\n",
            "[50,    30] loss: 0.013\n",
            "[50,    40] loss: 0.010\n",
            "[51,    10] loss: 0.004\n",
            "[51,    20] loss: 0.003\n",
            "[51,    30] loss: 0.004\n",
            "[51,    40] loss: 0.004\n",
            "[52,    10] loss: 0.002\n",
            "[52,    20] loss: 0.002\n",
            "[52,    30] loss: 0.002\n",
            "[52,    40] loss: 0.002\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.003\n",
            "[54,    10] loss: 0.003\n",
            "[54,    20] loss: 0.002\n",
            "[54,    30] loss: 0.002\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.001\n",
            "[56,    30] loss: 0.001\n",
            "[56,    40] loss: 0.006\n",
            "[57,    10] loss: 0.012\n",
            "[57,    20] loss: 0.012\n",
            "[57,    30] loss: 0.013\n",
            "[57,    40] loss: 0.006\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.005\n",
            "[60,    10] loss: 0.011\n",
            "[60,    20] loss: 0.023\n",
            "[60,    30] loss: 0.012\n",
            "[60,    40] loss: 0.012\n",
            "[61,    10] loss: 0.013\n",
            "[61,    20] loss: 0.011\n",
            "[61,    30] loss: 0.009\n",
            "[61,    40] loss: 0.012\n",
            "[62,    10] loss: 0.019\n",
            "[62,    20] loss: 0.016\n",
            "[62,    30] loss: 0.014\n",
            "[62,    40] loss: 0.009\n",
            "[63,    10] loss: 0.003\n",
            "[63,    20] loss: 0.003\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.003\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.003\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.002\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 57 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 79 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 85 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 86 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 1.006\n",
            "[1,    20] loss: 0.904\n",
            "[1,    30] loss: 0.815\n",
            "[1,    40] loss: 0.779\n",
            "[2,    10] loss: 0.765\n",
            "[2,    20] loss: 0.743\n",
            "[2,    30] loss: 0.730\n",
            "[2,    40] loss: 0.701\n",
            "[3,    10] loss: 0.702\n",
            "[3,    20] loss: 0.660\n",
            "[3,    30] loss: 0.665\n",
            "[3,    40] loss: 0.642\n",
            "[4,    10] loss: 0.647\n",
            "[4,    20] loss: 0.621\n",
            "[4,    30] loss: 0.593\n",
            "[4,    40] loss: 0.599\n",
            "[5,    10] loss: 0.604\n",
            "[5,    20] loss: 0.587\n",
            "[5,    30] loss: 0.567\n",
            "[5,    40] loss: 0.554\n",
            "[6,    10] loss: 0.553\n",
            "[6,    20] loss: 0.507\n",
            "[6,    30] loss: 0.514\n",
            "[6,    40] loss: 0.478\n",
            "[7,    10] loss: 0.514\n",
            "[7,    20] loss: 0.495\n",
            "[7,    30] loss: 0.477\n",
            "[7,    40] loss: 0.444\n",
            "[8,    10] loss: 0.417\n",
            "[8,    20] loss: 0.385\n",
            "[8,    30] loss: 0.384\n",
            "[8,    40] loss: 0.383\n",
            "[9,    10] loss: 0.340\n",
            "[9,    20] loss: 0.339\n",
            "[9,    30] loss: 0.328\n",
            "[9,    40] loss: 0.360\n",
            "[10,    10] loss: 0.349\n",
            "[10,    20] loss: 0.330\n",
            "[10,    30] loss: 0.325\n",
            "[10,    40] loss: 0.293\n",
            "[11,    10] loss: 0.218\n",
            "[11,    20] loss: 0.209\n",
            "[11,    30] loss: 0.222\n",
            "[11,    40] loss: 0.247\n",
            "[12,    10] loss: 0.237\n",
            "[12,    20] loss: 0.223\n",
            "[12,    30] loss: 0.220\n",
            "[12,    40] loss: 0.210\n",
            "[13,    10] loss: 0.257\n",
            "[13,    20] loss: 0.230\n",
            "[13,    30] loss: 0.263\n",
            "[13,    40] loss: 0.220\n",
            "[14,    10] loss: 0.181\n",
            "[14,    20] loss: 0.167\n",
            "[14,    30] loss: 0.157\n",
            "[14,    40] loss: 0.152\n",
            "[15,    10] loss: 0.174\n",
            "[15,    20] loss: 0.136\n",
            "[15,    30] loss: 0.121\n",
            "[15,    40] loss: 0.127\n",
            "[16,    10] loss: 0.175\n",
            "[16,    20] loss: 0.177\n",
            "[16,    30] loss: 0.174\n",
            "[16,    40] loss: 0.133\n",
            "[17,    10] loss: 0.065\n",
            "[17,    20] loss: 0.055\n",
            "[17,    30] loss: 0.055\n",
            "[17,    40] loss: 0.053\n",
            "[18,    10] loss: 0.058\n",
            "[18,    20] loss: 0.050\n",
            "[18,    30] loss: 0.039\n",
            "[18,    40] loss: 0.039\n",
            "[19,    10] loss: 0.022\n",
            "[19,    20] loss: 0.019\n",
            "[19,    30] loss: 0.019\n",
            "[19,    40] loss: 0.014\n",
            "[20,    10] loss: 0.007\n",
            "[20,    20] loss: 0.008\n",
            "[20,    30] loss: 0.005\n",
            "[20,    40] loss: 0.004\n",
            "[21,    10] loss: 0.002\n",
            "[21,    20] loss: 0.002\n",
            "[21,    30] loss: 0.002\n",
            "[21,    40] loss: 0.002\n",
            "[22,    10] loss: 0.001\n",
            "[22,    20] loss: 0.001\n",
            "[22,    30] loss: 0.001\n",
            "[22,    40] loss: 0.010\n",
            "[23,    10] loss: 0.028\n",
            "[23,    20] loss: 0.051\n",
            "[23,    30] loss: 0.045\n",
            "[23,    40] loss: 0.049\n",
            "[24,    10] loss: 0.226\n",
            "[24,    20] loss: 0.265\n",
            "[24,    30] loss: 0.256\n",
            "[24,    40] loss: 0.264\n",
            "[25,    10] loss: 0.298\n",
            "[25,    20] loss: 0.260\n",
            "[25,    30] loss: 0.248\n",
            "[25,    40] loss: 0.175\n",
            "[26,    10] loss: 0.115\n",
            "[26,    20] loss: 0.084\n",
            "[26,    30] loss: 0.087\n",
            "[26,    40] loss: 0.070\n",
            "[27,    10] loss: 0.057\n",
            "[27,    20] loss: 0.055\n",
            "[27,    30] loss: 0.047\n",
            "[27,    40] loss: 0.038\n",
            "[28,    10] loss: 0.032\n",
            "[28,    20] loss: 0.034\n",
            "[28,    30] loss: 0.033\n",
            "[28,    40] loss: 0.026\n",
            "[29,    10] loss: 0.025\n",
            "[29,    20] loss: 0.031\n",
            "[29,    30] loss: 0.023\n",
            "[29,    40] loss: 0.015\n",
            "[30,    10] loss: 0.010\n",
            "[30,    20] loss: 0.007\n",
            "[30,    30] loss: 0.006\n",
            "[30,    40] loss: 0.008\n",
            "[31,    10] loss: 0.005\n",
            "[31,    20] loss: 0.008\n",
            "[31,    30] loss: 0.005\n",
            "[31,    40] loss: 0.003\n",
            "[32,    10] loss: 0.002\n",
            "[32,    20] loss: 0.002\n",
            "[32,    30] loss: 0.002\n",
            "[32,    40] loss: 0.030\n",
            "[33,    10] loss: 0.217\n",
            "[33,    20] loss: 0.238\n",
            "[33,    30] loss: 0.186\n",
            "[33,    40] loss: 0.154\n",
            "[34,    10] loss: 0.108\n",
            "[34,    20] loss: 0.104\n",
            "[34,    30] loss: 0.079\n",
            "[34,    40] loss: 0.062\n",
            "[35,    10] loss: 0.032\n",
            "[35,    20] loss: 0.020\n",
            "[35,    30] loss: 0.018\n",
            "[35,    40] loss: 0.020\n",
            "[36,    10] loss: 0.019\n",
            "[36,    20] loss: 0.018\n",
            "[36,    30] loss: 0.014\n",
            "[36,    40] loss: 0.027\n",
            "[37,    10] loss: 0.048\n",
            "[37,    20] loss: 0.067\n",
            "[37,    30] loss: 0.037\n",
            "[37,    40] loss: 0.046\n",
            "[38,    10] loss: 0.056\n",
            "[38,    20] loss: 0.046\n",
            "[38,    30] loss: 0.039\n",
            "[38,    40] loss: 0.037\n",
            "[39,    10] loss: 0.019\n",
            "[39,    20] loss: 0.011\n",
            "[39,    30] loss: 0.014\n",
            "[39,    40] loss: 0.043\n",
            "[40,    10] loss: 0.137\n",
            "[40,    20] loss: 0.102\n",
            "[40,    30] loss: 0.081\n",
            "[40,    40] loss: 0.051\n",
            "[41,    10] loss: 0.023\n",
            "[41,    20] loss: 0.021\n",
            "[41,    30] loss: 0.017\n",
            "[41,    40] loss: 0.028\n",
            "[42,    10] loss: 0.082\n",
            "[42,    20] loss: 0.076\n",
            "[42,    30] loss: 0.056\n",
            "[42,    40] loss: 0.053\n",
            "[43,    10] loss: 0.063\n",
            "[43,    20] loss: 0.050\n",
            "[43,    30] loss: 0.035\n",
            "[43,    40] loss: 0.035\n",
            "[44,    10] loss: 0.011\n",
            "[44,    20] loss: 0.009\n",
            "[44,    30] loss: 0.005\n",
            "[44,    40] loss: 0.005\n",
            "[45,    10] loss: 0.002\n",
            "[45,    20] loss: 0.002\n",
            "[45,    30] loss: 0.002\n",
            "[45,    40] loss: 0.002\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.001\n",
            "[49,    10] loss: 0.001\n",
            "[49,    20] loss: 0.001\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.006\n",
            "[50,    10] loss: 0.007\n",
            "[50,    20] loss: 0.010\n",
            "[50,    30] loss: 0.007\n",
            "[50,    40] loss: 0.009\n",
            "[51,    10] loss: 0.005\n",
            "[51,    20] loss: 0.007\n",
            "[51,    30] loss: 0.004\n",
            "[51,    40] loss: 0.004\n",
            "[52,    10] loss: 0.002\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.002\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.002\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.001\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.001\n",
            "[55,    40] loss: 0.003\n",
            "[56,    10] loss: 0.002\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.002\n",
            "[56,    40] loss: 0.002\n",
            "[57,    10] loss: 0.001\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.004\n",
            "[58,    10] loss: 0.003\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.001\n",
            "[59,    10] loss: 0.001\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.000\n",
            "[60,    30] loss: 0.000\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.000\n",
            "[61,    20] loss: 0.000\n",
            "[61,    30] loss: 0.000\n",
            "[61,    40] loss: 0.000\n",
            "[62,    10] loss: 0.000\n",
            "[62,    20] loss: 0.000\n",
            "[62,    30] loss: 0.000\n",
            "[62,    40] loss: 0.000\n",
            "[63,    10] loss: 0.000\n",
            "[63,    20] loss: 0.000\n",
            "[63,    30] loss: 0.000\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.000\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.000\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.000\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.003\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.004\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.000\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.000\n",
            "[69,    10] loss: 0.000\n",
            "[69,    20] loss: 0.000\n",
            "[69,    30] loss: 0.000\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.000\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 57 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 91 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 0.958\n",
            "[1,    20] loss: 0.782\n",
            "[1,    30] loss: 0.663\n",
            "[1,    40] loss: 0.672\n",
            "[2,    10] loss: 0.613\n",
            "[2,    20] loss: 0.624\n",
            "[2,    30] loss: 0.587\n",
            "[2,    40] loss: 0.556\n",
            "[3,    10] loss: 0.525\n",
            "[3,    20] loss: 0.516\n",
            "[3,    30] loss: 0.511\n",
            "[3,    40] loss: 0.517\n",
            "[4,    10] loss: 0.487\n",
            "[4,    20] loss: 0.467\n",
            "[4,    30] loss: 0.463\n",
            "[4,    40] loss: 0.491\n",
            "[5,    10] loss: 0.498\n",
            "[5,    20] loss: 0.417\n",
            "[5,    30] loss: 0.392\n",
            "[5,    40] loss: 0.383\n",
            "[6,    10] loss: 0.341\n",
            "[6,    20] loss: 0.349\n",
            "[6,    30] loss: 0.329\n",
            "[6,    40] loss: 0.334\n",
            "[7,    10] loss: 0.298\n",
            "[7,    20] loss: 0.302\n",
            "[7,    30] loss: 0.305\n",
            "[7,    40] loss: 0.275\n",
            "[8,    10] loss: 0.249\n",
            "[8,    20] loss: 0.232\n",
            "[8,    30] loss: 0.224\n",
            "[8,    40] loss: 0.272\n",
            "[9,    10] loss: 0.409\n",
            "[9,    20] loss: 0.400\n",
            "[9,    30] loss: 0.357\n",
            "[9,    40] loss: 0.330\n",
            "[10,    10] loss: 0.275\n",
            "[10,    20] loss: 0.256\n",
            "[10,    30] loss: 0.245\n",
            "[10,    40] loss: 0.222\n",
            "[11,    10] loss: 0.163\n",
            "[11,    20] loss: 0.163\n",
            "[11,    30] loss: 0.132\n",
            "[11,    40] loss: 0.138\n",
            "[12,    10] loss: 0.112\n",
            "[12,    20] loss: 0.113\n",
            "[12,    30] loss: 0.106\n",
            "[12,    40] loss: 0.132\n",
            "[13,    10] loss: 0.176\n",
            "[13,    20] loss: 0.155\n",
            "[13,    30] loss: 0.144\n",
            "[13,    40] loss: 0.184\n",
            "[14,    10] loss: 0.319\n",
            "[14,    20] loss: 0.267\n",
            "[14,    30] loss: 0.218\n",
            "[14,    40] loss: 0.204\n",
            "[15,    10] loss: 0.177\n",
            "[15,    20] loss: 0.138\n",
            "[15,    30] loss: 0.112\n",
            "[15,    40] loss: 0.126\n",
            "[16,    10] loss: 0.195\n",
            "[16,    20] loss: 0.182\n",
            "[16,    30] loss: 0.140\n",
            "[16,    40] loss: 0.110\n",
            "[17,    10] loss: 0.097\n",
            "[17,    20] loss: 0.080\n",
            "[17,    30] loss: 0.070\n",
            "[17,    40] loss: 0.065\n",
            "[18,    10] loss: 0.065\n",
            "[18,    20] loss: 0.070\n",
            "[18,    30] loss: 0.060\n",
            "[18,    40] loss: 0.040\n",
            "[19,    10] loss: 0.022\n",
            "[19,    20] loss: 0.020\n",
            "[19,    30] loss: 0.017\n",
            "[19,    40] loss: 0.014\n",
            "[20,    10] loss: 0.009\n",
            "[20,    20] loss: 0.005\n",
            "[20,    30] loss: 0.005\n",
            "[20,    40] loss: 0.004\n",
            "[21,    10] loss: 0.003\n",
            "[21,    20] loss: 0.003\n",
            "[21,    30] loss: 0.003\n",
            "[21,    40] loss: 0.003\n",
            "[22,    10] loss: 0.003\n",
            "[22,    20] loss: 0.003\n",
            "[22,    30] loss: 0.002\n",
            "[22,    40] loss: 0.009\n",
            "[23,    10] loss: 0.033\n",
            "[23,    20] loss: 0.038\n",
            "[23,    30] loss: 0.028\n",
            "[23,    40] loss: 0.023\n",
            "[24,    10] loss: 0.013\n",
            "[24,    20] loss: 0.011\n",
            "[24,    30] loss: 0.007\n",
            "[24,    40] loss: 0.069\n",
            "[25,    10] loss: 0.241\n",
            "[25,    20] loss: 0.263\n",
            "[25,    30] loss: 0.242\n",
            "[25,    40] loss: 0.167\n",
            "[26,    10] loss: 0.113\n",
            "[26,    20] loss: 0.086\n",
            "[26,    30] loss: 0.072\n",
            "[26,    40] loss: 0.056\n",
            "[27,    10] loss: 0.044\n",
            "[27,    20] loss: 0.025\n",
            "[27,    30] loss: 0.027\n",
            "[27,    40] loss: 0.018\n",
            "[28,    10] loss: 0.009\n",
            "[28,    20] loss: 0.008\n",
            "[28,    30] loss: 0.006\n",
            "[28,    40] loss: 0.017\n",
            "[29,    10] loss: 0.073\n",
            "[29,    20] loss: 0.053\n",
            "[29,    30] loss: 0.047\n",
            "[29,    40] loss: 0.030\n",
            "[30,    10] loss: 0.012\n",
            "[30,    20] loss: 0.012\n",
            "[30,    30] loss: 0.010\n",
            "[30,    40] loss: 0.007\n",
            "[31,    10] loss: 0.004\n",
            "[31,    20] loss: 0.004\n",
            "[31,    30] loss: 0.003\n",
            "[31,    40] loss: 0.045\n",
            "[32,    10] loss: 0.099\n",
            "[32,    20] loss: 0.096\n",
            "[32,    30] loss: 0.082\n",
            "[32,    40] loss: 0.066\n",
            "[33,    10] loss: 0.035\n",
            "[33,    20] loss: 0.030\n",
            "[33,    30] loss: 0.021\n",
            "[33,    40] loss: 0.061\n",
            "[34,    10] loss: 0.108\n",
            "[34,    20] loss: 0.112\n",
            "[34,    30] loss: 0.085\n",
            "[34,    40] loss: 0.065\n",
            "[35,    10] loss: 0.032\n",
            "[35,    20] loss: 0.028\n",
            "[35,    30] loss: 0.017\n",
            "[35,    40] loss: 0.037\n",
            "[36,    10] loss: 0.064\n",
            "[36,    20] loss: 0.075\n",
            "[36,    30] loss: 0.058\n",
            "[36,    40] loss: 0.043\n",
            "[37,    10] loss: 0.059\n",
            "[37,    20] loss: 0.036\n",
            "[37,    30] loss: 0.025\n",
            "[37,    40] loss: 0.067\n",
            "[38,    10] loss: 0.093\n",
            "[38,    20] loss: 0.076\n",
            "[38,    30] loss: 0.069\n",
            "[38,    40] loss: 0.040\n",
            "[39,    10] loss: 0.020\n",
            "[39,    20] loss: 0.016\n",
            "[39,    30] loss: 0.012\n",
            "[39,    40] loss: 0.008\n",
            "[40,    10] loss: 0.003\n",
            "[40,    20] loss: 0.003\n",
            "[40,    30] loss: 0.003\n",
            "[40,    40] loss: 0.003\n",
            "[41,    10] loss: 0.002\n",
            "[41,    20] loss: 0.002\n",
            "[41,    30] loss: 0.002\n",
            "[41,    40] loss: 0.001\n",
            "[42,    10] loss: 0.001\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.002\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.002\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.004\n",
            "[45,    10] loss: 0.002\n",
            "[45,    20] loss: 0.005\n",
            "[45,    30] loss: 0.002\n",
            "[45,    40] loss: 0.003\n",
            "[46,    10] loss: 0.002\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.001\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.017\n",
            "[49,    10] loss: 0.052\n",
            "[49,    20] loss: 0.065\n",
            "[49,    30] loss: 0.029\n",
            "[49,    40] loss: 0.030\n",
            "[50,    10] loss: 0.028\n",
            "[50,    20] loss: 0.015\n",
            "[50,    30] loss: 0.018\n",
            "[50,    40] loss: 0.011\n",
            "[51,    10] loss: 0.005\n",
            "[51,    20] loss: 0.003\n",
            "[51,    30] loss: 0.003\n",
            "[51,    40] loss: 0.073\n",
            "[52,    10] loss: 0.153\n",
            "[52,    20] loss: 0.101\n",
            "[52,    30] loss: 0.108\n",
            "[52,    40] loss: 0.120\n",
            "[53,    10] loss: 0.157\n",
            "[53,    20] loss: 0.119\n",
            "[53,    30] loss: 0.075\n",
            "[53,    40] loss: 0.062\n",
            "[54,    10] loss: 0.052\n",
            "[54,    20] loss: 0.051\n",
            "[54,    30] loss: 0.028\n",
            "[54,    40] loss: 0.020\n",
            "[55,    10] loss: 0.010\n",
            "[55,    20] loss: 0.008\n",
            "[55,    30] loss: 0.006\n",
            "[55,    40] loss: 0.009\n",
            "[56,    10] loss: 0.007\n",
            "[56,    20] loss: 0.011\n",
            "[56,    30] loss: 0.006\n",
            "[56,    40] loss: 0.005\n",
            "[57,    10] loss: 0.003\n",
            "[57,    20] loss: 0.002\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.001\n",
            "[58,    30] loss: 0.001\n",
            "[58,    40] loss: 0.003\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.001\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.003\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.006\n",
            "[65,    10] loss: 0.007\n",
            "[65,    20] loss: 0.009\n",
            "[65,    30] loss: 0.007\n",
            "[65,    40] loss: 0.012\n",
            "[66,    10] loss: 0.031\n",
            "[66,    20] loss: 0.027\n",
            "[66,    30] loss: 0.011\n",
            "[66,    40] loss: 0.013\n",
            "[67,    10] loss: 0.026\n",
            "[67,    20] loss: 0.029\n",
            "[67,    30] loss: 0.023\n",
            "[67,    40] loss: 0.031\n",
            "[68,    10] loss: 0.063\n",
            "[68,    20] loss: 0.049\n",
            "[68,    30] loss: 0.028\n",
            "[68,    40] loss: 0.024\n",
            "[69,    10] loss: 0.040\n",
            "[69,    20] loss: 0.027\n",
            "[69,    30] loss: 0.021\n",
            "[69,    40] loss: 0.020\n",
            "[70,    10] loss: 0.013\n",
            "[70,    20] loss: 0.013\n",
            "[70,    30] loss: 0.007\n",
            "[70,    40] loss: 0.005\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 57 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 84 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 93 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 0.941\n",
            "[1,    20] loss: 0.737\n",
            "[1,    30] loss: 0.631\n",
            "[1,    40] loss: 0.591\n",
            "[2,    10] loss: 0.592\n",
            "[2,    20] loss: 0.555\n",
            "[2,    30] loss: 0.521\n",
            "[2,    40] loss: 0.497\n",
            "[3,    10] loss: 0.484\n",
            "[3,    20] loss: 0.437\n",
            "[3,    30] loss: 0.430\n",
            "[3,    40] loss: 0.407\n",
            "[4,    10] loss: 0.398\n",
            "[4,    20] loss: 0.374\n",
            "[4,    30] loss: 0.331\n",
            "[4,    40] loss: 0.340\n",
            "[5,    10] loss: 0.295\n",
            "[5,    20] loss: 0.301\n",
            "[5,    30] loss: 0.264\n",
            "[5,    40] loss: 0.250\n",
            "[6,    10] loss: 0.249\n",
            "[6,    20] loss: 0.223\n",
            "[6,    30] loss: 0.220\n",
            "[6,    40] loss: 0.277\n",
            "[7,    10] loss: 0.251\n",
            "[7,    20] loss: 0.264\n",
            "[7,    30] loss: 0.223\n",
            "[7,    40] loss: 0.200\n",
            "[8,    10] loss: 0.211\n",
            "[8,    20] loss: 0.197\n",
            "[8,    30] loss: 0.168\n",
            "[8,    40] loss: 0.170\n",
            "[9,    10] loss: 0.145\n",
            "[9,    20] loss: 0.129\n",
            "[9,    30] loss: 0.107\n",
            "[9,    40] loss: 0.117\n",
            "[10,    10] loss: 0.071\n",
            "[10,    20] loss: 0.083\n",
            "[10,    30] loss: 0.079\n",
            "[10,    40] loss: 0.170\n",
            "[11,    10] loss: 0.448\n",
            "[11,    20] loss: 0.300\n",
            "[11,    30] loss: 0.242\n",
            "[11,    40] loss: 0.236\n",
            "[12,    10] loss: 0.196\n",
            "[12,    20] loss: 0.166\n",
            "[12,    30] loss: 0.126\n",
            "[12,    40] loss: 0.126\n",
            "[13,    10] loss: 0.148\n",
            "[13,    20] loss: 0.134\n",
            "[13,    30] loss: 0.107\n",
            "[13,    40] loss: 0.093\n",
            "[14,    10] loss: 0.082\n",
            "[14,    20] loss: 0.070\n",
            "[14,    30] loss: 0.064\n",
            "[14,    40] loss: 0.052\n",
            "[15,    10] loss: 0.048\n",
            "[15,    20] loss: 0.031\n",
            "[15,    30] loss: 0.033\n",
            "[15,    40] loss: 0.042\n",
            "[16,    10] loss: 0.059\n",
            "[16,    20] loss: 0.062\n",
            "[16,    30] loss: 0.043\n",
            "[16,    40] loss: 0.060\n",
            "[17,    10] loss: 0.058\n",
            "[17,    20] loss: 0.060\n",
            "[17,    30] loss: 0.051\n",
            "[17,    40] loss: 0.030\n",
            "[18,    10] loss: 0.023\n",
            "[18,    20] loss: 0.015\n",
            "[18,    30] loss: 0.011\n",
            "[18,    40] loss: 0.011\n",
            "[19,    10] loss: 0.012\n",
            "[19,    20] loss: 0.011\n",
            "[19,    30] loss: 0.011\n",
            "[19,    40] loss: 0.008\n",
            "[20,    10] loss: 0.004\n",
            "[20,    20] loss: 0.004\n",
            "[20,    30] loss: 0.003\n",
            "[20,    40] loss: 0.003\n",
            "[21,    10] loss: 0.002\n",
            "[21,    20] loss: 0.002\n",
            "[21,    30] loss: 0.002\n",
            "[21,    40] loss: 0.002\n",
            "[22,    10] loss: 0.001\n",
            "[22,    20] loss: 0.002\n",
            "[22,    30] loss: 0.002\n",
            "[22,    40] loss: 0.005\n",
            "[23,    10] loss: 0.025\n",
            "[23,    20] loss: 0.027\n",
            "[23,    30] loss: 0.019\n",
            "[23,    40] loss: 0.012\n",
            "[24,    10] loss: 0.007\n",
            "[24,    20] loss: 0.004\n",
            "[24,    30] loss: 0.005\n",
            "[24,    40] loss: 0.004\n",
            "[25,    10] loss: 0.004\n",
            "[25,    20] loss: 0.003\n",
            "[25,    30] loss: 0.002\n",
            "[25,    40] loss: 0.010\n",
            "[26,    10] loss: 0.020\n",
            "[26,    20] loss: 0.032\n",
            "[26,    30] loss: 0.015\n",
            "[26,    40] loss: 0.018\n",
            "[27,    10] loss: 0.010\n",
            "[27,    20] loss: 0.005\n",
            "[27,    30] loss: 0.004\n",
            "[27,    40] loss: 0.032\n",
            "[28,    10] loss: 0.199\n",
            "[28,    20] loss: 0.190\n",
            "[28,    30] loss: 0.162\n",
            "[28,    40] loss: 0.128\n",
            "[29,    10] loss: 0.082\n",
            "[29,    20] loss: 0.058\n",
            "[29,    30] loss: 0.056\n",
            "[29,    40] loss: 0.035\n",
            "[30,    10] loss: 0.015\n",
            "[30,    20] loss: 0.015\n",
            "[30,    30] loss: 0.012\n",
            "[30,    40] loss: 0.038\n",
            "[31,    10] loss: 0.089\n",
            "[31,    20] loss: 0.069\n",
            "[31,    30] loss: 0.050\n",
            "[31,    40] loss: 0.039\n",
            "[32,    10] loss: 0.064\n",
            "[32,    20] loss: 0.040\n",
            "[32,    30] loss: 0.033\n",
            "[32,    40] loss: 0.038\n",
            "[33,    10] loss: 0.027\n",
            "[33,    20] loss: 0.022\n",
            "[33,    30] loss: 0.022\n",
            "[33,    40] loss: 0.015\n",
            "[34,    10] loss: 0.019\n",
            "[34,    20] loss: 0.015\n",
            "[34,    30] loss: 0.009\n",
            "[34,    40] loss: 0.009\n",
            "[35,    10] loss: 0.004\n",
            "[35,    20] loss: 0.003\n",
            "[35,    30] loss: 0.003\n",
            "[35,    40] loss: 0.002\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.001\n",
            "[37,    40] loss: 0.015\n",
            "[38,    10] loss: 0.047\n",
            "[38,    20] loss: 0.057\n",
            "[38,    30] loss: 0.024\n",
            "[38,    40] loss: 0.037\n",
            "[39,    10] loss: 0.044\n",
            "[39,    20] loss: 0.042\n",
            "[39,    30] loss: 0.037\n",
            "[39,    40] loss: 0.031\n",
            "[40,    10] loss: 0.024\n",
            "[40,    20] loss: 0.024\n",
            "[40,    30] loss: 0.016\n",
            "[40,    40] loss: 0.015\n",
            "[41,    10] loss: 0.007\n",
            "[41,    20] loss: 0.005\n",
            "[41,    30] loss: 0.004\n",
            "[41,    40] loss: 0.005\n",
            "[42,    10] loss: 0.003\n",
            "[42,    20] loss: 0.005\n",
            "[42,    30] loss: 0.003\n",
            "[42,    40] loss: 0.004\n",
            "[43,    10] loss: 0.003\n",
            "[43,    20] loss: 0.005\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.004\n",
            "[44,    10] loss: 0.003\n",
            "[44,    20] loss: 0.003\n",
            "[44,    30] loss: 0.004\n",
            "[44,    40] loss: 0.003\n",
            "[45,    10] loss: 0.002\n",
            "[45,    20] loss: 0.002\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.003\n",
            "[46,    10] loss: 0.004\n",
            "[46,    20] loss: 0.002\n",
            "[46,    30] loss: 0.002\n",
            "[46,    40] loss: 0.002\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.001\n",
            "[48,    10] loss: 0.001\n",
            "[48,    20] loss: 0.000\n",
            "[48,    30] loss: 0.001\n",
            "[48,    40] loss: 0.012\n",
            "[49,    10] loss: 0.052\n",
            "[49,    20] loss: 0.035\n",
            "[49,    30] loss: 0.032\n",
            "[49,    40] loss: 0.022\n",
            "[50,    10] loss: 0.008\n",
            "[50,    20] loss: 0.010\n",
            "[50,    30] loss: 0.007\n",
            "[50,    40] loss: 0.006\n",
            "[51,    10] loss: 0.004\n",
            "[51,    20] loss: 0.004\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.003\n",
            "[52,    10] loss: 0.004\n",
            "[52,    20] loss: 0.002\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.001\n",
            "[54,    10] loss: 0.001\n",
            "[54,    20] loss: 0.001\n",
            "[54,    30] loss: 0.001\n",
            "[54,    40] loss: 0.001\n",
            "[55,    10] loss: 0.000\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.000\n",
            "[55,    40] loss: 0.001\n",
            "[56,    10] loss: 0.000\n",
            "[56,    20] loss: 0.000\n",
            "[56,    30] loss: 0.000\n",
            "[56,    40] loss: 0.001\n",
            "[57,    10] loss: 0.000\n",
            "[57,    20] loss: 0.001\n",
            "[57,    30] loss: 0.001\n",
            "[57,    40] loss: 0.000\n",
            "[58,    10] loss: 0.000\n",
            "[58,    20] loss: 0.000\n",
            "[58,    30] loss: 0.000\n",
            "[58,    40] loss: 0.000\n",
            "[59,    10] loss: 0.000\n",
            "[59,    20] loss: 0.000\n",
            "[59,    30] loss: 0.000\n",
            "[59,    40] loss: 0.000\n",
            "[60,    10] loss: 0.000\n",
            "[60,    20] loss: 0.000\n",
            "[60,    30] loss: 0.000\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.000\n",
            "[61,    20] loss: 0.000\n",
            "[61,    30] loss: 0.000\n",
            "[61,    40] loss: 0.000\n",
            "[62,    10] loss: 0.000\n",
            "[62,    20] loss: 0.000\n",
            "[62,    30] loss: 0.000\n",
            "[62,    40] loss: 0.000\n",
            "[63,    10] loss: 0.000\n",
            "[63,    20] loss: 0.000\n",
            "[63,    30] loss: 0.000\n",
            "[63,    40] loss: 0.000\n",
            "[64,    10] loss: 0.000\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.000\n",
            "[64,    40] loss: 0.000\n",
            "[65,    10] loss: 0.000\n",
            "[65,    20] loss: 0.000\n",
            "[65,    30] loss: 0.000\n",
            "[65,    40] loss: 0.002\n",
            "[66,    10] loss: 0.005\n",
            "[66,    20] loss: 0.004\n",
            "[66,    30] loss: 0.003\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.000\n",
            "[68,    20] loss: 0.000\n",
            "[68,    30] loss: 0.000\n",
            "[68,    40] loss: 0.047\n",
            "[69,    10] loss: 0.251\n",
            "[69,    20] loss: 0.212\n",
            "[69,    30] loss: 0.163\n",
            "[69,    40] loss: 0.133\n",
            "[70,    10] loss: 0.147\n",
            "[70,    20] loss: 0.128\n",
            "[70,    30] loss: 0.088\n",
            "[70,    40] loss: 0.061\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 55 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 82 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 90 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 94 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 0.895\n",
            "[1,    20] loss: 0.641\n",
            "[1,    30] loss: 0.555\n",
            "[1,    40] loss: 0.529\n",
            "[2,    10] loss: 0.482\n",
            "[2,    20] loss: 0.450\n",
            "[2,    30] loss: 0.408\n",
            "[2,    40] loss: 0.428\n",
            "[3,    10] loss: 0.416\n",
            "[3,    20] loss: 0.371\n",
            "[3,    30] loss: 0.329\n",
            "[3,    40] loss: 0.366\n",
            "[4,    10] loss: 0.352\n",
            "[4,    20] loss: 0.305\n",
            "[4,    30] loss: 0.272\n",
            "[4,    40] loss: 0.264\n",
            "[5,    10] loss: 0.238\n",
            "[5,    20] loss: 0.235\n",
            "[5,    30] loss: 0.216\n",
            "[5,    40] loss: 0.214\n",
            "[6,    10] loss: 0.183\n",
            "[6,    20] loss: 0.182\n",
            "[6,    30] loss: 0.181\n",
            "[6,    40] loss: 0.165\n",
            "[7,    10] loss: 0.151\n",
            "[7,    20] loss: 0.132\n",
            "[7,    30] loss: 0.124\n",
            "[7,    40] loss: 0.099\n",
            "[8,    10] loss: 0.092\n",
            "[8,    20] loss: 0.070\n",
            "[8,    30] loss: 0.069\n",
            "[8,    40] loss: 0.056\n",
            "[9,    10] loss: 0.032\n",
            "[9,    20] loss: 0.028\n",
            "[9,    30] loss: 0.025\n",
            "[9,    40] loss: 0.031\n",
            "[10,    10] loss: 0.040\n",
            "[10,    20] loss: 0.050\n",
            "[10,    30] loss: 0.044\n",
            "[10,    40] loss: 0.157\n",
            "[11,    10] loss: 0.637\n",
            "[11,    20] loss: 0.435\n",
            "[11,    30] loss: 0.358\n",
            "[11,    40] loss: 0.308\n",
            "[12,    10] loss: 0.255\n",
            "[12,    20] loss: 0.220\n",
            "[12,    30] loss: 0.193\n",
            "[12,    40] loss: 0.215\n",
            "[13,    10] loss: 0.192\n",
            "[13,    20] loss: 0.153\n",
            "[13,    30] loss: 0.142\n",
            "[13,    40] loss: 0.141\n",
            "[14,    10] loss: 0.112\n",
            "[14,    20] loss: 0.097\n",
            "[14,    30] loss: 0.094\n",
            "[14,    40] loss: 0.077\n",
            "[15,    10] loss: 0.049\n",
            "[15,    20] loss: 0.043\n",
            "[15,    30] loss: 0.043\n",
            "[15,    40] loss: 0.081\n",
            "[16,    10] loss: 0.160\n",
            "[16,    20] loss: 0.139\n",
            "[16,    30] loss: 0.109\n",
            "[16,    40] loss: 0.092\n",
            "[17,    10] loss: 0.081\n",
            "[17,    20] loss: 0.058\n",
            "[17,    30] loss: 0.044\n",
            "[17,    40] loss: 0.043\n",
            "[18,    10] loss: 0.040\n",
            "[18,    20] loss: 0.038\n",
            "[18,    30] loss: 0.026\n",
            "[18,    40] loss: 0.037\n",
            "[19,    10] loss: 0.067\n",
            "[19,    20] loss: 0.045\n",
            "[19,    30] loss: 0.046\n",
            "[19,    40] loss: 0.050\n",
            "[20,    10] loss: 0.082\n",
            "[20,    20] loss: 0.067\n",
            "[20,    30] loss: 0.051\n",
            "[20,    40] loss: 0.031\n",
            "[21,    10] loss: 0.016\n",
            "[21,    20] loss: 0.014\n",
            "[21,    30] loss: 0.012\n",
            "[21,    40] loss: 0.014\n",
            "[22,    10] loss: 0.008\n",
            "[22,    20] loss: 0.008\n",
            "[22,    30] loss: 0.006\n",
            "[22,    40] loss: 0.007\n",
            "[23,    10] loss: 0.003\n",
            "[23,    20] loss: 0.003\n",
            "[23,    30] loss: 0.003\n",
            "[23,    40] loss: 0.003\n",
            "[24,    10] loss: 0.002\n",
            "[24,    20] loss: 0.002\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.001\n",
            "[25,    20] loss: 0.001\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.024\n",
            "[26,    10] loss: 0.126\n",
            "[26,    20] loss: 0.088\n",
            "[26,    30] loss: 0.073\n",
            "[26,    40] loss: 0.084\n",
            "[27,    10] loss: 0.157\n",
            "[27,    20] loss: 0.096\n",
            "[27,    30] loss: 0.065\n",
            "[27,    40] loss: 0.048\n",
            "[28,    10] loss: 0.027\n",
            "[28,    20] loss: 0.024\n",
            "[28,    30] loss: 0.022\n",
            "[28,    40] loss: 0.022\n",
            "[29,    10] loss: 0.026\n",
            "[29,    20] loss: 0.018\n",
            "[29,    30] loss: 0.010\n",
            "[29,    40] loss: 0.009\n",
            "[30,    10] loss: 0.005\n",
            "[30,    20] loss: 0.005\n",
            "[30,    30] loss: 0.004\n",
            "[30,    40] loss: 0.005\n",
            "[31,    10] loss: 0.003\n",
            "[31,    20] loss: 0.003\n",
            "[31,    30] loss: 0.003\n",
            "[31,    40] loss: 0.003\n",
            "[32,    10] loss: 0.002\n",
            "[32,    20] loss: 0.002\n",
            "[32,    30] loss: 0.002\n",
            "[32,    40] loss: 0.003\n",
            "[33,    10] loss: 0.002\n",
            "[33,    20] loss: 0.002\n",
            "[33,    30] loss: 0.002\n",
            "[33,    40] loss: 0.001\n",
            "[34,    10] loss: 0.001\n",
            "[34,    20] loss: 0.001\n",
            "[34,    30] loss: 0.001\n",
            "[34,    40] loss: 0.002\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.001\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.001\n",
            "[37,    40] loss: 0.002\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.002\n",
            "[38,    30] loss: 0.001\n",
            "[38,    40] loss: 0.021\n",
            "[39,    10] loss: 0.085\n",
            "[39,    20] loss: 0.063\n",
            "[39,    30] loss: 0.050\n",
            "[39,    40] loss: 0.034\n",
            "[40,    10] loss: 0.018\n",
            "[40,    20] loss: 0.015\n",
            "[40,    30] loss: 0.011\n",
            "[40,    40] loss: 0.009\n",
            "[41,    10] loss: 0.004\n",
            "[41,    20] loss: 0.004\n",
            "[41,    30] loss: 0.005\n",
            "[41,    40] loss: 0.003\n",
            "[42,    10] loss: 0.002\n",
            "[42,    20] loss: 0.002\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.003\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.002\n",
            "[43,    30] loss: 0.002\n",
            "[43,    40] loss: 0.001\n",
            "[44,    10] loss: 0.001\n",
            "[44,    20] loss: 0.001\n",
            "[44,    30] loss: 0.001\n",
            "[44,    40] loss: 0.001\n",
            "[45,    10] loss: 0.001\n",
            "[45,    20] loss: 0.001\n",
            "[45,    30] loss: 0.001\n",
            "[45,    40] loss: 0.001\n",
            "[46,    10] loss: 0.001\n",
            "[46,    20] loss: 0.001\n",
            "[46,    30] loss: 0.001\n",
            "[46,    40] loss: 0.001\n",
            "[47,    10] loss: 0.001\n",
            "[47,    20] loss: 0.001\n",
            "[47,    30] loss: 0.001\n",
            "[47,    40] loss: 0.008\n",
            "[48,    10] loss: 0.020\n",
            "[48,    20] loss: 0.010\n",
            "[48,    30] loss: 0.007\n",
            "[48,    40] loss: 0.019\n",
            "[49,    10] loss: 0.066\n",
            "[49,    20] loss: 0.038\n",
            "[49,    30] loss: 0.021\n",
            "[49,    40] loss: 0.013\n",
            "[50,    10] loss: 0.006\n",
            "[50,    20] loss: 0.006\n",
            "[50,    30] loss: 0.005\n",
            "[50,    40] loss: 0.005\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.001\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.001\n",
            "[52,    20] loss: 0.001\n",
            "[52,    30] loss: 0.001\n",
            "[52,    40] loss: 0.001\n",
            "[53,    10] loss: 0.001\n",
            "[53,    20] loss: 0.001\n",
            "[53,    30] loss: 0.001\n",
            "[53,    40] loss: 0.019\n",
            "[54,    10] loss: 0.126\n",
            "[54,    20] loss: 0.116\n",
            "[54,    30] loss: 0.078\n",
            "[54,    40] loss: 0.046\n",
            "[55,    10] loss: 0.016\n",
            "[55,    20] loss: 0.012\n",
            "[55,    30] loss: 0.012\n",
            "[55,    40] loss: 0.008\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.005\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.007\n",
            "[57,    10] loss: 0.011\n",
            "[57,    20] loss: 0.011\n",
            "[57,    30] loss: 0.009\n",
            "[57,    40] loss: 0.008\n",
            "[58,    10] loss: 0.005\n",
            "[58,    20] loss: 0.004\n",
            "[58,    30] loss: 0.003\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.001\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.007\n",
            "[61,    10] loss: 0.007\n",
            "[61,    20] loss: 0.012\n",
            "[61,    30] loss: 0.006\n",
            "[61,    40] loss: 0.004\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.000\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.000\n",
            "[65,    20] loss: 0.000\n",
            "[65,    30] loss: 0.000\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.003\n",
            "[68,    10] loss: 0.002\n",
            "[68,    20] loss: 0.002\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.000\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.002\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 55 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 82 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 0.881\n",
            "[1,    20] loss: 0.635\n",
            "[1,    30] loss: 0.537\n",
            "[1,    40] loss: 0.498\n",
            "[2,    10] loss: 0.475\n",
            "[2,    20] loss: 0.439\n",
            "[2,    30] loss: 0.412\n",
            "[2,    40] loss: 0.368\n",
            "[3,    10] loss: 0.360\n",
            "[3,    20] loss: 0.324\n",
            "[3,    30] loss: 0.318\n",
            "[3,    40] loss: 0.355\n",
            "[4,    10] loss: 0.308\n",
            "[4,    20] loss: 0.292\n",
            "[4,    30] loss: 0.263\n",
            "[4,    40] loss: 0.254\n",
            "[5,    10] loss: 0.277\n",
            "[5,    20] loss: 0.284\n",
            "[5,    30] loss: 0.234\n",
            "[5,    40] loss: 0.178\n",
            "[6,    10] loss: 0.178\n",
            "[6,    20] loss: 0.173\n",
            "[6,    30] loss: 0.142\n",
            "[6,    40] loss: 0.203\n",
            "[7,    10] loss: 0.287\n",
            "[7,    20] loss: 0.192\n",
            "[7,    30] loss: 0.177\n",
            "[7,    40] loss: 0.167\n",
            "[8,    10] loss: 0.209\n",
            "[8,    20] loss: 0.172\n",
            "[8,    30] loss: 0.130\n",
            "[8,    40] loss: 0.132\n",
            "[9,    10] loss: 0.112\n",
            "[9,    20] loss: 0.084\n",
            "[9,    30] loss: 0.074\n",
            "[9,    40] loss: 0.067\n",
            "[10,    10] loss: 0.060\n",
            "[10,    20] loss: 0.039\n",
            "[10,    30] loss: 0.039\n",
            "[10,    40] loss: 0.046\n",
            "[11,    10] loss: 0.063\n",
            "[11,    20] loss: 0.056\n",
            "[11,    30] loss: 0.060\n",
            "[11,    40] loss: 0.033\n",
            "[12,    10] loss: 0.021\n",
            "[12,    20] loss: 0.021\n",
            "[12,    30] loss: 0.018\n",
            "[12,    40] loss: 0.014\n",
            "[13,    10] loss: 0.017\n",
            "[13,    20] loss: 0.014\n",
            "[13,    30] loss: 0.011\n",
            "[13,    40] loss: 0.011\n",
            "[14,    10] loss: 0.006\n",
            "[14,    20] loss: 0.007\n",
            "[14,    30] loss: 0.006\n",
            "[14,    40] loss: 0.004\n",
            "[15,    10] loss: 0.003\n",
            "[15,    20] loss: 0.003\n",
            "[15,    30] loss: 0.003\n",
            "[15,    40] loss: 0.003\n",
            "[16,    10] loss: 0.003\n",
            "[16,    20] loss: 0.002\n",
            "[16,    30] loss: 0.002\n",
            "[16,    40] loss: 0.002\n",
            "[17,    10] loss: 0.002\n",
            "[17,    20] loss: 0.002\n",
            "[17,    30] loss: 0.002\n",
            "[17,    40] loss: 0.002\n",
            "[18,    10] loss: 0.002\n",
            "[18,    20] loss: 0.001\n",
            "[18,    30] loss: 0.001\n",
            "[18,    40] loss: 0.011\n",
            "[19,    10] loss: 0.057\n",
            "[19,    20] loss: 0.055\n",
            "[19,    30] loss: 0.033\n",
            "[19,    40] loss: 0.031\n",
            "[20,    10] loss: 0.063\n",
            "[20,    20] loss: 0.048\n",
            "[20,    30] loss: 0.038\n",
            "[20,    40] loss: 0.025\n",
            "[21,    10] loss: 0.030\n",
            "[21,    20] loss: 0.025\n",
            "[21,    30] loss: 0.023\n",
            "[21,    40] loss: 0.013\n",
            "[22,    10] loss: 0.009\n",
            "[22,    20] loss: 0.008\n",
            "[22,    30] loss: 0.008\n",
            "[22,    40] loss: 0.007\n",
            "[23,    10] loss: 0.004\n",
            "[23,    20] loss: 0.004\n",
            "[23,    30] loss: 0.002\n",
            "[23,    40] loss: 0.004\n",
            "[24,    10] loss: 0.002\n",
            "[24,    20] loss: 0.004\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.003\n",
            "[25,    10] loss: 0.002\n",
            "[25,    20] loss: 0.001\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.001\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.001\n",
            "[28,    10] loss: 0.000\n",
            "[28,    20] loss: 0.001\n",
            "[28,    30] loss: 0.001\n",
            "[28,    40] loss: 0.003\n",
            "[29,    10] loss: 0.009\n",
            "[29,    20] loss: 0.009\n",
            "[29,    30] loss: 0.007\n",
            "[29,    40] loss: 0.009\n",
            "[30,    10] loss: 0.006\n",
            "[30,    20] loss: 0.005\n",
            "[30,    30] loss: 0.005\n",
            "[30,    40] loss: 0.003\n",
            "[31,    10] loss: 0.002\n",
            "[31,    20] loss: 0.002\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.004\n",
            "[33,    10] loss: 0.007\n",
            "[33,    20] loss: 0.005\n",
            "[33,    30] loss: 0.006\n",
            "[33,    40] loss: 0.005\n",
            "[34,    10] loss: 0.003\n",
            "[34,    20] loss: 0.003\n",
            "[34,    30] loss: 0.002\n",
            "[34,    40] loss: 0.002\n",
            "[35,    10] loss: 0.001\n",
            "[35,    20] loss: 0.001\n",
            "[35,    30] loss: 0.001\n",
            "[35,    40] loss: 0.002\n",
            "[36,    10] loss: 0.001\n",
            "[36,    20] loss: 0.001\n",
            "[36,    30] loss: 0.001\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.000\n",
            "[37,    40] loss: 0.000\n",
            "[38,    10] loss: 0.000\n",
            "[38,    20] loss: 0.000\n",
            "[38,    30] loss: 0.000\n",
            "[38,    40] loss: 0.000\n",
            "[39,    10] loss: 0.000\n",
            "[39,    20] loss: 0.000\n",
            "[39,    30] loss: 0.000\n",
            "[39,    40] loss: 0.000\n",
            "[40,    10] loss: 0.000\n",
            "[40,    20] loss: 0.000\n",
            "[40,    30] loss: 0.000\n",
            "[40,    40] loss: 0.000\n",
            "[41,    10] loss: 0.000\n",
            "[41,    20] loss: 0.000\n",
            "[41,    30] loss: 0.000\n",
            "[41,    40] loss: 0.002\n",
            "[42,    10] loss: 0.002\n",
            "[42,    20] loss: 0.001\n",
            "[42,    30] loss: 0.001\n",
            "[42,    40] loss: 0.002\n",
            "[43,    10] loss: 0.001\n",
            "[43,    20] loss: 0.001\n",
            "[43,    30] loss: 0.001\n",
            "[43,    40] loss: 0.003\n",
            "[44,    10] loss: 0.019\n",
            "[44,    20] loss: 0.010\n",
            "[44,    30] loss: 0.016\n",
            "[44,    40] loss: 0.007\n",
            "[45,    10] loss: 0.005\n",
            "[45,    20] loss: 0.004\n",
            "[45,    30] loss: 0.002\n",
            "[45,    40] loss: 0.005\n",
            "[46,    10] loss: 0.010\n",
            "[46,    20] loss: 0.008\n",
            "[46,    30] loss: 0.010\n",
            "[46,    40] loss: 0.009\n",
            "[47,    10] loss: 0.013\n",
            "[47,    20] loss: 0.011\n",
            "[47,    30] loss: 0.008\n",
            "[47,    40] loss: 0.011\n",
            "[48,    10] loss: 0.009\n",
            "[48,    20] loss: 0.004\n",
            "[48,    30] loss: 0.005\n",
            "[48,    40] loss: 0.004\n",
            "[49,    10] loss: 0.003\n",
            "[49,    20] loss: 0.003\n",
            "[49,    30] loss: 0.001\n",
            "[49,    40] loss: 0.002\n",
            "[50,    10] loss: 0.001\n",
            "[50,    20] loss: 0.000\n",
            "[50,    30] loss: 0.001\n",
            "[50,    40] loss: 0.010\n",
            "[51,    10] loss: 0.083\n",
            "[51,    20] loss: 0.068\n",
            "[51,    30] loss: 0.057\n",
            "[51,    40] loss: 0.075\n",
            "[52,    10] loss: 0.190\n",
            "[52,    20] loss: 0.161\n",
            "[52,    30] loss: 0.126\n",
            "[52,    40] loss: 0.092\n",
            "[53,    10] loss: 0.060\n",
            "[53,    20] loss: 0.037\n",
            "[53,    30] loss: 0.029\n",
            "[53,    40] loss: 0.080\n",
            "[54,    10] loss: 0.099\n",
            "[54,    20] loss: 0.087\n",
            "[54,    30] loss: 0.044\n",
            "[54,    40] loss: 0.071\n",
            "[55,    10] loss: 0.142\n",
            "[55,    20] loss: 0.065\n",
            "[55,    30] loss: 0.047\n",
            "[55,    40] loss: 0.055\n",
            "[56,    10] loss: 0.045\n",
            "[56,    20] loss: 0.055\n",
            "[56,    30] loss: 0.024\n",
            "[56,    40] loss: 0.027\n",
            "[57,    10] loss: 0.025\n",
            "[57,    20] loss: 0.025\n",
            "[57,    30] loss: 0.013\n",
            "[57,    40] loss: 0.012\n",
            "[58,    10] loss: 0.011\n",
            "[58,    20] loss: 0.010\n",
            "[58,    30] loss: 0.007\n",
            "[58,    40] loss: 0.019\n",
            "[59,    10] loss: 0.032\n",
            "[59,    20] loss: 0.027\n",
            "[59,    30] loss: 0.025\n",
            "[59,    40] loss: 0.012\n",
            "[60,    10] loss: 0.007\n",
            "[60,    20] loss: 0.009\n",
            "[60,    30] loss: 0.007\n",
            "[60,    40] loss: 0.004\n",
            "[61,    10] loss: 0.003\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.003\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.014\n",
            "[64,    10] loss: 0.057\n",
            "[64,    20] loss: 0.043\n",
            "[64,    30] loss: 0.029\n",
            "[64,    40] loss: 0.038\n",
            "[65,    10] loss: 0.075\n",
            "[65,    20] loss: 0.056\n",
            "[65,    30] loss: 0.034\n",
            "[65,    40] loss: 0.019\n",
            "[66,    10] loss: 0.011\n",
            "[66,    20] loss: 0.007\n",
            "[66,    30] loss: 0.008\n",
            "[66,    40] loss: 0.008\n",
            "[67,    10] loss: 0.003\n",
            "[67,    20] loss: 0.003\n",
            "[67,    30] loss: 0.002\n",
            "[67,    40] loss: 0.004\n",
            "[68,    10] loss: 0.004\n",
            "[68,    20] loss: 0.004\n",
            "[68,    30] loss: 0.003\n",
            "[68,    40] loss: 0.004\n",
            "[69,    10] loss: 0.002\n",
            "[69,    20] loss: 0.003\n",
            "[69,    30] loss: 0.002\n",
            "[69,    40] loss: 0.004\n",
            "[70,    10] loss: 0.006\n",
            "[70,    20] loss: 0.005\n",
            "[70,    30] loss: 0.002\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 81 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 0.881\n",
            "[1,    20] loss: 0.607\n",
            "[1,    30] loss: 0.535\n",
            "[1,    40] loss: 0.500\n",
            "[2,    10] loss: 0.464\n",
            "[2,    20] loss: 0.410\n",
            "[2,    30] loss: 0.374\n",
            "[2,    40] loss: 0.392\n",
            "[3,    10] loss: 0.376\n",
            "[3,    20] loss: 0.327\n",
            "[3,    30] loss: 0.307\n",
            "[3,    40] loss: 0.250\n",
            "[4,    10] loss: 0.304\n",
            "[4,    20] loss: 0.269\n",
            "[4,    30] loss: 0.240\n",
            "[4,    40] loss: 0.209\n",
            "[5,    10] loss: 0.189\n",
            "[5,    20] loss: 0.185\n",
            "[5,    30] loss: 0.150\n",
            "[5,    40] loss: 0.149\n",
            "[6,    10] loss: 0.137\n",
            "[6,    20] loss: 0.141\n",
            "[6,    30] loss: 0.118\n",
            "[6,    40] loss: 0.101\n",
            "[7,    10] loss: 0.132\n",
            "[7,    20] loss: 0.107\n",
            "[7,    30] loss: 0.112\n",
            "[7,    40] loss: 0.090\n",
            "[8,    10] loss: 0.069\n",
            "[8,    20] loss: 0.060\n",
            "[8,    30] loss: 0.049\n",
            "[8,    40] loss: 0.061\n",
            "[9,    10] loss: 0.104\n",
            "[9,    20] loss: 0.096\n",
            "[9,    30] loss: 0.068\n",
            "[9,    40] loss: 0.052\n",
            "[10,    10] loss: 0.045\n",
            "[10,    20] loss: 0.031\n",
            "[10,    30] loss: 0.028\n",
            "[10,    40] loss: 0.024\n",
            "[11,    10] loss: 0.014\n",
            "[11,    20] loss: 0.014\n",
            "[11,    30] loss: 0.010\n",
            "[11,    40] loss: 0.010\n",
            "[12,    10] loss: 0.006\n",
            "[12,    20] loss: 0.006\n",
            "[12,    30] loss: 0.004\n",
            "[12,    40] loss: 0.004\n",
            "[13,    10] loss: 0.003\n",
            "[13,    20] loss: 0.003\n",
            "[13,    30] loss: 0.003\n",
            "[13,    40] loss: 0.019\n",
            "[14,    10] loss: 0.261\n",
            "[14,    20] loss: 0.243\n",
            "[14,    30] loss: 0.247\n",
            "[14,    40] loss: 0.163\n",
            "[15,    10] loss: 0.135\n",
            "[15,    20] loss: 0.110\n",
            "[15,    30] loss: 0.082\n",
            "[15,    40] loss: 0.073\n",
            "[16,    10] loss: 0.057\n",
            "[16,    20] loss: 0.055\n",
            "[16,    30] loss: 0.045\n",
            "[16,    40] loss: 0.087\n",
            "[17,    10] loss: 0.185\n",
            "[17,    20] loss: 0.170\n",
            "[17,    30] loss: 0.145\n",
            "[17,    40] loss: 0.129\n",
            "[18,    10] loss: 0.126\n",
            "[18,    20] loss: 0.086\n",
            "[18,    30] loss: 0.087\n",
            "[18,    40] loss: 0.060\n",
            "[19,    10] loss: 0.054\n",
            "[19,    20] loss: 0.044\n",
            "[19,    30] loss: 0.036\n",
            "[19,    40] loss: 0.026\n",
            "[20,    10] loss: 0.015\n",
            "[20,    20] loss: 0.010\n",
            "[20,    30] loss: 0.009\n",
            "[20,    40] loss: 0.010\n",
            "[21,    10] loss: 0.006\n",
            "[21,    20] loss: 0.004\n",
            "[21,    30] loss: 0.005\n",
            "[21,    40] loss: 0.012\n",
            "[22,    10] loss: 0.022\n",
            "[22,    20] loss: 0.013\n",
            "[22,    30] loss: 0.011\n",
            "[22,    40] loss: 0.013\n",
            "[23,    10] loss: 0.015\n",
            "[23,    20] loss: 0.012\n",
            "[23,    30] loss: 0.008\n",
            "[23,    40] loss: 0.005\n",
            "[24,    10] loss: 0.003\n",
            "[24,    20] loss: 0.003\n",
            "[24,    30] loss: 0.002\n",
            "[24,    40] loss: 0.002\n",
            "[25,    10] loss: 0.002\n",
            "[25,    20] loss: 0.001\n",
            "[25,    30] loss: 0.001\n",
            "[25,    40] loss: 0.002\n",
            "[26,    10] loss: 0.001\n",
            "[26,    20] loss: 0.001\n",
            "[26,    30] loss: 0.001\n",
            "[26,    40] loss: 0.001\n",
            "[27,    10] loss: 0.001\n",
            "[27,    20] loss: 0.001\n",
            "[27,    30] loss: 0.001\n",
            "[27,    40] loss: 0.004\n",
            "[28,    10] loss: 0.006\n",
            "[28,    20] loss: 0.004\n",
            "[28,    30] loss: 0.004\n",
            "[28,    40] loss: 0.005\n",
            "[29,    10] loss: 0.004\n",
            "[29,    20] loss: 0.003\n",
            "[29,    30] loss: 0.001\n",
            "[29,    40] loss: 0.002\n",
            "[30,    10] loss: 0.001\n",
            "[30,    20] loss: 0.001\n",
            "[30,    30] loss: 0.001\n",
            "[30,    40] loss: 0.001\n",
            "[31,    10] loss: 0.001\n",
            "[31,    20] loss: 0.001\n",
            "[31,    30] loss: 0.001\n",
            "[31,    40] loss: 0.001\n",
            "[32,    10] loss: 0.001\n",
            "[32,    20] loss: 0.001\n",
            "[32,    30] loss: 0.001\n",
            "[32,    40] loss: 0.001\n",
            "[33,    10] loss: 0.001\n",
            "[33,    20] loss: 0.001\n",
            "[33,    30] loss: 0.001\n",
            "[33,    40] loss: 0.000\n",
            "[34,    10] loss: 0.000\n",
            "[34,    20] loss: 0.000\n",
            "[34,    30] loss: 0.000\n",
            "[34,    40] loss: 0.001\n",
            "[35,    10] loss: 0.000\n",
            "[35,    20] loss: 0.000\n",
            "[35,    30] loss: 0.000\n",
            "[35,    40] loss: 0.000\n",
            "[36,    10] loss: 0.000\n",
            "[36,    20] loss: 0.000\n",
            "[36,    30] loss: 0.000\n",
            "[36,    40] loss: 0.001\n",
            "[37,    10] loss: 0.001\n",
            "[37,    20] loss: 0.001\n",
            "[37,    30] loss: 0.001\n",
            "[37,    40] loss: 0.001\n",
            "[38,    10] loss: 0.001\n",
            "[38,    20] loss: 0.000\n",
            "[38,    30] loss: 0.000\n",
            "[38,    40] loss: 0.001\n",
            "[39,    10] loss: 0.000\n",
            "[39,    20] loss: 0.000\n",
            "[39,    30] loss: 0.000\n",
            "[39,    40] loss: 0.001\n",
            "[40,    10] loss: 0.000\n",
            "[40,    20] loss: 0.000\n",
            "[40,    30] loss: 0.000\n",
            "[40,    40] loss: 0.001\n",
            "[41,    10] loss: 0.000\n",
            "[41,    20] loss: 0.000\n",
            "[41,    30] loss: 0.000\n",
            "[41,    40] loss: 0.000\n",
            "[42,    10] loss: 0.000\n",
            "[42,    20] loss: 0.000\n",
            "[42,    30] loss: 0.000\n",
            "[42,    40] loss: 0.000\n",
            "[43,    10] loss: 0.000\n",
            "[43,    20] loss: 0.000\n",
            "[43,    30] loss: 0.000\n",
            "[43,    40] loss: 0.000\n",
            "[44,    10] loss: 0.000\n",
            "[44,    20] loss: 0.000\n",
            "[44,    30] loss: 0.000\n",
            "[44,    40] loss: 0.000\n",
            "[45,    10] loss: 0.000\n",
            "[45,    20] loss: 0.000\n",
            "[45,    30] loss: 0.000\n",
            "[45,    40] loss: 0.000\n",
            "[46,    10] loss: 0.000\n",
            "[46,    20] loss: 0.000\n",
            "[46,    30] loss: 0.000\n",
            "[46,    40] loss: 0.004\n",
            "[47,    10] loss: 0.012\n",
            "[47,    20] loss: 0.015\n",
            "[47,    30] loss: 0.012\n",
            "[47,    40] loss: 0.054\n",
            "[48,    10] loss: 0.270\n",
            "[48,    20] loss: 0.233\n",
            "[48,    30] loss: 0.163\n",
            "[48,    40] loss: 0.150\n",
            "[49,    10] loss: 0.096\n",
            "[49,    20] loss: 0.072\n",
            "[49,    30] loss: 0.047\n",
            "[49,    40] loss: 0.048\n",
            "[50,    10] loss: 0.024\n",
            "[50,    20] loss: 0.018\n",
            "[50,    30] loss: 0.017\n",
            "[50,    40] loss: 0.010\n",
            "[51,    10] loss: 0.007\n",
            "[51,    20] loss: 0.008\n",
            "[51,    30] loss: 0.006\n",
            "[51,    40] loss: 0.007\n",
            "[52,    10] loss: 0.004\n",
            "[52,    20] loss: 0.004\n",
            "[52,    30] loss: 0.004\n",
            "[52,    40] loss: 0.008\n",
            "[53,    10] loss: 0.015\n",
            "[53,    20] loss: 0.012\n",
            "[53,    30] loss: 0.008\n",
            "[53,    40] loss: 0.008\n",
            "[54,    10] loss: 0.005\n",
            "[54,    20] loss: 0.008\n",
            "[54,    30] loss: 0.004\n",
            "[54,    40] loss: 0.003\n",
            "[55,    10] loss: 0.003\n",
            "[55,    20] loss: 0.001\n",
            "[55,    30] loss: 0.002\n",
            "[55,    40] loss: 0.002\n",
            "[56,    10] loss: 0.001\n",
            "[56,    20] loss: 0.002\n",
            "[56,    30] loss: 0.001\n",
            "[56,    40] loss: 0.008\n",
            "[57,    10] loss: 0.028\n",
            "[57,    20] loss: 0.022\n",
            "[57,    30] loss: 0.015\n",
            "[57,    40] loss: 0.013\n",
            "[58,    10] loss: 0.005\n",
            "[58,    20] loss: 0.005\n",
            "[58,    30] loss: 0.003\n",
            "[58,    40] loss: 0.003\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.001\n",
            "[59,    30] loss: 0.001\n",
            "[59,    40] loss: 0.001\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.001\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.005\n",
            "[64,    10] loss: 0.013\n",
            "[64,    20] loss: 0.010\n",
            "[64,    30] loss: 0.005\n",
            "[64,    40] loss: 0.008\n",
            "[65,    10] loss: 0.046\n",
            "[65,    20] loss: 0.036\n",
            "[65,    30] loss: 0.018\n",
            "[65,    40] loss: 0.055\n",
            "[66,    10] loss: 0.205\n",
            "[66,    20] loss: 0.217\n",
            "[66,    30] loss: 0.114\n",
            "[66,    40] loss: 0.086\n",
            "[67,    10] loss: 0.058\n",
            "[67,    20] loss: 0.045\n",
            "[67,    30] loss: 0.025\n",
            "[67,    40] loss: 0.023\n",
            "[68,    10] loss: 0.011\n",
            "[68,    20] loss: 0.006\n",
            "[68,    30] loss: 0.007\n",
            "[68,    40] loss: 0.011\n",
            "[69,    10] loss: 0.014\n",
            "[69,    20] loss: 0.016\n",
            "[69,    30] loss: 0.010\n",
            "[69,    40] loss: 0.012\n",
            "[70,    10] loss: 0.037\n",
            "[70,    20] loss: 0.046\n",
            "[70,    30] loss: 0.025\n",
            "[70,    40] loss: 0.016\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 53 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 66 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 79 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AbZaQekCfVjN",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ouBomi5DfVjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "85e290c4-e3ca-4308-dc92-809412a0c05b"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "plt.title(\"alpha value 0.5\")\n",
        "plt.legend()\n",
        "fig.savefig(\"Figure.pdf\")\n",
        "fig.savefig(\"Figure.png\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlY1RknNfVjb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uMVKmRRhfVjd",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}