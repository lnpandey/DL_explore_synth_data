{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "complexity_type4_first_layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm4jv30eD_hl"
      },
      "source": [
        "# data =  [{\"mosaic_list\":mosaic_list_of_images, \"mosaic_label\": mosaic_label, \"fore_idx\":fore_idx}]\n",
        "# np.save(\"mosaic_data.npy\",data)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\n",
        "mosaic_label = data[0][\"mosaic_label\"]\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,300)  #,self.output)\n",
        "        #self.linear2 = nn.Linear(6,12)\n",
        "        self.linear2 = nn.Linear(300,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,300], dtype=torch.float64)\n",
        "        features = torch.zeros([batch,self.K,300],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x) \n",
        "      #x = F.relu(self.linear2(x))\n",
        "      x = self.linear2(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,200)\n",
        "        #self.linear2 = nn.Linear(6,12)\n",
        "        self.linear2 = nn.Linear(200,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      #x = F.relu(self.linear2(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOfxUJZ_eFKw"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(2,1,9,2).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(300,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.01)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.01)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 1000\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEabNK9Q1bTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "ad5cbc88-30ab-470a-aec1-bb22c6ecdc69"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa906003690>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "004f7b0b-6948-4a3f-b9d8-61e9c6929684"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([62.47166667, 22.115     ,  1.24833333, 14.165     ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"synthetic_first_300_200.csv\",index=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "PzR8ISPlOSbP",
        "outputId": "066ea1fb-3b58-48de-d398-0fda5ea1de6e"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.166667</td>\n",
              "      <td>28.833333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>80.266667</td>\n",
              "      <td>19.633333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>95.800000</td>\n",
              "      <td>4.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>68.733333</td>\n",
              "      <td>31.066667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.900000</td>\n",
              "      <td>25.800000</td>\n",
              "      <td>9.600000</td>\n",
              "      <td>43.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>36.766667</td>\n",
              "      <td>1.833333</td>\n",
              "      <td>61.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>98.166667</td>\n",
              "      <td>1.566667</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>69.166667</td>\n",
              "      <td>30.733333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>95.000000</td>\n",
              "      <td>4.900000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>79.433333</td>\n",
              "      <td>20.466667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.366667</td>\n",
              "      <td>36.400000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>62.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>99.466667</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>68.733333</td>\n",
              "      <td>31.066667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>72.266667</td>\n",
              "      <td>27.566667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>69.433333</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>91.766667</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>68.900000</td>\n",
              "      <td>31.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>36.766667</td>\n",
              "      <td>1.733333</td>\n",
              "      <td>61.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.166667</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>9.733333</td>\n",
              "      <td>53.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>99.700000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         FTPT       FFPT      FTPF       FFPF\n",
              "0   71.166667  28.833333  0.000000   0.000000\n",
              "1   80.266667  19.633333  0.100000   0.000000\n",
              "2   95.800000   4.100000  0.100000   0.000000\n",
              "3   68.733333  31.066667  0.100000   0.100000\n",
              "4   20.900000  25.800000  9.600000  43.700000\n",
              "5    0.000000  36.766667  1.833333  61.400000\n",
              "6   98.166667   1.566667  0.266667   0.000000\n",
              "7   69.166667  30.733333  0.100000   0.000000\n",
              "8   95.000000   4.900000  0.100000   0.000000\n",
              "9   79.433333  20.466667  0.100000   0.000000\n",
              "10   0.366667  36.400000  0.333333  62.900000\n",
              "11  99.466667   0.300000  0.233333   0.000000\n",
              "12  68.733333  31.066667  0.133333   0.066667\n",
              "13  72.266667  27.566667  0.066667   0.100000\n",
              "14  69.433333  30.500000  0.033333   0.033333\n",
              "15  91.766667   8.000000  0.233333   0.000000\n",
              "16  68.900000  31.100000  0.000000   0.000000\n",
              "17   0.000000  36.766667  1.733333  61.500000\n",
              "18   0.166667  36.600000  9.733333  53.500000\n",
              "19  99.700000   0.133333  0.166667   0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}