{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type4_First_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\r\n",
        "mosaic_label = data[0][\"mosaic_label\"]\r\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,50)  #,self.output)\n",
        "        self.linear2 = nn.Linear(50,50)\n",
        "        self.linear3 = nn.Linear(50,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,50], dtype=torch.float64)\n",
        "        features = torch.zeros([batch,self.K,50],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x) \n",
        "      x = F.relu(self.linear2(x))\n",
        "      x = self.linear3(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,50)\n",
        "        #self.linear2 = nn.Linear(6,12)\n",
        "        self.linear2 = nn.Linear(50,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      #x = F.relu(self.linear2(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOfxUJZ_eFKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a122b4d7-9644-4533-cf8b-703a26500986"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(2,1,9,2).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(50,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.001)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.001)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 2500\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: [0 ] loss: 1.199\n",
            "epoch: [1] loss: 1.191\n",
            "epoch: [2] loss: 1.187\n",
            "epoch: [3] loss: 1.183\n",
            "epoch: [4] loss: 1.178\n",
            "epoch: [5] loss: 1.172\n",
            "epoch: [6] loss: 1.165\n",
            "epoch: [7] loss: 1.156\n",
            "epoch: [8] loss: 1.143\n",
            "epoch: [9] loss: 1.128\n",
            "epoch: [10] loss: 1.103\n",
            "epoch: [11] loss: 1.081\n",
            "epoch: [12] loss: 1.042\n",
            "epoch: [13] loss: 1.004\n",
            "epoch: [14] loss: 0.982\n",
            "epoch: [15] loss: 0.934\n",
            "epoch: [16] loss: 0.876\n",
            "epoch: [17] loss: 0.827\n",
            "epoch: [18] loss: 0.775\n",
            "epoch: [19] loss: 0.720\n",
            "epoch: [20] loss: 0.669\n",
            "epoch: [21] loss: 0.604\n",
            "epoch: [22] loss: 0.553\n",
            "epoch: [23] loss: 0.474\n",
            "epoch: [24] loss: 0.415\n",
            "epoch: [25] loss: 0.355\n",
            "epoch: [26] loss: 0.301\n",
            "epoch: [27] loss: 0.250\n",
            "epoch: [28] loss: 0.212\n",
            "epoch: [29] loss: 0.174\n",
            "epoch: [30] loss: 0.147\n",
            "epoch: [31] loss: 0.125\n",
            "epoch: [32] loss: 0.110\n",
            "epoch: [33] loss: 0.096\n",
            "epoch: [34] loss: 0.081\n",
            "epoch: [35] loss: 0.072\n",
            "epoch: [36] loss: 0.064\n",
            "epoch: [37] loss: 0.056\n",
            "epoch: [38] loss: 0.049\n",
            "epoch: [39] loss: 0.044\n",
            "epoch: [40] loss: 0.038\n",
            "epoch: [41] loss: 0.035\n",
            "epoch: [42] loss: 0.031\n",
            "epoch: [43] loss: 0.028\n",
            "epoch: [44] loss: 0.025\n",
            "epoch: [45] loss: 0.023\n",
            "epoch: [46] loss: 0.020\n",
            "epoch: [47] loss: 0.018\n",
            "epoch: [48] loss: 0.017\n",
            "epoch: [49] loss: 0.015\n",
            "epoch: [50] loss: 0.013\n",
            "epoch: [51] loss: 0.012\n",
            "epoch: [52] loss: 0.011\n",
            "epoch: [53] loss: 0.010\n",
            "Finished Training run 0\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.211\n",
            "epoch: [1] loss: 1.196\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.185\n",
            "epoch: [4] loss: 1.181\n",
            "epoch: [5] loss: 1.175\n",
            "epoch: [6] loss: 1.169\n",
            "epoch: [7] loss: 1.160\n",
            "epoch: [8] loss: 1.149\n",
            "epoch: [9] loss: 1.131\n",
            "epoch: [10] loss: 1.102\n",
            "epoch: [11] loss: 1.060\n",
            "epoch: [12] loss: 1.002\n",
            "epoch: [13] loss: 0.936\n",
            "epoch: [14] loss: 0.863\n",
            "epoch: [15] loss: 0.790\n",
            "epoch: [16] loss: 0.723\n",
            "epoch: [17] loss: 0.658\n",
            "epoch: [18] loss: 0.595\n",
            "epoch: [19] loss: 0.541\n",
            "epoch: [20] loss: 0.482\n",
            "epoch: [21] loss: 0.424\n",
            "epoch: [22] loss: 0.367\n",
            "epoch: [23] loss: 0.310\n",
            "epoch: [24] loss: 0.262\n",
            "epoch: [25] loss: 0.219\n",
            "epoch: [26] loss: 0.186\n",
            "epoch: [27] loss: 0.160\n",
            "epoch: [28] loss: 0.134\n",
            "epoch: [29] loss: 0.116\n",
            "epoch: [30] loss: 0.100\n",
            "epoch: [31] loss: 0.088\n",
            "epoch: [32] loss: 0.075\n",
            "epoch: [33] loss: 0.066\n",
            "epoch: [34] loss: 0.056\n",
            "epoch: [35] loss: 0.050\n",
            "epoch: [36] loss: 0.044\n",
            "epoch: [37] loss: 0.039\n",
            "epoch: [38] loss: 0.035\n",
            "epoch: [39] loss: 0.032\n",
            "epoch: [40] loss: 0.029\n",
            "epoch: [41] loss: 0.026\n",
            "epoch: [42] loss: 0.024\n",
            "epoch: [43] loss: 0.022\n",
            "epoch: [44] loss: 0.020\n",
            "epoch: [45] loss: 0.022\n",
            "epoch: [46] loss: 0.018\n",
            "epoch: [47] loss: 0.017\n",
            "epoch: [48] loss: 0.015\n",
            "epoch: [49] loss: 0.014\n",
            "epoch: [50] loss: 0.013\n",
            "epoch: [51] loss: 0.012\n",
            "epoch: [52] loss: 0.012\n",
            "epoch: [53] loss: 0.011\n",
            "epoch: [54] loss: 0.010\n",
            "epoch: [55] loss: 0.010\n",
            "epoch: [56] loss: 0.009\n",
            "Finished Training run 1\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.193\n",
            "epoch: [3] loss: 1.191\n",
            "epoch: [4] loss: 1.189\n",
            "epoch: [5] loss: 1.186\n",
            "epoch: [6] loss: 1.183\n",
            "epoch: [7] loss: 1.180\n",
            "epoch: [8] loss: 1.175\n",
            "epoch: [9] loss: 1.169\n",
            "epoch: [10] loss: 1.161\n",
            "epoch: [11] loss: 1.148\n",
            "epoch: [12] loss: 1.131\n",
            "epoch: [13] loss: 1.105\n",
            "epoch: [14] loss: 1.073\n",
            "epoch: [15] loss: 1.029\n",
            "epoch: [16] loss: 0.977\n",
            "epoch: [17] loss: 0.915\n",
            "epoch: [18] loss: 0.846\n",
            "epoch: [19] loss: 0.780\n",
            "epoch: [20] loss: 0.721\n",
            "epoch: [21] loss: 0.645\n",
            "epoch: [22] loss: 0.573\n",
            "epoch: [23] loss: 0.500\n",
            "epoch: [24] loss: 0.420\n",
            "epoch: [25] loss: 0.326\n",
            "epoch: [26] loss: 0.240\n",
            "epoch: [27] loss: 0.167\n",
            "epoch: [28] loss: 0.120\n",
            "epoch: [29] loss: 0.082\n",
            "epoch: [30] loss: 0.061\n",
            "epoch: [31] loss: 0.046\n",
            "epoch: [32] loss: 0.037\n",
            "epoch: [33] loss: 0.029\n",
            "epoch: [34] loss: 0.024\n",
            "epoch: [35] loss: 0.020\n",
            "epoch: [36] loss: 0.017\n",
            "epoch: [37] loss: 0.014\n",
            "epoch: [38] loss: 0.013\n",
            "epoch: [39] loss: 0.011\n",
            "epoch: [40] loss: 0.010\n",
            "Finished Training run 2\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.207\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.192\n",
            "epoch: [3] loss: 1.189\n",
            "epoch: [4] loss: 1.185\n",
            "epoch: [5] loss: 1.180\n",
            "epoch: [6] loss: 1.173\n",
            "epoch: [7] loss: 1.165\n",
            "epoch: [8] loss: 1.157\n",
            "epoch: [9] loss: 1.141\n",
            "epoch: [10] loss: 1.123\n",
            "epoch: [11] loss: 1.093\n",
            "epoch: [12] loss: 1.053\n",
            "epoch: [13] loss: 1.014\n",
            "epoch: [14] loss: 0.968\n",
            "epoch: [15] loss: 0.914\n",
            "epoch: [16] loss: 0.875\n",
            "epoch: [17] loss: 0.808\n",
            "epoch: [18] loss: 0.757\n",
            "epoch: [19] loss: 0.694\n",
            "epoch: [20] loss: 0.639\n",
            "epoch: [21] loss: 0.587\n",
            "epoch: [22] loss: 0.541\n",
            "epoch: [23] loss: 0.493\n",
            "epoch: [24] loss: 0.436\n",
            "epoch: [25] loss: 0.380\n",
            "epoch: [26] loss: 0.323\n",
            "epoch: [27] loss: 0.258\n",
            "epoch: [28] loss: 0.198\n",
            "epoch: [29] loss: 0.150\n",
            "epoch: [30] loss: 0.113\n",
            "epoch: [31] loss: 0.087\n",
            "epoch: [32] loss: 0.068\n",
            "epoch: [33] loss: 0.054\n",
            "epoch: [34] loss: 0.045\n",
            "epoch: [35] loss: 0.037\n",
            "epoch: [36] loss: 0.031\n",
            "epoch: [37] loss: 0.026\n",
            "epoch: [38] loss: 0.022\n",
            "epoch: [39] loss: 0.018\n",
            "epoch: [40] loss: 0.016\n",
            "epoch: [41] loss: 0.013\n",
            "epoch: [42] loss: 0.012\n",
            "epoch: [43] loss: 0.010\n",
            "epoch: [44] loss: 0.009\n",
            "Finished Training run 3\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.188\n",
            "epoch: [2] loss: 1.181\n",
            "epoch: [3] loss: 1.174\n",
            "epoch: [4] loss: 1.166\n",
            "epoch: [5] loss: 1.155\n",
            "epoch: [6] loss: 1.141\n",
            "epoch: [7] loss: 1.118\n",
            "epoch: [8] loss: 1.085\n",
            "epoch: [9] loss: 1.038\n",
            "epoch: [10] loss: 0.989\n",
            "epoch: [11] loss: 0.938\n",
            "epoch: [12] loss: 0.886\n",
            "epoch: [13] loss: 0.840\n",
            "epoch: [14] loss: 0.797\n",
            "epoch: [15] loss: 0.748\n",
            "epoch: [16] loss: 0.708\n",
            "epoch: [17] loss: 0.673\n",
            "epoch: [18] loss: 0.629\n",
            "epoch: [19] loss: 0.591\n",
            "epoch: [20] loss: 0.543\n",
            "epoch: [21] loss: 0.489\n",
            "epoch: [22] loss: 0.432\n",
            "epoch: [23] loss: 0.374\n",
            "epoch: [24] loss: 0.334\n",
            "epoch: [25] loss: 0.289\n",
            "epoch: [26] loss: 0.255\n",
            "epoch: [27] loss: 0.227\n",
            "epoch: [28] loss: 0.205\n",
            "epoch: [29] loss: 0.185\n",
            "epoch: [30] loss: 0.168\n",
            "epoch: [31] loss: 0.149\n",
            "epoch: [32] loss: 0.135\n",
            "epoch: [33] loss: 0.125\n",
            "epoch: [34] loss: 0.114\n",
            "epoch: [35] loss: 0.105\n",
            "epoch: [36] loss: 0.102\n",
            "epoch: [37] loss: 0.098\n",
            "epoch: [38] loss: 0.084\n",
            "epoch: [39] loss: 0.078\n",
            "epoch: [40] loss: 0.074\n",
            "epoch: [41] loss: 0.070\n",
            "epoch: [42] loss: 0.064\n",
            "epoch: [43] loss: 0.059\n",
            "epoch: [44] loss: 0.056\n",
            "epoch: [45] loss: 0.053\n",
            "epoch: [46] loss: 0.055\n",
            "epoch: [47] loss: 0.046\n",
            "epoch: [48] loss: 0.043\n",
            "epoch: [49] loss: 0.043\n",
            "epoch: [50] loss: 0.038\n",
            "epoch: [51] loss: 0.035\n",
            "epoch: [52] loss: 0.034\n",
            "epoch: [53] loss: 0.033\n",
            "epoch: [54] loss: 0.032\n",
            "epoch: [55] loss: 0.028\n",
            "epoch: [56] loss: 0.025\n",
            "epoch: [57] loss: 0.025\n",
            "epoch: [58] loss: 0.024\n",
            "epoch: [59] loss: 0.024\n",
            "epoch: [60] loss: 0.021\n",
            "epoch: [61] loss: 0.021\n",
            "epoch: [62] loss: 0.019\n",
            "epoch: [63] loss: 0.017\n",
            "epoch: [64] loss: 0.017\n",
            "epoch: [65] loss: 0.017\n",
            "epoch: [66] loss: 0.017\n",
            "epoch: [67] loss: 0.014\n",
            "epoch: [68] loss: 0.015\n",
            "epoch: [69] loss: 0.013\n",
            "epoch: [70] loss: 0.014\n",
            "epoch: [71] loss: 0.013\n",
            "epoch: [72] loss: 0.013\n",
            "epoch: [73] loss: 0.011\n",
            "epoch: [74] loss: 0.011\n",
            "epoch: [75] loss: 0.011\n",
            "epoch: [76] loss: 0.010\n",
            "epoch: [77] loss: 0.010\n",
            "epoch: [78] loss: 0.009\n",
            "Finished Training run 4\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.223\n",
            "epoch: [1] loss: 1.193\n",
            "epoch: [2] loss: 1.187\n",
            "epoch: [3] loss: 1.182\n",
            "epoch: [4] loss: 1.176\n",
            "epoch: [5] loss: 1.168\n",
            "epoch: [6] loss: 1.156\n",
            "epoch: [7] loss: 1.140\n",
            "epoch: [8] loss: 1.120\n",
            "epoch: [9] loss: 1.085\n",
            "epoch: [10] loss: 1.042\n",
            "epoch: [11] loss: 0.995\n",
            "epoch: [12] loss: 0.941\n",
            "epoch: [13] loss: 0.888\n",
            "epoch: [14] loss: 0.838\n",
            "epoch: [15] loss: 0.790\n",
            "epoch: [16] loss: 0.751\n",
            "epoch: [17] loss: 0.707\n",
            "epoch: [18] loss: 0.673\n",
            "epoch: [19] loss: 0.637\n",
            "epoch: [20] loss: 0.607\n",
            "epoch: [21] loss: 0.577\n",
            "epoch: [22] loss: 0.545\n",
            "epoch: [23] loss: 0.511\n",
            "epoch: [24] loss: 0.465\n",
            "epoch: [25] loss: 0.416\n",
            "epoch: [26] loss: 0.370\n",
            "epoch: [27] loss: 0.314\n",
            "epoch: [28] loss: 0.263\n",
            "epoch: [29] loss: 0.222\n",
            "epoch: [30] loss: 0.183\n",
            "epoch: [31] loss: 0.154\n",
            "epoch: [32] loss: 0.139\n",
            "epoch: [33] loss: 0.114\n",
            "epoch: [34] loss: 0.099\n",
            "epoch: [35] loss: 0.086\n",
            "epoch: [36] loss: 0.076\n",
            "epoch: [37] loss: 0.066\n",
            "epoch: [38] loss: 0.061\n",
            "epoch: [39] loss: 0.055\n",
            "epoch: [40] loss: 0.050\n",
            "epoch: [41] loss: 0.045\n",
            "epoch: [42] loss: 0.042\n",
            "epoch: [43] loss: 0.038\n",
            "epoch: [44] loss: 0.035\n",
            "epoch: [45] loss: 0.034\n",
            "epoch: [46] loss: 0.033\n",
            "epoch: [47] loss: 0.029\n",
            "epoch: [48] loss: 0.026\n",
            "epoch: [49] loss: 0.025\n",
            "epoch: [50] loss: 0.025\n",
            "epoch: [51] loss: 0.023\n",
            "epoch: [52] loss: 0.021\n",
            "epoch: [53] loss: 0.020\n",
            "epoch: [54] loss: 0.018\n",
            "epoch: [55] loss: 0.017\n",
            "epoch: [56] loss: 0.017\n",
            "epoch: [57] loss: 0.016\n",
            "epoch: [58] loss: 0.015\n",
            "epoch: [59] loss: 0.014\n",
            "epoch: [60] loss: 0.015\n",
            "epoch: [61] loss: 0.014\n",
            "epoch: [62] loss: 0.012\n",
            "epoch: [63] loss: 0.011\n",
            "epoch: [64] loss: 0.011\n",
            "epoch: [65] loss: 0.010\n",
            "epoch: [66] loss: 0.010\n",
            "epoch: [67] loss: 0.009\n",
            "Finished Training run 5\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.228\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.187\n",
            "epoch: [3] loss: 1.183\n",
            "epoch: [4] loss: 1.177\n",
            "epoch: [5] loss: 1.173\n",
            "epoch: [6] loss: 1.165\n",
            "epoch: [7] loss: 1.156\n",
            "epoch: [8] loss: 1.144\n",
            "epoch: [9] loss: 1.132\n",
            "epoch: [10] loss: 1.105\n",
            "epoch: [11] loss: 1.080\n",
            "epoch: [12] loss: 1.053\n",
            "epoch: [13] loss: 1.002\n",
            "epoch: [14] loss: 0.950\n",
            "epoch: [15] loss: 0.897\n",
            "epoch: [16] loss: 0.837\n",
            "epoch: [17] loss: 0.788\n",
            "epoch: [18] loss: 0.727\n",
            "epoch: [19] loss: 0.677\n",
            "epoch: [20] loss: 0.633\n",
            "epoch: [21] loss: 0.595\n",
            "epoch: [22] loss: 0.552\n",
            "epoch: [23] loss: 0.517\n",
            "epoch: [24] loss: 0.472\n",
            "epoch: [25] loss: 0.433\n",
            "epoch: [26] loss: 0.379\n",
            "epoch: [27] loss: 0.330\n",
            "epoch: [28] loss: 0.277\n",
            "epoch: [29] loss: 0.215\n",
            "epoch: [30] loss: 0.174\n",
            "epoch: [31] loss: 0.139\n",
            "epoch: [32] loss: 0.125\n",
            "epoch: [33] loss: 0.100\n",
            "epoch: [34] loss: 0.084\n",
            "epoch: [35] loss: 0.074\n",
            "epoch: [36] loss: 0.068\n",
            "epoch: [37] loss: 0.058\n",
            "epoch: [38] loss: 0.052\n",
            "epoch: [39] loss: 0.048\n",
            "epoch: [40] loss: 0.043\n",
            "epoch: [41] loss: 0.039\n",
            "epoch: [42] loss: 0.036\n",
            "epoch: [43] loss: 0.037\n",
            "epoch: [44] loss: 0.034\n",
            "epoch: [45] loss: 0.029\n",
            "epoch: [46] loss: 0.027\n",
            "epoch: [47] loss: 0.025\n",
            "epoch: [48] loss: 0.028\n",
            "epoch: [49] loss: 0.022\n",
            "epoch: [50] loss: 0.021\n",
            "epoch: [51] loss: 0.022\n",
            "epoch: [52] loss: 0.019\n",
            "epoch: [53] loss: 0.018\n",
            "epoch: [54] loss: 0.017\n",
            "epoch: [55] loss: 0.017\n",
            "epoch: [56] loss: 0.015\n",
            "epoch: [57] loss: 0.015\n",
            "epoch: [58] loss: 0.015\n",
            "epoch: [59] loss: 0.013\n",
            "epoch: [60] loss: 0.013\n",
            "epoch: [61] loss: 0.013\n",
            "epoch: [62] loss: 0.012\n",
            "epoch: [63] loss: 0.011\n",
            "epoch: [64] loss: 0.011\n",
            "epoch: [65] loss: 0.013\n",
            "epoch: [66] loss: 0.011\n",
            "epoch: [67] loss: 0.010\n",
            "epoch: [68] loss: 0.010\n",
            "epoch: [69] loss: 0.009\n",
            "Finished Training run 6\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.203\n",
            "epoch: [1] loss: 1.191\n",
            "epoch: [2] loss: 1.186\n",
            "epoch: [3] loss: 1.181\n",
            "epoch: [4] loss: 1.176\n",
            "epoch: [5] loss: 1.169\n",
            "epoch: [6] loss: 1.160\n",
            "epoch: [7] loss: 1.150\n",
            "epoch: [8] loss: 1.143\n",
            "epoch: [9] loss: 1.124\n",
            "epoch: [10] loss: 1.095\n",
            "epoch: [11] loss: 1.068\n",
            "epoch: [12] loss: 1.025\n",
            "epoch: [13] loss: 0.985\n",
            "epoch: [14] loss: 0.939\n",
            "epoch: [15] loss: 0.897\n",
            "epoch: [16] loss: 0.851\n",
            "epoch: [17] loss: 0.802\n",
            "epoch: [18] loss: 0.753\n",
            "epoch: [19] loss: 0.703\n",
            "epoch: [20] loss: 0.655\n",
            "epoch: [21] loss: 0.611\n",
            "epoch: [22] loss: 0.567\n",
            "epoch: [23] loss: 0.516\n",
            "epoch: [24] loss: 0.463\n",
            "epoch: [25] loss: 0.407\n",
            "epoch: [26] loss: 0.343\n",
            "epoch: [27] loss: 0.290\n",
            "epoch: [28] loss: 0.239\n",
            "epoch: [29] loss: 0.196\n",
            "epoch: [30] loss: 0.156\n",
            "epoch: [31] loss: 0.125\n",
            "epoch: [32] loss: 0.101\n",
            "epoch: [33] loss: 0.086\n",
            "epoch: [34] loss: 0.070\n",
            "epoch: [35] loss: 0.060\n",
            "epoch: [36] loss: 0.054\n",
            "epoch: [37] loss: 0.046\n",
            "epoch: [38] loss: 0.041\n",
            "epoch: [39] loss: 0.036\n",
            "epoch: [40] loss: 0.032\n",
            "epoch: [41] loss: 0.029\n",
            "epoch: [42] loss: 0.027\n",
            "epoch: [43] loss: 0.024\n",
            "epoch: [44] loss: 0.023\n",
            "epoch: [45] loss: 0.022\n",
            "epoch: [46] loss: 0.021\n",
            "epoch: [47] loss: 0.019\n",
            "epoch: [48] loss: 0.017\n",
            "epoch: [49] loss: 0.016\n",
            "epoch: [50] loss: 0.017\n",
            "epoch: [51] loss: 0.014\n",
            "epoch: [52] loss: 0.014\n",
            "epoch: [53] loss: 0.014\n",
            "epoch: [54] loss: 0.015\n",
            "epoch: [55] loss: 0.012\n",
            "epoch: [56] loss: 0.011\n",
            "epoch: [57] loss: 0.011\n",
            "epoch: [58] loss: 0.010\n",
            "epoch: [59] loss: 0.010\n",
            "Finished Training run 7\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.204\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.186\n",
            "epoch: [3] loss: 1.180\n",
            "epoch: [4] loss: 1.173\n",
            "epoch: [5] loss: 1.168\n",
            "epoch: [6] loss: 1.159\n",
            "epoch: [7] loss: 1.145\n",
            "epoch: [8] loss: 1.123\n",
            "epoch: [9] loss: 1.092\n",
            "epoch: [10] loss: 1.058\n",
            "epoch: [11] loss: 1.023\n",
            "epoch: [12] loss: 0.974\n",
            "epoch: [13] loss: 0.921\n",
            "epoch: [14] loss: 0.864\n",
            "epoch: [15] loss: 0.807\n",
            "epoch: [16] loss: 0.758\n",
            "epoch: [17] loss: 0.695\n",
            "epoch: [18] loss: 0.647\n",
            "epoch: [19] loss: 0.593\n",
            "epoch: [20] loss: 0.539\n",
            "epoch: [21] loss: 0.492\n",
            "epoch: [22] loss: 0.442\n",
            "epoch: [23] loss: 0.394\n",
            "epoch: [24] loss: 0.343\n",
            "epoch: [25] loss: 0.295\n",
            "epoch: [26] loss: 0.245\n",
            "epoch: [27] loss: 0.205\n",
            "epoch: [28] loss: 0.171\n",
            "epoch: [29] loss: 0.142\n",
            "epoch: [30] loss: 0.124\n",
            "epoch: [31] loss: 0.099\n",
            "epoch: [32] loss: 0.083\n",
            "epoch: [33] loss: 0.070\n",
            "epoch: [34] loss: 0.059\n",
            "epoch: [35] loss: 0.051\n",
            "epoch: [36] loss: 0.043\n",
            "epoch: [37] loss: 0.037\n",
            "epoch: [38] loss: 0.033\n",
            "epoch: [39] loss: 0.028\n",
            "epoch: [40] loss: 0.026\n",
            "epoch: [41] loss: 0.023\n",
            "epoch: [42] loss: 0.020\n",
            "epoch: [43] loss: 0.019\n",
            "epoch: [44] loss: 0.017\n",
            "epoch: [45] loss: 0.015\n",
            "epoch: [46] loss: 0.013\n",
            "epoch: [47] loss: 0.012\n",
            "epoch: [48] loss: 0.011\n",
            "epoch: [49] loss: 0.012\n",
            "epoch: [50] loss: 0.010\n",
            "epoch: [51] loss: 0.009\n",
            "Finished Training run 8\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.213\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.187\n",
            "epoch: [3] loss: 1.183\n",
            "epoch: [4] loss: 1.178\n",
            "epoch: [5] loss: 1.174\n",
            "epoch: [6] loss: 1.166\n",
            "epoch: [7] loss: 1.157\n",
            "epoch: [8] loss: 1.142\n",
            "epoch: [9] loss: 1.127\n",
            "epoch: [10] loss: 1.088\n",
            "epoch: [11] loss: 1.054\n",
            "epoch: [12] loss: 1.015\n",
            "epoch: [13] loss: 0.988\n",
            "epoch: [14] loss: 0.935\n",
            "epoch: [15] loss: 0.889\n",
            "epoch: [16] loss: 0.833\n",
            "epoch: [17] loss: 0.763\n",
            "epoch: [18] loss: 0.698\n",
            "epoch: [19] loss: 0.637\n",
            "epoch: [20] loss: 0.584\n",
            "epoch: [21] loss: 0.531\n",
            "epoch: [22] loss: 0.482\n",
            "epoch: [23] loss: 0.431\n",
            "epoch: [24] loss: 0.373\n",
            "epoch: [25] loss: 0.327\n",
            "epoch: [26] loss: 0.281\n",
            "epoch: [27] loss: 0.235\n",
            "epoch: [28] loss: 0.196\n",
            "epoch: [29] loss: 0.172\n",
            "epoch: [30] loss: 0.153\n",
            "epoch: [31] loss: 0.127\n",
            "epoch: [32] loss: 0.108\n",
            "epoch: [33] loss: 0.094\n",
            "epoch: [34] loss: 0.080\n",
            "epoch: [35] loss: 0.069\n",
            "epoch: [36] loss: 0.058\n",
            "epoch: [37] loss: 0.050\n",
            "epoch: [38] loss: 0.043\n",
            "epoch: [39] loss: 0.036\n",
            "epoch: [40] loss: 0.032\n",
            "epoch: [41] loss: 0.027\n",
            "epoch: [42] loss: 0.024\n",
            "epoch: [43] loss: 0.021\n",
            "epoch: [44] loss: 0.019\n",
            "epoch: [45] loss: 0.017\n",
            "epoch: [46] loss: 0.015\n",
            "epoch: [47] loss: 0.014\n",
            "epoch: [48] loss: 0.012\n",
            "epoch: [49] loss: 0.011\n",
            "epoch: [50] loss: 0.010\n",
            "epoch: [51] loss: 0.009\n",
            "Finished Training run 9\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.204\n",
            "epoch: [1] loss: 1.196\n",
            "epoch: [2] loss: 1.190\n",
            "epoch: [3] loss: 1.186\n",
            "epoch: [4] loss: 1.181\n",
            "epoch: [5] loss: 1.175\n",
            "epoch: [6] loss: 1.170\n",
            "epoch: [7] loss: 1.161\n",
            "epoch: [8] loss: 1.150\n",
            "epoch: [9] loss: 1.137\n",
            "epoch: [10] loss: 1.117\n",
            "epoch: [11] loss: 1.090\n",
            "epoch: [12] loss: 1.050\n",
            "epoch: [13] loss: 0.999\n",
            "epoch: [14] loss: 0.942\n",
            "epoch: [15] loss: 0.881\n",
            "epoch: [16] loss: 0.817\n",
            "epoch: [17] loss: 0.755\n",
            "epoch: [18] loss: 0.692\n",
            "epoch: [19] loss: 0.645\n",
            "epoch: [20] loss: 0.566\n",
            "epoch: [21] loss: 0.504\n",
            "epoch: [22] loss: 0.444\n",
            "epoch: [23] loss: 0.395\n",
            "epoch: [24] loss: 0.334\n",
            "epoch: [25] loss: 0.289\n",
            "epoch: [26] loss: 0.244\n",
            "epoch: [27] loss: 0.208\n",
            "epoch: [28] loss: 0.174\n",
            "epoch: [29] loss: 0.149\n",
            "epoch: [30] loss: 0.135\n",
            "epoch: [31] loss: 0.109\n",
            "epoch: [32] loss: 0.095\n",
            "epoch: [33] loss: 0.080\n",
            "epoch: [34] loss: 0.069\n",
            "epoch: [35] loss: 0.061\n",
            "epoch: [36] loss: 0.053\n",
            "epoch: [37] loss: 0.047\n",
            "epoch: [38] loss: 0.043\n",
            "epoch: [39] loss: 0.037\n",
            "epoch: [40] loss: 0.034\n",
            "epoch: [41] loss: 0.030\n",
            "epoch: [42] loss: 0.027\n",
            "epoch: [43] loss: 0.025\n",
            "epoch: [44] loss: 0.022\n",
            "epoch: [45] loss: 0.021\n",
            "epoch: [46] loss: 0.018\n",
            "epoch: [47] loss: 0.017\n",
            "epoch: [48] loss: 0.016\n",
            "epoch: [49] loss: 0.015\n",
            "epoch: [50] loss: 0.014\n",
            "epoch: [51] loss: 0.012\n",
            "epoch: [52] loss: 0.011\n",
            "epoch: [53] loss: 0.011\n",
            "epoch: [54] loss: 0.010\n",
            "epoch: [55] loss: 0.009\n",
            "Finished Training run 10\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.210\n",
            "epoch: [1] loss: 1.191\n",
            "epoch: [2] loss: 1.185\n",
            "epoch: [3] loss: 1.178\n",
            "epoch: [4] loss: 1.171\n",
            "epoch: [5] loss: 1.162\n",
            "epoch: [6] loss: 1.152\n",
            "epoch: [7] loss: 1.137\n",
            "epoch: [8] loss: 1.112\n",
            "epoch: [9] loss: 1.081\n",
            "epoch: [10] loss: 1.042\n",
            "epoch: [11] loss: 0.995\n",
            "epoch: [12] loss: 0.943\n",
            "epoch: [13] loss: 0.884\n",
            "epoch: [14] loss: 0.827\n",
            "epoch: [15] loss: 0.777\n",
            "epoch: [16] loss: 0.715\n",
            "epoch: [17] loss: 0.651\n",
            "epoch: [18] loss: 0.593\n",
            "epoch: [19] loss: 0.534\n",
            "epoch: [20] loss: 0.480\n",
            "epoch: [21] loss: 0.415\n",
            "epoch: [22] loss: 0.344\n",
            "epoch: [23] loss: 0.291\n",
            "epoch: [24] loss: 0.219\n",
            "epoch: [25] loss: 0.169\n",
            "epoch: [26] loss: 0.126\n",
            "epoch: [27] loss: 0.098\n",
            "epoch: [28] loss: 0.078\n",
            "epoch: [29] loss: 0.061\n",
            "epoch: [30] loss: 0.050\n",
            "epoch: [31] loss: 0.043\n",
            "epoch: [32] loss: 0.037\n",
            "epoch: [33] loss: 0.032\n",
            "epoch: [34] loss: 0.033\n",
            "epoch: [35] loss: 0.024\n",
            "epoch: [36] loss: 0.021\n",
            "epoch: [37] loss: 0.018\n",
            "epoch: [38] loss: 0.016\n",
            "epoch: [39] loss: 0.015\n",
            "epoch: [40] loss: 0.013\n",
            "epoch: [41] loss: 0.012\n",
            "epoch: [42] loss: 0.011\n",
            "epoch: [43] loss: 0.011\n",
            "epoch: [44] loss: 0.010\n",
            "Finished Training run 11\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.212\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.187\n",
            "epoch: [3] loss: 1.182\n",
            "epoch: [4] loss: 1.176\n",
            "epoch: [5] loss: 1.169\n",
            "epoch: [6] loss: 1.160\n",
            "epoch: [7] loss: 1.149\n",
            "epoch: [8] loss: 1.134\n",
            "epoch: [9] loss: 1.110\n",
            "epoch: [10] loss: 1.071\n",
            "epoch: [11] loss: 1.030\n",
            "epoch: [12] loss: 0.986\n",
            "epoch: [13] loss: 0.951\n",
            "epoch: [14] loss: 0.914\n",
            "epoch: [15] loss: 0.876\n",
            "epoch: [16] loss: 0.838\n",
            "epoch: [17] loss: 0.801\n",
            "epoch: [18] loss: 0.752\n",
            "epoch: [19] loss: 0.708\n",
            "epoch: [20] loss: 0.663\n",
            "epoch: [21] loss: 0.619\n",
            "epoch: [22] loss: 0.577\n",
            "epoch: [23] loss: 0.529\n",
            "epoch: [24] loss: 0.461\n",
            "epoch: [25] loss: 0.413\n",
            "epoch: [26] loss: 0.351\n",
            "epoch: [27] loss: 0.294\n",
            "epoch: [28] loss: 0.242\n",
            "epoch: [29] loss: 0.194\n",
            "epoch: [30] loss: 0.153\n",
            "epoch: [31] loss: 0.119\n",
            "epoch: [32] loss: 0.096\n",
            "epoch: [33] loss: 0.079\n",
            "epoch: [34] loss: 0.067\n",
            "epoch: [35] loss: 0.055\n",
            "epoch: [36] loss: 0.047\n",
            "epoch: [37] loss: 0.041\n",
            "epoch: [38] loss: 0.037\n",
            "epoch: [39] loss: 0.032\n",
            "epoch: [40] loss: 0.028\n",
            "epoch: [41] loss: 0.025\n",
            "epoch: [42] loss: 0.024\n",
            "epoch: [43] loss: 0.021\n",
            "epoch: [44] loss: 0.019\n",
            "epoch: [45] loss: 0.018\n",
            "epoch: [46] loss: 0.017\n",
            "epoch: [47] loss: 0.015\n",
            "epoch: [48] loss: 0.014\n",
            "epoch: [49] loss: 0.013\n",
            "epoch: [50] loss: 0.012\n",
            "epoch: [51] loss: 0.011\n",
            "epoch: [52] loss: 0.010\n",
            "epoch: [53] loss: 0.009\n",
            "Finished Training run 12\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.207\n",
            "epoch: [1] loss: 1.193\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.182\n",
            "epoch: [4] loss: 1.174\n",
            "epoch: [5] loss: 1.166\n",
            "epoch: [6] loss: 1.153\n",
            "epoch: [7] loss: 1.135\n",
            "epoch: [8] loss: 1.110\n",
            "epoch: [9] loss: 1.075\n",
            "epoch: [10] loss: 1.038\n",
            "epoch: [11] loss: 1.000\n",
            "epoch: [12] loss: 0.961\n",
            "epoch: [13] loss: 0.923\n",
            "epoch: [14] loss: 0.884\n",
            "epoch: [15] loss: 0.829\n",
            "epoch: [16] loss: 0.784\n",
            "epoch: [17] loss: 0.734\n",
            "epoch: [18] loss: 0.687\n",
            "epoch: [19] loss: 0.635\n",
            "epoch: [20] loss: 0.586\n",
            "epoch: [21] loss: 0.530\n",
            "epoch: [22] loss: 0.469\n",
            "epoch: [23] loss: 0.397\n",
            "epoch: [24] loss: 0.328\n",
            "epoch: [25] loss: 0.268\n",
            "epoch: [26] loss: 0.208\n",
            "epoch: [27] loss: 0.170\n",
            "epoch: [28] loss: 0.137\n",
            "epoch: [29] loss: 0.113\n",
            "epoch: [30] loss: 0.095\n",
            "epoch: [31] loss: 0.085\n",
            "epoch: [32] loss: 0.071\n",
            "epoch: [33] loss: 0.063\n",
            "epoch: [34] loss: 0.056\n",
            "epoch: [35] loss: 0.056\n",
            "epoch: [36] loss: 0.045\n",
            "epoch: [37] loss: 0.053\n",
            "epoch: [38] loss: 0.037\n",
            "epoch: [39] loss: 0.033\n",
            "epoch: [40] loss: 0.030\n",
            "epoch: [41] loss: 0.028\n",
            "epoch: [42] loss: 0.026\n",
            "epoch: [43] loss: 0.025\n",
            "epoch: [44] loss: 0.023\n",
            "epoch: [45] loss: 0.021\n",
            "epoch: [46] loss: 0.020\n",
            "epoch: [47] loss: 0.019\n",
            "epoch: [48] loss: 0.018\n",
            "epoch: [49] loss: 0.018\n",
            "epoch: [50] loss: 0.017\n",
            "epoch: [51] loss: 0.015\n",
            "epoch: [52] loss: 0.015\n",
            "epoch: [53] loss: 0.015\n",
            "epoch: [54] loss: 0.013\n",
            "epoch: [55] loss: 0.013\n",
            "epoch: [56] loss: 0.013\n",
            "epoch: [57] loss: 0.012\n",
            "epoch: [58] loss: 0.011\n",
            "epoch: [59] loss: 0.010\n",
            "epoch: [60] loss: 0.012\n",
            "epoch: [61] loss: 0.010\n",
            "Finished Training run 13\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.259\n",
            "epoch: [1] loss: 1.201\n",
            "epoch: [2] loss: 1.191\n",
            "epoch: [3] loss: 1.186\n",
            "epoch: [4] loss: 1.183\n",
            "epoch: [5] loss: 1.180\n",
            "epoch: [6] loss: 1.176\n",
            "epoch: [7] loss: 1.171\n",
            "epoch: [8] loss: 1.166\n",
            "epoch: [9] loss: 1.160\n",
            "epoch: [10] loss: 1.147\n",
            "epoch: [11] loss: 1.128\n",
            "epoch: [12] loss: 1.097\n",
            "epoch: [13] loss: 1.041\n",
            "epoch: [14] loss: 0.975\n",
            "epoch: [15] loss: 0.897\n",
            "epoch: [16] loss: 0.813\n",
            "epoch: [17] loss: 0.732\n",
            "epoch: [18] loss: 0.653\n",
            "epoch: [19] loss: 0.581\n",
            "epoch: [20] loss: 0.508\n",
            "epoch: [21] loss: 0.431\n",
            "epoch: [22] loss: 0.352\n",
            "epoch: [23] loss: 0.262\n",
            "epoch: [24] loss: 0.191\n",
            "epoch: [25] loss: 0.137\n",
            "epoch: [26] loss: 0.097\n",
            "epoch: [27] loss: 0.070\n",
            "epoch: [28] loss: 0.053\n",
            "epoch: [29] loss: 0.041\n",
            "epoch: [30] loss: 0.032\n",
            "epoch: [31] loss: 0.027\n",
            "epoch: [32] loss: 0.022\n",
            "epoch: [33] loss: 0.019\n",
            "epoch: [34] loss: 0.016\n",
            "epoch: [35] loss: 0.014\n",
            "epoch: [36] loss: 0.012\n",
            "epoch: [37] loss: 0.011\n",
            "epoch: [38] loss: 0.010\n",
            "Finished Training run 14\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.185\n",
            "epoch: [4] loss: 1.181\n",
            "epoch: [5] loss: 1.175\n",
            "epoch: [6] loss: 1.172\n",
            "epoch: [7] loss: 1.159\n",
            "epoch: [8] loss: 1.149\n",
            "epoch: [9] loss: 1.123\n",
            "epoch: [10] loss: 1.092\n",
            "epoch: [11] loss: 1.048\n",
            "epoch: [12] loss: 0.991\n",
            "epoch: [13] loss: 0.922\n",
            "epoch: [14] loss: 0.847\n",
            "epoch: [15] loss: 0.770\n",
            "epoch: [16] loss: 0.701\n",
            "epoch: [17] loss: 0.632\n",
            "epoch: [18] loss: 0.570\n",
            "epoch: [19] loss: 0.513\n",
            "epoch: [20] loss: 0.446\n",
            "epoch: [21] loss: 0.390\n",
            "epoch: [22] loss: 0.324\n",
            "epoch: [23] loss: 0.268\n",
            "epoch: [24] loss: 0.222\n",
            "epoch: [25] loss: 0.183\n",
            "epoch: [26] loss: 0.150\n",
            "epoch: [27] loss: 0.145\n",
            "epoch: [28] loss: 0.113\n",
            "epoch: [29] loss: 0.095\n",
            "epoch: [30] loss: 0.088\n",
            "epoch: [31] loss: 0.068\n",
            "epoch: [32] loss: 0.064\n",
            "epoch: [33] loss: 0.057\n",
            "epoch: [34] loss: 0.056\n",
            "epoch: [35] loss: 0.045\n",
            "epoch: [36] loss: 0.040\n",
            "epoch: [37] loss: 0.040\n",
            "epoch: [38] loss: 0.035\n",
            "epoch: [39] loss: 0.032\n",
            "epoch: [40] loss: 0.031\n",
            "epoch: [41] loss: 0.029\n",
            "epoch: [42] loss: 0.029\n",
            "epoch: [43] loss: 0.025\n",
            "epoch: [44] loss: 0.022\n",
            "epoch: [45] loss: 0.021\n",
            "epoch: [46] loss: 0.020\n",
            "epoch: [47] loss: 0.019\n",
            "epoch: [48] loss: 0.019\n",
            "epoch: [49] loss: 0.017\n",
            "epoch: [50] loss: 0.016\n",
            "epoch: [51] loss: 0.015\n",
            "epoch: [52] loss: 0.015\n",
            "epoch: [53] loss: 0.015\n",
            "epoch: [54] loss: 0.013\n",
            "epoch: [55] loss: 0.012\n",
            "epoch: [56] loss: 0.012\n",
            "epoch: [57] loss: 0.014\n",
            "epoch: [58] loss: 0.011\n",
            "epoch: [59] loss: 0.010\n",
            "epoch: [60] loss: 0.010\n",
            "Finished Training run 15\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.197\n",
            "epoch: [1] loss: 1.190\n",
            "epoch: [2] loss: 1.183\n",
            "epoch: [3] loss: 1.175\n",
            "epoch: [4] loss: 1.168\n",
            "epoch: [5] loss: 1.160\n",
            "epoch: [6] loss: 1.152\n",
            "epoch: [7] loss: 1.140\n",
            "epoch: [8] loss: 1.125\n",
            "epoch: [9] loss: 1.106\n",
            "epoch: [10] loss: 1.081\n",
            "epoch: [11] loss: 1.052\n",
            "epoch: [12] loss: 1.024\n",
            "epoch: [13] loss: 0.996\n",
            "epoch: [14] loss: 0.969\n",
            "epoch: [15] loss: 0.941\n",
            "epoch: [16] loss: 0.915\n",
            "epoch: [17] loss: 0.880\n",
            "epoch: [18] loss: 0.840\n",
            "epoch: [19] loss: 0.804\n",
            "epoch: [20] loss: 0.763\n",
            "epoch: [21] loss: 0.721\n",
            "epoch: [22] loss: 0.668\n",
            "epoch: [23] loss: 0.619\n",
            "epoch: [24] loss: 0.565\n",
            "epoch: [25] loss: 0.517\n",
            "epoch: [26] loss: 0.471\n",
            "epoch: [27] loss: 0.399\n",
            "epoch: [28] loss: 0.344\n",
            "epoch: [29] loss: 0.288\n",
            "epoch: [30] loss: 0.243\n",
            "epoch: [31] loss: 0.202\n",
            "epoch: [32] loss: 0.168\n",
            "epoch: [33] loss: 0.139\n",
            "epoch: [34] loss: 0.120\n",
            "epoch: [35] loss: 0.100\n",
            "epoch: [36] loss: 0.086\n",
            "epoch: [37] loss: 0.078\n",
            "epoch: [38] loss: 0.068\n",
            "epoch: [39] loss: 0.062\n",
            "epoch: [40] loss: 0.056\n",
            "epoch: [41] loss: 0.047\n",
            "epoch: [42] loss: 0.044\n",
            "epoch: [43] loss: 0.037\n",
            "epoch: [44] loss: 0.034\n",
            "epoch: [45] loss: 0.030\n",
            "epoch: [46] loss: 0.027\n",
            "epoch: [47] loss: 0.026\n",
            "epoch: [48] loss: 0.026\n",
            "epoch: [49] loss: 0.022\n",
            "epoch: [50] loss: 0.020\n",
            "epoch: [51] loss: 0.019\n",
            "epoch: [52] loss: 0.018\n",
            "epoch: [53] loss: 0.017\n",
            "epoch: [54] loss: 0.018\n",
            "epoch: [55] loss: 0.015\n",
            "epoch: [56] loss: 0.015\n",
            "epoch: [57] loss: 0.013\n",
            "epoch: [58] loss: 0.012\n",
            "epoch: [59] loss: 0.012\n",
            "epoch: [60] loss: 0.011\n",
            "epoch: [61] loss: 0.011\n",
            "epoch: [62] loss: 0.010\n",
            "epoch: [63] loss: 0.010\n",
            "Finished Training run 16\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.203\n",
            "epoch: [1] loss: 1.190\n",
            "epoch: [2] loss: 1.185\n",
            "epoch: [3] loss: 1.180\n",
            "epoch: [4] loss: 1.176\n",
            "epoch: [5] loss: 1.169\n",
            "epoch: [6] loss: 1.160\n",
            "epoch: [7] loss: 1.146\n",
            "epoch: [8] loss: 1.125\n",
            "epoch: [9] loss: 1.095\n",
            "epoch: [10] loss: 1.049\n",
            "epoch: [11] loss: 0.996\n",
            "epoch: [12] loss: 0.941\n",
            "epoch: [13] loss: 0.894\n",
            "epoch: [14] loss: 0.842\n",
            "epoch: [15] loss: 0.787\n",
            "epoch: [16] loss: 0.736\n",
            "epoch: [17] loss: 0.689\n",
            "epoch: [18] loss: 0.641\n",
            "epoch: [19] loss: 0.598\n",
            "epoch: [20] loss: 0.549\n",
            "epoch: [21] loss: 0.502\n",
            "epoch: [22] loss: 0.453\n",
            "epoch: [23] loss: 0.410\n",
            "epoch: [24] loss: 0.368\n",
            "epoch: [25] loss: 0.324\n",
            "epoch: [26] loss: 0.283\n",
            "epoch: [27] loss: 0.239\n",
            "epoch: [28] loss: 0.209\n",
            "epoch: [29] loss: 0.182\n",
            "epoch: [30] loss: 0.152\n",
            "epoch: [31] loss: 0.139\n",
            "epoch: [32] loss: 0.119\n",
            "epoch: [33] loss: 0.105\n",
            "epoch: [34] loss: 0.099\n",
            "epoch: [35] loss: 0.090\n",
            "epoch: [36] loss: 0.081\n",
            "epoch: [37] loss: 0.071\n",
            "epoch: [38] loss: 0.064\n",
            "epoch: [39] loss: 0.061\n",
            "epoch: [40] loss: 0.053\n",
            "epoch: [41] loss: 0.048\n",
            "epoch: [42] loss: 0.046\n",
            "epoch: [43] loss: 0.041\n",
            "epoch: [44] loss: 0.036\n",
            "epoch: [45] loss: 0.033\n",
            "epoch: [46] loss: 0.031\n",
            "epoch: [47] loss: 0.030\n",
            "epoch: [48] loss: 0.026\n",
            "epoch: [49] loss: 0.026\n",
            "epoch: [50] loss: 0.023\n",
            "epoch: [51] loss: 0.022\n",
            "epoch: [52] loss: 0.019\n",
            "epoch: [53] loss: 0.018\n",
            "epoch: [54] loss: 0.016\n",
            "epoch: [55] loss: 0.015\n",
            "epoch: [56] loss: 0.014\n",
            "epoch: [57] loss: 0.014\n",
            "epoch: [58] loss: 0.013\n",
            "epoch: [59] loss: 0.012\n",
            "epoch: [60] loss: 0.011\n",
            "epoch: [61] loss: 0.011\n",
            "epoch: [62] loss: 0.010\n",
            "epoch: [63] loss: 0.010\n",
            "Finished Training run 17\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.213\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.185\n",
            "epoch: [3] loss: 1.179\n",
            "epoch: [4] loss: 1.172\n",
            "epoch: [5] loss: 1.165\n",
            "epoch: [6] loss: 1.155\n",
            "epoch: [7] loss: 1.140\n",
            "epoch: [8] loss: 1.128\n",
            "epoch: [9] loss: 1.097\n",
            "epoch: [10] loss: 1.063\n",
            "epoch: [11] loss: 1.018\n",
            "epoch: [12] loss: 0.964\n",
            "epoch: [13] loss: 0.914\n",
            "epoch: [14] loss: 0.865\n",
            "epoch: [15] loss: 0.819\n",
            "epoch: [16] loss: 0.777\n",
            "epoch: [17] loss: 0.723\n",
            "epoch: [18] loss: 0.678\n",
            "epoch: [19] loss: 0.620\n",
            "epoch: [20] loss: 0.567\n",
            "epoch: [21] loss: 0.514\n",
            "epoch: [22] loss: 0.465\n",
            "epoch: [23] loss: 0.409\n",
            "epoch: [24] loss: 0.359\n",
            "epoch: [25] loss: 0.297\n",
            "epoch: [26] loss: 0.234\n",
            "epoch: [27] loss: 0.174\n",
            "epoch: [28] loss: 0.128\n",
            "epoch: [29] loss: 0.095\n",
            "epoch: [30] loss: 0.075\n",
            "epoch: [31] loss: 0.056\n",
            "epoch: [32] loss: 0.044\n",
            "epoch: [33] loss: 0.036\n",
            "epoch: [34] loss: 0.029\n",
            "epoch: [35] loss: 0.025\n",
            "epoch: [36] loss: 0.022\n",
            "epoch: [37] loss: 0.019\n",
            "epoch: [38] loss: 0.017\n",
            "epoch: [39] loss: 0.015\n",
            "epoch: [40] loss: 0.013\n",
            "epoch: [41] loss: 0.012\n",
            "epoch: [42] loss: 0.011\n",
            "epoch: [43] loss: 0.010\n",
            "Finished Training run 18\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.190\n",
            "epoch: [3] loss: 1.187\n",
            "epoch: [4] loss: 1.184\n",
            "epoch: [5] loss: 1.181\n",
            "epoch: [6] loss: 1.176\n",
            "epoch: [7] loss: 1.172\n",
            "epoch: [8] loss: 1.166\n",
            "epoch: [9] loss: 1.160\n",
            "epoch: [10] loss: 1.152\n",
            "epoch: [11] loss: 1.142\n",
            "epoch: [12] loss: 1.120\n",
            "epoch: [13] loss: 1.099\n",
            "epoch: [14] loss: 1.061\n",
            "epoch: [15] loss: 1.020\n",
            "epoch: [16] loss: 0.971\n",
            "epoch: [17] loss: 0.928\n",
            "epoch: [18] loss: 0.878\n",
            "epoch: [19] loss: 0.824\n",
            "epoch: [20] loss: 0.778\n",
            "epoch: [21] loss: 0.719\n",
            "epoch: [22] loss: 0.663\n",
            "epoch: [23] loss: 0.608\n",
            "epoch: [24] loss: 0.560\n",
            "epoch: [25] loss: 0.513\n",
            "epoch: [26] loss: 0.467\n",
            "epoch: [27] loss: 0.423\n",
            "epoch: [28] loss: 0.369\n",
            "epoch: [29] loss: 0.316\n",
            "epoch: [30] loss: 0.267\n",
            "epoch: [31] loss: 0.223\n",
            "epoch: [32] loss: 0.187\n",
            "epoch: [33] loss: 0.155\n",
            "epoch: [34] loss: 0.136\n",
            "epoch: [35] loss: 0.109\n",
            "epoch: [36] loss: 0.090\n",
            "epoch: [37] loss: 0.084\n",
            "epoch: [38] loss: 0.068\n",
            "epoch: [39] loss: 0.059\n",
            "epoch: [40] loss: 0.053\n",
            "epoch: [41] loss: 0.048\n",
            "epoch: [42] loss: 0.040\n",
            "epoch: [43] loss: 0.035\n",
            "epoch: [44] loss: 0.031\n",
            "epoch: [45] loss: 0.028\n",
            "epoch: [46] loss: 0.025\n",
            "epoch: [47] loss: 0.023\n",
            "epoch: [48] loss: 0.020\n",
            "epoch: [49] loss: 0.019\n",
            "epoch: [50] loss: 0.017\n",
            "epoch: [51] loss: 0.016\n",
            "epoch: [52] loss: 0.014\n",
            "epoch: [53] loss: 0.013\n",
            "epoch: [54] loss: 0.012\n",
            "epoch: [55] loss: 0.011\n",
            "epoch: [56] loss: 0.010\n",
            "epoch: [57] loss: 0.010\n",
            "Finished Training run 19\n",
            "Accuracy of the network on the 3000 train images: 100 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEabNK9Q1bTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "142f8d55-b641-43f4-ec62-13f5c15d5ef0"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f00336d5110>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "78457a03-d246-48b6-b970-6df0db8173e9"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.13750000e+01, 8.60333333e+00, 1.83333333e-02, 3.33333333e-03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"synthetic_first.csv\",index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "PzR8ISPlOSbP",
        "outputId": "38762582-77d8-4d92-aa92-64f484f50f9e"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>99.933333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>99.700000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>99.366667</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>98.600000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>72.800000</td>\n",
              "      <td>27.133333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>71.033333</td>\n",
              "      <td>28.866667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>97.733333</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>99.633333</td>\n",
              "      <td>0.366667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>81.466667</td>\n",
              "      <td>18.533333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>98.933333</td>\n",
              "      <td>1.066667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>82.066667</td>\n",
              "      <td>17.933333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>90.666667</td>\n",
              "      <td>9.300000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>78.866667</td>\n",
              "      <td>21.100000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>91.433333</td>\n",
              "      <td>8.533333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>94.766667</td>\n",
              "      <td>5.200000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>98.533333</td>\n",
              "      <td>1.466667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>96.733333</td>\n",
              "      <td>3.266667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>80.233333</td>\n",
              "      <td>19.766667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>95.000000</td>\n",
              "      <td>4.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          FTPT       FFPT      FTPF      FFPF\n",
              "0    99.933333   0.066667  0.000000  0.000000\n",
              "1    99.700000   0.300000  0.000000  0.000000\n",
              "2    99.366667   0.600000  0.033333  0.000000\n",
              "3    98.600000   1.400000  0.000000  0.000000\n",
              "4    72.800000  27.133333  0.066667  0.000000\n",
              "5    71.033333  28.866667  0.033333  0.066667\n",
              "6    97.733333   2.200000  0.066667  0.000000\n",
              "7    99.633333   0.366667  0.000000  0.000000\n",
              "8    81.466667  18.533333  0.000000  0.000000\n",
              "9    98.933333   1.066667  0.000000  0.000000\n",
              "10   82.066667  17.933333  0.000000  0.000000\n",
              "11   90.666667   9.300000  0.033333  0.000000\n",
              "12   78.866667  21.100000  0.033333  0.000000\n",
              "13   91.433333   8.533333  0.033333  0.000000\n",
              "14   94.766667   5.200000  0.033333  0.000000\n",
              "15   98.533333   1.466667  0.000000  0.000000\n",
              "16   96.733333   3.266667  0.000000  0.000000\n",
              "17   80.233333  19.766667  0.000000  0.000000\n",
              "18   95.000000   4.966667  0.033333  0.000000\n",
              "19  100.000000   0.000000  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}