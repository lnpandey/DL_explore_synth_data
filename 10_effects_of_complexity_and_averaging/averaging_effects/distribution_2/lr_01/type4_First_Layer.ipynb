{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type4_First_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\r\n",
        "mosaic_label = data[0][\"mosaic_label\"]\r\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,50)  #,self.output)\n",
        "        self.linear2 = nn.Linear(50,50)\n",
        "        self.linear3 = nn.Linear(50,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,50], dtype=torch.float64)\n",
        "        features = torch.zeros([batch,self.K,50],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x) \n",
        "      x = F.relu(self.linear2(x))\n",
        "      x = self.linear3(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,50)\n",
        "        #self.linear2 = nn.Linear(6,12)\n",
        "        self.linear2 = nn.Linear(50,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      #x = F.relu(self.linear2(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOfxUJZ_eFKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1506d3e3-06de-428e-bd9c-3d5f6f70f603"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(2,1,9,2).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(50,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.01)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.01)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 2500\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: [0 ] loss: 1.199\n",
            "epoch: [1] loss: 1.203\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.176\n",
            "epoch: [4] loss: 1.144\n",
            "epoch: [5] loss: 1.031\n",
            "epoch: [6] loss: 0.834\n",
            "epoch: [7] loss: 0.546\n",
            "epoch: [8] loss: 0.248\n",
            "epoch: [9] loss: 0.149\n",
            "epoch: [10] loss: 0.056\n",
            "epoch: [11] loss: 0.042\n",
            "epoch: [12] loss: 0.014\n",
            "epoch: [13] loss: 0.010\n",
            "epoch: [14] loss: 0.011\n",
            "epoch: [15] loss: 0.009\n",
            "Finished Training run 0\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.211\n",
            "epoch: [1] loss: 1.190\n",
            "epoch: [2] loss: 1.146\n",
            "epoch: [3] loss: 0.965\n",
            "epoch: [4] loss: 0.666\n",
            "epoch: [5] loss: 0.344\n",
            "epoch: [6] loss: 0.124\n",
            "epoch: [7] loss: 0.048\n",
            "epoch: [8] loss: 0.021\n",
            "epoch: [9] loss: 0.009\n",
            "Finished Training run 1\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.182\n",
            "epoch: [3] loss: 1.166\n",
            "epoch: [4] loss: 1.131\n",
            "epoch: [5] loss: 1.021\n",
            "epoch: [6] loss: 0.757\n",
            "epoch: [7] loss: 0.523\n",
            "epoch: [8] loss: 0.255\n",
            "epoch: [9] loss: 0.070\n",
            "epoch: [10] loss: 0.114\n",
            "epoch: [11] loss: 0.101\n",
            "epoch: [12] loss: 0.036\n",
            "epoch: [13] loss: 0.017\n",
            "epoch: [14] loss: 0.010\n",
            "Finished Training run 2\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.207\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.188\n",
            "epoch: [3] loss: 1.163\n",
            "epoch: [4] loss: 1.042\n",
            "epoch: [5] loss: 0.842\n",
            "epoch: [6] loss: 0.554\n",
            "epoch: [7] loss: 0.318\n",
            "epoch: [8] loss: 0.097\n",
            "epoch: [9] loss: 0.028\n",
            "epoch: [10] loss: 0.014\n",
            "epoch: [11] loss: 0.007\n",
            "Finished Training run 3\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.197\n",
            "epoch: [2] loss: 1.193\n",
            "epoch: [3] loss: 1.186\n",
            "epoch: [4] loss: 1.158\n",
            "epoch: [5] loss: 1.011\n",
            "epoch: [6] loss: 0.820\n",
            "epoch: [7] loss: 0.608\n",
            "epoch: [8] loss: 0.404\n",
            "epoch: [9] loss: 0.256\n",
            "epoch: [10] loss: 0.075\n",
            "epoch: [11] loss: 0.064\n",
            "epoch: [12] loss: 0.018\n",
            "epoch: [13] loss: 0.011\n",
            "epoch: [14] loss: 0.008\n",
            "Finished Training run 4\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.223\n",
            "epoch: [1] loss: 1.188\n",
            "epoch: [2] loss: 1.145\n",
            "epoch: [3] loss: 1.023\n",
            "epoch: [4] loss: 0.876\n",
            "epoch: [5] loss: 0.710\n",
            "epoch: [6] loss: 0.595\n",
            "epoch: [7] loss: 0.451\n",
            "epoch: [8] loss: 0.238\n",
            "epoch: [9] loss: 0.076\n",
            "epoch: [10] loss: 0.027\n",
            "epoch: [11] loss: 0.018\n",
            "epoch: [12] loss: 0.008\n",
            "Finished Training run 5\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.228\n",
            "epoch: [1] loss: 1.186\n",
            "epoch: [2] loss: 1.161\n",
            "epoch: [3] loss: 1.087\n",
            "epoch: [4] loss: 0.942\n",
            "epoch: [5] loss: 0.686\n",
            "epoch: [6] loss: 0.517\n",
            "epoch: [7] loss: 0.347\n",
            "epoch: [8] loss: 0.088\n",
            "epoch: [9] loss: 0.019\n",
            "epoch: [10] loss: 0.007\n",
            "Finished Training run 6\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.203\n",
            "epoch: [1] loss: 1.181\n",
            "epoch: [2] loss: 1.153\n",
            "epoch: [3] loss: 1.080\n",
            "epoch: [4] loss: 0.894\n",
            "epoch: [5] loss: 0.707\n",
            "epoch: [6] loss: 0.507\n",
            "epoch: [7] loss: 0.332\n",
            "epoch: [8] loss: 0.156\n",
            "epoch: [9] loss: 0.090\n",
            "epoch: [10] loss: 0.071\n",
            "epoch: [11] loss: 0.034\n",
            "epoch: [12] loss: 0.024\n",
            "epoch: [13] loss: 0.020\n",
            "epoch: [14] loss: 0.021\n",
            "epoch: [15] loss: 0.013\n",
            "epoch: [16] loss: 0.009\n",
            "Finished Training run 7\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.204\n",
            "epoch: [1] loss: 1.186\n",
            "epoch: [2] loss: 1.156\n",
            "epoch: [3] loss: 1.082\n",
            "epoch: [4] loss: 0.896\n",
            "epoch: [5] loss: 0.616\n",
            "epoch: [6] loss: 0.403\n",
            "epoch: [7] loss: 0.126\n",
            "epoch: [8] loss: 0.035\n",
            "epoch: [9] loss: 0.008\n",
            "Finished Training run 8\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.213\n",
            "epoch: [1] loss: 1.188\n",
            "epoch: [2] loss: 1.162\n",
            "epoch: [3] loss: 1.045\n",
            "epoch: [4] loss: 0.870\n",
            "epoch: [5] loss: 0.642\n",
            "epoch: [6] loss: 0.398\n",
            "epoch: [7] loss: 0.190\n",
            "epoch: [8] loss: 0.109\n",
            "epoch: [9] loss: 0.052\n",
            "epoch: [10] loss: 0.026\n",
            "epoch: [11] loss: 0.017\n",
            "epoch: [12] loss: 0.010\n",
            "epoch: [13] loss: 0.008\n",
            "Finished Training run 9\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.204\n",
            "epoch: [1] loss: 1.198\n",
            "epoch: [2] loss: 1.192\n",
            "epoch: [3] loss: 1.187\n",
            "epoch: [4] loss: 1.180\n",
            "epoch: [5] loss: 1.125\n",
            "epoch: [6] loss: 0.927\n",
            "epoch: [7] loss: 0.504\n",
            "epoch: [8] loss: 0.184\n",
            "epoch: [9] loss: 0.633\n",
            "epoch: [10] loss: 0.103\n",
            "epoch: [11] loss: 0.034\n",
            "epoch: [12] loss: 0.019\n",
            "epoch: [13] loss: 0.010\n",
            "epoch: [14] loss: 0.009\n",
            "Finished Training run 10\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.210\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.161\n",
            "epoch: [3] loss: 1.066\n",
            "epoch: [4] loss: 0.914\n",
            "epoch: [5] loss: 0.722\n",
            "epoch: [6] loss: 0.462\n",
            "epoch: [7] loss: 0.178\n",
            "epoch: [8] loss: 0.040\n",
            "epoch: [9] loss: 0.015\n",
            "epoch: [10] loss: 0.011\n",
            "epoch: [11] loss: 0.004\n",
            "Finished Training run 11\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.212\n",
            "epoch: [1] loss: 1.200\n",
            "epoch: [2] loss: 1.181\n",
            "epoch: [3] loss: 1.141\n",
            "epoch: [4] loss: 0.968\n",
            "epoch: [5] loss: 0.797\n",
            "epoch: [6] loss: 0.602\n",
            "epoch: [7] loss: 0.221\n",
            "epoch: [8] loss: 0.146\n",
            "epoch: [9] loss: 0.013\n",
            "epoch: [10] loss: 0.008\n",
            "Finished Training run 12\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.207\n",
            "epoch: [1] loss: 1.177\n",
            "epoch: [2] loss: 1.112\n",
            "epoch: [3] loss: 0.925\n",
            "epoch: [4] loss: 0.714\n",
            "epoch: [5] loss: 0.441\n",
            "epoch: [6] loss: 0.192\n",
            "epoch: [7] loss: 0.025\n",
            "epoch: [8] loss: 0.004\n",
            "Finished Training run 13\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.259\n",
            "epoch: [1] loss: 1.188\n",
            "epoch: [2] loss: 1.178\n",
            "epoch: [3] loss: 1.132\n",
            "epoch: [4] loss: 0.938\n",
            "epoch: [5] loss: 0.625\n",
            "epoch: [6] loss: 0.375\n",
            "epoch: [7] loss: 0.107\n",
            "epoch: [8] loss: 0.034\n",
            "epoch: [9] loss: 0.009\n",
            "Finished Training run 14\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.200\n",
            "epoch: [2] loss: 1.197\n",
            "epoch: [3] loss: 1.193\n",
            "epoch: [4] loss: 1.180\n",
            "epoch: [5] loss: 1.110\n",
            "epoch: [6] loss: 0.841\n",
            "epoch: [7] loss: 0.600\n",
            "epoch: [8] loss: 0.494\n",
            "epoch: [9] loss: 0.286\n",
            "epoch: [10] loss: 0.057\n",
            "epoch: [11] loss: 0.025\n",
            "epoch: [12] loss: 0.007\n",
            "Finished Training run 15\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.197\n",
            "epoch: [1] loss: 1.189\n",
            "epoch: [2] loss: 1.152\n",
            "epoch: [3] loss: 1.104\n",
            "epoch: [4] loss: 0.999\n",
            "epoch: [5] loss: 0.774\n",
            "epoch: [6] loss: 0.531\n",
            "epoch: [7] loss: 0.308\n",
            "epoch: [8] loss: 0.091\n",
            "epoch: [9] loss: 0.048\n",
            "epoch: [10] loss: 0.029\n",
            "epoch: [11] loss: 0.017\n",
            "epoch: [12] loss: 0.013\n",
            "epoch: [13] loss: 0.032\n",
            "epoch: [14] loss: 0.024\n",
            "epoch: [15] loss: 0.047\n",
            "epoch: [16] loss: 0.024\n",
            "epoch: [17] loss: 0.011\n",
            "epoch: [18] loss: 0.010\n",
            "epoch: [19] loss: 0.007\n",
            "Finished Training run 16\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.203\n",
            "epoch: [1] loss: 1.174\n",
            "epoch: [2] loss: 1.086\n",
            "epoch: [3] loss: 0.883\n",
            "epoch: [4] loss: 0.681\n",
            "epoch: [5] loss: 0.530\n",
            "epoch: [6] loss: 0.344\n",
            "epoch: [7] loss: 0.141\n",
            "epoch: [8] loss: 0.122\n",
            "epoch: [9] loss: 0.031\n",
            "epoch: [10] loss: 0.011\n",
            "epoch: [11] loss: 0.006\n",
            "Finished Training run 17\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.213\n",
            "epoch: [1] loss: 1.179\n",
            "epoch: [2] loss: 1.148\n",
            "epoch: [3] loss: 1.090\n",
            "epoch: [4] loss: 0.852\n",
            "epoch: [5] loss: 0.620\n",
            "epoch: [6] loss: 0.400\n",
            "epoch: [7] loss: 0.121\n",
            "epoch: [8] loss: 0.016\n",
            "epoch: [9] loss: 0.005\n",
            "Finished Training run 18\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.182\n",
            "epoch: [2] loss: 1.164\n",
            "epoch: [3] loss: 1.116\n",
            "epoch: [4] loss: 0.916\n",
            "epoch: [5] loss: 0.599\n",
            "epoch: [6] loss: 0.319\n",
            "epoch: [7] loss: 0.069\n",
            "epoch: [8] loss: 0.048\n",
            "epoch: [9] loss: 0.009\n",
            "Finished Training run 19\n",
            "Accuracy of the network on the 3000 train images: 99 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEabNK9Q1bTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "44c94a7e-5067-4c43-93bb-aeaeccaf9a19"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0e4b4dfb90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "0ea9ed9b-6715-4f8b-fb3a-5c877f5c6398"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.05450000e+01, 9.40666667e+00, 4.66666667e-02, 1.66666667e-03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"synthetic_first.csv\",index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "PzR8ISPlOSbP",
        "outputId": "56139df5-4947-4557-ecec-ec29d79936ee"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>94.700000</td>\n",
              "      <td>5.300000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>97.000000</td>\n",
              "      <td>2.933333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>71.766667</td>\n",
              "      <td>28.133333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>97.200000</td>\n",
              "      <td>2.733333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>99.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>71.100000</td>\n",
              "      <td>28.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>98.966667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>99.633333</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>91.966667</td>\n",
              "      <td>7.966667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>99.900000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>99.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>97.066667</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>78.700000</td>\n",
              "      <td>21.233333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>99.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>80.200000</td>\n",
              "      <td>19.766667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>71.533333</td>\n",
              "      <td>28.466667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>99.766667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>77.666667</td>\n",
              "      <td>22.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>83.966667</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>99.933333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         FTPT       FFPT      FTPF      FFPF\n",
              "0   94.700000   5.300000  0.000000  0.000000\n",
              "1   97.000000   2.933333  0.066667  0.000000\n",
              "2   71.766667  28.133333  0.066667  0.033333\n",
              "3   97.200000   2.733333  0.066667  0.000000\n",
              "4   99.966667   0.033333  0.000000  0.000000\n",
              "5   71.100000  28.900000  0.000000  0.000000\n",
              "6   98.966667   1.000000  0.033333  0.000000\n",
              "7   99.633333   0.300000  0.066667  0.000000\n",
              "8   91.966667   7.966667  0.066667  0.000000\n",
              "9   99.900000   0.100000  0.000000  0.000000\n",
              "10  99.900000   0.000000  0.100000  0.000000\n",
              "11  97.066667   2.900000  0.033333  0.000000\n",
              "12  78.700000  21.233333  0.066667  0.000000\n",
              "13  99.966667   0.033333  0.000000  0.000000\n",
              "14  80.200000  19.766667  0.033333  0.000000\n",
              "15  71.533333  28.466667  0.000000  0.000000\n",
              "16  99.766667   0.000000  0.233333  0.000000\n",
              "17  77.666667  22.333333  0.000000  0.000000\n",
              "18  83.966667  16.000000  0.033333  0.000000\n",
              "19  99.933333   0.000000  0.066667  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}