{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Second_Layer_averaging_Complexity Synthetic elliptical blobs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEp-LtqiWAf"
      },
      "source": [
        "# mu1 = np.array([3,3,3,3,0])\n",
        "# sigma1 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu2 = np.array([4,4,4,4,0])\n",
        "# sigma2 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu3 = np.array([10,5,5,10,0])\n",
        "# sigma3 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu4 = np.array([-10,-10,-10,-10,0])\n",
        "# sigma4 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu5 = np.array([-21,4,4,-21,0])\n",
        "# sigma5 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu6 = np.array([-10,18,18,-10,0])\n",
        "# sigma6 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu7 = np.array([4,20,4,20,0])\n",
        "# sigma7 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu8 = np.array([4,-20,-20,4,0])\n",
        "# sigma8 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu9 = np.array([20,20,20,20,0])\n",
        "# sigma9 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu10 = np.array([20,-10,-10,20,0])\n",
        "# sigma10 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "# sample1 = np.random.multivariate_normal(mean=mu1,cov= sigma1,size=500)\n",
        "# sample2 = np.random.multivariate_normal(mean=mu2,cov= sigma2,size=500)\n",
        "# sample3 = np.random.multivariate_normal(mean=mu3,cov= sigma3,size=500)\n",
        "# sample4 = np.random.multivariate_normal(mean=mu4,cov= sigma4,size=500)\n",
        "# sample5 = np.random.multivariate_normal(mean=mu5,cov= sigma5,size=500)\n",
        "# sample6 = np.random.multivariate_normal(mean=mu6,cov= sigma6,size=500)\n",
        "# sample7 = np.random.multivariate_normal(mean=mu7,cov= sigma7,size=500)\n",
        "# sample8 = np.random.multivariate_normal(mean=mu8,cov= sigma8,size=500)\n",
        "# sample9 = np.random.multivariate_normal(mean=mu9,cov= sigma9,size=500)\n",
        "# sample10 = np.random.multivariate_normal(mean=mu10,cov= sigma10,size=500)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YDnxeP-2_1V"
      },
      "source": [
        "# X = np.concatenate((sample1,sample2,sample3,sample4,sample5,sample6,sample7,sample8,sample9,sample10),axis=0)\n",
        "# Y = np.concatenate((np.zeros((500,1)),np.ones((500,1)),2*np.ones((500,1)),3*np.ones((500,1)),4*np.ones((500,1)),\n",
        "#                     5*np.ones((500,1)),6*np.ones((500,1)),7*np.ones((500,1)),8*np.ones((500,1)),9*np.ones((500,1))),axis=0).astype(int)\n",
        "# print(X.shape,Y.shape)\n",
        "# plt.scatter(sample1[:,0],sample1[:,1],label=\"class_0\")\n",
        "# plt.scatter(sample2[:,0],sample2[:,1],label=\"class_1\")\n",
        "# plt.scatter(sample3[:,0],sample3[:,1],label=\"class_2\")\n",
        "# plt.scatter(sample4[:,0],sample4[:,1],label=\"class_3\")\n",
        "# plt.scatter(sample5[:,0],sample5[:,1],label=\"class_4\")\n",
        "# plt.scatter(sample6[:,0],sample6[:,1],label=\"class_5\")\n",
        "# plt.scatter(sample7[:,0],sample7[:,1],label=\"class_6\")\n",
        "# plt.scatter(sample8[:,0],sample8[:,1],label=\"class_7\")\n",
        "# plt.scatter(sample9[:,0],sample9[:,1],label=\"class_8\")\n",
        "# plt.scatter(sample10[:,0],sample10[:,1],label=\"class_9\")\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6YzqPUf3CHa"
      },
      "source": [
        "# class SyntheticDataset(Dataset):\n",
        "#   \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "#   def __init__(self, x, y):\n",
        "#     \"\"\"\n",
        "#       Args:\n",
        "#         csv_file (string): Path to the csv file with annotations.\n",
        "#         root_dir (string): Directory with all the images.\n",
        "#         transform (callable, optional): Optional transform to be applied\n",
        "#             on a sample.\n",
        "#     \"\"\"\n",
        "#     self.x = x\n",
        "#     self.y = y\n",
        "#     #self.fore_idx = fore_idx\n",
        "    \n",
        "#   def __len__(self):\n",
        "#     return len(self.y)\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "#     return self.x[idx] , self.y[idx] #, self.fore_idx[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mi3nL5-4D7_"
      },
      "source": [
        "# trainset = SyntheticDataset(X,Y)\n",
        "# #\n",
        "\n",
        "# # testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKzc7IgwqoU2"
      },
      "source": [
        "# classes = ('zero','one','two','three','four','five','six','seven','eight','nine')\n",
        "\n",
        "# foreground_classes = {'zero','one','two'}\n",
        "# fg_used = '012'\n",
        "# fg1, fg2, fg3 = 0,1,2\n",
        "\n",
        "\n",
        "# all_classes = {'zero','one','two','three','four','five','six','seven','eight','nine'}\n",
        "# background_classes = all_classes - foreground_classes\n",
        "# background_classes"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT6iKHutquR8"
      },
      "source": [
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWKzXkPSq5KU"
      },
      "source": [
        "# dataiter = iter(trainloader)\n",
        "# background_data=[]\n",
        "# background_label=[]\n",
        "# foreground_data=[]\n",
        "# foreground_label=[]\n",
        "# batch_size=100\n",
        "\n",
        "# for i in range(50):\n",
        "#   images, labels = dataiter.next()\n",
        "#   for j in range(batch_size):\n",
        "#     if(classes[labels[j]] in background_classes):\n",
        "#       img = images[j].tolist()\n",
        "#       background_data.append(img)\n",
        "#       background_label.append(labels[j])\n",
        "#     else:\n",
        "#       img = images[j].tolist()\n",
        "#       foreground_data.append(img)\n",
        "#       foreground_label.append(labels[j])\n",
        "            \n",
        "# foreground_data = torch.tensor(foreground_data)\n",
        "# foreground_label = torch.tensor(foreground_label)\n",
        "# background_data = torch.tensor(background_data)\n",
        "# background_label = torch.tensor(background_label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChdziOP3rF1G"
      },
      "source": [
        "# def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "#   \"\"\"\n",
        "#   bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "#   fg_idx : index of image to be used as foreground image from foreground data\n",
        "#   fg : at what position/index foreground image has to be stored out of 0-8\n",
        "#   \"\"\"\n",
        "#   image_list=[]\n",
        "#   j=0\n",
        "#   for i in range(9):\n",
        "#     if i != fg:\n",
        "#       image_list.append(background_data[bg_idx[j]])\n",
        "#       j+=1\n",
        "#     else: \n",
        "#       image_list.append(foreground_data[fg_idx])\n",
        "#       label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
        "#   #image_list = np.concatenate(image_list ,axis=0)\n",
        "#   image_list = torch.stack(image_list) \n",
        "#   return image_list,label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASrmPqErIDM"
      },
      "source": [
        "# desired_num = 3000\n",
        "# mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "# fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "# mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "# list_set_labels = [] \n",
        "# for i in range(desired_num):\n",
        "#   set_idx = set()\n",
        "#   np.random.seed(i)\n",
        "#   bg_idx = np.random.randint(0,3500,8)\n",
        "#   set_idx = set(background_label[bg_idx].tolist())\n",
        "#   fg_idx = np.random.randint(0,1500)\n",
        "#   set_idx.add(foreground_label[fg_idx].item())\n",
        "#   fg = np.random.randint(0,9)\n",
        "#   fore_idx.append(fg)\n",
        "#   image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "#   mosaic_list_of_images.append(image_list)\n",
        "#   mosaic_label.append(label)\n",
        "#   list_set_labels.append(set_idx)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm4jv30eD_hl"
      },
      "source": [
        "# data =  [{\"mosaic_list\":mosaic_list_of_images, \"mosaic_label\": mosaic_label, \"fore_idx\":fore_idx}]\r\n",
        "# np.save(\"mosaic_data.npy\",data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"mosaic_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\r\n",
        "mosaic_label = data[0][\"mosaic_label\"]\r\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,6)  #,self.output)\n",
        "        self.linear2 = nn.Linear(6,12)\n",
        "        self.linear3 = nn.Linear(12,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,12], dtype=torch.float64)   # number of features of output\n",
        "        features = torch.zeros([batch,self.K,12],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x = F.relu(x) \n",
        "      x = self.linear2(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.linear3(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,6)\n",
        "        self.linear2 = nn.Linear(6,12)\n",
        "        self.linear3 = nn.Linear(12,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      x = F.relu(self.linear2(x))\n",
        "      x = self.linear3(x)\n",
        "      return x    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOfxUJZ_eFKw"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(5,1,9,5).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(12,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.01)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.01)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 2500\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEabNK9Q1bTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "49deeef8-6508-4fe9-c317-e4c98ee381af"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faf5656e210>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "83b41c25-9f9b-4c86-dfeb-4e2688addc46"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.67566667e+01, 3.22333333e+00, 1.83333333e-02, 1.66666667e-03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"synthetic_second.csv\",index=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzR8ISPlOSbP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "d17e8b17-a2d1-48ab-92a8-9190245cd2ed"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>99.766667</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>66.700000</td>\n",
              "      <td>33.200000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>99.066667</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>85.800000</td>\n",
              "      <td>14.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>87.000000</td>\n",
              "      <td>12.933333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>97.300000</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>99.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>99.966667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>99.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          FTPT       FFPT      FTPF      FFPF\n",
              "0   100.000000   0.000000  0.000000  0.000000\n",
              "1   100.000000   0.000000  0.000000  0.000000\n",
              "2   100.000000   0.000000  0.000000  0.000000\n",
              "3    99.966667   0.033333  0.000000  0.000000\n",
              "4    99.766667   0.233333  0.000000  0.000000\n",
              "5   100.000000   0.000000  0.000000  0.000000\n",
              "6   100.000000   0.000000  0.000000  0.000000\n",
              "7    66.700000  33.200000  0.066667  0.033333\n",
              "8   100.000000   0.000000  0.000000  0.000000\n",
              "9    99.066667   0.933333  0.000000  0.000000\n",
              "10  100.000000   0.000000  0.000000  0.000000\n",
              "11   85.800000  14.100000  0.100000  0.000000\n",
              "12   87.000000  12.933333  0.066667  0.000000\n",
              "13   97.300000   2.700000  0.000000  0.000000\n",
              "14   99.900000   0.000000  0.100000  0.000000\n",
              "15  100.000000   0.000000  0.000000  0.000000\n",
              "16  100.000000   0.000000  0.000000  0.000000\n",
              "17   99.966667   0.000000  0.033333  0.000000\n",
              "18   99.666667   0.333333  0.000000  0.000000\n",
              "19  100.000000   0.000000  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}