{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generalization_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvhP2fR9tuc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AKOgwmmuDVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01a252ed-36d6-455f-809e-9e6af3abbde7"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRNXNPHWxCT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=False)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "fg1,fg2,fg3 = 0,1,2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nlXuo8mxHs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3bTKPpixLFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]])#.type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx])#.type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]- fg1  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho0BsIqqxOj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desired_num = 30000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "#fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  np.random.seed(i)\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  #fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tebDf0O3xSrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx]\n",
        "\n",
        "batch = 125\n",
        "msd = MosaicDataset(mosaic_list_of_images, mosaic_label )\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9ggP3kz9Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images =[]        #list of mosaic images, each mosaic image is saved as laist of 9 images\n",
        "fore_idx_test =[]                   #list of indexes at which foreground image is present in a mosaic image                \n",
        "test_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(10000):\n",
        "  np.random.seed(i+30000)\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx_test.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  test_images.append(image_list)\n",
        "  test_label.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPEFFNfrz9Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = MosaicDataset(test_images,test_label)\n",
        "test_loader = DataLoader( test_data,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU6nQ692xZHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Focus(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Focus, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0)\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.fc2 = nn.Linear(512, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "    self.fc4 = nn.Linear(10,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    # print(x.shape)\n",
        "    x = (F.relu(self.conv3(x)))\n",
        "    x =  x.view(x.size(0), -1)\n",
        "    # print(x.shape)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTEbTBRXxajy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classification(nn.Module):\n",
        "  def __init__(self,focus_net):\n",
        "    super(Classification, self).__init__()\n",
        "    self.module1 = focus_net\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=0)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0)\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.fc2 = nn.Linear(512, 64)\n",
        "    self.fc3 = nn.Linear(64, 10)\n",
        "    self.fc4 = nn.Linear(10,3)\n",
        "\n",
        "  def forward(self,z):  #z batch of list of 9 images\n",
        "    y = torch.zeros([batch,3, 32,32], dtype=torch.float64)\n",
        "    x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "    x = x.to(\"cuda\")\n",
        "    y = y.to(\"cuda\")\n",
        "\n",
        "    for i in range(9):\n",
        "        x[:,i] = self.module1.forward(z[:,i])[:,0]\n",
        "\n",
        "    x = F.softmax(x,dim=1)\n",
        "\n",
        "    x1 = x[:,0]\n",
        "    torch.mul(x1[:,None,None,None],z[:,0])\n",
        "\n",
        "    for i in range(9):            \n",
        "      x1 = x[:,i]          \n",
        "      y = y + torch.mul(x1[:,None,None,None],z[:,i])\n",
        "    \n",
        "    y1 = self.pool(F.relu(self.conv1(y)))\n",
        "    y1 = self.pool(F.relu(self.conv2(y1)))\n",
        "    # print(x.shape)\n",
        "    y1 = (F.relu(self.conv3(y1)))\n",
        "    y1 =  y1.view(y1.size(0), -1)\n",
        "    # print(x.shape)\n",
        "    y1 = F.relu(self.fc1(y1))\n",
        "    y1 = F.relu(self.fc2(y1))\n",
        "    y1 = F.relu(self.fc3(y1))\n",
        "    y1 = self.fc4(y1)\n",
        "\n",
        "    return y1 , x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_u8JyyLxy6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "focus_net = Focus().double()\n",
        "for params in focus_net.parameters():\n",
        "  params.requires_grad = True\n",
        "classify = Classification(focus_net).double()\n",
        "classify = classify.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_bdCUU0Xmm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion_classify = nn.CrossEntropyLoss()\n",
        "optimizer_classify = optim.SGD(classify.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp9SRr_fVLAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04bc6f66-8871-4aca-a567-973a05cd04c3"
      },
      "source": [
        "nos_epochs =300\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times  \n",
        "  running_loss = 0.0\n",
        "  epoch_loss = []\n",
        "  cnt=0\n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    # zero the parameter gradients\n",
        "    \n",
        "    optimizer_classify.zero_grad()\n",
        "    outputs, alphas, avg_images = classify(inputs)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = criterion_classify(outputs, labels) \n",
        "    loss.backward()\n",
        "    optimizer_classify.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini = 60\n",
        "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
        "      epoch_loss.append(running_loss/mini)\n",
        "      running_loss = 0.0\n",
        "    cnt=cnt+1\n",
        "  if(np.mean(epoch_loss) <= 0.03):\n",
        "      break;\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    60] loss: 1.100\n",
            "[1,   120] loss: 1.099\n",
            "[1,   180] loss: 1.099\n",
            "[1,   240] loss: 1.099\n",
            "[2,    60] loss: 1.099\n",
            "[2,   120] loss: 1.099\n",
            "[2,   180] loss: 1.099\n",
            "[2,   240] loss: 1.099\n",
            "[3,    60] loss: 1.099\n",
            "[3,   120] loss: 1.099\n",
            "[3,   180] loss: 1.099\n",
            "[3,   240] loss: 1.098\n",
            "[4,    60] loss: 1.099\n",
            "[4,   120] loss: 1.098\n",
            "[4,   180] loss: 1.098\n",
            "[4,   240] loss: 1.098\n",
            "[5,    60] loss: 1.098\n",
            "[5,   120] loss: 1.098\n",
            "[5,   180] loss: 1.098\n",
            "[5,   240] loss: 1.097\n",
            "[6,    60] loss: 1.096\n",
            "[6,   120] loss: 1.096\n",
            "[6,   180] loss: 1.095\n",
            "[6,   240] loss: 1.093\n",
            "[7,    60] loss: 1.092\n",
            "[7,   120] loss: 1.090\n",
            "[7,   180] loss: 1.086\n",
            "[7,   240] loss: 1.085\n",
            "[8,    60] loss: 1.081\n",
            "[8,   120] loss: 1.079\n",
            "[8,   180] loss: 1.073\n",
            "[8,   240] loss: 1.072\n",
            "[9,    60] loss: 1.068\n",
            "[9,   120] loss: 1.072\n",
            "[9,   180] loss: 1.068\n",
            "[9,   240] loss: 1.066\n",
            "[10,    60] loss: 1.060\n",
            "[10,   120] loss: 1.066\n",
            "[10,   180] loss: 1.063\n",
            "[10,   240] loss: 1.061\n",
            "[11,    60] loss: 1.057\n",
            "[11,   120] loss: 1.058\n",
            "[11,   180] loss: 1.062\n",
            "[11,   240] loss: 1.048\n",
            "[12,    60] loss: 1.051\n",
            "[12,   120] loss: 1.053\n",
            "[12,   180] loss: 1.052\n",
            "[12,   240] loss: 1.047\n",
            "[13,    60] loss: 1.049\n",
            "[13,   120] loss: 1.049\n",
            "[13,   180] loss: 1.045\n",
            "[13,   240] loss: 1.048\n",
            "[14,    60] loss: 1.046\n",
            "[14,   120] loss: 1.042\n",
            "[14,   180] loss: 1.045\n",
            "[14,   240] loss: 1.037\n",
            "[15,    60] loss: 1.030\n",
            "[15,   120] loss: 1.038\n",
            "[15,   180] loss: 1.036\n",
            "[15,   240] loss: 1.020\n",
            "[16,    60] loss: 1.020\n",
            "[16,   120] loss: 1.009\n",
            "[16,   180] loss: 1.006\n",
            "[16,   240] loss: 0.997\n",
            "[17,    60] loss: 0.982\n",
            "[17,   120] loss: 0.962\n",
            "[17,   180] loss: 0.954\n",
            "[17,   240] loss: 0.943\n",
            "[18,    60] loss: 0.918\n",
            "[18,   120] loss: 0.895\n",
            "[18,   180] loss: 0.889\n",
            "[18,   240] loss: 0.885\n",
            "[19,    60] loss: 0.860\n",
            "[19,   120] loss: 0.856\n",
            "[19,   180] loss: 0.830\n",
            "[19,   240] loss: 0.811\n",
            "[20,    60] loss: 0.802\n",
            "[20,   120] loss: 0.763\n",
            "[20,   180] loss: 0.758\n",
            "[20,   240] loss: 0.741\n",
            "[21,    60] loss: 0.728\n",
            "[21,   120] loss: 0.714\n",
            "[21,   180] loss: 0.680\n",
            "[21,   240] loss: 0.675\n",
            "[22,    60] loss: 0.648\n",
            "[22,   120] loss: 0.632\n",
            "[22,   180] loss: 0.636\n",
            "[22,   240] loss: 0.632\n",
            "[23,    60] loss: 0.588\n",
            "[23,   120] loss: 0.600\n",
            "[23,   180] loss: 0.563\n",
            "[23,   240] loss: 0.564\n",
            "[24,    60] loss: 0.506\n",
            "[24,   120] loss: 0.525\n",
            "[24,   180] loss: 0.536\n",
            "[24,   240] loss: 0.532\n",
            "[25,    60] loss: 0.468\n",
            "[25,   120] loss: 0.460\n",
            "[25,   180] loss: 0.502\n",
            "[25,   240] loss: 0.471\n",
            "[26,    60] loss: 0.423\n",
            "[26,   120] loss: 0.464\n",
            "[26,   180] loss: 0.426\n",
            "[26,   240] loss: 0.414\n",
            "[27,    60] loss: 0.378\n",
            "[27,   120] loss: 0.388\n",
            "[27,   180] loss: 0.412\n",
            "[27,   240] loss: 0.392\n",
            "[28,    60] loss: 0.350\n",
            "[28,   120] loss: 0.369\n",
            "[28,   180] loss: 0.361\n",
            "[28,   240] loss: 0.372\n",
            "[29,    60] loss: 0.322\n",
            "[29,   120] loss: 0.325\n",
            "[29,   180] loss: 0.345\n",
            "[29,   240] loss: 0.353\n",
            "[30,    60] loss: 0.307\n",
            "[30,   120] loss: 0.294\n",
            "[30,   180] loss: 0.328\n",
            "[30,   240] loss: 0.308\n",
            "[31,    60] loss: 0.292\n",
            "[31,   120] loss: 0.271\n",
            "[31,   180] loss: 0.281\n",
            "[31,   240] loss: 0.294\n",
            "[32,    60] loss: 0.241\n",
            "[32,   120] loss: 0.253\n",
            "[32,   180] loss: 0.260\n",
            "[32,   240] loss: 0.257\n",
            "[33,    60] loss: 0.221\n",
            "[33,   120] loss: 0.236\n",
            "[33,   180] loss: 0.243\n",
            "[33,   240] loss: 0.229\n",
            "[34,    60] loss: 0.216\n",
            "[34,   120] loss: 0.225\n",
            "[34,   180] loss: 0.241\n",
            "[34,   240] loss: 0.228\n",
            "[35,    60] loss: 0.207\n",
            "[35,   120] loss: 0.201\n",
            "[35,   180] loss: 0.211\n",
            "[35,   240] loss: 0.196\n",
            "[36,    60] loss: 0.165\n",
            "[36,   120] loss: 0.184\n",
            "[36,   180] loss: 0.187\n",
            "[36,   240] loss: 0.207\n",
            "[37,    60] loss: 0.156\n",
            "[37,   120] loss: 0.157\n",
            "[37,   180] loss: 0.170\n",
            "[37,   240] loss: 0.193\n",
            "[38,    60] loss: 0.155\n",
            "[38,   120] loss: 0.164\n",
            "[38,   180] loss: 0.166\n",
            "[38,   240] loss: 0.153\n",
            "[39,    60] loss: 0.144\n",
            "[39,   120] loss: 0.140\n",
            "[39,   180] loss: 0.151\n",
            "[39,   240] loss: 0.154\n",
            "[40,    60] loss: 0.122\n",
            "[40,   120] loss: 0.125\n",
            "[40,   180] loss: 0.152\n",
            "[40,   240] loss: 0.152\n",
            "[41,    60] loss: 0.112\n",
            "[41,   120] loss: 0.133\n",
            "[41,   180] loss: 0.135\n",
            "[41,   240] loss: 0.122\n",
            "[42,    60] loss: 0.103\n",
            "[42,   120] loss: 0.114\n",
            "[42,   180] loss: 0.113\n",
            "[42,   240] loss: 0.141\n",
            "[43,    60] loss: 0.126\n",
            "[43,   120] loss: 0.110\n",
            "[43,   180] loss: 0.128\n",
            "[43,   240] loss: 0.128\n",
            "[44,    60] loss: 0.094\n",
            "[44,   120] loss: 0.092\n",
            "[44,   180] loss: 0.102\n",
            "[44,   240] loss: 0.124\n",
            "[45,    60] loss: 0.096\n",
            "[45,   120] loss: 0.106\n",
            "[45,   180] loss: 0.116\n",
            "[45,   240] loss: 0.103\n",
            "[46,    60] loss: 0.100\n",
            "[46,   120] loss: 0.097\n",
            "[46,   180] loss: 0.099\n",
            "[46,   240] loss: 0.098\n",
            "[47,    60] loss: 0.084\n",
            "[47,   120] loss: 0.083\n",
            "[47,   180] loss: 0.093\n",
            "[47,   240] loss: 0.082\n",
            "[48,    60] loss: 0.123\n",
            "[48,   120] loss: 0.090\n",
            "[48,   180] loss: 0.137\n",
            "[48,   240] loss: 0.098\n",
            "[49,    60] loss: 0.069\n",
            "[49,   120] loss: 0.074\n",
            "[49,   180] loss: 0.068\n",
            "[49,   240] loss: 0.068\n",
            "[50,    60] loss: 0.051\n",
            "[50,   120] loss: 0.065\n",
            "[50,   180] loss: 0.079\n",
            "[50,   240] loss: 0.073\n",
            "[51,    60] loss: 0.059\n",
            "[51,   120] loss: 0.054\n",
            "[51,   180] loss: 0.053\n",
            "[51,   240] loss: 0.061\n",
            "[52,    60] loss: 0.056\n",
            "[52,   120] loss: 0.058\n",
            "[52,   180] loss: 0.045\n",
            "[52,   240] loss: 0.070\n",
            "[53,    60] loss: 0.045\n",
            "[53,   120] loss: 0.047\n",
            "[53,   180] loss: 0.044\n",
            "[53,   240] loss: 0.050\n",
            "[54,    60] loss: 0.042\n",
            "[54,   120] loss: 0.046\n",
            "[54,   180] loss: 0.057\n",
            "[54,   240] loss: 0.054\n",
            "[55,    60] loss: 0.044\n",
            "[55,   120] loss: 0.034\n",
            "[55,   180] loss: 0.041\n",
            "[55,   240] loss: 0.048\n",
            "[56,    60] loss: 0.031\n",
            "[56,   120] loss: 0.040\n",
            "[56,   180] loss: 0.059\n",
            "[56,   240] loss: 0.067\n",
            "[57,    60] loss: 0.057\n",
            "[57,   120] loss: 0.074\n",
            "[57,   180] loss: 0.062\n",
            "[57,   240] loss: 0.069\n",
            "[58,    60] loss: 0.037\n",
            "[58,   120] loss: 0.040\n",
            "[58,   180] loss: 0.042\n",
            "[58,   240] loss: 0.049\n",
            "[59,    60] loss: 0.041\n",
            "[59,   120] loss: 0.060\n",
            "[59,   180] loss: 0.040\n",
            "[59,   240] loss: 0.051\n",
            "[60,    60] loss: 0.030\n",
            "[60,   120] loss: 0.043\n",
            "[60,   180] loss: 0.034\n",
            "[60,   240] loss: 0.038\n",
            "[61,    60] loss: 0.038\n",
            "[61,   120] loss: 0.049\n",
            "[61,   180] loss: 0.044\n",
            "[61,   240] loss: 0.040\n",
            "[62,    60] loss: 0.037\n",
            "[62,   120] loss: 0.018\n",
            "[62,   180] loss: 0.030\n",
            "[62,   240] loss: 0.029\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6R_j82EoJuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.save(classify.state_dict(),\"/content/brandom_btrain.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q5GJPPfyK1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d6b562a-92a2-4f53-d3d0-4763b4f71675"
      },
      "source": [
        "classify.load_state_dict( torch.load(\"/content/brandom_ctrain.pt\"))\n",
        "#classify.load_state_dict( torch.load(\"/content/drive/My Drive/Cheating_data/16_experiments_on_cnn_3layers/4_focus_random_classify_random_train_classify.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dqtbDtjzX4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d74e064b-c96a-4d97-cb0b-e5de23bc1901"
      },
      "source": [
        "classify.eval()\n",
        "total = 0 \n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels  = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels  = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "    outputs, alphas, avg_images = classify(inputs)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 30000 train images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 30000 train images: 99 %\n",
            "total correct 29773\n",
            "total train set images 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-15xPM280v4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "642b52e8-daaa-43ad-883c-173d6f74735a"
      },
      "source": [
        "classify.eval()\n",
        "total = 0 \n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels  = data\n",
        "    inputs = inputs.double() \n",
        "    inputs, labels  = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "    outputs, alphas, avg_images = classify(inputs)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 41 %\n",
            "total correct 4155\n",
            "total train set images 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKazVW7WHueg",
        "colab_type": "text"
      },
      "source": [
        "# Analysis on both random train classify Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQB7lW6r16ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN = 100000\n",
        "MAX = -100000\n",
        "i=0\n",
        "for param in classify.parameters():\n",
        "  if i %2 == 0:\n",
        "    min_value = np.min(param.cpu().detach().numpy())\n",
        "    max_value = np.max(param.cpu().detach().numpy())\n",
        "    if MIN > min_value:\n",
        "      MIN = min_value\n",
        "    if MAX < max_value:\n",
        "      MAX = max_value \n",
        "  i =i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_tDHGCaIGqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86c496fe-4c9e-4d8b-e8fa-66115ec8ec27"
      },
      "source": [
        "MIN,MAX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.473424726375127, 1.930031799345379)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm8-wt7gIHif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46bdc306-91f0-4d8f-bb10-3d74f0b81449"
      },
      "source": [
        "classify.eval()\n",
        "margin = []\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels  = data\n",
        "    inputs = inputs.double() \n",
        "    inputs  = inputs.to(\"cuda\")\n",
        "    output, alphas, avg_images = classify(inputs)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    indexes = np.arange(output.shape[0]), np.argsort(output, axis=1)[:, -2]\n",
        "    second_largest = output[indexes]\n",
        "    margin.append(np.diag(output[:,labels]) - second_largest)\n",
        "\n",
        "margin = np.concatenate(margin,axis=0)\n",
        "margin = np.percentile(margin[margin>0],5)\n",
        "print(margin)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0872364954705107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P78adR7jIHM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV082_iNIGMC",
        "colab_type": "text"
      },
      "source": [
        "# Analysis on both random train both Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scFA7u5LILOR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN = 100000\n",
        "MAX = -100000\n",
        "i = 0\n",
        "for param in classify.parameters():\n",
        "  if i %2 == 0:\n",
        "    min_value = np.min(param.cpu().detach().numpy())\n",
        "    max_value = np.max(param.cpu().detach().numpy())\n",
        "    if MIN > min_value:\n",
        "      MIN = min_value\n",
        "    if MAX < max_value:\n",
        "      MAX = max_value \n",
        "  i = i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfy5-OToKK5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccf9f021-c8a9-4098-a3cb-cb3fbb0ab462"
      },
      "source": [
        "MIN,MAX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.093979748167102, 1.5115582056525738)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7FMmE3iKMha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82d27b18-63de-42fc-9ffc-7bddcd19f195"
      },
      "source": [
        "classify.eval()\n",
        "margin = []\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels  = data\n",
        "    inputs = inputs.double() \n",
        "    inputs  = inputs.to(\"cuda\")\n",
        "    output, alphas, avg_images = classify(inputs)\n",
        "    output = output.cpu().detach().numpy()\n",
        "    indexes = np.arange(output.shape[0]), np.argsort(output, axis=1)[:, -2]\n",
        "    second_largest = output[indexes]\n",
        "    margin.append(np.diag(output[:,labels]) - second_largest)\n",
        "\n",
        "margin = np.concatenate(margin,axis=0)\n",
        "margin = np.percentile(margin[margin>0],5)\n",
        "print(margin)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.4494927697682702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MVv2vIOsDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}