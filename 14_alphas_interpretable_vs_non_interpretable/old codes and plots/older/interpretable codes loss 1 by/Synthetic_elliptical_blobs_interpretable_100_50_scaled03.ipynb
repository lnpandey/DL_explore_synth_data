{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Synthetic_elliptical_blobs_interpretable_100_50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        " import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEp-LtqiWAf"
      },
      "source": [
        "mu1 = np.array([3,3,3,3,0])\n",
        "sigma1 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu2 = np.array([4,4,4,4,0])\n",
        "sigma2 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu3 = np.array([10,5,5,10,0])\n",
        "sigma3 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu4 = np.array([-10,-10,-10,-10,0])\n",
        "sigma4 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu5 = np.array([-21,4,4,-21,0])\n",
        "sigma5 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu6 = np.array([-10,18,18,-10,0])\n",
        "sigma6 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu7 = np.array([4,20,4,20,0])\n",
        "sigma7 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu8 = np.array([4,-20,-20,4,0])\n",
        "sigma8 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu9 = np.array([20,20,20,20,0])\n",
        "sigma9 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu10 = np.array([20,-10,-10,20,0])\n",
        "sigma10 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "sample1 = np.random.multivariate_normal(mean=mu1,cov= sigma1,size=500)\n",
        "sample2 = np.random.multivariate_normal(mean=mu2,cov= sigma2,size=500)\n",
        "sample3 = np.random.multivariate_normal(mean=mu3,cov= sigma3,size=500)\n",
        "sample4 = np.random.multivariate_normal(mean=mu4,cov= sigma4,size=500)\n",
        "sample5 = np.random.multivariate_normal(mean=mu5,cov= sigma5,size=500)\n",
        "sample6 = np.random.multivariate_normal(mean=mu6,cov= sigma6,size=500)\n",
        "sample7 = np.random.multivariate_normal(mean=mu7,cov= sigma7,size=500)\n",
        "sample8 = np.random.multivariate_normal(mean=mu8,cov= sigma8,size=500)\n",
        "sample9 = np.random.multivariate_normal(mean=mu9,cov= sigma9,size=500)\n",
        "sample10 = np.random.multivariate_normal(mean=mu10,cov= sigma10,size=500)\n"
      ],
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NshDNGjY2T3w"
      },
      "source": [
        "# mu1 = np.array([3,3,0,0,0])\n",
        "# sigma1 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu2 = np.array([4,4,0,0,0])\n",
        "# sigma2 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu3 = np.array([10,5,0,0,0])\n",
        "# sigma3 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu4 = np.array([-10,-10,0,0,0])\n",
        "# sigma4 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu5 = np.array([-21,4,0,0,0])\n",
        "# sigma5 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu6 = np.array([-10,18,0,0,0])\n",
        "# sigma6 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu7 = np.array([4,20,0,0,0])\n",
        "# sigma7 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu8 = np.array([4,-20,0,0,0])\n",
        "# sigma8 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu9 = np.array([20,20,0,0,0])\n",
        "# sigma9 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu10 = np.array([20,-10,0,0,0])\n",
        "# sigma10 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "# sample1 = np.random.multivariate_normal(mean=mu1,cov= sigma1,size=500)\n",
        "# sample2 = np.random.multivariate_normal(mean=mu2,cov= sigma2,size=500)\n",
        "# sample3 = np.random.multivariate_normal(mean=mu3,cov= sigma3,size=500)\n",
        "# sample4 = np.random.multivariate_normal(mean=mu4,cov= sigma4,size=500)\n",
        "# sample5 = np.random.multivariate_normal(mean=mu5,cov= sigma5,size=500)\n",
        "# sample6 = np.random.multivariate_normal(mean=mu6,cov= sigma6,size=500)\n",
        "# sample7 = np.random.multivariate_normal(mean=mu7,cov= sigma7,size=500)\n",
        "# sample8 = np.random.multivariate_normal(mean=mu8,cov= sigma8,size=500)\n",
        "# sample9 = np.random.multivariate_normal(mean=mu9,cov= sigma9,size=500)\n",
        "# sample10 = np.random.multivariate_normal(mean=mu10,cov= sigma10,size=500)"
      ],
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YDnxeP-2_1V",
        "outputId": "c05719b6-eb75-489c-aef9-f94854e0572e"
      },
      "source": [
        "X = np.concatenate((sample1,sample2,sample3,sample4,sample5,sample6,sample7,sample8,sample9,sample10),axis=0)\n",
        "Y = np.concatenate((np.zeros((500,1)),np.ones((500,1)),2*np.ones((500,1)),3*np.ones((500,1)),4*np.ones((500,1)),\n",
        "                    5*np.ones((500,1)),6*np.ones((500,1)),7*np.ones((500,1)),8*np.ones((500,1)),9*np.ones((500,1))),axis=0).astype(int)\n",
        "print(X.shape,Y.shape)\n",
        "# plt.scatter(sample1[:,0],sample1[:,1],label=\"class_0\")\n",
        "# plt.scatter(sample2[:,0],sample2[:,1],label=\"class_1\")\n",
        "# plt.scatter(sample3[:,0],sample3[:,1],label=\"class_2\")\n",
        "# plt.scatter(sample4[:,0],sample4[:,1],label=\"class_3\")\n",
        "# plt.scatter(sample5[:,0],sample5[:,1],label=\"class_4\")\n",
        "# plt.scatter(sample6[:,0],sample6[:,1],label=\"class_5\")\n",
        "# plt.scatter(sample7[:,0],sample7[:,1],label=\"class_6\")\n",
        "# plt.scatter(sample8[:,0],sample8[:,1],label=\"class_7\")\n",
        "# plt.scatter(sample9[:,0],sample9[:,1],label=\"class_8\")\n",
        "# plt.scatter(sample10[:,0],sample10[:,1],label=\"class_9\")\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 5) (5000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6YzqPUf3CHa"
      },
      "source": [
        "class SyntheticDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, x, y):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx] , self.y[idx] #, self.fore_idx[idx]"
      ],
      "execution_count": 359,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mi3nL5-4D7_"
      },
      "source": [
        "trainset = SyntheticDataset(X,Y)\n",
        "\n",
        "\n",
        "# testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKzc7IgwqoU2",
        "outputId": "e12699a7-84d0-4c14-910e-e58d52cb8fe4"
      },
      "source": [
        "classes = ('zero','one','two','three','four','five','six','seven','eight','nine')\n",
        "\n",
        "foreground_classes = {'zero','one','two'}\n",
        "fg_used = '012'\n",
        "fg1, fg2, fg3 = 0,1,2\n",
        "\n",
        "\n",
        "all_classes = {'zero','one','two','three','four','five','six','seven','eight','nine'}\n",
        "background_classes = all_classes - foreground_classes\n",
        "background_classes"
      ],
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eight', 'five', 'four', 'nine', 'seven', 'six', 'three'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 361
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT6iKHutquR8"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)"
      ],
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWKzXkPSq5KU"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=100\n",
        "\n",
        "for i in range(50):\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)"
      ],
      "execution_count": 363,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChdziOP3rF1G"
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]])\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx])\n",
        "      label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 364,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASrmPqErIDM"
      },
      "source": [
        "desired_num = 3000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "list_set_labels = [] \n",
        "for i in range(desired_num):\n",
        "  set_idx = set()\n",
        "  np.random.seed(i)\n",
        "  bg_idx = np.random.randint(0,3500,8)\n",
        "  set_idx = set(background_label[bg_idx].tolist())\n",
        "  fg_idx = np.random.randint(0,1500)\n",
        "  set_idx.add(foreground_label[fg_idx].item())\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "  list_set_labels.append(set_idx)"
      ],
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDFN7dCarmmR"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([5], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return torch.stack(avg_image_dataset) , torch.stack(labels) , foreground_index"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgF90qBIt8yN"
      },
      "source": [
        "def calculate_loss(dataloader,model,criter):\n",
        "  model.eval()\n",
        "  r_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      outputs = model(inputs)\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  return r_loss/i"
      ],
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 368,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPfrih82Bg"
      },
      "source": [
        "**Focus Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,100)  #,self.output)\n",
        "        self.linear2 = nn.Linear(100,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,self.d], dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            x[:,i] = self.helper(z[:,i] )[:,0]  # self.d*i:self.d*i+self.d\n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        x1 = x[:,0]\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],z[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      x = self.linear2(x)\n",
        "      return x\n"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrL0Zb484KO"
      },
      "source": [
        "**Classification Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,50)\n",
        "        self.linear2 = nn.Linear(50,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKHrKis88lW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAPjSKkrd0ru"
      },
      "source": [
        "where = Focus_deep(5,1,9,5).double()\n",
        "what = Classification_deep(5,3).double()\n",
        "where = where.to(\"cuda\")\n",
        "what = what.to(\"cuda\")"
      ],
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "\n",
        "      mx,_ = torch.max(alpha,1)\n",
        "      entropy = np.mean(-np.log2(mx.cpu().detach().numpy()))\n",
        "      # print(\"entropy of batch\", entropy)\n",
        "\n",
        "      loss = criter(outputs, labels) + 0.3*entropy\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MOfxUJZ_eFKw",
        "outputId": "716e9248-c65d-4a6b-87f8-7639b0d2c66b"
      },
      "source": [
        "print(\"--\"*40)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_where = optim.Adam(where.parameters(),lr =0.001)\n",
        "optimizer_what = optim.Adam(what.parameters(), lr=0.001)\n",
        "acti = []\n",
        "loss_curi = []\n",
        "analysis_data = []\n",
        "epochs = 1000\n",
        "running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "loss_curi.append(running_loss)\n",
        "analysis_data.append(anlys_data)\n",
        "print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  what.train()\n",
        "  where.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_ = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "    # zero the parameter gradients\n",
        "    optimizer_where.zero_grad()\n",
        "    optimizer_what.zero_grad()\n",
        "    # forward + backward + optimize\n",
        "    avg, alpha = where(inputs)\n",
        "    outputs = what(avg)\n",
        "\n",
        "    mx,_ = torch.max(alpha,1)\n",
        "    entropy = np.mean(-np.log2(mx.cpu().detach().numpy()))\n",
        "    # print(\"entropy of batch\", entropy)\n",
        "\n",
        "    loss = criterion(outputs, labels) + 0.3*entropy\n",
        "\n",
        "    # loss = criterion(outputs, labels)\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer_where.step()\n",
        "    optimizer_what.step()\n",
        "\n",
        "  running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  analysis_data.append(anls_data)\n",
        "  print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "  loss_curi.append(running_loss)   #loss per epoch\n",
        "  if running_loss<=0.01:\n",
        "    break\n",
        "print('Finished Training')\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    images, labels,_ = data\n",
        "    images = images.double()\n",
        "    images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    avg, alpha = where(images)\n",
        "    outputs  = what(avg)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 2.571\n",
            "epoch: [1] loss: 1.668\n",
            "epoch: [2] loss: 1.483\n",
            "epoch: [3] loss: 1.290\n",
            "epoch: [4] loss: 1.137\n",
            "epoch: [5] loss: 1.024\n",
            "epoch: [6] loss: 0.922\n",
            "epoch: [7] loss: 0.831\n",
            "epoch: [8] loss: 0.748\n",
            "epoch: [9] loss: 0.675\n",
            "epoch: [10] loss: 0.614\n",
            "epoch: [11] loss: 0.561\n",
            "epoch: [12] loss: 0.516\n",
            "epoch: [13] loss: 0.475\n",
            "epoch: [14] loss: 0.439\n",
            "epoch: [15] loss: 0.408\n",
            "epoch: [16] loss: 0.380\n",
            "epoch: [17] loss: 0.355\n",
            "epoch: [18] loss: 0.333\n",
            "epoch: [19] loss: 0.312\n",
            "epoch: [20] loss: 0.294\n",
            "epoch: [21] loss: 0.276\n",
            "epoch: [22] loss: 0.262\n",
            "epoch: [23] loss: 0.249\n",
            "epoch: [24] loss: 0.236\n",
            "epoch: [25] loss: 0.225\n",
            "epoch: [26] loss: 0.215\n",
            "epoch: [27] loss: 0.207\n",
            "epoch: [28] loss: 0.198\n",
            "epoch: [29] loss: 0.190\n",
            "epoch: [30] loss: 0.183\n",
            "epoch: [31] loss: 0.176\n",
            "epoch: [32] loss: 0.170\n",
            "epoch: [33] loss: 0.164\n",
            "epoch: [34] loss: 0.159\n",
            "epoch: [35] loss: 0.154\n",
            "epoch: [36] loss: 0.148\n",
            "epoch: [37] loss: 0.144\n",
            "epoch: [38] loss: 0.140\n",
            "epoch: [39] loss: 0.136\n",
            "epoch: [40] loss: 0.132\n",
            "epoch: [41] loss: 0.129\n",
            "epoch: [42] loss: 0.125\n",
            "epoch: [43] loss: 0.122\n",
            "epoch: [44] loss: 0.119\n",
            "epoch: [45] loss: 0.116\n",
            "epoch: [46] loss: 0.113\n",
            "epoch: [47] loss: 0.110\n",
            "epoch: [48] loss: 0.108\n",
            "epoch: [49] loss: 0.106\n",
            "epoch: [50] loss: 0.103\n",
            "epoch: [51] loss: 0.101\n",
            "epoch: [52] loss: 0.099\n",
            "epoch: [53] loss: 0.097\n",
            "epoch: [54] loss: 0.095\n",
            "epoch: [55] loss: 0.093\n",
            "epoch: [56] loss: 0.091\n",
            "epoch: [57] loss: 0.089\n",
            "epoch: [58] loss: 0.088\n",
            "epoch: [59] loss: 0.086\n",
            "epoch: [60] loss: 0.085\n",
            "epoch: [61] loss: 0.083\n",
            "epoch: [62] loss: 0.082\n",
            "epoch: [63] loss: 0.081\n",
            "epoch: [64] loss: 0.079\n",
            "epoch: [65] loss: 0.078\n",
            "epoch: [66] loss: 0.076\n",
            "epoch: [67] loss: 0.075\n",
            "epoch: [68] loss: 0.073\n",
            "epoch: [69] loss: 0.073\n",
            "epoch: [70] loss: 0.071\n",
            "epoch: [71] loss: 0.070\n",
            "epoch: [72] loss: 0.069\n",
            "epoch: [73] loss: 0.068\n",
            "epoch: [74] loss: 0.067\n",
            "epoch: [75] loss: 0.066\n",
            "epoch: [76] loss: 0.064\n",
            "epoch: [77] loss: 0.063\n",
            "epoch: [78] loss: 0.063\n",
            "epoch: [79] loss: 0.062\n",
            "epoch: [80] loss: 0.061\n",
            "epoch: [81] loss: 0.060\n",
            "epoch: [82] loss: 0.059\n",
            "epoch: [83] loss: 0.058\n",
            "epoch: [84] loss: 0.058\n",
            "epoch: [85] loss: 0.057\n",
            "epoch: [86] loss: 0.057\n",
            "epoch: [87] loss: 0.056\n",
            "epoch: [88] loss: 0.055\n",
            "epoch: [89] loss: 0.054\n",
            "epoch: [90] loss: 0.053\n",
            "epoch: [91] loss: 0.053\n",
            "epoch: [92] loss: 0.052\n",
            "epoch: [93] loss: 0.052\n",
            "epoch: [94] loss: 0.051\n",
            "epoch: [95] loss: 0.050\n",
            "epoch: [96] loss: 0.050\n",
            "epoch: [97] loss: 0.049\n",
            "epoch: [98] loss: 0.049\n",
            "epoch: [99] loss: 0.048\n",
            "epoch: [100] loss: 0.047\n",
            "epoch: [101] loss: 0.047\n",
            "epoch: [102] loss: 0.046\n",
            "epoch: [103] loss: 0.046\n",
            "epoch: [104] loss: 0.046\n",
            "epoch: [105] loss: 0.046\n",
            "epoch: [106] loss: 0.045\n",
            "epoch: [107] loss: 0.044\n",
            "epoch: [108] loss: 0.044\n",
            "epoch: [109] loss: 0.044\n",
            "epoch: [110] loss: 0.043\n",
            "epoch: [111] loss: 0.043\n",
            "epoch: [112] loss: 0.042\n",
            "epoch: [113] loss: 0.041\n",
            "epoch: [114] loss: 0.041\n",
            "epoch: [115] loss: 0.041\n",
            "epoch: [116] loss: 0.040\n",
            "epoch: [117] loss: 0.040\n",
            "epoch: [118] loss: 0.039\n",
            "epoch: [119] loss: 0.039\n",
            "epoch: [120] loss: 0.039\n",
            "epoch: [121] loss: 0.038\n",
            "epoch: [122] loss: 0.038\n",
            "epoch: [123] loss: 0.038\n",
            "epoch: [124] loss: 0.038\n",
            "epoch: [125] loss: 0.037\n",
            "epoch: [126] loss: 0.037\n",
            "epoch: [127] loss: 0.037\n",
            "epoch: [128] loss: 0.036\n",
            "epoch: [129] loss: 0.036\n",
            "epoch: [130] loss: 0.036\n",
            "epoch: [131] loss: 0.036\n",
            "epoch: [132] loss: 0.036\n",
            "epoch: [133] loss: 0.035\n",
            "epoch: [134] loss: 0.035\n",
            "epoch: [135] loss: 0.034\n",
            "epoch: [136] loss: 0.034\n",
            "epoch: [137] loss: 0.034\n",
            "epoch: [138] loss: 0.033\n",
            "epoch: [139] loss: 0.033\n",
            "epoch: [140] loss: 0.033\n",
            "epoch: [141] loss: 0.033\n",
            "epoch: [142] loss: 0.033\n",
            "epoch: [143] loss: 0.032\n",
            "epoch: [144] loss: 0.032\n",
            "epoch: [145] loss: 0.032\n",
            "epoch: [146] loss: 0.032\n",
            "epoch: [147] loss: 0.032\n",
            "epoch: [148] loss: 0.032\n",
            "epoch: [149] loss: 0.031\n",
            "epoch: [150] loss: 0.031\n",
            "epoch: [151] loss: 0.031\n",
            "epoch: [152] loss: 0.031\n",
            "epoch: [153] loss: 0.030\n",
            "epoch: [154] loss: 0.030\n",
            "epoch: [155] loss: 0.030\n",
            "epoch: [156] loss: 0.030\n",
            "epoch: [157] loss: 0.030\n",
            "epoch: [158] loss: 0.030\n",
            "epoch: [159] loss: 0.029\n",
            "epoch: [160] loss: 0.029\n",
            "epoch: [161] loss: 0.029\n",
            "epoch: [162] loss: 0.029\n",
            "epoch: [163] loss: 0.029\n",
            "epoch: [164] loss: 0.029\n",
            "epoch: [165] loss: 0.029\n",
            "epoch: [166] loss: 0.029\n",
            "epoch: [167] loss: 0.029\n",
            "epoch: [168] loss: 0.029\n",
            "epoch: [169] loss: 0.029\n",
            "epoch: [170] loss: 0.029\n",
            "epoch: [171] loss: 0.028\n",
            "epoch: [172] loss: 0.028\n",
            "epoch: [173] loss: 0.028\n",
            "epoch: [174] loss: 0.028\n",
            "epoch: [175] loss: 0.028\n",
            "epoch: [176] loss: 0.028\n",
            "epoch: [177] loss: 0.027\n",
            "epoch: [178] loss: 0.027\n",
            "epoch: [179] loss: 0.027\n",
            "epoch: [180] loss: 0.027\n",
            "epoch: [181] loss: 0.027\n",
            "epoch: [182] loss: 0.027\n",
            "epoch: [183] loss: 0.027\n",
            "epoch: [184] loss: 0.027\n",
            "epoch: [185] loss: 0.027\n",
            "epoch: [186] loss: 0.027\n",
            "epoch: [187] loss: 0.027\n",
            "epoch: [188] loss: 0.026\n",
            "epoch: [189] loss: 0.026\n",
            "epoch: [190] loss: 0.026\n",
            "epoch: [191] loss: 0.026\n",
            "epoch: [192] loss: 0.026\n",
            "epoch: [193] loss: 0.026\n",
            "epoch: [194] loss: 0.026\n",
            "epoch: [195] loss: 0.026\n",
            "epoch: [196] loss: 0.026\n",
            "epoch: [197] loss: 0.026\n",
            "epoch: [198] loss: 0.026\n",
            "epoch: [199] loss: 0.026\n",
            "epoch: [200] loss: 0.026\n",
            "epoch: [201] loss: 0.026\n",
            "epoch: [202] loss: 0.026\n",
            "epoch: [203] loss: 0.026\n",
            "epoch: [204] loss: 0.026\n",
            "epoch: [205] loss: 0.026\n",
            "epoch: [206] loss: 0.026\n",
            "epoch: [207] loss: 0.026\n",
            "epoch: [208] loss: 0.026\n",
            "epoch: [209] loss: 0.025\n",
            "epoch: [210] loss: 0.025\n",
            "epoch: [211] loss: 0.025\n",
            "epoch: [212] loss: 0.025\n",
            "epoch: [213] loss: 0.025\n",
            "epoch: [214] loss: 0.025\n",
            "epoch: [215] loss: 0.025\n",
            "epoch: [216] loss: 0.025\n",
            "epoch: [217] loss: 0.025\n",
            "epoch: [218] loss: 0.025\n",
            "epoch: [219] loss: 0.025\n",
            "epoch: [220] loss: 0.025\n",
            "epoch: [221] loss: 0.025\n",
            "epoch: [222] loss: 0.025\n",
            "epoch: [223] loss: 0.025\n",
            "epoch: [224] loss: 0.025\n",
            "epoch: [225] loss: 0.025\n",
            "epoch: [226] loss: 0.025\n",
            "epoch: [227] loss: 0.025\n",
            "epoch: [228] loss: 0.025\n",
            "epoch: [229] loss: 0.024\n",
            "epoch: [230] loss: 0.025\n",
            "epoch: [231] loss: 0.024\n",
            "epoch: [232] loss: 0.024\n",
            "epoch: [233] loss: 0.025\n",
            "epoch: [234] loss: 0.024\n",
            "epoch: [235] loss: 0.024\n",
            "epoch: [236] loss: 0.024\n",
            "epoch: [237] loss: 0.024\n",
            "epoch: [238] loss: 0.024\n",
            "epoch: [239] loss: 0.024\n",
            "epoch: [240] loss: 0.024\n",
            "epoch: [241] loss: 0.024\n",
            "epoch: [242] loss: 0.024\n",
            "epoch: [243] loss: 0.024\n",
            "epoch: [244] loss: 0.024\n",
            "epoch: [245] loss: 0.024\n",
            "epoch: [246] loss: 0.024\n",
            "epoch: [247] loss: 0.024\n",
            "epoch: [248] loss: 0.024\n",
            "epoch: [249] loss: 0.024\n",
            "epoch: [250] loss: 0.024\n",
            "epoch: [251] loss: 0.024\n",
            "epoch: [252] loss: 0.024\n",
            "epoch: [253] loss: 0.024\n",
            "epoch: [254] loss: 0.024\n",
            "epoch: [255] loss: 0.024\n",
            "epoch: [256] loss: 0.024\n",
            "epoch: [257] loss: 0.024\n",
            "epoch: [258] loss: 0.024\n",
            "epoch: [259] loss: 0.024\n",
            "epoch: [260] loss: 0.024\n",
            "epoch: [261] loss: 0.024\n",
            "epoch: [262] loss: 0.024\n",
            "epoch: [263] loss: 0.024\n",
            "epoch: [264] loss: 0.024\n",
            "epoch: [265] loss: 0.024\n",
            "epoch: [266] loss: 0.024\n",
            "epoch: [267] loss: 0.024\n",
            "epoch: [268] loss: 0.024\n",
            "epoch: [269] loss: 0.024\n",
            "epoch: [270] loss: 0.024\n",
            "epoch: [271] loss: 0.024\n",
            "epoch: [272] loss: 0.024\n",
            "epoch: [273] loss: 0.024\n",
            "epoch: [274] loss: 0.024\n",
            "epoch: [275] loss: 0.024\n",
            "epoch: [276] loss: 0.024\n",
            "epoch: [277] loss: 0.024\n",
            "epoch: [278] loss: 0.024\n",
            "epoch: [279] loss: 0.024\n",
            "epoch: [280] loss: 0.024\n",
            "epoch: [281] loss: 0.024\n",
            "epoch: [282] loss: 0.024\n",
            "epoch: [283] loss: 0.025\n",
            "epoch: [284] loss: 0.025\n",
            "epoch: [285] loss: 0.025\n",
            "epoch: [286] loss: 0.025\n",
            "epoch: [287] loss: 0.025\n",
            "epoch: [288] loss: 0.025\n",
            "epoch: [289] loss: 0.025\n",
            "epoch: [290] loss: 0.025\n",
            "epoch: [291] loss: 0.025\n",
            "epoch: [292] loss: 0.025\n",
            "epoch: [293] loss: 0.025\n",
            "epoch: [294] loss: 0.025\n",
            "epoch: [295] loss: 0.024\n",
            "epoch: [296] loss: 0.025\n",
            "epoch: [297] loss: 0.025\n",
            "epoch: [298] loss: 0.024\n",
            "epoch: [299] loss: 0.024\n",
            "epoch: [300] loss: 0.025\n",
            "epoch: [301] loss: 0.025\n",
            "epoch: [302] loss: 0.025\n",
            "epoch: [303] loss: 0.025\n",
            "epoch: [304] loss: 0.025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-375-9fb361f7a5a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer_what\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-370-228145e2baf6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# self.d*i:self.d*i+self.d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-370-228145e2baf6>\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "9da7750a-4193-4b8b-9a1f-17b3cfd736e4"
      },
      "source": [
        "analysis_data = np.array(analysis_data)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.savefig(\"trends_synthetic_300_300.png\",bbox_inches=\"tight\")\n",
        "plt.savefig(\"trends_synthetic_300_300.pdf\",bbox_inches=\"tight\")\n"
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-376-7a0997c64b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manalysis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ftpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ffpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ftpf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (306,) and (305,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5mag3jZ-LMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe528a0-0125-4b2b-dabe-afddb19f2857"
      },
      "source": [
        "analysis_data[-1,:2]/3000"
      ],
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.99333333e-01, 6.66666667e-04])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSxFtBWQ1M8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9674c3e0-fb1a-4304-f5d5-31def30c8785"
      },
      "source": [
        "running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\r\n",
        "print(running_loss, anls_data)"
      ],
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.02456110093563588 [2998, 2, 0, 0, 2972, 28]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncIi9Jc92a4u"
      },
      "source": [
        "what.eval()\r\n",
        "where.eval()\r\n",
        "alphas = []\r\n",
        "max_alpha =[]\r\n",
        "alpha_ftpt=[]\r\n",
        "alpha_ffpt=[]\r\n",
        "alpha_ftpf=[]\r\n",
        "alpha_ffpf=[]\r\n",
        "argmax_more_than_half=0\r\n",
        "argmax_less_than_half=0\r\n",
        "cnt =0\r\n",
        "with torch.no_grad():\r\n",
        "  for i, data in enumerate(train_loader, 0):\r\n",
        "    inputs, labels, fidx = data\r\n",
        "    inputs = inputs.double()\r\n",
        "    inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\r\n",
        "    avg, alphas = where(inputs)\r\n",
        "    outputs = what(avg)\r\n",
        "    _, predicted = torch.max(outputs.data, 1)\r\n",
        "    batch = len(predicted)\r\n",
        "    mx,_ = torch.max(alphas,1)\r\n",
        "    max_alpha.append(mx.cpu().detach().numpy())\r\n",
        "    for j in range (batch):\r\n",
        "      cnt+=1\r\n",
        "      focus = torch.argmax(alphas[j]).item()\r\n",
        "      if alphas[j][focus] >= 0.5 :\r\n",
        "        argmax_more_than_half += 1\r\n",
        "      else:\r\n",
        "        argmax_less_than_half += 1\r\n",
        "\r\n",
        "      if (focus == fidx[j].item() and predicted[j].item() == labels[j].item()):\r\n",
        "          alpha_ftpt.append(alphas[j][focus].item())\r\n",
        "          # print(focus, fore_idx[j].item(), predicted[j].item() , labels[j].item() )\r\n",
        "\r\n",
        "      elif (focus != fidx[j].item() and predicted[j].item() == labels[j].item()):\r\n",
        "          alpha_ffpt.append(alphas[j][focus].item())\r\n",
        "\r\n",
        "      elif (focus == fidx[j].item() and predicted[j].item() != labels[j].item()):\r\n",
        "          alpha_ftpf.append(alphas[j][focus].item())\r\n",
        "\r\n",
        "      elif (focus != fidx[j].item() and predicted[j].item() != labels[j].item()):\r\n",
        "          alpha_ffpf.append(alphas[j][focus].item())\r\n"
      ],
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7vDw6cn1q9M",
        "outputId": "a4e44919-708e-43b2-aaa8-672844d5128e"
      },
      "source": [
        "np.mean(-np.log2(mx.cpu().detach().numpy()))"
      ],
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07573719085751252"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc43myxx2yGI"
      },
      "source": [
        "a = np.array([0.8,0.9])"
      ],
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUdhdSpB23BL",
        "outputId": "852b9d71-2a48-4f49-b5d6-2ad00843cba6"
      },
      "source": [
        "-np.log2(a)"
      ],
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.32192809, 0.15200309])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 382
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyEk81R43gPZ",
        "outputId": "f22463e9-3fbf-428f-8245-0be77ea0682a"
      },
      "source": [
        "np.mean(-np.log2(a))"
      ],
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23696559416620613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPE_6NQd3VHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a847ade1-ac6e-423b-91a1-39816e5546c5"
      },
      "source": [
        "max_alpha = np.concatenate(max_alpha,axis=0)\r\n",
        "print(max_alpha.shape, cnt)"
      ],
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000,) 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvgu92LY3Zke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0307ad07-6df6-4b41-9e19-a41940599662"
      },
      "source": [
        "np.array(alpha_ftpt).size, np.array(alpha_ffpt).size, np.array(alpha_ftpf).size, np.array(alpha_ffpf).size"
      ],
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2998, 2, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XtgiDDpZ8qH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "fc6c4afc-4ec1-4c4d-84f4-0295ff507016"
      },
      "source": [
        "plt.figure(figsize=(6,6))\r\n",
        "_,bins,_ = plt.hist(max_alpha,bins=50,color =\"c\")\r\n",
        "plt.title(\"alpha values histogram\")\r\n",
        "plt.savefig(\"attention_model_2_hist\")"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uTx4G6PeOgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "3fd16f61-7932-49d4-ec93-50972999466a"
      },
      "source": [
        "plt.figure(figsize=(6,6))\r\n",
        "_,bins,_ = plt.hist(np.array(alpha_ftpt),bins=50,color =\"c\")\r\n",
        "plt.title(\"alpha values in ftpt\")\r\n",
        "plt.savefig(\"attention_model_2_hist\")"
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2Nn1IneTkT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "db4e20a7-096a-4156-a907-7e410f26d232"
      },
      "source": [
        "plt.figure(figsize=(6,6))\r\n",
        "_,bins,_ = plt.hist(np.array(alpha_ffpt),bins=50,color =\"c\")\r\n",
        "plt.title(\"alpha values in ffpt\")\r\n",
        "plt.savefig(\"attention_model_2_hist\")"
      ],
      "execution_count": 388,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZSZor21zD_f"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": 388,
      "outputs": []
    }
  ]
}