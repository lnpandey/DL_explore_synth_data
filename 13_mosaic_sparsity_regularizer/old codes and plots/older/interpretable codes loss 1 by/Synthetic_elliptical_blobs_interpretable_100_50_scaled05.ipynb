{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Synthetic_elliptical_blobs_interpretable_100_50_scaled05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        " import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEp-LtqiWAf"
      },
      "source": [
        "mu1 = np.array([3,3,3,3,0])\n",
        "sigma1 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu2 = np.array([4,4,4,4,0])\n",
        "sigma2 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu3 = np.array([10,5,5,10,0])\n",
        "sigma3 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu4 = np.array([-10,-10,-10,-10,0])\n",
        "sigma4 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu5 = np.array([-21,4,4,-21,0])\n",
        "sigma5 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu6 = np.array([-10,18,18,-10,0])\n",
        "sigma6 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu7 = np.array([4,20,4,20,0])\n",
        "sigma7 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu8 = np.array([4,-20,-20,4,0])\n",
        "sigma8 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu9 = np.array([20,20,20,20,0])\n",
        "sigma9 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "mu10 = np.array([20,-10,-10,20,0])\n",
        "sigma10 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "sample1 = np.random.multivariate_normal(mean=mu1,cov= sigma1,size=500)\n",
        "sample2 = np.random.multivariate_normal(mean=mu2,cov= sigma2,size=500)\n",
        "sample3 = np.random.multivariate_normal(mean=mu3,cov= sigma3,size=500)\n",
        "sample4 = np.random.multivariate_normal(mean=mu4,cov= sigma4,size=500)\n",
        "sample5 = np.random.multivariate_normal(mean=mu5,cov= sigma5,size=500)\n",
        "sample6 = np.random.multivariate_normal(mean=mu6,cov= sigma6,size=500)\n",
        "sample7 = np.random.multivariate_normal(mean=mu7,cov= sigma7,size=500)\n",
        "sample8 = np.random.multivariate_normal(mean=mu8,cov= sigma8,size=500)\n",
        "sample9 = np.random.multivariate_normal(mean=mu9,cov= sigma9,size=500)\n",
        "sample10 = np.random.multivariate_normal(mean=mu10,cov= sigma10,size=500)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NshDNGjY2T3w"
      },
      "source": [
        "# mu1 = np.array([3,3,0,0,0])\n",
        "# sigma1 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu2 = np.array([4,4,0,0,0])\n",
        "# sigma2 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu3 = np.array([10,5,0,0,0])\n",
        "# sigma3 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu4 = np.array([-10,-10,0,0,0])\n",
        "# sigma4 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu5 = np.array([-21,4,0,0,0])\n",
        "# sigma5 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu6 = np.array([-10,18,0,0,0])\n",
        "# sigma6 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu7 = np.array([4,20,0,0,0])\n",
        "# sigma7 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu8 = np.array([4,-20,0,0,0])\n",
        "# sigma8 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu9 = np.array([20,20,0,0,0])\n",
        "# sigma9 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu10 = np.array([20,-10,0,0,0])\n",
        "# sigma10 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "# sample1 = np.random.multivariate_normal(mean=mu1,cov= sigma1,size=500)\n",
        "# sample2 = np.random.multivariate_normal(mean=mu2,cov= sigma2,size=500)\n",
        "# sample3 = np.random.multivariate_normal(mean=mu3,cov= sigma3,size=500)\n",
        "# sample4 = np.random.multivariate_normal(mean=mu4,cov= sigma4,size=500)\n",
        "# sample5 = np.random.multivariate_normal(mean=mu5,cov= sigma5,size=500)\n",
        "# sample6 = np.random.multivariate_normal(mean=mu6,cov= sigma6,size=500)\n",
        "# sample7 = np.random.multivariate_normal(mean=mu7,cov= sigma7,size=500)\n",
        "# sample8 = np.random.multivariate_normal(mean=mu8,cov= sigma8,size=500)\n",
        "# sample9 = np.random.multivariate_normal(mean=mu9,cov= sigma9,size=500)\n",
        "# sample10 = np.random.multivariate_normal(mean=mu10,cov= sigma10,size=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YDnxeP-2_1V",
        "outputId": "c014b184-0549-4349-98b1-17536eb76bc5"
      },
      "source": [
        "X = np.concatenate((sample1,sample2,sample3,sample4,sample5,sample6,sample7,sample8,sample9,sample10),axis=0)\n",
        "Y = np.concatenate((np.zeros((500,1)),np.ones((500,1)),2*np.ones((500,1)),3*np.ones((500,1)),4*np.ones((500,1)),\n",
        "                    5*np.ones((500,1)),6*np.ones((500,1)),7*np.ones((500,1)),8*np.ones((500,1)),9*np.ones((500,1))),axis=0).astype(int)\n",
        "print(X.shape,Y.shape)\n",
        "# plt.scatter(sample1[:,0],sample1[:,1],label=\"class_0\")\n",
        "# plt.scatter(sample2[:,0],sample2[:,1],label=\"class_1\")\n",
        "# plt.scatter(sample3[:,0],sample3[:,1],label=\"class_2\")\n",
        "# plt.scatter(sample4[:,0],sample4[:,1],label=\"class_3\")\n",
        "# plt.scatter(sample5[:,0],sample5[:,1],label=\"class_4\")\n",
        "# plt.scatter(sample6[:,0],sample6[:,1],label=\"class_5\")\n",
        "# plt.scatter(sample7[:,0],sample7[:,1],label=\"class_6\")\n",
        "# plt.scatter(sample8[:,0],sample8[:,1],label=\"class_7\")\n",
        "# plt.scatter(sample9[:,0],sample9[:,1],label=\"class_8\")\n",
        "# plt.scatter(sample10[:,0],sample10[:,1],label=\"class_9\")\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 5) (5000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6YzqPUf3CHa"
      },
      "source": [
        "class SyntheticDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, x, y):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx] , self.y[idx] #, self.fore_idx[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mi3nL5-4D7_"
      },
      "source": [
        "trainset = SyntheticDataset(X,Y)\n",
        "\n",
        "\n",
        "# testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKzc7IgwqoU2",
        "outputId": "5565a684-9e0c-4f26-9d52-958e45cfb0d5"
      },
      "source": [
        "classes = ('zero','one','two','three','four','five','six','seven','eight','nine')\n",
        "\n",
        "foreground_classes = {'zero','one','two'}\n",
        "fg_used = '012'\n",
        "fg1, fg2, fg3 = 0,1,2\n",
        "\n",
        "\n",
        "all_classes = {'zero','one','two','three','four','five','six','seven','eight','nine'}\n",
        "background_classes = all_classes - foreground_classes\n",
        "background_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eight', 'five', 'four', 'nine', 'seven', 'six', 'three'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 327
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT6iKHutquR8"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWKzXkPSq5KU"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=100\n",
        "\n",
        "for i in range(50):\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChdziOP3rF1G"
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]])\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx])\n",
        "      label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASrmPqErIDM"
      },
      "source": [
        "desired_num = 3000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "list_set_labels = [] \n",
        "for i in range(desired_num):\n",
        "  set_idx = set()\n",
        "  np.random.seed(i)\n",
        "  bg_idx = np.random.randint(0,3500,8)\n",
        "  set_idx = set(background_label[bg_idx].tolist())\n",
        "  fg_idx = np.random.randint(0,1500)\n",
        "  set_idx.add(foreground_label[fg_idx].item())\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "  list_set_labels.append(set_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDFN7dCarmmR"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([5], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return torch.stack(avg_image_dataset) , torch.stack(labels) , foreground_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgF90qBIt8yN"
      },
      "source": [
        "def calculate_loss(dataloader,model,criter):\n",
        "  model.eval()\n",
        "  r_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      outputs = model(inputs)\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  return r_loss/i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzPfrih82Bg"
      },
      "source": [
        "**Focus Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,100)  #,self.output)\n",
        "        self.linear2 = nn.Linear(100,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,self.d], dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            x[:,i] = self.helper(z[:,i] )[:,0]  # self.d*i:self.d*i+self.d\n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        x1 = x[:,0]\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],z[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      x = self.linear2(x)\n",
        "      return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjrL0Zb484KO"
      },
      "source": [
        "**Classification Net**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,50)\n",
        "        self.linear2 = nn.Linear(50,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByKHrKis88lW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAPjSKkrd0ru"
      },
      "source": [
        "where = Focus_deep(5,1,9,5).double()\n",
        "what = Classification_deep(5,3).double()\n",
        "where = where.to(\"cuda\")\n",
        "what = what.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "\n",
        "      mx,_ = torch.max(alpha,1)\n",
        "      entropy = np.mean(-np.log2(mx.cpu().detach().numpy()))\n",
        "      # print(\"entropy of batch\", entropy)\n",
        "\n",
        "      loss = criter(outputs, labels) + 0.5*entropy\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MOfxUJZ_eFKw",
        "outputId": "29e11699-f87a-4f20-b20a-df079575f910"
      },
      "source": [
        "print(\"--\"*40)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_where = optim.Adam(where.parameters(),lr =0.001)\n",
        "optimizer_what = optim.Adam(what.parameters(), lr=0.001)\n",
        "acti = []\n",
        "loss_curi = []\n",
        "analysis_data = []\n",
        "epochs = 1000\n",
        "running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "loss_curi.append(running_loss)\n",
        "analysis_data.append(anlys_data)\n",
        "print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "for epoch in range(epochs): # loop over the dataset multiple times\n",
        "  ep_lossi = []\n",
        "  running_loss = 0.0\n",
        "  what.train()\n",
        "  where.train()\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # get the inputs\n",
        "    inputs, labels,_ = data\n",
        "    inputs = inputs.double()\n",
        "    inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "    # zero the parameter gradients\n",
        "    optimizer_where.zero_grad()\n",
        "    optimizer_what.zero_grad()\n",
        "    # forward + backward + optimize\n",
        "    avg, alpha = where(inputs)\n",
        "    outputs = what(avg)\n",
        "\n",
        "    mx,_ = torch.max(alpha,1)\n",
        "    entropy = np.mean(-np.log2(mx.cpu().detach().numpy()))\n",
        "    # print(\"entropy of batch\", entropy)\n",
        "\n",
        "    loss = criterion(outputs, labels) + 0.5*entropy\n",
        "\n",
        "    # loss = criterion(outputs, labels)\n",
        "    # print statistics\n",
        "    running_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer_where.step()\n",
        "    optimizer_what.step()\n",
        "\n",
        "  running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  analysis_data.append(anls_data)\n",
        "  print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "  loss_curi.append(running_loss)   #loss per epoch\n",
        "  if running_loss<=0.01:\n",
        "    break\n",
        "print('Finished Training')\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    images, labels,_ = data\n",
        "    images = images.double()\n",
        "    images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    avg, alpha = where(images)\n",
        "    outputs  = what(avg)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 2.362\n",
            "epoch: [1] loss: 2.046\n",
            "epoch: [2] loss: 1.778\n",
            "epoch: [3] loss: 1.573\n",
            "epoch: [4] loss: 1.407\n",
            "epoch: [5] loss: 1.107\n",
            "epoch: [6] loss: 0.976\n",
            "epoch: [7] loss: 0.867\n",
            "epoch: [8] loss: 0.784\n",
            "epoch: [9] loss: 0.716\n",
            "epoch: [10] loss: 0.656\n",
            "epoch: [11] loss: 0.599\n",
            "epoch: [12] loss: 0.547\n",
            "epoch: [13] loss: 0.498\n",
            "epoch: [14] loss: 0.459\n",
            "epoch: [15] loss: 0.418\n",
            "epoch: [16] loss: 0.387\n",
            "epoch: [17] loss: 0.357\n",
            "epoch: [18] loss: 0.341\n",
            "epoch: [19] loss: 0.310\n",
            "epoch: [20] loss: 0.301\n",
            "epoch: [21] loss: 0.280\n",
            "epoch: [22] loss: 0.268\n",
            "epoch: [23] loss: 0.253\n",
            "epoch: [24] loss: 0.243\n",
            "epoch: [25] loss: 0.232\n",
            "epoch: [26] loss: 0.222\n",
            "epoch: [27] loss: 0.213\n",
            "epoch: [28] loss: 0.206\n",
            "epoch: [29] loss: 0.197\n",
            "epoch: [30] loss: 0.191\n",
            "epoch: [31] loss: 0.185\n",
            "epoch: [32] loss: 0.178\n",
            "epoch: [33] loss: 0.172\n",
            "epoch: [34] loss: 0.168\n",
            "epoch: [35] loss: 0.163\n",
            "epoch: [36] loss: 0.159\n",
            "epoch: [37] loss: 0.153\n",
            "epoch: [38] loss: 0.150\n",
            "epoch: [39] loss: 0.146\n",
            "epoch: [40] loss: 0.142\n",
            "epoch: [41] loss: 0.138\n",
            "epoch: [42] loss: 0.134\n",
            "epoch: [43] loss: 0.132\n",
            "epoch: [44] loss: 0.128\n",
            "epoch: [45] loss: 0.124\n",
            "epoch: [46] loss: 0.121\n",
            "epoch: [47] loss: 0.119\n",
            "epoch: [48] loss: 0.115\n",
            "epoch: [49] loss: 0.114\n",
            "epoch: [50] loss: 0.111\n",
            "epoch: [51] loss: 0.108\n",
            "epoch: [52] loss: 0.105\n",
            "epoch: [53] loss: 0.104\n",
            "epoch: [54] loss: 0.102\n",
            "epoch: [55] loss: 0.099\n",
            "epoch: [56] loss: 0.096\n",
            "epoch: [57] loss: 0.095\n",
            "epoch: [58] loss: 0.093\n",
            "epoch: [59] loss: 0.092\n",
            "epoch: [60] loss: 0.090\n",
            "epoch: [61] loss: 0.089\n",
            "epoch: [62] loss: 0.087\n",
            "epoch: [63] loss: 0.085\n",
            "epoch: [64] loss: 0.084\n",
            "epoch: [65] loss: 0.082\n",
            "epoch: [66] loss: 0.081\n",
            "epoch: [67] loss: 0.081\n",
            "epoch: [68] loss: 0.079\n",
            "epoch: [69] loss: 0.078\n",
            "epoch: [70] loss: 0.076\n",
            "epoch: [71] loss: 0.075\n",
            "epoch: [72] loss: 0.074\n",
            "epoch: [73] loss: 0.073\n",
            "epoch: [74] loss: 0.071\n",
            "epoch: [75] loss: 0.071\n",
            "epoch: [76] loss: 0.070\n",
            "epoch: [77] loss: 0.069\n",
            "epoch: [78] loss: 0.068\n",
            "epoch: [79] loss: 0.067\n",
            "epoch: [80] loss: 0.067\n",
            "epoch: [81] loss: 0.066\n",
            "epoch: [82] loss: 0.065\n",
            "epoch: [83] loss: 0.064\n",
            "epoch: [84] loss: 0.063\n",
            "epoch: [85] loss: 0.062\n",
            "epoch: [86] loss: 0.061\n",
            "epoch: [87] loss: 0.060\n",
            "epoch: [88] loss: 0.060\n",
            "epoch: [89] loss: 0.060\n",
            "epoch: [90] loss: 0.060\n",
            "epoch: [91] loss: 0.059\n",
            "epoch: [92] loss: 0.058\n",
            "epoch: [93] loss: 0.057\n",
            "epoch: [94] loss: 0.057\n",
            "epoch: [95] loss: 0.056\n",
            "epoch: [96] loss: 0.055\n",
            "epoch: [97] loss: 0.055\n",
            "epoch: [98] loss: 0.054\n",
            "epoch: [99] loss: 0.054\n",
            "epoch: [100] loss: 0.053\n",
            "epoch: [101] loss: 0.052\n",
            "epoch: [102] loss: 0.052\n",
            "epoch: [103] loss: 0.051\n",
            "epoch: [104] loss: 0.051\n",
            "epoch: [105] loss: 0.050\n",
            "epoch: [106] loss: 0.049\n",
            "epoch: [107] loss: 0.049\n",
            "epoch: [108] loss: 0.049\n",
            "epoch: [109] loss: 0.048\n",
            "epoch: [110] loss: 0.048\n",
            "epoch: [111] loss: 0.047\n",
            "epoch: [112] loss: 0.047\n",
            "epoch: [113] loss: 0.047\n",
            "epoch: [114] loss: 0.046\n",
            "epoch: [115] loss: 0.045\n",
            "epoch: [116] loss: 0.045\n",
            "epoch: [117] loss: 0.045\n",
            "epoch: [118] loss: 0.045\n",
            "epoch: [119] loss: 0.045\n",
            "epoch: [120] loss: 0.044\n",
            "epoch: [121] loss: 0.044\n",
            "epoch: [122] loss: 0.044\n",
            "epoch: [123] loss: 0.043\n",
            "epoch: [124] loss: 0.043\n",
            "epoch: [125] loss: 0.043\n",
            "epoch: [126] loss: 0.042\n",
            "epoch: [127] loss: 0.042\n",
            "epoch: [128] loss: 0.041\n",
            "epoch: [129] loss: 0.041\n",
            "epoch: [130] loss: 0.041\n",
            "epoch: [131] loss: 0.040\n",
            "epoch: [132] loss: 0.040\n",
            "epoch: [133] loss: 0.040\n",
            "epoch: [134] loss: 0.040\n",
            "epoch: [135] loss: 0.040\n",
            "epoch: [136] loss: 0.040\n",
            "epoch: [137] loss: 0.040\n",
            "epoch: [138] loss: 0.040\n",
            "epoch: [139] loss: 0.039\n",
            "epoch: [140] loss: 0.039\n",
            "epoch: [141] loss: 0.039\n",
            "epoch: [142] loss: 0.039\n",
            "epoch: [143] loss: 0.038\n",
            "epoch: [144] loss: 0.038\n",
            "epoch: [145] loss: 0.038\n",
            "epoch: [146] loss: 0.037\n",
            "epoch: [147] loss: 0.037\n",
            "epoch: [148] loss: 0.037\n",
            "epoch: [149] loss: 0.037\n",
            "epoch: [150] loss: 0.037\n",
            "epoch: [151] loss: 0.036\n",
            "epoch: [152] loss: 0.036\n",
            "epoch: [153] loss: 0.036\n",
            "epoch: [154] loss: 0.036\n",
            "epoch: [155] loss: 0.036\n",
            "epoch: [156] loss: 0.035\n",
            "epoch: [157] loss: 0.035\n",
            "epoch: [158] loss: 0.036\n",
            "epoch: [159] loss: 0.035\n",
            "epoch: [160] loss: 0.036\n",
            "epoch: [161] loss: 0.036\n",
            "epoch: [162] loss: 0.035\n",
            "epoch: [163] loss: 0.034\n",
            "epoch: [164] loss: 0.034\n",
            "epoch: [165] loss: 0.034\n",
            "epoch: [166] loss: 0.034\n",
            "epoch: [167] loss: 0.033\n",
            "epoch: [168] loss: 0.034\n",
            "epoch: [169] loss: 0.033\n",
            "epoch: [170] loss: 0.033\n",
            "epoch: [171] loss: 0.033\n",
            "epoch: [172] loss: 0.033\n",
            "epoch: [173] loss: 0.033\n",
            "epoch: [174] loss: 0.032\n",
            "epoch: [175] loss: 0.032\n",
            "epoch: [176] loss: 0.032\n",
            "epoch: [177] loss: 0.032\n",
            "epoch: [178] loss: 0.032\n",
            "epoch: [179] loss: 0.032\n",
            "epoch: [180] loss: 0.032\n",
            "epoch: [181] loss: 0.031\n",
            "epoch: [182] loss: 0.031\n",
            "epoch: [183] loss: 0.031\n",
            "epoch: [184] loss: 0.031\n",
            "epoch: [185] loss: 0.031\n",
            "epoch: [186] loss: 0.031\n",
            "epoch: [187] loss: 0.031\n",
            "epoch: [188] loss: 0.030\n",
            "epoch: [189] loss: 0.030\n",
            "epoch: [190] loss: 0.030\n",
            "epoch: [191] loss: 0.031\n",
            "epoch: [192] loss: 0.030\n",
            "epoch: [193] loss: 0.030\n",
            "epoch: [194] loss: 0.030\n",
            "epoch: [195] loss: 0.030\n",
            "epoch: [196] loss: 0.030\n",
            "epoch: [197] loss: 0.030\n",
            "epoch: [198] loss: 0.030\n",
            "epoch: [199] loss: 0.030\n",
            "epoch: [200] loss: 0.030\n",
            "epoch: [201] loss: 0.030\n",
            "epoch: [202] loss: 0.030\n",
            "epoch: [203] loss: 0.030\n",
            "epoch: [204] loss: 0.030\n",
            "epoch: [205] loss: 0.029\n",
            "epoch: [206] loss: 0.029\n",
            "epoch: [207] loss: 0.029\n",
            "epoch: [208] loss: 0.029\n",
            "epoch: [209] loss: 0.029\n",
            "epoch: [210] loss: 0.029\n",
            "epoch: [211] loss: 0.029\n",
            "epoch: [212] loss: 0.029\n",
            "epoch: [213] loss: 0.029\n",
            "epoch: [214] loss: 0.029\n",
            "epoch: [215] loss: 0.028\n",
            "epoch: [216] loss: 0.029\n",
            "epoch: [217] loss: 0.028\n",
            "epoch: [218] loss: 0.028\n",
            "epoch: [219] loss: 0.028\n",
            "epoch: [220] loss: 0.028\n",
            "epoch: [221] loss: 0.028\n",
            "epoch: [222] loss: 0.028\n",
            "epoch: [223] loss: 0.028\n",
            "epoch: [224] loss: 0.028\n",
            "epoch: [225] loss: 0.028\n",
            "epoch: [226] loss: 0.028\n",
            "epoch: [227] loss: 0.028\n",
            "epoch: [228] loss: 0.028\n",
            "epoch: [229] loss: 0.027\n",
            "epoch: [230] loss: 0.027\n",
            "epoch: [231] loss: 0.027\n",
            "epoch: [232] loss: 0.028\n",
            "epoch: [233] loss: 0.027\n",
            "epoch: [234] loss: 0.027\n",
            "epoch: [235] loss: 0.027\n",
            "epoch: [236] loss: 0.027\n",
            "epoch: [237] loss: 0.027\n",
            "epoch: [238] loss: 0.027\n",
            "epoch: [239] loss: 0.027\n",
            "epoch: [240] loss: 0.026\n",
            "epoch: [241] loss: 0.026\n",
            "epoch: [242] loss: 0.026\n",
            "epoch: [243] loss: 0.026\n",
            "epoch: [244] loss: 0.026\n",
            "epoch: [245] loss: 0.026\n",
            "epoch: [246] loss: 0.026\n",
            "epoch: [247] loss: 0.026\n",
            "epoch: [248] loss: 0.026\n",
            "epoch: [249] loss: 0.026\n",
            "epoch: [250] loss: 0.026\n",
            "epoch: [251] loss: 0.026\n",
            "epoch: [252] loss: 0.026\n",
            "epoch: [253] loss: 0.026\n",
            "epoch: [254] loss: 0.026\n",
            "epoch: [255] loss: 0.026\n",
            "epoch: [256] loss: 0.026\n",
            "epoch: [257] loss: 0.026\n",
            "epoch: [258] loss: 0.026\n",
            "epoch: [259] loss: 0.025\n",
            "epoch: [260] loss: 0.025\n",
            "epoch: [261] loss: 0.025\n",
            "epoch: [262] loss: 0.025\n",
            "epoch: [263] loss: 0.025\n",
            "epoch: [264] loss: 0.025\n",
            "epoch: [265] loss: 0.025\n",
            "epoch: [266] loss: 0.025\n",
            "epoch: [267] loss: 0.025\n",
            "epoch: [268] loss: 0.025\n",
            "epoch: [269] loss: 0.025\n",
            "epoch: [270] loss: 0.025\n",
            "epoch: [271] loss: 0.025\n",
            "epoch: [272] loss: 0.025\n",
            "epoch: [273] loss: 0.025\n",
            "epoch: [274] loss: 0.025\n",
            "epoch: [275] loss: 0.025\n",
            "epoch: [276] loss: 0.025\n",
            "epoch: [277] loss: 0.025\n",
            "epoch: [278] loss: 0.025\n",
            "epoch: [279] loss: 0.025\n",
            "epoch: [280] loss: 0.025\n",
            "epoch: [281] loss: 0.025\n",
            "epoch: [282] loss: 0.025\n",
            "epoch: [283] loss: 0.025\n",
            "epoch: [284] loss: 0.025\n",
            "epoch: [285] loss: 0.024\n",
            "epoch: [286] loss: 0.025\n",
            "epoch: [287] loss: 0.025\n",
            "epoch: [288] loss: 0.025\n",
            "epoch: [289] loss: 0.025\n",
            "epoch: [290] loss: 0.025\n",
            "epoch: [291] loss: 0.025\n",
            "epoch: [292] loss: 0.025\n",
            "epoch: [293] loss: 0.025\n",
            "epoch: [294] loss: 0.025\n",
            "epoch: [295] loss: 0.025\n",
            "epoch: [296] loss: 0.025\n",
            "epoch: [297] loss: 0.025\n",
            "epoch: [298] loss: 0.025\n",
            "epoch: [299] loss: 0.025\n",
            "epoch: [300] loss: 0.025\n",
            "epoch: [301] loss: 0.025\n",
            "epoch: [302] loss: 0.025\n",
            "epoch: [303] loss: 0.025\n",
            "epoch: [304] loss: 0.025\n",
            "epoch: [305] loss: 0.025\n",
            "epoch: [306] loss: 0.026\n",
            "epoch: [307] loss: 0.025\n",
            "epoch: [308] loss: 0.025\n",
            "epoch: [309] loss: 0.025\n",
            "epoch: [310] loss: 0.026\n",
            "epoch: [311] loss: 0.026\n",
            "epoch: [312] loss: 0.026\n",
            "epoch: [313] loss: 0.026\n",
            "epoch: [314] loss: 0.026\n",
            "epoch: [315] loss: 0.026\n",
            "epoch: [316] loss: 0.026\n",
            "epoch: [317] loss: 0.026\n",
            "epoch: [318] loss: 0.026\n",
            "epoch: [319] loss: 0.026\n",
            "epoch: [320] loss: 0.026\n",
            "epoch: [321] loss: 0.027\n",
            "epoch: [322] loss: 0.027\n",
            "epoch: [323] loss: 0.027\n",
            "epoch: [324] loss: 0.027\n",
            "epoch: [325] loss: 0.027\n",
            "epoch: [326] loss: 0.027\n",
            "epoch: [327] loss: 0.027\n",
            "epoch: [328] loss: 0.027\n",
            "epoch: [329] loss: 0.027\n",
            "epoch: [330] loss: 0.027\n",
            "epoch: [331] loss: 0.027\n",
            "epoch: [332] loss: 0.027\n",
            "epoch: [333] loss: 0.027\n",
            "epoch: [334] loss: 0.028\n",
            "epoch: [335] loss: 0.028\n",
            "epoch: [336] loss: 0.028\n",
            "epoch: [337] loss: 0.028\n",
            "epoch: [338] loss: 0.028\n",
            "epoch: [339] loss: 0.028\n",
            "epoch: [340] loss: 0.028\n",
            "epoch: [341] loss: 0.028\n",
            "epoch: [342] loss: 0.028\n",
            "epoch: [343] loss: 0.028\n",
            "epoch: [344] loss: 0.028\n",
            "epoch: [345] loss: 0.028\n",
            "epoch: [346] loss: 0.028\n",
            "epoch: [347] loss: 0.028\n",
            "epoch: [348] loss: 0.028\n",
            "epoch: [349] loss: 0.028\n",
            "epoch: [350] loss: 0.028\n",
            "epoch: [351] loss: 0.028\n",
            "epoch: [352] loss: 0.029\n",
            "epoch: [353] loss: 0.028\n",
            "epoch: [354] loss: 0.029\n",
            "epoch: [355] loss: 0.029\n",
            "epoch: [356] loss: 0.029\n",
            "epoch: [357] loss: 0.029\n",
            "epoch: [358] loss: 0.029\n",
            "epoch: [359] loss: 0.029\n",
            "epoch: [360] loss: 0.029\n",
            "epoch: [361] loss: 0.029\n",
            "epoch: [362] loss: 0.029\n",
            "epoch: [363] loss: 0.029\n",
            "epoch: [364] loss: 0.029\n",
            "epoch: [365] loss: 0.029\n",
            "epoch: [366] loss: 0.029\n",
            "epoch: [367] loss: 0.029\n",
            "epoch: [368] loss: 0.029\n",
            "epoch: [369] loss: 0.029\n",
            "epoch: [370] loss: 0.029\n",
            "epoch: [371] loss: 0.029\n",
            "epoch: [372] loss: 0.030\n",
            "epoch: [373] loss: 0.029\n",
            "epoch: [374] loss: 0.029\n",
            "epoch: [375] loss: 0.030\n",
            "epoch: [376] loss: 0.029\n",
            "epoch: [377] loss: 0.030\n",
            "epoch: [378] loss: 0.030\n",
            "epoch: [379] loss: 0.030\n",
            "epoch: [380] loss: 0.030\n",
            "epoch: [381] loss: 0.030\n",
            "epoch: [382] loss: 0.030\n",
            "epoch: [383] loss: 0.030\n",
            "epoch: [384] loss: 0.030\n",
            "epoch: [385] loss: 0.030\n",
            "epoch: [386] loss: 0.030\n",
            "epoch: [387] loss: 0.029\n",
            "epoch: [388] loss: 0.030\n",
            "epoch: [389] loss: 0.030\n",
            "epoch: [390] loss: 0.030\n",
            "epoch: [391] loss: 0.030\n",
            "epoch: [392] loss: 0.030\n",
            "epoch: [393] loss: 0.030\n",
            "epoch: [394] loss: 0.030\n",
            "epoch: [395] loss: 0.030\n",
            "epoch: [396] loss: 0.030\n",
            "epoch: [397] loss: 0.030\n",
            "epoch: [398] loss: 0.030\n",
            "epoch: [399] loss: 0.030\n",
            "epoch: [400] loss: 0.030\n",
            "epoch: [401] loss: 0.031\n",
            "epoch: [402] loss: 0.030\n",
            "epoch: [403] loss: 0.031\n",
            "epoch: [404] loss: 0.030\n",
            "epoch: [405] loss: 0.031\n",
            "epoch: [406] loss: 0.031\n",
            "epoch: [407] loss: 0.031\n",
            "epoch: [408] loss: 0.031\n",
            "epoch: [409] loss: 0.031\n",
            "epoch: [410] loss: 0.031\n",
            "epoch: [411] loss: 0.031\n",
            "epoch: [412] loss: 0.031\n",
            "epoch: [413] loss: 0.031\n",
            "epoch: [414] loss: 0.031\n",
            "epoch: [415] loss: 0.031\n",
            "epoch: [416] loss: 0.031\n",
            "epoch: [417] loss: 0.031\n",
            "epoch: [418] loss: 0.031\n",
            "epoch: [419] loss: 0.032\n",
            "epoch: [420] loss: 0.031\n",
            "epoch: [421] loss: 0.031\n",
            "epoch: [422] loss: 0.031\n",
            "epoch: [423] loss: 0.032\n",
            "epoch: [424] loss: 0.031\n",
            "epoch: [425] loss: 0.032\n",
            "epoch: [426] loss: 0.032\n",
            "epoch: [427] loss: 0.032\n",
            "epoch: [428] loss: 0.031\n",
            "epoch: [429] loss: 0.032\n",
            "epoch: [430] loss: 0.032\n",
            "epoch: [431] loss: 0.032\n",
            "epoch: [432] loss: 0.032\n",
            "epoch: [433] loss: 0.032\n",
            "epoch: [434] loss: 0.032\n",
            "epoch: [435] loss: 0.032\n",
            "epoch: [436] loss: 0.032\n",
            "epoch: [437] loss: 0.032\n",
            "epoch: [438] loss: 0.032\n",
            "epoch: [439] loss: 0.032\n",
            "epoch: [440] loss: 0.033\n",
            "epoch: [441] loss: 0.032\n",
            "epoch: [442] loss: 0.033\n",
            "epoch: [443] loss: 0.033\n",
            "epoch: [444] loss: 0.033\n",
            "epoch: [445] loss: 0.033\n",
            "epoch: [446] loss: 0.033\n",
            "epoch: [447] loss: 0.033\n",
            "epoch: [448] loss: 0.033\n",
            "epoch: [449] loss: 0.033\n",
            "epoch: [450] loss: 0.033\n",
            "epoch: [451] loss: 0.033\n",
            "epoch: [452] loss: 0.033\n",
            "epoch: [453] loss: 0.033\n",
            "epoch: [454] loss: 0.033\n",
            "epoch: [455] loss: 0.033\n",
            "epoch: [456] loss: 0.034\n",
            "epoch: [457] loss: 0.033\n",
            "epoch: [458] loss: 0.033\n",
            "epoch: [459] loss: 0.034\n",
            "epoch: [460] loss: 0.034\n",
            "epoch: [461] loss: 0.033\n",
            "epoch: [462] loss: 0.034\n",
            "epoch: [463] loss: 0.034\n",
            "epoch: [464] loss: 0.034\n",
            "epoch: [465] loss: 0.034\n",
            "epoch: [466] loss: 0.034\n",
            "epoch: [467] loss: 0.034\n",
            "epoch: [468] loss: 0.034\n",
            "epoch: [469] loss: 0.034\n",
            "epoch: [470] loss: 0.034\n",
            "epoch: [471] loss: 0.034\n",
            "epoch: [472] loss: 0.034\n",
            "epoch: [473] loss: 0.034\n",
            "epoch: [474] loss: 0.034\n",
            "epoch: [475] loss: 0.035\n",
            "epoch: [476] loss: 0.034\n",
            "epoch: [477] loss: 0.035\n",
            "epoch: [478] loss: 0.035\n",
            "epoch: [479] loss: 0.035\n",
            "epoch: [480] loss: 0.035\n",
            "epoch: [481] loss: 0.035\n",
            "epoch: [482] loss: 0.035\n",
            "epoch: [483] loss: 0.035\n",
            "epoch: [484] loss: 0.035\n",
            "epoch: [485] loss: 0.035\n",
            "epoch: [486] loss: 0.035\n",
            "epoch: [487] loss: 0.035\n",
            "epoch: [488] loss: 0.036\n",
            "epoch: [489] loss: 0.036\n",
            "epoch: [490] loss: 0.035\n",
            "epoch: [491] loss: 0.036\n",
            "epoch: [492] loss: 0.036\n",
            "epoch: [493] loss: 0.036\n",
            "epoch: [494] loss: 0.036\n",
            "epoch: [495] loss: 0.036\n",
            "epoch: [496] loss: 0.036\n",
            "epoch: [497] loss: 0.036\n",
            "epoch: [498] loss: 0.036\n",
            "epoch: [499] loss: 0.037\n",
            "epoch: [500] loss: 0.036\n",
            "epoch: [501] loss: 0.037\n",
            "epoch: [502] loss: 0.037\n",
            "epoch: [503] loss: 0.037\n",
            "epoch: [504] loss: 0.036\n",
            "epoch: [505] loss: 0.037\n",
            "epoch: [506] loss: 0.037\n",
            "epoch: [507] loss: 0.037\n",
            "epoch: [508] loss: 0.037\n",
            "epoch: [509] loss: 0.037\n",
            "epoch: [510] loss: 0.037\n",
            "epoch: [511] loss: 0.037\n",
            "epoch: [512] loss: 0.037\n",
            "epoch: [513] loss: 0.038\n",
            "epoch: [514] loss: 0.038\n",
            "epoch: [515] loss: 0.038\n",
            "epoch: [516] loss: 0.038\n",
            "epoch: [517] loss: 0.038\n",
            "epoch: [518] loss: 0.038\n",
            "epoch: [519] loss: 0.038\n",
            "epoch: [520] loss: 0.039\n",
            "epoch: [521] loss: 0.039\n",
            "epoch: [522] loss: 0.039\n",
            "epoch: [523] loss: 0.039\n",
            "epoch: [524] loss: 0.039\n",
            "epoch: [525] loss: 0.039\n",
            "epoch: [526] loss: 0.039\n",
            "epoch: [527] loss: 0.039\n",
            "epoch: [528] loss: 0.039\n",
            "epoch: [529] loss: 0.040\n",
            "epoch: [530] loss: 0.039\n",
            "epoch: [531] loss: 0.040\n",
            "epoch: [532] loss: 0.040\n",
            "epoch: [533] loss: 0.040\n",
            "epoch: [534] loss: 0.040\n",
            "epoch: [535] loss: 0.040\n",
            "epoch: [536] loss: 0.040\n",
            "epoch: [537] loss: 0.040\n",
            "epoch: [538] loss: 0.040\n",
            "epoch: [539] loss: 0.040\n",
            "epoch: [540] loss: 0.040\n",
            "epoch: [541] loss: 0.041\n",
            "epoch: [542] loss: 0.041\n",
            "epoch: [543] loss: 0.041\n",
            "epoch: [544] loss: 0.041\n",
            "epoch: [545] loss: 0.041\n",
            "epoch: [546] loss: 0.041\n",
            "epoch: [547] loss: 0.041\n",
            "epoch: [548] loss: 0.041\n",
            "epoch: [549] loss: 0.041\n",
            "epoch: [550] loss: 0.042\n",
            "epoch: [551] loss: 0.041\n",
            "epoch: [552] loss: 0.041\n",
            "epoch: [553] loss: 0.042\n",
            "epoch: [554] loss: 0.042\n",
            "epoch: [555] loss: 0.042\n",
            "epoch: [556] loss: 0.042\n",
            "epoch: [557] loss: 0.042\n",
            "epoch: [558] loss: 0.042\n",
            "epoch: [559] loss: 0.042\n",
            "epoch: [560] loss: 0.041\n",
            "epoch: [561] loss: 0.042\n",
            "epoch: [562] loss: 0.042\n",
            "epoch: [563] loss: 0.043\n",
            "epoch: [564] loss: 0.042\n",
            "epoch: [565] loss: 0.042\n",
            "epoch: [566] loss: 0.043\n",
            "epoch: [567] loss: 0.043\n",
            "epoch: [568] loss: 0.043\n",
            "epoch: [569] loss: 0.043\n",
            "epoch: [570] loss: 0.043\n",
            "epoch: [571] loss: 0.043\n",
            "epoch: [572] loss: 0.043\n",
            "epoch: [573] loss: 0.043\n",
            "epoch: [574] loss: 0.043\n",
            "epoch: [575] loss: 0.043\n",
            "epoch: [576] loss: 0.043\n",
            "epoch: [577] loss: 0.044\n",
            "epoch: [578] loss: 0.044\n",
            "epoch: [579] loss: 0.044\n",
            "epoch: [580] loss: 0.044\n",
            "epoch: [581] loss: 0.044\n",
            "epoch: [582] loss: 0.044\n",
            "epoch: [583] loss: 0.044\n",
            "epoch: [584] loss: 0.044\n",
            "epoch: [585] loss: 0.044\n",
            "epoch: [586] loss: 0.044\n",
            "epoch: [587] loss: 0.044\n",
            "epoch: [588] loss: 0.044\n",
            "epoch: [589] loss: 0.044\n",
            "epoch: [590] loss: 0.044\n",
            "epoch: [591] loss: 0.044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-341-700df77a6096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moptimizer_what\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manls_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_attn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0manalysis_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manls_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: [%d] loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-339-cf207a1acd7a>\u001b[0m in \u001b[0;36mcalculate_attn_loss\u001b[0;34m(dataloader, what, where, criter)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-336-228145e2baf6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# self.d*i:self.d*i+self.d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "21c4c9a4-48e2-47d6-967a-f68685d1f52f"
      },
      "source": [
        "analysis_data = np.array(analysis_data)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.savefig(\"trends_synthetic_300_300.png\",bbox_inches=\"tight\")\n",
        "plt.savefig(\"trends_synthetic_300_300.pdf\",bbox_inches=\"tight\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-342-7a0997c64b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manalysis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ftpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ffpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manalysis_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ftpf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (593,) and (592,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5mag3jZ-LMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2ccaec-90ab-46f9-b021-370e2c43be28"
      },
      "source": [
        "analysis_data[-1,:2]/3000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSxFtBWQ1M8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba917fce-9352-46fd-a577-739a6f8b9139"
      },
      "source": [
        "running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\r\n",
        "print(running_loss, anls_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0446853677593435 [3000, 0, 0, 0, 3000, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncIi9Jc92a4u"
      },
      "source": [
        "what.eval()\r\n",
        "where.eval()\r\n",
        "alphas = []\r\n",
        "max_alpha =[]\r\n",
        "alpha_ftpt=[]\r\n",
        "alpha_ffpt=[]\r\n",
        "alpha_ftpf=[]\r\n",
        "alpha_ffpf=[]\r\n",
        "argmax_more_than_half=0\r\n",
        "argmax_less_than_half=0\r\n",
        "cnt =0\r\n",
        "with torch.no_grad():\r\n",
        "  for i, data in enumerate(train_loader, 0):\r\n",
        "    inputs, labels, fidx = data\r\n",
        "    inputs = inputs.double()\r\n",
        "    inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\r\n",
        "    avg, alphas = where(inputs)\r\n",
        "    outputs = what(avg)\r\n",
        "    _, predicted = torch.max(outputs.data, 1)\r\n",
        "    batch = len(predicted)\r\n",
        "    mx,_ = torch.max(alphas,1)\r\n",
        "    max_alpha.append(mx.cpu().detach().numpy())\r\n",
        "    for j in range (batch):\r\n",
        "      cnt+=1\r\n",
        "      focus = torch.argmax(alphas[j]).item()\r\n",
        "      if alphas[j][focus] >= 0.5 :\r\n",
        "        argmax_more_than_half += 1\r\n",
        "      else:\r\n",
        "        argmax_less_than_half += 1\r\n",
        "\r\n",
        "      if (focus == fidx[j].item() and predicted[j].item() == labels[j].item()):\r\n",
        "          alpha_ftpt.append(alphas[j][focus].item())\r\n",
        "          # print(focus, fore_idx[j].item(), predicted[j].item() , labels[j].item() )\r\n",
        "\r\n",
        "      elif (focus != fidx[j].item() and predicted[j].item() == labels[j].item()):\r\n",
        "          alpha_ffpt.append(alphas[j][focus].item())\r\n",
        "\r\n",
        "      elif (focus == fidx[j].item() and predicted[j].item() != labels[j].item()):\r\n",
        "          alpha_ftpf.append(alphas[j][focus].item())\r\n",
        "\r\n",
        "      elif (focus != fidx[j].item() and predicted[j].item() != labels[j].item()):\r\n",
        "          alpha_ffpf.append(alphas[j][focus].item())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7vDw6cn1q9M",
        "outputId": "ff7faf43-67bc-4a63-d114-e08bd0f47e6a"
      },
      "source": [
        "np.mean(-np.log2(mx.cpu().detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08580094622975085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 346
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc43myxx2yGI"
      },
      "source": [
        "a = np.array([0.8,0.9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUdhdSpB23BL",
        "outputId": "fbe36f7e-8391-4334-a391-c3ea9f885431"
      },
      "source": [
        "-np.log2(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.32192809, 0.15200309])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyEk81R43gPZ",
        "outputId": "be45245b-3fff-428c-f986-8e6bfecfba3d"
      },
      "source": [
        "np.mean(-np.log2(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23696559416620613"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPE_6NQd3VHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde7016f-01ca-452d-e9d1-390d67d6822d"
      },
      "source": [
        "max_alpha = np.concatenate(max_alpha,axis=0)\r\n",
        "print(max_alpha.shape, cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000,) 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvgu92LY3Zke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9db248-d30b-405d-e816-b449535f820c"
      },
      "source": [
        "np.array(alpha_ftpt).size, np.array(alpha_ffpt).size, np.array(alpha_ftpf).size, np.array(alpha_ffpf).size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 351
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XtgiDDpZ8qH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "30a82808-1439-4447-ecbf-2ed4e15cfac1"
      },
      "source": [
        "plt.figure(figsize=(6,6))\r\n",
        "_,bins,_ = plt.hist(max_alpha,bins=50,color =\"c\")\r\n",
        "plt.title(\"alpha values histogram\")\r\n",
        "plt.savefig(\"attention_model_2_hist\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uTx4G6PeOgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "031254ec-5d4e-4cb5-a124-4479d82624d0"
      },
      "source": [
        "plt.figure(figsize=(6,6))\r\n",
        "_,bins,_ = plt.hist(np.array(alpha_ftpt),bins=50,color =\"c\")\r\n",
        "plt.title(\"alpha values in ftpt\")\r\n",
        "plt.savefig(\"attention_model_2_hist\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ2Nn1IneTkT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "f32e4862-26d2-4999-ac00-fe26bdbd6f76"
      },
      "source": [
        "plt.figure(figsize=(6,6))\r\n",
        "_,bins,_ = plt.hist(np.array(alpha_ffpt),bins=50,color =\"c\")\r\n",
        "plt.title(\"alpha values in ffpt\")\r\n",
        "plt.savefig(\"attention_model_2_hist\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAF1CAYAAADlbe0oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVaUlEQVR4nO3df7DldX3f8edLVkgoCggLIguuCrZddUadW9BJ/NGgCEx1TWMtWIfVkJixtSYxSUvGaTFoppJEcYy2EcWE4hgwpNGt1BIUkSkDyEWtFVJkQREQ5MoCEVFw9d0/zhd7vHOXe++es/dy9/18zOxwvt/v55zv53N39z7P+X7vaqoKSVJfj1vtCUiSVpchkKTmDIEkNWcIJKk5QyBJzRkCSWrOEOgxIckbkvyvaY/dnZJcnuTXVvB8RyZ5IMleu/j8Nyf5zvAaByX5hSQ3DduvnvZ8tXYYAmmNqKpvVdV+VfXj5T43yeOB9wLHD69xD3Am8IFh+5OLPH9jkkqybtdmr8cyQyD1cCjwc8D1Y/ueOm9bTRkCrZgkpye5Ocn3ktyQ5JcfZWwleWuSW5J8N8kfJ3ncvDF/kuTeJN9IcuLY/jcm+bvhPLck+Y2dnGOfJPclefbYvvVJfpDkkCQHJvl0krnhPJ9OsmEnr/WOJB8b2/6Zd9BJ9k9ybpI7k9yR5F2PXOJJclSSLyS5f1jrhTs5x/zXvDzJO5NcOaz1b5McvMDzngncOGzel+SyJDcDTwf++3BpaJ/h9f5Tki8m+fskn0rypOF5V4w9/4EkL1xojlqbDIFW0s3Ai4D9gT8APpbksEcZ/8vADPB8YDPwq2PHjmX0ze1g4I+Ac5NkOHY38M+AJwJvBM5O8vz5L15VDwH/DThlbPdrgS9U1d2M/n78OaN3zkcCPwA+sIz1jvsLYAdwFPA84HjgkfsL7wT+FjgQ2AD86TJe93WM1ngIsDfwu/MHVNXXgWcNmwdU1S9V1TOAbwGvHC4NPTQcP5XR1/mwYb7vH/a/eOz5+1XVVcuYox7jDIFWTFX9VVV9u6p+UlUXAjcBxzzKU86qqu1V9S3gffzsN+xbq+rDw/Xy8xh94zp0OM/FVXVzjXyB0TfZF+3kHB8HTh7bft2wj6q6p6r+uqoerKrvAX8IvGS5605yKHAS8FtV9f0hMmePnfdHjGLzlKr6YVUt50b4n1fV16vqB8AngOcud37znF9VX6uq7wP/AXjtrt6c1tphCLRikpya5CvD5Zj7gGczeke/M7eNPb4VeMrY9l2PPKiqB4eH+w3nOTHJ1Um2D+c56VHO83lg3yTHJtnI6Bvp3wyvs2+SDyW5NcnfM7o8csAufGN8KvB44M6xtX+I0bt4gH8HBPhikuuT/OpOXmchd409fpDhazCB+V/zx/Pov0faA/gTAFoRSZ4KfBg4Driqqn6c5CuMvgHuzBH8/5uZRwLfXsJ59gH+mtEljk9V1Y+SfHJn5xnm8QlGnza+A3x6ePcP8DvAPwSOraq7kjwX+PJOXuv7wL5j208ee3wb8BBwcFXtWGAOdwG/Psz/F4HPJrmiqrYttt7d4Iixx0cy+rTyXUaXrLSH8hOBVso/AAqYg9ENXUafCB7N7w03bI8AfhNY8CbqPHsD+wzn2THcRD5+ked8HPiXwL8aHj/iCYzuC9w33DQ941Fe4yvAi4ef9d8f+P1HDlTVnYwuT70nyROTPC7JM5K8BCDJvxi7CX0vo6/TT5aw1t3h9Uk2JdmX0Y+XXjRcfpsb5vT0VZqXdiNDoBVRVTcA7wGuYvTO+znAlYs87VPAdYy+yV4MnLuE83wPeCuj6+X3Mrrmv3WR51zD6B39U4DPjB16H/DzjN4RXw38z0d5jUsZheqrw5w/PW/IqYwidcMwr4sY3dcA+CfANUkeGOb6m1V1yyJL3V3OZ3Rj+y5GP276Vvjp5bc/BK4cLm+9YJXmp90g/h/T6LEoSQFHr9LlkZaSXA58rKo+stpz0cryE4EkNWcIJKk5Lw1JUnN+IpCk5gyBJDW3Jv9B2cEHH1wbN25c7WlI0ppy3XXXfbeq1s/fvyZDsHHjRmZnZ1d7GpK0piS5daH9XhqSpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKam0oIkpyQ5MYk25KcvsDxfZJcOBy/JsnGecePTPJAkt+dxnwkSUs3cQiS7AV8EDgR2ASckmTTvGGnAfdW1VHA2cBZ846/F/jMpHORJC3fND4RHANsq6pbquph4AJg87wxm4HzhscXAcclCUCSVwPfAK6fwlwkScs0jRAcDtw2tn37sG/BMVW1A7gfOCjJfsC/B/5gsZMkeVOS2SSzc3NzU5i2JAlW/2bxO4Czq+qBxQZW1TlVNVNVM+vXr9/9M5OkJtZN4TXuAI4Y294w7FtozO1J1gH7A/cAxwKvSfJHwAHAT5L8sKo+MIV5SZKWYBohuBY4OsnTGH3DPxl43bwxW4EtwFXAa4DLqqqAFz0yIMk7gAeMgCStrIlDUFU7krwFuATYC/hoVV2f5Exgtqq2AucC5yfZBmxnFAtJ0mNARm/M15aZmZmanZ1d7WlI0pqS5Lqqmpm/f7VvFkuSVpkhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmphKCJCckuTHJtiSnL3B8nyQXDsevSbJx2P/yJNcl+T/Df39pGvORJC3dxCFIshfwQeBEYBNwSpJN84adBtxbVUcBZwNnDfu/C7yyqp4DbAHOn3Q+kqTlmcYngmOAbVV1S1U9DFwAbJ43ZjNw3vD4IuC4JKmqL1fVt4f91wM/n2SfKcxJkrRE0wjB4cBtY9u3D/sWHFNVO4D7gYPmjfkV4EtV9dBCJ0nypiSzSWbn5uamMG1JEjxGbhYneRajy0W/sbMxVXVOVc1U1cz69etXbnKStIebRgjuAI4Y294w7FtwTJJ1wP7APcP2BuBvgFOr6uYpzEeStAzTCMG1wNFJnpZkb+BkYOu8MVsZ3QwGeA1wWVVVkgOAi4HTq+rKKcxFkrRME4dguOb/FuAS4O+AT1TV9UnOTPKqYdi5wEFJtgFvAx75EdO3AEcB/zHJV4Zfh0w6J0nS0qWqVnsOyzYzM1Ozs7OrPQ1JWlOSXFdVM/P3PyZuFkuSVo8hkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmphKCJCckuTHJtiSnL3B8nyQXDsevSbJx7NjvD/tvTPKKacxHkrR0E4cgyV7AB4ETgU3AKUk2zRt2GnBvVR0FnA2cNTx3E3Ay8CzgBOA/D68nSVoh0/hEcAywrapuqaqHgQuAzfPGbAbOGx5fBByXJMP+C6rqoar6BrBteD1J0gqZRggOB24b27592LfgmKraAdwPHLTE50qSdqM1c7M4yZuSzCaZnZubW+3pSNIeYxohuAM4Ymx7w7BvwTFJ1gH7A/cs8bkAVNU5VTVTVTPr16+fwrQlSTCdEFwLHJ3kaUn2ZnTzd+u8MVuBLcPj1wCXVVUN+08efqroacDRwBenMCdJ0hKtm/QFqmpHkrcAlwB7AR+tquuTnAnMVtVW4Fzg/CTbgO2MYsEw7hPADcAO4N9U1Y8nnZMkaekyemO+tszMzNTs7OxqT0OS1pQk11XVzPz9a+ZmsSRp9zAEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5iYKQZInJbk0yU3Dfw/cybgtw5ibkmwZ9u2b5OIk/zfJ9UnePclcJEm7ZtJPBKcDn6uqo4HPDds/I8mTgDOAY4FjgDPGgvEnVfWPgOcBv5DkxAnnI0lapklDsBk4b3h8HvDqBca8Ari0qrZX1b3ApcAJVfVgVX0eoKoeBr4EbJhwPpKkZZo0BIdW1Z3D47uAQxcYczhw29j27cO+n0pyAPBKRp8qFpTkTUlmk8zOzc1NNmtJ0k+tW2xAks8CT17g0NvHN6qqktRyJ5BkHfCXwPur6padjauqc4BzAGZmZpZ9HknSwhYNQVW9bGfHknwnyWFVdWeSw4C7Fxh2B/DSse0NwOVj2+cAN1XV+5Y0Y0nSVE16aWgrsGV4vAX41AJjLgGOT3LgcJP4+GEfSd4F7A/81oTzkCTtoklD8G7g5UluAl42bJNkJslHAKpqO/BO4Nrh15lVtT3JBkaXlzYBX0rylSS/NuF8JEnLlKq1d7l9ZmamZmdnV3sakrSmJLmuqmbm7/dfFktSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpqbKARJnpTk0iQ3Df89cCfjtgxjbkqyZYHjW5N8bZK5SJJ2zaSfCE4HPldVRwOfG7Z/RpInAWcAxwLHAGeMByPJPwcemHAekqRdNGkINgPnDY/PA169wJhXAJdW1faquhe4FDgBIMl+wNuAd004D0nSLpo0BIdW1Z3D47uAQxcYczhw29j27cM+gHcC7wEeXOxESd6UZDbJ7Nzc3ARTliSNW7fYgCSfBZ68wKG3j29UVSWppZ44yXOBZ1TVbyfZuNj4qjoHOAdgZmZmyeeRJD26RUNQVS/b2bEk30lyWFXdmeQw4O4Fht0BvHRsewNwOfBCYCbJN4d5HJLk8qp6KZKkFTPppaGtwCM/BbQF+NQCYy4Bjk9y4HCT+Hjgkqr6L1X1lKraCPwi8HUjIEkrb9IQvBt4eZKbgJcN2ySZSfIRgKrazuhewLXDrzOHfZKkx4BUrb3L7TMzMzU7O7va05CkNSXJdVU1M3+//7JYkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqLlW12nNYtiRzwK27+PSDge9OcTprgWvuoduau60XJl/zU6tq/fydazIEk0gyW1Uzqz2PleSae+i25m7rhd23Zi8NSVJzhkCSmusYgnNWewKrwDX30G3N3dYLu2nN7e4RSJJ+VsdPBJKkMXtsCJKckOTGJNuSnL7A8X2SXDgcvybJxpWf5fQsYb1vS3JDkq8m+VySp67GPKdpsTWPjfuVJJVkzf+EyVLWnOS1w+/19Uk+vtJznLYl/Nk+Msnnk3x5+PN90mrMc1qSfDTJ3Um+tpPjSfL+4evx1STPn/ikVbXH/QL2Am4Gng7sDfxvYNO8Mf8a+LPh8cnAhas979283n8K7Ds8fvNaXu9S1zyMewJwBXA1MLPa816B3+ejgS8DBw7bh6z2vFdgzecAbx4ebwK+udrznnDNLwaeD3xtJ8dPAj4DBHgBcM2k59xTPxEcA2yrqluq6mHgAmDzvDGbgfOGxxcBxyXJCs5xmhZdb1V9vqoeHDavBjas8BynbSm/xwDvBM4CfriSk9tNlrLmXwc+WFX3AlTV3Ss8x2lbypoLeOLweH/g2ys4v6mrqiuA7Y8yZDPwX2vkauCAJIdNcs49NQSHA7eNbd8+7FtwTFXtAO4HDlqR2U3fUtY77jRG7yjWskXXPHxkPqKqLl7Jie1GS/l9fibwzCRXJrk6yQkrNrvdYylrfgfw+iS3A/8D+LcrM7VVs9y/74taN9F0tOYkeT0wA7xkteeyOyV5HPBe4A2rPJWVto7R5aGXMvrUd0WS51TVfas6q93rFOAvquo9SV4InJ/k2VX1k9We2Fqxp34iuAM4Ymx7w7BvwTFJ1jH6SHnPisxu+payXpK8DHg78KqqemiF5ra7LLbmJwDPBi5P8k1G11K3rvEbxkv5fb4d2FpVP6qqbwBfZxSGtWopaz4N+ARAVV0F/Byj/02ePdWS/r4vx54agmuBo5M8LcnejG4Gb503ZiuwZXj8GuCyGu7ErEGLrjfJ84APMYrAWr9uDIusuarur6qDq2pjVW1kdF/kVVU1uzrTnYql/Ln+JKNPAyQ5mNGloltWcpJTtpQ1fws4DiDJP2YUgrkVneXK2gqcOvz00AuA+6vqzklecI+8NFRVO5K8BbiE0U8dfLSqrk9yJjBbVVuBcxl9hNzG6MbMyas348kscb1/DOwH/NVwT/xbVfWqVZv0hJa45j3KEtd8CXB8khuAHwO/V1Vr9ZPuUtf8O8CHk/w2oxvHb1jDb+pI8peMYn7wcN/jDODxAFX1Z4zug5wEbAMeBN448TnX8NdLkjQFe+qlIUnSEhkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1Zwgkqbn/B+GLhuDqQ4CAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZSZor21zD_f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}