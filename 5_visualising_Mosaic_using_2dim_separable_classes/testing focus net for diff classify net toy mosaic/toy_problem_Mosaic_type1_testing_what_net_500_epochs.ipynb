{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "toy_problem_Mosaic_type1_testing_what_net_500_epochs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2_J4Rw2r0SQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67b019a-985b-48f5-9871-a213674f0c15"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqdXHO0Cr0Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5d66de-0d26-4bf0-bfca-2e0201874cfd"
      },
      "source": [
        "y = np.random.randint(0,10,5000)\n",
        "idx= []\n",
        "for i in range(10):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 474\n",
            "1 481\n",
            "2 513\n",
            "3 478\n",
            "4 515\n",
            "5 516\n",
            "6 525\n",
            "7 464\n",
            "8 514\n",
            "9 520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "source": [
        "x = np.zeros((5000,2))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "source": [
        "x[idx[0],:] = np.random.multivariate_normal(mean = [2,5],cov=[[0.1,0],[0,0.1]],size=sum(idx[0]))\n",
        "\n",
        "x[idx[1],:] = np.random.multivariate_normal(mean = [-15,-7],cov=[[0.1,0],[0,0.1]],size=sum(idx[1]))\n",
        "\n",
        "x[idx[2],:] = np.random.multivariate_normal(mean = [2,-2],cov=[[0.1,0],[0,0.1]],size=sum(idx[2]))\n",
        "\n",
        "x[idx[3],:] = np.random.multivariate_normal(mean = [-2,0],cov=[[0.1,0],[0,0.1]],size=sum(idx[3]))\n",
        "\n",
        "\n",
        "x[idx[4],:] = np.random.multivariate_normal(mean = [15,8],cov=[[0.1,0],[0,0.1]],size=sum(idx[4]))\n",
        "\n",
        "x[idx[5],:] = np.random.multivariate_normal(mean = [-15,6],cov=[[0.1,0],[0,0.1]],size=sum(idx[5]))\n",
        "\n",
        "x[idx[6],:] = np.random.multivariate_normal(mean = [2,-18],cov=[[0.1,0],[0,0.1]],size=sum(idx[6]))\n",
        "\n",
        "x[idx[7],:] = np.random.multivariate_normal(mean = [10,-10],cov=[[0.1,0],[0,0.1]],size=sum(idx[7]))\n",
        "\n",
        "x[idx[8],:] = np.random.multivariate_normal(mean = [-10,-15],cov=[[0.1,0],[0,0.1]],size=sum(idx[8]))\n",
        "\n",
        "x[idx[9],:] = np.random.multivariate_normal(mean = [-2,10],cov=[[0.1,0],[0,0.1]],size=sum(idx[9]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8Jm7YUr0St",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "557a5b43-4c35-4413-9303-285363c8f815"
      },
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe9caff8ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "source": [
        "foreground_classes = {'class_0','class_1', 'class_2'}\n",
        "\n",
        "background_classes = {'class_3','class_4', 'class_5', 'class_6','class_7', 'class_8', 'class_9'}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplNpNQVr0S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9567a952-5a65-4e3a-adf1-6b0455a36473"
      },
      "source": [
        "fg_class  = np.random.randint(0,3)\n",
        "fg_idx = np.random.randint(0,9)\n",
        "\n",
        "a = []\n",
        "for i in range(9):\n",
        "    if i == fg_idx:\n",
        "        b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "    else:\n",
        "        bg_class = np.random.randint(3,10)\n",
        "        b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "a = np.concatenate(a,axis=0)\n",
        "print(a.shape)\n",
        "\n",
        "print(fg_class , fg_idx)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "background 5 present at 0\n",
            "background 9 present at 1\n",
            "background 5 present at 2\n",
            "background 8 present at 3\n",
            "foreground 1 present at 4\n",
            "background 9 present at 5\n",
            "background 3 present at 6\n",
            "background 9 present at 7\n",
            "background 7 present at 8\n",
            "(9, 2)\n",
            "1 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwZVmmRBr0S8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67a3f22-2cba-454f-dfe9-e7c09b70cfa4"
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoxzYI-ur0S_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c862ad09-672f-4b59-d17b-49867df78d40"
      },
      "source": [
        "np.reshape(a,(18,1))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-15.65866604],\n",
              "       [  5.91233713],\n",
              "       [ -1.81356219],\n",
              "       [  9.69495179],\n",
              "       [-14.99297081],\n",
              "       [  6.14735087],\n",
              "       [-10.28402091],\n",
              "       [-14.51136049],\n",
              "       [-14.61751239],\n",
              "       [ -6.81268516],\n",
              "       [ -1.95651493],\n",
              "       [ 10.23181881],\n",
              "       [ -2.31701742],\n",
              "       [ -0.25525232],\n",
              "       [ -2.20197386],\n",
              "       [ 10.67907208],\n",
              "       [ 10.31340073],\n",
              "       [-10.2293761 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ruI0cxr0TE"
      },
      "source": [
        "a=np.reshape(a,(3,6))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUTFhJIr0TI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "182e5e5b-d795-4b5f-ddd6-f78c18be81c6"
      },
      "source": [
        "plt.imshow(a)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe9cab00438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAADKCAYAAACmA/sWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANL0lEQVR4nO3df6xfdX3H8edr5YJbgRWHGU1bBbMGx9wP8AZccIaALMAYXTJMIJmi0TQhkuEmbuASzEyWsGVhmz8CI0iEzYDLcK7bmmEXUTQbyoUVoWVoR0hoJVYpgkUHXHjvj3tY7i637W2/53tP7/08H8k39/z43PN+n97c1z0953zPN1WFJGn5+4mhG5AkLQ4DX5IaYeBLUiMMfElqhIEvSY0w8CWpESMFfpLXJtmS5Nvd1+P2Me6lJFu716ZRakqSDk1GuQ8/yZ8Be6rquiRXA8dV1R/OM25vVR09Qp+SpBGNGviPAmdV1ZNJVgNfrqqT5xln4EvSwEYN/B9U1apuOsDTr8zPGTcNbAWmgeuq6gv72N5GYCPACla85ac49pB7O9wde8pLQ7cwVt/bu3x/dm9a9d2hWxirxx86ZugWxurYU6aHbmGsdm579vtV9br51h0w8JP8G3DCPKv+CLh1dsAnebqqXnUeP8maqtqV5I3Al4Bzquq/91f32Ly2zsg5++1tKTv7oeeGbmGs/vqes4duYWzuvej6oVsYq8vWL9/fO4Bzp3YP3cJYffgXvnh/VU3Ot+6IA31zVb1jX+uSfDfJ6lmndOb9l6yqXd3Xx5J8GTgV2G/gS5L6NeptmZuAy7rpy4B/nDsgyXFJjuqmjwfOBLaPWFeSdJBGDfzrgHOTfBt4RzdPkskkN3djfh6YSvIgcDcz5/ANfElaZAc8pbM/VfUU8KoTflU1Bby/m/534BdHqSNJGp3vtJWkRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5Ia0UvgJzkvyaNJdiS5ep71RyX5XLf+60lO7KOuJGnhRg78JCuATwHnA6cAlyY5Zc6w9wFPV9XPAX8B/OmodSVJB6ePI/zTgR1V9VhVvQDcAWyYM2YDcGs3/ffAOUnSQ21J0gL1EfhrgCdmze/sls07pqqmgWeAn5m7oSQbk0wlmXqR53toTZL0isPqom1V3VRVk1U1OcFRQ7cjSctKH4G/C1g3a35tt2zeMUmOAH4aeKqH2pKkBeoj8O8D1ic5KcmRwCXApjljNgGXddMXA1+qquqhtiRpgY4YdQNVNZ3kCuAuYAVwS1VtS/IxYKqqNgGfBv4myQ5gDzN/FCRJi2jkwAeoqs3A5jnLrp01/T/AO/uoJUk6NIfVRVtJ0vgY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGtFL4Cc5L8mjSXYkuXqe9e9J8r0kW7vX+/uoK0lauJE/xDzJCuBTwLnATuC+JJuqavucoZ+rqitGrSdJOjR9HOGfDuyoqseq6gXgDmBDD9uVJPVo5CN8YA3wxKz5ncAZ84z77SRvB74F/F5VPTF3QJKNwEaAo16zium3vaWH9g5PX/m1HUO3MFZvevmRoVsYm/MfvGroFsbq6T9/eegWxurOVZ8YuoWx+vB+1i3WRdt/Ak6sql8CtgC3zjeoqm6qqsmqmpyYWLlIrUlSG/oI/F3Aulnza7tl/6eqnqqq57vZm4Hle+guSYepPgL/PmB9kpOSHAlcAmyaPSDJ6lmzFwHL9//7knSYGvkcflVNJ7kCuAtYAdxSVduSfAyYqqpNwO8muQiYBvYA7xm1riTp4PRx0Zaq2gxsnrPs2lnT1wDX9FFLknRofKetJDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1IheAj/JLUl2J3l4H+uT5ONJdiT5ZpLT+qgrSVq4vo7wPwOct5/15wPru9dG4Iae6kqSFqiXwK+qe4A9+xmyAbitZtwLrEqyuo/akqSFWaxz+GuAJ2bN7+yW/T9JNiaZSjL14ovPLVJrktSGw+qibVXdVFWTVTU5MbFy6HYkaVlZrMDfBaybNb+2WyZJWiSLFfibgHd3d+u8FXimqp5cpNqSJOCIPjaS5HbgLOD4JDuBjwITAFV1I7AZuADYAfwIeG8fdSVJC9dL4FfVpQdYX8AH+qglSTo0h9VFW0nS+Bj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5Ia0UvgJ7klye4kD+9j/VlJnkmytXtd20ddSdLC9fIh5sBngE8Ct+1nzFer6sKe6kmSDlIvR/hVdQ+wp49tSZLGo68j/IX41SQPAt8BrqqqbXMHJNkIbARYcdxxPP6bE4vY3iJ7zxuH7mCsas9RQ7cwNo+984ahWxir5+vFoVsYqwveffnQLYzZNftcs1gXbR8A3lBVvwx8AvjCfIOq6qaqmqyqyRVHr1yk1iSpDYsS+FX1bFXt7aY3AxNJjl+M2pKkGYsS+ElOSJJu+vSu7lOLUVuSNKOXc/hJbgfOAo5PshP4KDABUFU3AhcDlyeZBn4MXFJV1UdtSdLC9BL4VXXpAdZ/kpnbNiVJA/GdtpLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjRg78JOuS3J1ke5JtSa6cZ0ySfDzJjiTfTHLaqHUlSQenjw8xnwY+VFUPJDkGuD/JlqraPmvM+cD67nUGcEP3VZK0SEY+wq+qJ6vqgW76h8AjwJo5wzYAt9WMe4FVSVaPWluStHC9nsNPciJwKvD1OavWAE/Mmt/Jq/8okGRjkqkkUy/tfa7P1iSpeb0FfpKjgTuBD1bVs4eyjaq6qaomq2pyxdEr+2pNkkRPgZ9kgpmw/2xVfX6eIbuAdbPm13bLJEmLpI+7dAJ8Gnikqq7fx7BNwLu7u3XeCjxTVU+OWluStHB93KVzJvAu4KEkW7tlHwFeD1BVNwKbgQuAHcCPgPf2UFeSdBBGDvyq+hqQA4wp4AOj1pIkHTrfaStJjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1YuTAT7Iuyd1JtifZluTKecacleSZJFu717Wj1pUkHZyRP8QcmAY+VFUPJDkGuD/JlqraPmfcV6vqwh7qSZIOwchH+FX1ZFU90E3/EHgEWDPqdiVJ/UpV9bex5ETgHuDNVfXsrOVnAXcCO4HvAFdV1bZ5vn8jsLGbPRl4tLfmDux44PuLWG+xuX9Lm/u3dC32vr2hql4334reAj/J0cBXgD+pqs/PWXcs8HJV7U1yAfBXVbW+l8I9STJVVZND9zEu7t/S5v4tXYfTvvVyl06SCWaO4D87N+wBqurZqtrbTW8GJpIc30dtSdLC9HGXToBPA49U1fX7GHNCN44kp3d1nxq1tiRp4fq4S+dM4F3AQ0m2dss+ArweoKpuBC4GLk8yDfwYuKT6vHjQj5uGbmDM3L+lzf1bug6bfev1oq0k6fDlO20lqREGviQ1wsAHkpyX5NEkO5JcPXQ/fUpyS5LdSR4eupdxWMijPZaqJK9J8o0kD3b79sdD9zQOSVYk+c8k/zx0L31L8niSh7pHykwN3k/r5/CTrAC+BZzLzBvD7gMunefREEtSkrcDe4HbqurNQ/fTtySrgdWzH+0B/NZy+Pl1d7at7N6/MgF8Dbiyqu4duLVeJfl9YBI4drk9fiXJ48BkVR0WbyrzCB9OB3ZU1WNV9QJwB7Bh4J56U1X3AHuG7mNclvOjPWrG3m52onstqyO0JGuB3wBuHrqXFhj4M+HwxKz5nSyTwGhN92iPU4GvD9tJf7rTHVuB3cCWqlo2+9b5S+APgJeHbmRMCvhikvu7R8cMysDXstA92uNO4IOzn+O01FXVS1X1K8Ba4PQky+a0XJILgd1Vdf/QvYzR26rqNOB84APdKdbBGPiwC1g3a35tt0xLxIEe7bEcVNUPgLuB84bupUdnAhd157nvAM5O8rfDttSvqtrVfd0N/AMzp5AHY+DPXKRdn+SkJEcClwCbBu5JC7SQR3ssVUlel2RVN/2TzNxY8F/DdtWfqrqmqtZW1YnM/N59qap+Z+C2epNkZXcjAUlWAr8ODHq3XPOBX1XTwBXAXcxc8Pu7+R7dvFQluR34D+DkJDuTvG/onnr2yqM9zp71iWoXDN1UT1YDdyf5JjMHJluqatnduriM/SzwtSQPAt8A/qWq/nXIhpq/LVOSWtH8Eb4ktcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY34X3mvtXgNGCY9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbvfbwVr0TN"
      },
      "source": [
        "desired_num = 1000\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "for j in range(desired_num):\n",
        "    fg_class  = np.random.randint(0,3)\n",
        "    fg_idx = 0\n",
        "    a = []\n",
        "    for i in range(9):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(3,10)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(18,1)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "source": [
        "mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n",
        "# print(mosaic_list)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2PnW7aQr0TT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a468877-563f-4996-996d-24729754d7fe"
      },
      "source": [
        "print(np.shape(mosaic_list_of_images))\n",
        "print(np.shape(fore_idx))\n",
        "print(np.shape(mosaic_label))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 18)\n",
            "(1000,)\n",
            "(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  cnt = 0\n",
        "  counter = np.array([0,0,0,0,0,0,0,0,0])\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([18], dtype=torch.float64)\n",
        "    np.random.seed(dataset_number*10000 + i)\n",
        "    give_pref = foreground_index[i] #np.random.randint(0,9)\n",
        "    # print(\"outside\", give_pref,foreground_index[i])\n",
        "    for j in range(9):\n",
        "      if j == give_pref:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "\n",
        "    if give_pref == foreground_index[i] :\n",
        "      # print(\"equal are\", give_pref,foreground_index[i])\n",
        "      cnt += 1\n",
        "      counter[give_pref] += 1\n",
        "    else :\n",
        "      counter[give_pref] += 1\n",
        "\n",
        "    avg_image_dataset.append(img)\n",
        "\n",
        "  print(\"number of correct averaging happened for dataset \"+str(dataset_number)+\" is \"+str(cnt)) \n",
        "  print(\"the averaging are done as \", counter) \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZAjix3x8CM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a49741-c7f8-49b2-ed39-29ee688c4646"
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 9)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of correct averaging happened for dataset 1 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 2 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 3 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 4 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 5 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 6 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 7 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 8 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n",
            "number of correct averaging happened for dataset 9 is 1000\n",
            "the averaging are done as  [1000    0    0    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL0BRf8er0TX"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY2l62APygaV"
      },
      "source": [
        "batch = 200\n",
        "epochs = 300\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVRXgwwNr0Tb"
      },
      "source": [
        "class Wherenet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Wherenet,self).__init__()\n",
        "        self.linear1 = nn.Linear(2,128)\n",
        "        self.linear2 = nn.Linear(128,256)\n",
        "        self.linear3 = nn.Linear(256,128)\n",
        "        self.linear4 = nn.Linear(128,64)\n",
        "        self.linear5 = nn.Linear(64,1)\n",
        "    def forward(self,z):\n",
        "        x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,2], dtype=torch.float64)\n",
        "        #x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        for i in range(9):\n",
        "            x[:,i] = self.helper(z[:,2*i:2*i+2])[:,0]\n",
        "            #print(k[:,0].shape,x[:,i].shape)\n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        x1 = x[:,0]\n",
        "        for i in range(9):\n",
        "            x1 = x[:,i]          \n",
        "            #print()\n",
        "            y = y+torch.mul(x1[:,None],z[:,2*i:2*i+2])\n",
        "        return y , x \n",
        "\n",
        "    \n",
        "    def helper(self,x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = F.relu(self.linear4(x))\n",
        "        x = self.linear5(x)\n",
        "        return x\n",
        "\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Ek05Kxr0Te",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca0c3c0-0902-4a98-bf70-05b1150cc3d2"
      },
      "source": [
        "trainiter = iter(trainloader_1)\n",
        "input1,labels1 = trainiter.next()\n",
        "\n",
        "input1.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxEmWZI6r0Ti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abec705f-6eb2-4a5d-b058-25edfce046f2"
      },
      "source": [
        "where = Wherenet().double()\n",
        "where = where\n",
        "out_where,alphas = where(input1)\n",
        "out_where.shape,alphas.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([200, 2]), torch.Size([200, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_XeIUk0r0Tl"
      },
      "source": [
        "class Whatnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Whatnet,self).__init__()\n",
        "        self.linear1 = nn.Linear(2,128)\n",
        "        self.linear2 = nn.Linear(128,256)\n",
        "        self.linear3 = nn.Linear(256,128)\n",
        "        self.linear4 = nn.Linear(128,64)\n",
        "        self.linear5 = nn.Linear(64,3)\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = F.relu(self.linear4(x))\n",
        "        x = self.linear5(x)\n",
        "        return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35i9bIlr0Tp"
      },
      "source": [
        "# what = Whatnet().double()\n",
        "# what(out_where)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uALi25pmzQHV"
      },
      "source": [
        "def test_all(number, testloader,what, where):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            # images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            avg_inp,alphas = where(inputs)        \n",
        "            outputs = what(avg_inp)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmNprlPzTjP"
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    where = Wherenet().double()\n",
        "    what = Whatnet().double()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer_where = optim.SGD(where.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer_where = optim.Adam(where.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
        "    # optimizer_what = optim.SGD(what.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer_what = optim.Adam(what.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 500\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            # inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_what.zero_grad()\n",
        "            optimizer_where.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            avg_inp,alphas = where(inputs) \n",
        "            # print(avg_inp.shape)       \n",
        "            outputs = what(avg_inp)\n",
        "            \n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_what.step()\n",
        "            optimizer_where.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            mini = 4\n",
        "            if i % mini == mini-1:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / mini))\n",
        "                ep_lossi.append(running_loss/mini) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "        if (np.mean(ep_lossi) <= 0.45):\n",
        "            break\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    # torch.save(inc.state_dict(),\"train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            inputs, labels = data\n",
        "            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            avg_inp,alphas = where(inputs)        \n",
        "            outputs = what(avg_inp)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,what, where)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgvXIxE6A7l1"
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQoPST5zW2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7cb0ec-b3b8-4ca9-a8bb-22e8363a7387"
      },
      "source": [
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,     4] loss: 1.107\n",
            "[2,     4] loss: 1.035\n",
            "[3,     4] loss: 1.035\n",
            "[4,     4] loss: 1.018\n",
            "[5,     4] loss: 1.023\n",
            "[6,     4] loss: 1.014\n",
            "[7,     4] loss: 1.011\n",
            "[8,     4] loss: 1.009\n",
            "[9,     4] loss: 1.011\n",
            "[10,     4] loss: 1.019\n",
            "[11,     4] loss: 1.009\n",
            "[12,     4] loss: 1.020\n",
            "[13,     4] loss: 1.013\n",
            "[14,     4] loss: 1.014\n",
            "[15,     4] loss: 1.010\n",
            "[16,     4] loss: 1.021\n",
            "[17,     4] loss: 1.023\n",
            "[18,     4] loss: 1.010\n",
            "[19,     4] loss: 1.010\n",
            "[20,     4] loss: 1.018\n",
            "[21,     4] loss: 1.022\n",
            "[22,     4] loss: 1.011\n",
            "[23,     4] loss: 1.005\n",
            "[24,     4] loss: 1.010\n",
            "[25,     4] loss: 1.008\n",
            "[26,     4] loss: 1.011\n",
            "[27,     4] loss: 1.024\n",
            "[28,     4] loss: 1.012\n",
            "[29,     4] loss: 1.009\n",
            "[30,     4] loss: 1.012\n",
            "[31,     4] loss: 1.006\n",
            "[32,     4] loss: 1.014\n",
            "[33,     4] loss: 1.013\n",
            "[34,     4] loss: 1.012\n",
            "[35,     4] loss: 1.020\n",
            "[36,     4] loss: 1.010\n",
            "[37,     4] loss: 1.005\n",
            "[38,     4] loss: 0.996\n",
            "[39,     4] loss: 1.012\n",
            "[40,     4] loss: 1.019\n",
            "[41,     4] loss: 1.017\n",
            "[42,     4] loss: 1.011\n",
            "[43,     4] loss: 1.020\n",
            "[44,     4] loss: 1.010\n",
            "[45,     4] loss: 1.022\n",
            "[46,     4] loss: 1.006\n",
            "[47,     4] loss: 1.006\n",
            "[48,     4] loss: 1.018\n",
            "[49,     4] loss: 1.016\n",
            "[50,     4] loss: 1.021\n",
            "[51,     4] loss: 1.004\n",
            "[52,     4] loss: 1.026\n",
            "[53,     4] loss: 1.010\n",
            "[54,     4] loss: 1.017\n",
            "[55,     4] loss: 1.011\n",
            "[56,     4] loss: 1.009\n",
            "[57,     4] loss: 1.002\n",
            "[58,     4] loss: 1.009\n",
            "[59,     4] loss: 1.005\n",
            "[60,     4] loss: 1.008\n",
            "[61,     4] loss: 0.999\n",
            "[62,     4] loss: 1.009\n",
            "[63,     4] loss: 1.009\n",
            "[64,     4] loss: 1.010\n",
            "[65,     4] loss: 1.012\n",
            "[66,     4] loss: 1.010\n",
            "[67,     4] loss: 1.004\n",
            "[68,     4] loss: 1.002\n",
            "[69,     4] loss: 1.008\n",
            "[70,     4] loss: 1.004\n",
            "[71,     4] loss: 1.013\n",
            "[72,     4] loss: 1.005\n",
            "[73,     4] loss: 1.012\n",
            "[74,     4] loss: 1.001\n",
            "[75,     4] loss: 1.004\n",
            "[76,     4] loss: 1.010\n",
            "[77,     4] loss: 1.006\n",
            "[78,     4] loss: 1.010\n",
            "[79,     4] loss: 1.015\n",
            "[80,     4] loss: 1.000\n",
            "[81,     4] loss: 0.992\n",
            "[82,     4] loss: 1.004\n",
            "[83,     4] loss: 1.004\n",
            "[84,     4] loss: 1.016\n",
            "[85,     4] loss: 1.015\n",
            "[86,     4] loss: 1.004\n",
            "[87,     4] loss: 1.004\n",
            "[88,     4] loss: 1.015\n",
            "[89,     4] loss: 1.013\n",
            "[90,     4] loss: 1.000\n",
            "[91,     4] loss: 0.994\n",
            "[92,     4] loss: 1.010\n",
            "[93,     4] loss: 1.005\n",
            "[94,     4] loss: 0.999\n",
            "[95,     4] loss: 1.003\n",
            "[96,     4] loss: 1.003\n",
            "[97,     4] loss: 1.000\n",
            "[98,     4] loss: 1.004\n",
            "[99,     4] loss: 1.016\n",
            "[100,     4] loss: 1.003\n",
            "[101,     4] loss: 1.002\n",
            "[102,     4] loss: 1.012\n",
            "[103,     4] loss: 1.010\n",
            "[104,     4] loss: 1.000\n",
            "[105,     4] loss: 1.008\n",
            "[106,     4] loss: 1.002\n",
            "[107,     4] loss: 1.000\n",
            "[108,     4] loss: 1.010\n",
            "[109,     4] loss: 0.996\n",
            "[110,     4] loss: 0.996\n",
            "[111,     4] loss: 1.008\n",
            "[112,     4] loss: 1.011\n",
            "[113,     4] loss: 1.007\n",
            "[114,     4] loss: 1.006\n",
            "[115,     4] loss: 1.015\n",
            "[116,     4] loss: 1.006\n",
            "[117,     4] loss: 0.996\n",
            "[118,     4] loss: 1.001\n",
            "[119,     4] loss: 0.998\n",
            "[120,     4] loss: 1.006\n",
            "[121,     4] loss: 0.999\n",
            "[122,     4] loss: 1.017\n",
            "[123,     4] loss: 1.003\n",
            "[124,     4] loss: 1.006\n",
            "[125,     4] loss: 1.015\n",
            "[126,     4] loss: 1.006\n",
            "[127,     4] loss: 0.984\n",
            "[128,     4] loss: 0.995\n",
            "[129,     4] loss: 1.013\n",
            "[130,     4] loss: 1.012\n",
            "[131,     4] loss: 0.994\n",
            "[132,     4] loss: 1.000\n",
            "[133,     4] loss: 1.004\n",
            "[134,     4] loss: 1.006\n",
            "[135,     4] loss: 1.003\n",
            "[136,     4] loss: 0.995\n",
            "[137,     4] loss: 0.993\n",
            "[138,     4] loss: 0.995\n",
            "[139,     4] loss: 1.004\n",
            "[140,     4] loss: 1.013\n",
            "[141,     4] loss: 1.004\n",
            "[142,     4] loss: 1.006\n",
            "[143,     4] loss: 1.002\n",
            "[144,     4] loss: 1.002\n",
            "[145,     4] loss: 1.007\n",
            "[146,     4] loss: 0.996\n",
            "[147,     4] loss: 0.997\n",
            "[148,     4] loss: 0.997\n",
            "[149,     4] loss: 1.004\n",
            "[150,     4] loss: 0.999\n",
            "[151,     4] loss: 1.004\n",
            "[152,     4] loss: 1.000\n",
            "[153,     4] loss: 1.005\n",
            "[154,     4] loss: 1.001\n",
            "[155,     4] loss: 1.002\n",
            "[156,     4] loss: 1.001\n",
            "[157,     4] loss: 0.994\n",
            "[158,     4] loss: 0.999\n",
            "[159,     4] loss: 1.004\n",
            "[160,     4] loss: 0.997\n",
            "[161,     4] loss: 1.006\n",
            "[162,     4] loss: 1.006\n",
            "[163,     4] loss: 0.991\n",
            "[164,     4] loss: 1.009\n",
            "[165,     4] loss: 1.002\n",
            "[166,     4] loss: 0.994\n",
            "[167,     4] loss: 0.997\n",
            "[168,     4] loss: 1.002\n",
            "[169,     4] loss: 0.990\n",
            "[170,     4] loss: 0.994\n",
            "[171,     4] loss: 1.002\n",
            "[172,     4] loss: 1.006\n",
            "[173,     4] loss: 1.009\n",
            "[174,     4] loss: 1.008\n",
            "[175,     4] loss: 0.989\n",
            "[176,     4] loss: 1.014\n",
            "[177,     4] loss: 1.001\n",
            "[178,     4] loss: 0.992\n",
            "[179,     4] loss: 0.991\n",
            "[180,     4] loss: 0.992\n",
            "[181,     4] loss: 0.996\n",
            "[182,     4] loss: 1.002\n",
            "[183,     4] loss: 1.004\n",
            "[184,     4] loss: 1.001\n",
            "[185,     4] loss: 1.003\n",
            "[186,     4] loss: 0.996\n",
            "[187,     4] loss: 0.999\n",
            "[188,     4] loss: 1.009\n",
            "[189,     4] loss: 1.013\n",
            "[190,     4] loss: 1.000\n",
            "[191,     4] loss: 1.005\n",
            "[192,     4] loss: 0.994\n",
            "[193,     4] loss: 0.995\n",
            "[194,     4] loss: 0.995\n",
            "[195,     4] loss: 1.009\n",
            "[196,     4] loss: 0.996\n",
            "[197,     4] loss: 1.005\n",
            "[198,     4] loss: 1.010\n",
            "[199,     4] loss: 1.006\n",
            "[200,     4] loss: 0.997\n",
            "[201,     4] loss: 1.001\n",
            "[202,     4] loss: 0.991\n",
            "[203,     4] loss: 0.998\n",
            "[204,     4] loss: 1.001\n",
            "[205,     4] loss: 0.996\n",
            "[206,     4] loss: 0.981\n",
            "[207,     4] loss: 0.997\n",
            "[208,     4] loss: 1.006\n",
            "[209,     4] loss: 0.999\n",
            "[210,     4] loss: 1.007\n",
            "[211,     4] loss: 1.000\n",
            "[212,     4] loss: 1.000\n",
            "[213,     4] loss: 0.991\n",
            "[214,     4] loss: 0.990\n",
            "[215,     4] loss: 0.990\n",
            "[216,     4] loss: 0.996\n",
            "[217,     4] loss: 1.013\n",
            "[218,     4] loss: 0.999\n",
            "[219,     4] loss: 0.996\n",
            "[220,     4] loss: 1.003\n",
            "[221,     4] loss: 0.997\n",
            "[222,     4] loss: 0.998\n",
            "[223,     4] loss: 0.983\n",
            "[224,     4] loss: 0.989\n",
            "[225,     4] loss: 0.986\n",
            "[226,     4] loss: 0.993\n",
            "[227,     4] loss: 0.996\n",
            "[228,     4] loss: 0.998\n",
            "[229,     4] loss: 1.003\n",
            "[230,     4] loss: 1.006\n",
            "[231,     4] loss: 0.994\n",
            "[232,     4] loss: 1.003\n",
            "[233,     4] loss: 0.990\n",
            "[234,     4] loss: 1.010\n",
            "[235,     4] loss: 0.999\n",
            "[236,     4] loss: 0.999\n",
            "[237,     4] loss: 0.991\n",
            "[238,     4] loss: 0.996\n",
            "[239,     4] loss: 0.980\n",
            "[240,     4] loss: 0.973\n",
            "[241,     4] loss: 0.991\n",
            "[242,     4] loss: 0.997\n",
            "[243,     4] loss: 0.997\n",
            "[244,     4] loss: 0.990\n",
            "[245,     4] loss: 1.003\n",
            "[246,     4] loss: 1.007\n",
            "[247,     4] loss: 0.991\n",
            "[248,     4] loss: 0.991\n",
            "[249,     4] loss: 1.003\n",
            "[250,     4] loss: 0.996\n",
            "[251,     4] loss: 1.001\n",
            "[252,     4] loss: 1.007\n",
            "[253,     4] loss: 1.012\n",
            "[254,     4] loss: 0.978\n",
            "[255,     4] loss: 1.000\n",
            "[256,     4] loss: 0.990\n",
            "[257,     4] loss: 0.991\n",
            "[258,     4] loss: 1.002\n",
            "[259,     4] loss: 0.993\n",
            "[260,     4] loss: 0.998\n",
            "[261,     4] loss: 0.979\n",
            "[262,     4] loss: 0.994\n",
            "[263,     4] loss: 0.995\n",
            "[264,     4] loss: 0.991\n",
            "[265,     4] loss: 0.986\n",
            "[266,     4] loss: 1.010\n",
            "[267,     4] loss: 0.994\n",
            "[268,     4] loss: 0.998\n",
            "[269,     4] loss: 0.996\n",
            "[270,     4] loss: 0.988\n",
            "[271,     4] loss: 1.004\n",
            "[272,     4] loss: 0.993\n",
            "[273,     4] loss: 0.987\n",
            "[274,     4] loss: 0.999\n",
            "[275,     4] loss: 1.001\n",
            "[276,     4] loss: 0.990\n",
            "[277,     4] loss: 0.996\n",
            "[278,     4] loss: 0.997\n",
            "[279,     4] loss: 0.993\n",
            "[280,     4] loss: 0.993\n",
            "[281,     4] loss: 0.989\n",
            "[282,     4] loss: 0.986\n",
            "[283,     4] loss: 0.997\n",
            "[284,     4] loss: 0.990\n",
            "[285,     4] loss: 0.997\n",
            "[286,     4] loss: 0.984\n",
            "[287,     4] loss: 0.981\n",
            "[288,     4] loss: 0.993\n",
            "[289,     4] loss: 0.992\n",
            "[290,     4] loss: 0.980\n",
            "[291,     4] loss: 0.989\n",
            "[292,     4] loss: 0.988\n",
            "[293,     4] loss: 0.978\n",
            "[294,     4] loss: 0.997\n",
            "[295,     4] loss: 0.980\n",
            "[296,     4] loss: 1.001\n",
            "[297,     4] loss: 0.989\n",
            "[298,     4] loss: 0.988\n",
            "[299,     4] loss: 0.973\n",
            "[300,     4] loss: 0.985\n",
            "[301,     4] loss: 0.987\n",
            "[302,     4] loss: 0.983\n",
            "[303,     4] loss: 0.985\n",
            "[304,     4] loss: 0.980\n",
            "[305,     4] loss: 0.998\n",
            "[306,     4] loss: 0.989\n",
            "[307,     4] loss: 0.991\n",
            "[308,     4] loss: 0.981\n",
            "[309,     4] loss: 0.997\n",
            "[310,     4] loss: 0.991\n",
            "[311,     4] loss: 0.986\n",
            "[312,     4] loss: 0.986\n",
            "[313,     4] loss: 0.981\n",
            "[314,     4] loss: 1.002\n",
            "[315,     4] loss: 0.990\n",
            "[316,     4] loss: 0.990\n",
            "[317,     4] loss: 0.984\n",
            "[318,     4] loss: 0.984\n",
            "[319,     4] loss: 0.978\n",
            "[320,     4] loss: 1.003\n",
            "[321,     4] loss: 0.996\n",
            "[322,     4] loss: 0.992\n",
            "[323,     4] loss: 0.990\n",
            "[324,     4] loss: 0.984\n",
            "[325,     4] loss: 0.993\n",
            "[326,     4] loss: 1.006\n",
            "[327,     4] loss: 0.996\n",
            "[328,     4] loss: 0.982\n",
            "[329,     4] loss: 0.988\n",
            "[330,     4] loss: 0.985\n",
            "[331,     4] loss: 0.981\n",
            "[332,     4] loss: 0.984\n",
            "[333,     4] loss: 0.983\n",
            "[334,     4] loss: 0.999\n",
            "[335,     4] loss: 0.984\n",
            "[336,     4] loss: 0.985\n",
            "[337,     4] loss: 0.988\n",
            "[338,     4] loss: 0.992\n",
            "[339,     4] loss: 0.989\n",
            "[340,     4] loss: 0.988\n",
            "[341,     4] loss: 0.994\n",
            "[342,     4] loss: 0.985\n",
            "[343,     4] loss: 0.984\n",
            "[344,     4] loss: 0.999\n",
            "[345,     4] loss: 0.987\n",
            "[346,     4] loss: 0.974\n",
            "[347,     4] loss: 0.994\n",
            "[348,     4] loss: 0.970\n",
            "[349,     4] loss: 0.977\n",
            "[350,     4] loss: 0.985\n",
            "[351,     4] loss: 0.983\n",
            "[352,     4] loss: 0.978\n",
            "[353,     4] loss: 0.981\n",
            "[354,     4] loss: 0.978\n",
            "[355,     4] loss: 0.987\n",
            "[356,     4] loss: 1.001\n",
            "[357,     4] loss: 0.970\n",
            "[358,     4] loss: 1.032\n",
            "[359,     4] loss: 0.984\n",
            "[360,     4] loss: 0.982\n",
            "[361,     4] loss: 0.990\n",
            "[362,     4] loss: 0.991\n",
            "[363,     4] loss: 0.996\n",
            "[364,     4] loss: 0.986\n",
            "[365,     4] loss: 0.986\n",
            "[366,     4] loss: 0.990\n",
            "[367,     4] loss: 0.980\n",
            "[368,     4] loss: 0.988\n",
            "[369,     4] loss: 0.987\n",
            "[370,     4] loss: 0.985\n",
            "[371,     4] loss: 0.985\n",
            "[372,     4] loss: 0.986\n",
            "[373,     4] loss: 1.001\n",
            "[374,     4] loss: 0.983\n",
            "[375,     4] loss: 0.991\n",
            "[376,     4] loss: 0.978\n",
            "[377,     4] loss: 0.987\n",
            "[378,     4] loss: 0.984\n",
            "[379,     4] loss: 0.990\n",
            "[380,     4] loss: 0.992\n",
            "[381,     4] loss: 0.978\n",
            "[382,     4] loss: 0.991\n",
            "[383,     4] loss: 0.989\n",
            "[384,     4] loss: 0.988\n",
            "[385,     4] loss: 0.982\n",
            "[386,     4] loss: 0.982\n",
            "[387,     4] loss: 0.995\n",
            "[388,     4] loss: 0.992\n",
            "[389,     4] loss: 0.999\n",
            "[390,     4] loss: 0.974\n",
            "[391,     4] loss: 0.996\n",
            "[392,     4] loss: 0.985\n",
            "[393,     4] loss: 0.988\n",
            "[394,     4] loss: 0.974\n",
            "[395,     4] loss: 0.986\n",
            "[396,     4] loss: 0.968\n",
            "[397,     4] loss: 0.991\n",
            "[398,     4] loss: 0.995\n",
            "[399,     4] loss: 0.993\n",
            "[400,     4] loss: 0.987\n",
            "[401,     4] loss: 0.982\n",
            "[402,     4] loss: 0.976\n",
            "[403,     4] loss: 0.982\n",
            "[404,     4] loss: 0.989\n",
            "[405,     4] loss: 0.971\n",
            "[406,     4] loss: 0.999\n",
            "[407,     4] loss: 0.980\n",
            "[408,     4] loss: 0.982\n",
            "[409,     4] loss: 0.975\n",
            "[410,     4] loss: 0.983\n",
            "[411,     4] loss: 0.982\n",
            "[412,     4] loss: 0.991\n",
            "[413,     4] loss: 0.981\n",
            "[414,     4] loss: 0.976\n",
            "[415,     4] loss: 0.975\n",
            "[416,     4] loss: 0.989\n",
            "[417,     4] loss: 0.992\n",
            "[418,     4] loss: 0.980\n",
            "[419,     4] loss: 0.986\n",
            "[420,     4] loss: 0.986\n",
            "[421,     4] loss: 0.984\n",
            "[422,     4] loss: 0.987\n",
            "[423,     4] loss: 0.987\n",
            "[424,     4] loss: 0.993\n",
            "[425,     4] loss: 0.980\n",
            "[426,     4] loss: 0.988\n",
            "[427,     4] loss: 0.973\n",
            "[428,     4] loss: 0.977\n",
            "[429,     4] loss: 0.977\n",
            "[430,     4] loss: 0.982\n",
            "[431,     4] loss: 0.987\n",
            "[432,     4] loss: 0.967\n",
            "[433,     4] loss: 0.972\n",
            "[434,     4] loss: 0.988\n",
            "[435,     4] loss: 0.967\n",
            "[436,     4] loss: 0.982\n",
            "[437,     4] loss: 0.980\n",
            "[438,     4] loss: 0.994\n",
            "[439,     4] loss: 0.987\n",
            "[440,     4] loss: 0.973\n",
            "[441,     4] loss: 0.980\n",
            "[442,     4] loss: 0.976\n",
            "[443,     4] loss: 0.980\n",
            "[444,     4] loss: 0.978\n",
            "[445,     4] loss: 0.975\n",
            "[446,     4] loss: 0.980\n",
            "[447,     4] loss: 0.970\n",
            "[448,     4] loss: 0.989\n",
            "[449,     4] loss: 0.995\n",
            "[450,     4] loss: 0.984\n",
            "[451,     4] loss: 0.989\n",
            "[452,     4] loss: 0.993\n",
            "[453,     4] loss: 0.994\n",
            "[454,     4] loss: 0.992\n",
            "[455,     4] loss: 0.997\n",
            "[456,     4] loss: 0.993\n",
            "[457,     4] loss: 0.977\n",
            "[458,     4] loss: 0.992\n",
            "[459,     4] loss: 0.995\n",
            "[460,     4] loss: 0.979\n",
            "[461,     4] loss: 0.982\n",
            "[462,     4] loss: 0.966\n",
            "[463,     4] loss: 0.988\n",
            "[464,     4] loss: 0.988\n",
            "[465,     4] loss: 0.988\n",
            "[466,     4] loss: 0.975\n",
            "[467,     4] loss: 0.974\n",
            "[468,     4] loss: 0.974\n",
            "[469,     4] loss: 0.987\n",
            "[470,     4] loss: 0.977\n",
            "[471,     4] loss: 0.994\n",
            "[472,     4] loss: 0.977\n",
            "[473,     4] loss: 0.981\n",
            "[474,     4] loss: 0.982\n",
            "[475,     4] loss: 0.979\n",
            "[476,     4] loss: 0.970\n",
            "[477,     4] loss: 0.969\n",
            "[478,     4] loss: 0.971\n",
            "[479,     4] loss: 0.990\n",
            "[480,     4] loss: 0.959\n",
            "[481,     4] loss: 0.984\n",
            "[482,     4] loss: 0.972\n",
            "[483,     4] loss: 0.980\n",
            "[484,     4] loss: 0.976\n",
            "[485,     4] loss: 0.977\n",
            "[486,     4] loss: 0.977\n",
            "[487,     4] loss: 0.968\n",
            "[488,     4] loss: 0.974\n",
            "[489,     4] loss: 0.990\n",
            "[490,     4] loss: 0.976\n",
            "[491,     4] loss: 0.960\n",
            "[492,     4] loss: 0.985\n",
            "[493,     4] loss: 0.979\n",
            "[494,     4] loss: 0.978\n",
            "[495,     4] loss: 0.972\n",
            "[496,     4] loss: 0.970\n",
            "[497,     4] loss: 0.996\n",
            "[498,     4] loss: 0.971\n",
            "[499,     4] loss: 0.975\n",
            "[500,     4] loss: 0.964\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 50 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 50 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 55 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 59 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 67 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 68 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,     4] loss: 1.076\n",
            "[2,     4] loss: 0.900\n",
            "[3,     4] loss: 0.870\n",
            "[4,     4] loss: 0.868\n",
            "[5,     4] loss: 0.861\n",
            "[6,     4] loss: 0.867\n",
            "[7,     4] loss: 0.867\n",
            "[8,     4] loss: 0.880\n",
            "[9,     4] loss: 0.876\n",
            "[10,     4] loss: 0.871\n",
            "[11,     4] loss: 0.845\n",
            "[12,     4] loss: 0.859\n",
            "[13,     4] loss: 0.852\n",
            "[14,     4] loss: 0.861\n",
            "[15,     4] loss: 0.869\n",
            "[16,     4] loss: 0.853\n",
            "[17,     4] loss: 0.858\n",
            "[18,     4] loss: 0.863\n",
            "[19,     4] loss: 0.857\n",
            "[20,     4] loss: 0.861\n",
            "[21,     4] loss: 0.845\n",
            "[22,     4] loss: 0.860\n",
            "[23,     4] loss: 0.850\n",
            "[24,     4] loss: 0.870\n",
            "[25,     4] loss: 0.867\n",
            "[26,     4] loss: 0.861\n",
            "[27,     4] loss: 0.862\n",
            "[28,     4] loss: 0.863\n",
            "[29,     4] loss: 0.854\n",
            "[30,     4] loss: 0.869\n",
            "[31,     4] loss: 0.862\n",
            "[32,     4] loss: 0.855\n",
            "[33,     4] loss: 0.868\n",
            "[34,     4] loss: 0.851\n",
            "[35,     4] loss: 0.863\n",
            "[36,     4] loss: 0.856\n",
            "[37,     4] loss: 0.860\n",
            "[38,     4] loss: 0.861\n",
            "[39,     4] loss: 0.861\n",
            "[40,     4] loss: 0.858\n",
            "[41,     4] loss: 0.833\n",
            "[42,     4] loss: 0.858\n",
            "[43,     4] loss: 0.868\n",
            "[44,     4] loss: 0.850\n",
            "[45,     4] loss: 0.851\n",
            "[46,     4] loss: 0.848\n",
            "[47,     4] loss: 0.847\n",
            "[48,     4] loss: 0.869\n",
            "[49,     4] loss: 0.855\n",
            "[50,     4] loss: 0.871\n",
            "[51,     4] loss: 0.864\n",
            "[52,     4] loss: 0.839\n",
            "[53,     4] loss: 0.852\n",
            "[54,     4] loss: 0.877\n",
            "[55,     4] loss: 0.871\n",
            "[56,     4] loss: 0.853\n",
            "[57,     4] loss: 0.877\n",
            "[58,     4] loss: 0.857\n",
            "[59,     4] loss: 0.862\n",
            "[60,     4] loss: 0.856\n",
            "[61,     4] loss: 0.847\n",
            "[62,     4] loss: 0.835\n",
            "[63,     4] loss: 0.872\n",
            "[64,     4] loss: 0.866\n",
            "[65,     4] loss: 0.872\n",
            "[66,     4] loss: 0.871\n",
            "[67,     4] loss: 0.876\n",
            "[68,     4] loss: 0.870\n",
            "[69,     4] loss: 0.869\n",
            "[70,     4] loss: 0.862\n",
            "[71,     4] loss: 0.845\n",
            "[72,     4] loss: 0.867\n",
            "[73,     4] loss: 0.856\n",
            "[74,     4] loss: 0.855\n",
            "[75,     4] loss: 0.845\n",
            "[76,     4] loss: 0.859\n",
            "[77,     4] loss: 0.857\n",
            "[78,     4] loss: 0.848\n",
            "[79,     4] loss: 0.858\n",
            "[80,     4] loss: 0.852\n",
            "[81,     4] loss: 0.849\n",
            "[82,     4] loss: 0.861\n",
            "[83,     4] loss: 0.868\n",
            "[84,     4] loss: 0.858\n",
            "[85,     4] loss: 0.857\n",
            "[86,     4] loss: 0.853\n",
            "[87,     4] loss: 0.855\n",
            "[88,     4] loss: 0.843\n",
            "[89,     4] loss: 0.866\n",
            "[90,     4] loss: 0.860\n",
            "[91,     4] loss: 0.869\n",
            "[92,     4] loss: 0.858\n",
            "[93,     4] loss: 0.851\n",
            "[94,     4] loss: 0.835\n",
            "[95,     4] loss: 0.861\n",
            "[96,     4] loss: 0.864\n",
            "[97,     4] loss: 0.869\n",
            "[98,     4] loss: 0.841\n",
            "[99,     4] loss: 0.855\n",
            "[100,     4] loss: 0.863\n",
            "[101,     4] loss: 0.841\n",
            "[102,     4] loss: 0.857\n",
            "[103,     4] loss: 0.844\n",
            "[104,     4] loss: 0.850\n",
            "[105,     4] loss: 0.838\n",
            "[106,     4] loss: 0.840\n",
            "[107,     4] loss: 0.847\n",
            "[108,     4] loss: 0.846\n",
            "[109,     4] loss: 0.852\n",
            "[110,     4] loss: 0.847\n",
            "[111,     4] loss: 0.864\n",
            "[112,     4] loss: 0.860\n",
            "[113,     4] loss: 0.866\n",
            "[114,     4] loss: 0.849\n",
            "[115,     4] loss: 0.871\n",
            "[116,     4] loss: 0.857\n",
            "[117,     4] loss: 0.830\n",
            "[118,     4] loss: 0.854\n",
            "[119,     4] loss: 0.851\n",
            "[120,     4] loss: 0.850\n",
            "[121,     4] loss: 0.839\n",
            "[122,     4] loss: 0.857\n",
            "[123,     4] loss: 0.847\n",
            "[124,     4] loss: 0.851\n",
            "[125,     4] loss: 0.834\n",
            "[126,     4] loss: 0.836\n",
            "[127,     4] loss: 0.847\n",
            "[128,     4] loss: 0.868\n",
            "[129,     4] loss: 0.851\n",
            "[130,     4] loss: 0.855\n",
            "[131,     4] loss: 0.839\n",
            "[132,     4] loss: 0.848\n",
            "[133,     4] loss: 0.831\n",
            "[134,     4] loss: 0.855\n",
            "[135,     4] loss: 0.849\n",
            "[136,     4] loss: 0.843\n",
            "[137,     4] loss: 0.849\n",
            "[138,     4] loss: 0.843\n",
            "[139,     4] loss: 0.855\n",
            "[140,     4] loss: 0.850\n",
            "[141,     4] loss: 0.846\n",
            "[142,     4] loss: 0.847\n",
            "[143,     4] loss: 0.845\n",
            "[144,     4] loss: 0.850\n",
            "[145,     4] loss: 0.841\n",
            "[146,     4] loss: 0.859\n",
            "[147,     4] loss: 0.853\n",
            "[148,     4] loss: 0.843\n",
            "[149,     4] loss: 0.857\n",
            "[150,     4] loss: 0.848\n",
            "[151,     4] loss: 0.849\n",
            "[152,     4] loss: 0.850\n",
            "[153,     4] loss: 0.846\n",
            "[154,     4] loss: 0.859\n",
            "[155,     4] loss: 0.860\n",
            "[156,     4] loss: 0.859\n",
            "[157,     4] loss: 0.853\n",
            "[158,     4] loss: 0.835\n",
            "[159,     4] loss: 0.871\n",
            "[160,     4] loss: 0.856\n",
            "[161,     4] loss: 0.853\n",
            "[162,     4] loss: 0.852\n",
            "[163,     4] loss: 0.852\n",
            "[164,     4] loss: 0.850\n",
            "[165,     4] loss: 0.844\n",
            "[166,     4] loss: 0.840\n",
            "[167,     4] loss: 0.861\n",
            "[168,     4] loss: 0.847\n",
            "[169,     4] loss: 0.852\n",
            "[170,     4] loss: 0.847\n",
            "[171,     4] loss: 0.840\n",
            "[172,     4] loss: 0.828\n",
            "[173,     4] loss: 0.852\n",
            "[174,     4] loss: 0.849\n",
            "[175,     4] loss: 0.843\n",
            "[176,     4] loss: 0.847\n",
            "[177,     4] loss: 0.850\n",
            "[178,     4] loss: 0.840\n",
            "[179,     4] loss: 0.844\n",
            "[180,     4] loss: 0.855\n",
            "[181,     4] loss: 0.846\n",
            "[182,     4] loss: 0.855\n",
            "[183,     4] loss: 0.828\n",
            "[184,     4] loss: 0.851\n",
            "[185,     4] loss: 0.858\n",
            "[186,     4] loss: 0.846\n",
            "[187,     4] loss: 0.850\n",
            "[188,     4] loss: 0.833\n",
            "[189,     4] loss: 0.853\n",
            "[190,     4] loss: 0.845\n",
            "[191,     4] loss: 0.861\n",
            "[192,     4] loss: 0.851\n",
            "[193,     4] loss: 0.833\n",
            "[194,     4] loss: 0.832\n",
            "[195,     4] loss: 0.849\n",
            "[196,     4] loss: 0.847\n",
            "[197,     4] loss: 0.845\n",
            "[198,     4] loss: 0.826\n",
            "[199,     4] loss: 0.835\n",
            "[200,     4] loss: 0.842\n",
            "[201,     4] loss: 0.840\n",
            "[202,     4] loss: 0.856\n",
            "[203,     4] loss: 0.851\n",
            "[204,     4] loss: 0.848\n",
            "[205,     4] loss: 0.830\n",
            "[206,     4] loss: 0.834\n",
            "[207,     4] loss: 0.850\n",
            "[208,     4] loss: 0.848\n",
            "[209,     4] loss: 0.847\n",
            "[210,     4] loss: 0.846\n",
            "[211,     4] loss: 0.848\n",
            "[212,     4] loss: 0.867\n",
            "[213,     4] loss: 0.836\n",
            "[214,     4] loss: 0.851\n",
            "[215,     4] loss: 0.863\n",
            "[216,     4] loss: 0.835\n",
            "[217,     4] loss: 0.832\n",
            "[218,     4] loss: 0.852\n",
            "[219,     4] loss: 0.851\n",
            "[220,     4] loss: 0.851\n",
            "[221,     4] loss: 0.846\n",
            "[222,     4] loss: 0.851\n",
            "[223,     4] loss: 0.847\n",
            "[224,     4] loss: 0.848\n",
            "[225,     4] loss: 0.840\n",
            "[226,     4] loss: 0.856\n",
            "[227,     4] loss: 0.853\n",
            "[228,     4] loss: 0.846\n",
            "[229,     4] loss: 0.846\n",
            "[230,     4] loss: 0.839\n",
            "[231,     4] loss: 0.841\n",
            "[232,     4] loss: 0.848\n",
            "[233,     4] loss: 0.830\n",
            "[234,     4] loss: 0.849\n",
            "[235,     4] loss: 0.845\n",
            "[236,     4] loss: 0.841\n",
            "[237,     4] loss: 0.851\n",
            "[238,     4] loss: 0.860\n",
            "[239,     4] loss: 0.842\n",
            "[240,     4] loss: 0.842\n",
            "[241,     4] loss: 0.829\n",
            "[242,     4] loss: 0.844\n",
            "[243,     4] loss: 0.843\n",
            "[244,     4] loss: 0.839\n",
            "[245,     4] loss: 0.832\n",
            "[246,     4] loss: 0.852\n",
            "[247,     4] loss: 0.845\n",
            "[248,     4] loss: 0.818\n",
            "[249,     4] loss: 0.838\n",
            "[250,     4] loss: 0.837\n",
            "[251,     4] loss: 0.843\n",
            "[252,     4] loss: 0.842\n",
            "[253,     4] loss: 0.850\n",
            "[254,     4] loss: 0.839\n",
            "[255,     4] loss: 0.843\n",
            "[256,     4] loss: 0.854\n",
            "[257,     4] loss: 0.851\n",
            "[258,     4] loss: 0.852\n",
            "[259,     4] loss: 0.841\n",
            "[260,     4] loss: 0.855\n",
            "[261,     4] loss: 0.852\n",
            "[262,     4] loss: 0.857\n",
            "[263,     4] loss: 0.843\n",
            "[264,     4] loss: 0.837\n",
            "[265,     4] loss: 0.854\n",
            "[266,     4] loss: 0.850\n",
            "[267,     4] loss: 0.860\n",
            "[268,     4] loss: 0.845\n",
            "[269,     4] loss: 0.845\n",
            "[270,     4] loss: 0.853\n",
            "[271,     4] loss: 0.840\n",
            "[272,     4] loss: 0.833\n",
            "[273,     4] loss: 0.836\n",
            "[274,     4] loss: 0.842\n",
            "[275,     4] loss: 0.844\n",
            "[276,     4] loss: 0.847\n",
            "[277,     4] loss: 0.822\n",
            "[278,     4] loss: 0.833\n",
            "[279,     4] loss: 0.841\n",
            "[280,     4] loss: 0.862\n",
            "[281,     4] loss: 0.855\n",
            "[282,     4] loss: 0.844\n",
            "[283,     4] loss: 0.833\n",
            "[284,     4] loss: 0.846\n",
            "[285,     4] loss: 0.846\n",
            "[286,     4] loss: 0.856\n",
            "[287,     4] loss: 0.853\n",
            "[288,     4] loss: 0.830\n",
            "[289,     4] loss: 0.852\n",
            "[290,     4] loss: 0.843\n",
            "[291,     4] loss: 0.838\n",
            "[292,     4] loss: 0.839\n",
            "[293,     4] loss: 0.839\n",
            "[294,     4] loss: 0.833\n",
            "[295,     4] loss: 0.852\n",
            "[296,     4] loss: 0.836\n",
            "[297,     4] loss: 0.839\n",
            "[298,     4] loss: 0.864\n",
            "[299,     4] loss: 0.846\n",
            "[300,     4] loss: 0.837\n",
            "[301,     4] loss: 0.854\n",
            "[302,     4] loss: 0.849\n",
            "[303,     4] loss: 0.824\n",
            "[304,     4] loss: 0.840\n",
            "[305,     4] loss: 0.838\n",
            "[306,     4] loss: 0.855\n",
            "[307,     4] loss: 0.841\n",
            "[308,     4] loss: 0.842\n",
            "[309,     4] loss: 0.855\n",
            "[310,     4] loss: 0.836\n",
            "[311,     4] loss: 0.830\n",
            "[312,     4] loss: 0.852\n",
            "[313,     4] loss: 0.836\n",
            "[314,     4] loss: 0.842\n",
            "[315,     4] loss: 0.837\n",
            "[316,     4] loss: 0.835\n",
            "[317,     4] loss: 0.830\n",
            "[318,     4] loss: 0.843\n",
            "[319,     4] loss: 0.840\n",
            "[320,     4] loss: 0.848\n",
            "[321,     4] loss: 0.841\n",
            "[322,     4] loss: 0.844\n",
            "[323,     4] loss: 0.847\n",
            "[324,     4] loss: 0.838\n",
            "[325,     4] loss: 0.845\n",
            "[326,     4] loss: 0.854\n",
            "[327,     4] loss: 0.860\n",
            "[328,     4] loss: 0.851\n",
            "[329,     4] loss: 0.852\n",
            "[330,     4] loss: 0.856\n",
            "[331,     4] loss: 0.857\n",
            "[332,     4] loss: 0.832\n",
            "[333,     4] loss: 0.848\n",
            "[334,     4] loss: 0.842\n",
            "[335,     4] loss: 0.835\n",
            "[336,     4] loss: 0.841\n",
            "[337,     4] loss: 0.848\n",
            "[338,     4] loss: 0.834\n",
            "[339,     4] loss: 0.835\n",
            "[340,     4] loss: 0.835\n",
            "[341,     4] loss: 0.850\n",
            "[342,     4] loss: 0.835\n",
            "[343,     4] loss: 0.831\n",
            "[344,     4] loss: 0.828\n",
            "[345,     4] loss: 0.836\n",
            "[346,     4] loss: 0.836\n",
            "[347,     4] loss: 0.835\n",
            "[348,     4] loss: 0.810\n",
            "[349,     4] loss: 0.843\n",
            "[350,     4] loss: 0.846\n",
            "[351,     4] loss: 0.821\n",
            "[352,     4] loss: 0.843\n",
            "[353,     4] loss: 0.849\n",
            "[354,     4] loss: 0.851\n",
            "[355,     4] loss: 0.847\n",
            "[356,     4] loss: 0.854\n",
            "[357,     4] loss: 0.830\n",
            "[358,     4] loss: 0.842\n",
            "[359,     4] loss: 0.834\n",
            "[360,     4] loss: 0.839\n",
            "[361,     4] loss: 0.853\n",
            "[362,     4] loss: 0.841\n",
            "[363,     4] loss: 0.832\n",
            "[364,     4] loss: 0.838\n",
            "[365,     4] loss: 0.842\n",
            "[366,     4] loss: 0.846\n",
            "[367,     4] loss: 0.821\n",
            "[368,     4] loss: 0.849\n",
            "[369,     4] loss: 0.856\n",
            "[370,     4] loss: 0.850\n",
            "[371,     4] loss: 0.836\n",
            "[372,     4] loss: 0.845\n",
            "[373,     4] loss: 0.850\n",
            "[374,     4] loss: 0.830\n",
            "[375,     4] loss: 0.830\n",
            "[376,     4] loss: 0.855\n",
            "[377,     4] loss: 0.843\n",
            "[378,     4] loss: 0.833\n",
            "[379,     4] loss: 0.832\n",
            "[380,     4] loss: 0.832\n",
            "[381,     4] loss: 0.857\n",
            "[382,     4] loss: 0.832\n",
            "[383,     4] loss: 0.841\n",
            "[384,     4] loss: 0.832\n",
            "[385,     4] loss: 0.829\n",
            "[386,     4] loss: 0.840\n",
            "[387,     4] loss: 0.830\n",
            "[388,     4] loss: 0.845\n",
            "[389,     4] loss: 0.814\n",
            "[390,     4] loss: 0.835\n",
            "[391,     4] loss: 0.830\n",
            "[392,     4] loss: 0.833\n",
            "[393,     4] loss: 0.835\n",
            "[394,     4] loss: 0.828\n",
            "[395,     4] loss: 0.856\n",
            "[396,     4] loss: 0.831\n",
            "[397,     4] loss: 0.846\n",
            "[398,     4] loss: 0.849\n",
            "[399,     4] loss: 0.833\n",
            "[400,     4] loss: 0.829\n",
            "[401,     4] loss: 0.804\n",
            "[402,     4] loss: 0.824\n",
            "[403,     4] loss: 0.834\n",
            "[404,     4] loss: 0.834\n",
            "[405,     4] loss: 0.843\n",
            "[406,     4] loss: 0.843\n",
            "[407,     4] loss: 0.852\n",
            "[408,     4] loss: 0.837\n",
            "[409,     4] loss: 0.839\n",
            "[410,     4] loss: 0.816\n",
            "[411,     4] loss: 0.824\n",
            "[412,     4] loss: 0.829\n",
            "[413,     4] loss: 0.845\n",
            "[414,     4] loss: 0.844\n",
            "[415,     4] loss: 0.825\n",
            "[416,     4] loss: 0.852\n",
            "[417,     4] loss: 0.838\n",
            "[418,     4] loss: 0.846\n",
            "[419,     4] loss: 0.821\n",
            "[420,     4] loss: 0.824\n",
            "[421,     4] loss: 0.828\n",
            "[422,     4] loss: 0.823\n",
            "[423,     4] loss: 0.823\n",
            "[424,     4] loss: 0.828\n",
            "[425,     4] loss: 0.830\n",
            "[426,     4] loss: 0.842\n",
            "[427,     4] loss: 0.838\n",
            "[428,     4] loss: 0.832\n",
            "[429,     4] loss: 0.826\n",
            "[430,     4] loss: 0.853\n",
            "[431,     4] loss: 0.842\n",
            "[432,     4] loss: 0.826\n",
            "[433,     4] loss: 0.837\n",
            "[434,     4] loss: 0.822\n",
            "[435,     4] loss: 0.820\n",
            "[436,     4] loss: 0.853\n",
            "[437,     4] loss: 0.848\n",
            "[438,     4] loss: 0.844\n",
            "[439,     4] loss: 0.839\n",
            "[440,     4] loss: 0.842\n",
            "[441,     4] loss: 0.842\n",
            "[442,     4] loss: 0.838\n",
            "[443,     4] loss: 0.842\n",
            "[444,     4] loss: 0.847\n",
            "[445,     4] loss: 0.838\n",
            "[446,     4] loss: 0.830\n",
            "[447,     4] loss: 0.832\n",
            "[448,     4] loss: 0.849\n",
            "[449,     4] loss: 0.821\n",
            "[450,     4] loss: 0.848\n",
            "[451,     4] loss: 0.834\n",
            "[452,     4] loss: 0.844\n",
            "[453,     4] loss: 0.839\n",
            "[454,     4] loss: 0.830\n",
            "[455,     4] loss: 0.841\n",
            "[456,     4] loss: 0.831\n",
            "[457,     4] loss: 0.841\n",
            "[458,     4] loss: 0.829\n",
            "[459,     4] loss: 0.830\n",
            "[460,     4] loss: 0.840\n",
            "[461,     4] loss: 0.855\n",
            "[462,     4] loss: 0.834\n",
            "[463,     4] loss: 0.839\n",
            "[464,     4] loss: 0.831\n",
            "[465,     4] loss: 0.828\n",
            "[466,     4] loss: 0.835\n",
            "[467,     4] loss: 0.827\n",
            "[468,     4] loss: 0.851\n",
            "[469,     4] loss: 0.843\n",
            "[470,     4] loss: 0.841\n",
            "[471,     4] loss: 0.861\n",
            "[472,     4] loss: 0.837\n",
            "[473,     4] loss: 0.831\n",
            "[474,     4] loss: 0.850\n",
            "[475,     4] loss: 0.836\n",
            "[476,     4] loss: 0.843\n",
            "[477,     4] loss: 0.845\n",
            "[478,     4] loss: 0.838\n",
            "[479,     4] loss: 0.827\n",
            "[480,     4] loss: 0.830\n",
            "[481,     4] loss: 0.845\n",
            "[482,     4] loss: 0.844\n",
            "[483,     4] loss: 0.848\n",
            "[484,     4] loss: 0.836\n",
            "[485,     4] loss: 0.823\n",
            "[486,     4] loss: 0.831\n",
            "[487,     4] loss: 0.844\n",
            "[488,     4] loss: 0.833\n",
            "[489,     4] loss: 0.835\n",
            "[490,     4] loss: 0.837\n",
            "[491,     4] loss: 0.830\n",
            "[492,     4] loss: 0.848\n",
            "[493,     4] loss: 0.844\n",
            "[494,     4] loss: 0.858\n",
            "[495,     4] loss: 0.825\n",
            "[496,     4] loss: 0.830\n",
            "[497,     4] loss: 0.841\n",
            "[498,     4] loss: 0.831\n",
            "[499,     4] loss: 0.831\n",
            "[500,     4] loss: 0.834\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 62 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 63 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,     4] loss: 1.125\n",
            "[2,     4] loss: 0.706\n",
            "[3,     4] loss: 0.764\n",
            "[4,     4] loss: 0.700\n",
            "[5,     4] loss: 0.676\n",
            "[6,     4] loss: 0.664\n",
            "[7,     4] loss: 0.652\n",
            "[8,     4] loss: 0.646\n",
            "[9,     4] loss: 0.642\n",
            "[10,     4] loss: 0.667\n",
            "[11,     4] loss: 0.652\n",
            "[12,     4] loss: 0.661\n",
            "[13,     4] loss: 0.662\n",
            "[14,     4] loss: 0.663\n",
            "[15,     4] loss: 0.668\n",
            "[16,     4] loss: 0.647\n",
            "[17,     4] loss: 0.671\n",
            "[18,     4] loss: 0.656\n",
            "[19,     4] loss: 0.645\n",
            "[20,     4] loss: 0.630\n",
            "[21,     4] loss: 0.665\n",
            "[22,     4] loss: 0.646\n",
            "[23,     4] loss: 0.651\n",
            "[24,     4] loss: 0.650\n",
            "[25,     4] loss: 0.650\n",
            "[26,     4] loss: 0.670\n",
            "[27,     4] loss: 0.636\n",
            "[28,     4] loss: 0.655\n",
            "[29,     4] loss: 0.670\n",
            "[30,     4] loss: 0.637\n",
            "[31,     4] loss: 0.656\n",
            "[32,     4] loss: 0.650\n",
            "[33,     4] loss: 0.670\n",
            "[34,     4] loss: 0.657\n",
            "[35,     4] loss: 0.646\n",
            "[36,     4] loss: 0.643\n",
            "[37,     4] loss: 0.667\n",
            "[38,     4] loss: 0.667\n",
            "[39,     4] loss: 0.655\n",
            "[40,     4] loss: 0.649\n",
            "[41,     4] loss: 0.634\n",
            "[42,     4] loss: 0.653\n",
            "[43,     4] loss: 0.654\n",
            "[44,     4] loss: 0.647\n",
            "[45,     4] loss: 0.659\n",
            "[46,     4] loss: 0.653\n",
            "[47,     4] loss: 0.647\n",
            "[48,     4] loss: 0.652\n",
            "[49,     4] loss: 0.647\n",
            "[50,     4] loss: 0.664\n",
            "[51,     4] loss: 0.655\n",
            "[52,     4] loss: 0.642\n",
            "[53,     4] loss: 0.650\n",
            "[54,     4] loss: 0.643\n",
            "[55,     4] loss: 0.658\n",
            "[56,     4] loss: 0.659\n",
            "[57,     4] loss: 0.650\n",
            "[58,     4] loss: 0.643\n",
            "[59,     4] loss: 0.651\n",
            "[60,     4] loss: 0.647\n",
            "[61,     4] loss: 0.647\n",
            "[62,     4] loss: 0.651\n",
            "[63,     4] loss: 0.642\n",
            "[64,     4] loss: 0.641\n",
            "[65,     4] loss: 0.657\n",
            "[66,     4] loss: 0.648\n",
            "[67,     4] loss: 0.659\n",
            "[68,     4] loss: 0.657\n",
            "[69,     4] loss: 0.656\n",
            "[70,     4] loss: 0.651\n",
            "[71,     4] loss: 0.645\n",
            "[72,     4] loss: 0.652\n",
            "[73,     4] loss: 0.638\n",
            "[74,     4] loss: 0.624\n",
            "[75,     4] loss: 0.666\n",
            "[76,     4] loss: 0.646\n",
            "[77,     4] loss: 0.626\n",
            "[78,     4] loss: 0.638\n",
            "[79,     4] loss: 0.648\n",
            "[80,     4] loss: 0.648\n",
            "[81,     4] loss: 0.663\n",
            "[82,     4] loss: 0.667\n",
            "[83,     4] loss: 0.648\n",
            "[84,     4] loss: 0.661\n",
            "[85,     4] loss: 0.655\n",
            "[86,     4] loss: 0.652\n",
            "[87,     4] loss: 0.633\n",
            "[88,     4] loss: 0.657\n",
            "[89,     4] loss: 0.648\n",
            "[90,     4] loss: 0.652\n",
            "[91,     4] loss: 0.640\n",
            "[92,     4] loss: 0.659\n",
            "[93,     4] loss: 0.650\n",
            "[94,     4] loss: 0.643\n",
            "[95,     4] loss: 0.655\n",
            "[96,     4] loss: 0.644\n",
            "[97,     4] loss: 0.663\n",
            "[98,     4] loss: 0.630\n",
            "[99,     4] loss: 0.652\n",
            "[100,     4] loss: 0.639\n",
            "[101,     4] loss: 0.652\n",
            "[102,     4] loss: 0.660\n",
            "[103,     4] loss: 0.642\n",
            "[104,     4] loss: 0.639\n",
            "[105,     4] loss: 0.647\n",
            "[106,     4] loss: 0.655\n",
            "[107,     4] loss: 0.636\n",
            "[108,     4] loss: 0.641\n",
            "[109,     4] loss: 0.641\n",
            "[110,     4] loss: 0.649\n",
            "[111,     4] loss: 0.641\n",
            "[112,     4] loss: 0.656\n",
            "[113,     4] loss: 0.651\n",
            "[114,     4] loss: 0.656\n",
            "[115,     4] loss: 0.654\n",
            "[116,     4] loss: 0.634\n",
            "[117,     4] loss: 0.650\n",
            "[118,     4] loss: 0.643\n",
            "[119,     4] loss: 0.632\n",
            "[120,     4] loss: 0.640\n",
            "[121,     4] loss: 0.658\n",
            "[122,     4] loss: 0.646\n",
            "[123,     4] loss: 0.636\n",
            "[124,     4] loss: 0.635\n",
            "[125,     4] loss: 0.655\n",
            "[126,     4] loss: 0.649\n",
            "[127,     4] loss: 0.650\n",
            "[128,     4] loss: 0.648\n",
            "[129,     4] loss: 0.629\n",
            "[130,     4] loss: 0.636\n",
            "[131,     4] loss: 0.657\n",
            "[132,     4] loss: 0.643\n",
            "[133,     4] loss: 0.629\n",
            "[134,     4] loss: 0.639\n",
            "[135,     4] loss: 0.637\n",
            "[136,     4] loss: 0.639\n",
            "[137,     4] loss: 0.648\n",
            "[138,     4] loss: 0.645\n",
            "[139,     4] loss: 0.647\n",
            "[140,     4] loss: 0.654\n",
            "[141,     4] loss: 0.656\n",
            "[142,     4] loss: 0.655\n",
            "[143,     4] loss: 0.642\n",
            "[144,     4] loss: 0.645\n",
            "[145,     4] loss: 0.630\n",
            "[146,     4] loss: 0.643\n",
            "[147,     4] loss: 0.646\n",
            "[148,     4] loss: 0.662\n",
            "[149,     4] loss: 0.631\n",
            "[150,     4] loss: 0.657\n",
            "[151,     4] loss: 0.643\n",
            "[152,     4] loss: 0.641\n",
            "[153,     4] loss: 0.658\n",
            "[154,     4] loss: 0.647\n",
            "[155,     4] loss: 0.633\n",
            "[156,     4] loss: 0.647\n",
            "[157,     4] loss: 0.643\n",
            "[158,     4] loss: 0.656\n",
            "[159,     4] loss: 0.642\n",
            "[160,     4] loss: 0.655\n",
            "[161,     4] loss: 0.633\n",
            "[162,     4] loss: 0.648\n",
            "[163,     4] loss: 0.660\n",
            "[164,     4] loss: 0.661\n",
            "[165,     4] loss: 0.649\n",
            "[166,     4] loss: 0.640\n",
            "[167,     4] loss: 0.634\n",
            "[168,     4] loss: 0.652\n",
            "[169,     4] loss: 0.638\n",
            "[170,     4] loss: 0.635\n",
            "[171,     4] loss: 0.625\n",
            "[172,     4] loss: 0.642\n",
            "[173,     4] loss: 0.639\n",
            "[174,     4] loss: 0.644\n",
            "[175,     4] loss: 0.647\n",
            "[176,     4] loss: 0.655\n",
            "[177,     4] loss: 0.636\n",
            "[178,     4] loss: 0.644\n",
            "[179,     4] loss: 0.651\n",
            "[180,     4] loss: 0.656\n",
            "[181,     4] loss: 0.646\n",
            "[182,     4] loss: 0.657\n",
            "[183,     4] loss: 0.642\n",
            "[184,     4] loss: 0.652\n",
            "[185,     4] loss: 0.641\n",
            "[186,     4] loss: 0.649\n",
            "[187,     4] loss: 0.639\n",
            "[188,     4] loss: 0.644\n",
            "[189,     4] loss: 0.654\n",
            "[190,     4] loss: 0.647\n",
            "[191,     4] loss: 0.650\n",
            "[192,     4] loss: 0.637\n",
            "[193,     4] loss: 0.655\n",
            "[194,     4] loss: 0.639\n",
            "[195,     4] loss: 0.630\n",
            "[196,     4] loss: 0.646\n",
            "[197,     4] loss: 0.643\n",
            "[198,     4] loss: 0.646\n",
            "[199,     4] loss: 0.645\n",
            "[200,     4] loss: 0.647\n",
            "[201,     4] loss: 0.649\n",
            "[202,     4] loss: 0.642\n",
            "[203,     4] loss: 0.655\n",
            "[204,     4] loss: 0.654\n",
            "[205,     4] loss: 0.661\n",
            "[206,     4] loss: 0.644\n",
            "[207,     4] loss: 0.637\n",
            "[208,     4] loss: 0.645\n",
            "[209,     4] loss: 0.638\n",
            "[210,     4] loss: 0.653\n",
            "[211,     4] loss: 0.656\n",
            "[212,     4] loss: 0.668\n",
            "[213,     4] loss: 0.651\n",
            "[214,     4] loss: 0.634\n",
            "[215,     4] loss: 0.659\n",
            "[216,     4] loss: 0.647\n",
            "[217,     4] loss: 0.656\n",
            "[218,     4] loss: 0.640\n",
            "[219,     4] loss: 0.638\n",
            "[220,     4] loss: 0.640\n",
            "[221,     4] loss: 0.634\n",
            "[222,     4] loss: 0.640\n",
            "[223,     4] loss: 0.644\n",
            "[224,     4] loss: 0.632\n",
            "[225,     4] loss: 0.631\n",
            "[226,     4] loss: 0.646\n",
            "[227,     4] loss: 0.641\n",
            "[228,     4] loss: 0.647\n",
            "[229,     4] loss: 0.642\n",
            "[230,     4] loss: 0.657\n",
            "[231,     4] loss: 0.642\n",
            "[232,     4] loss: 0.656\n",
            "[233,     4] loss: 0.639\n",
            "[234,     4] loss: 0.651\n",
            "[235,     4] loss: 0.649\n",
            "[236,     4] loss: 0.642\n",
            "[237,     4] loss: 0.645\n",
            "[238,     4] loss: 0.634\n",
            "[239,     4] loss: 0.637\n",
            "[240,     4] loss: 0.648\n",
            "[241,     4] loss: 0.638\n",
            "[242,     4] loss: 0.622\n",
            "[243,     4] loss: 0.647\n",
            "[244,     4] loss: 0.652\n",
            "[245,     4] loss: 0.646\n",
            "[246,     4] loss: 0.643\n",
            "[247,     4] loss: 0.641\n",
            "[248,     4] loss: 0.637\n",
            "[249,     4] loss: 0.650\n",
            "[250,     4] loss: 0.647\n",
            "[251,     4] loss: 0.645\n",
            "[252,     4] loss: 0.640\n",
            "[253,     4] loss: 0.637\n",
            "[254,     4] loss: 0.639\n",
            "[255,     4] loss: 0.632\n",
            "[256,     4] loss: 0.635\n",
            "[257,     4] loss: 0.639\n",
            "[258,     4] loss: 0.643\n",
            "[259,     4] loss: 0.634\n",
            "[260,     4] loss: 0.621\n",
            "[261,     4] loss: 0.636\n",
            "[262,     4] loss: 0.629\n",
            "[263,     4] loss: 0.643\n",
            "[264,     4] loss: 0.634\n",
            "[265,     4] loss: 0.653\n",
            "[266,     4] loss: 0.662\n",
            "[267,     4] loss: 0.662\n",
            "[268,     4] loss: 0.643\n",
            "[269,     4] loss: 0.641\n",
            "[270,     4] loss: 0.640\n",
            "[271,     4] loss: 0.634\n",
            "[272,     4] loss: 0.642\n",
            "[273,     4] loss: 0.646\n",
            "[274,     4] loss: 0.645\n",
            "[275,     4] loss: 0.643\n",
            "[276,     4] loss: 0.640\n",
            "[277,     4] loss: 0.634\n",
            "[278,     4] loss: 0.632\n",
            "[279,     4] loss: 0.649\n",
            "[280,     4] loss: 0.648\n",
            "[281,     4] loss: 0.634\n",
            "[282,     4] loss: 0.655\n",
            "[283,     4] loss: 0.630\n",
            "[284,     4] loss: 0.635\n",
            "[285,     4] loss: 0.655\n",
            "[286,     4] loss: 0.634\n",
            "[287,     4] loss: 0.635\n",
            "[288,     4] loss: 0.647\n",
            "[289,     4] loss: 0.656\n",
            "[290,     4] loss: 0.609\n",
            "[291,     4] loss: 0.645\n",
            "[292,     4] loss: 0.638\n",
            "[293,     4] loss: 0.655\n",
            "[294,     4] loss: 0.647\n",
            "[295,     4] loss: 0.627\n",
            "[296,     4] loss: 0.637\n",
            "[297,     4] loss: 0.631\n",
            "[298,     4] loss: 0.645\n",
            "[299,     4] loss: 0.631\n",
            "[300,     4] loss: 0.639\n",
            "[301,     4] loss: 0.634\n",
            "[302,     4] loss: 0.638\n",
            "[303,     4] loss: 0.656\n",
            "[304,     4] loss: 0.641\n",
            "[305,     4] loss: 0.655\n",
            "[306,     4] loss: 0.628\n",
            "[307,     4] loss: 0.626\n",
            "[308,     4] loss: 0.654\n",
            "[309,     4] loss: 0.633\n",
            "[310,     4] loss: 0.646\n",
            "[311,     4] loss: 0.654\n",
            "[312,     4] loss: 0.645\n",
            "[313,     4] loss: 0.637\n",
            "[314,     4] loss: 0.636\n",
            "[315,     4] loss: 0.645\n",
            "[316,     4] loss: 0.639\n",
            "[317,     4] loss: 0.655\n",
            "[318,     4] loss: 0.657\n",
            "[319,     4] loss: 0.639\n",
            "[320,     4] loss: 0.645\n",
            "[321,     4] loss: 0.635\n",
            "[322,     4] loss: 0.631\n",
            "[323,     4] loss: 0.644\n",
            "[324,     4] loss: 0.635\n",
            "[325,     4] loss: 0.666\n",
            "[326,     4] loss: 0.626\n",
            "[327,     4] loss: 0.636\n",
            "[328,     4] loss: 0.644\n",
            "[329,     4] loss: 0.641\n",
            "[330,     4] loss: 0.626\n",
            "[331,     4] loss: 0.640\n",
            "[332,     4] loss: 0.629\n",
            "[333,     4] loss: 0.623\n",
            "[334,     4] loss: 0.650\n",
            "[335,     4] loss: 0.642\n",
            "[336,     4] loss: 0.652\n",
            "[337,     4] loss: 0.631\n",
            "[338,     4] loss: 0.651\n",
            "[339,     4] loss: 0.648\n",
            "[340,     4] loss: 0.661\n",
            "[341,     4] loss: 0.642\n",
            "[342,     4] loss: 0.657\n",
            "[343,     4] loss: 0.648\n",
            "[344,     4] loss: 0.651\n",
            "[345,     4] loss: 0.660\n",
            "[346,     4] loss: 0.632\n",
            "[347,     4] loss: 0.639\n",
            "[348,     4] loss: 0.641\n",
            "[349,     4] loss: 0.645\n",
            "[350,     4] loss: 0.653\n",
            "[351,     4] loss: 0.625\n",
            "[352,     4] loss: 0.661\n",
            "[353,     4] loss: 0.639\n",
            "[354,     4] loss: 0.636\n",
            "[355,     4] loss: 0.650\n",
            "[356,     4] loss: 0.636\n",
            "[357,     4] loss: 0.650\n",
            "[358,     4] loss: 0.641\n",
            "[359,     4] loss: 0.635\n",
            "[360,     4] loss: 0.632\n",
            "[361,     4] loss: 0.643\n",
            "[362,     4] loss: 0.641\n",
            "[363,     4] loss: 0.649\n",
            "[364,     4] loss: 0.639\n",
            "[365,     4] loss: 0.647\n",
            "[366,     4] loss: 0.643\n",
            "[367,     4] loss: 0.641\n",
            "[368,     4] loss: 0.646\n",
            "[369,     4] loss: 0.633\n",
            "[370,     4] loss: 0.634\n",
            "[371,     4] loss: 0.650\n",
            "[372,     4] loss: 0.638\n",
            "[373,     4] loss: 0.651\n",
            "[374,     4] loss: 0.633\n",
            "[375,     4] loss: 0.641\n",
            "[376,     4] loss: 0.644\n",
            "[377,     4] loss: 0.646\n",
            "[378,     4] loss: 0.647\n",
            "[379,     4] loss: 0.629\n",
            "[380,     4] loss: 0.637\n",
            "[381,     4] loss: 0.649\n",
            "[382,     4] loss: 0.633\n",
            "[383,     4] loss: 0.644\n",
            "[384,     4] loss: 0.640\n",
            "[385,     4] loss: 0.643\n",
            "[386,     4] loss: 0.638\n",
            "[387,     4] loss: 0.644\n",
            "[388,     4] loss: 0.631\n",
            "[389,     4] loss: 0.638\n",
            "[390,     4] loss: 0.677\n",
            "[391,     4] loss: 0.651\n",
            "[392,     4] loss: 0.645\n",
            "[393,     4] loss: 0.640\n",
            "[394,     4] loss: 0.651\n",
            "[395,     4] loss: 0.632\n",
            "[396,     4] loss: 0.638\n",
            "[397,     4] loss: 0.640\n",
            "[398,     4] loss: 0.623\n",
            "[399,     4] loss: 0.640\n",
            "[400,     4] loss: 0.629\n",
            "[401,     4] loss: 0.651\n",
            "[402,     4] loss: 0.647\n",
            "[403,     4] loss: 0.637\n",
            "[404,     4] loss: 0.640\n",
            "[405,     4] loss: 0.640\n",
            "[406,     4] loss: 0.636\n",
            "[407,     4] loss: 0.650\n",
            "[408,     4] loss: 0.634\n",
            "[409,     4] loss: 0.651\n",
            "[410,     4] loss: 0.641\n",
            "[411,     4] loss: 0.641\n",
            "[412,     4] loss: 0.638\n",
            "[413,     4] loss: 0.649\n",
            "[414,     4] loss: 0.648\n",
            "[415,     4] loss: 0.645\n",
            "[416,     4] loss: 0.653\n",
            "[417,     4] loss: 0.643\n",
            "[418,     4] loss: 0.646\n",
            "[419,     4] loss: 0.642\n",
            "[420,     4] loss: 0.634\n",
            "[421,     4] loss: 0.639\n",
            "[422,     4] loss: 0.630\n",
            "[423,     4] loss: 0.628\n",
            "[424,     4] loss: 0.636\n",
            "[425,     4] loss: 0.629\n",
            "[426,     4] loss: 0.654\n",
            "[427,     4] loss: 0.633\n",
            "[428,     4] loss: 0.650\n",
            "[429,     4] loss: 0.640\n",
            "[430,     4] loss: 0.653\n",
            "[431,     4] loss: 0.632\n",
            "[432,     4] loss: 0.634\n",
            "[433,     4] loss: 0.644\n",
            "[434,     4] loss: 0.636\n",
            "[435,     4] loss: 0.658\n",
            "[436,     4] loss: 0.647\n",
            "[437,     4] loss: 0.633\n",
            "[438,     4] loss: 0.646\n",
            "[439,     4] loss: 0.641\n",
            "[440,     4] loss: 0.625\n",
            "[441,     4] loss: 0.635\n",
            "[442,     4] loss: 0.639\n",
            "[443,     4] loss: 0.627\n",
            "[444,     4] loss: 0.622\n",
            "[445,     4] loss: 0.634\n",
            "[446,     4] loss: 0.641\n",
            "[447,     4] loss: 0.646\n",
            "[448,     4] loss: 0.622\n",
            "[449,     4] loss: 0.636\n",
            "[450,     4] loss: 0.622\n",
            "[451,     4] loss: 0.632\n",
            "[452,     4] loss: 0.637\n",
            "[453,     4] loss: 0.634\n",
            "[454,     4] loss: 0.629\n",
            "[455,     4] loss: 0.616\n",
            "[456,     4] loss: 0.627\n",
            "[457,     4] loss: 0.633\n",
            "[458,     4] loss: 0.653\n",
            "[459,     4] loss: 0.628\n",
            "[460,     4] loss: 0.642\n",
            "[461,     4] loss: 0.656\n",
            "[462,     4] loss: 0.650\n",
            "[463,     4] loss: 0.627\n",
            "[464,     4] loss: 0.643\n",
            "[465,     4] loss: 0.638\n",
            "[466,     4] loss: 0.630\n",
            "[467,     4] loss: 0.638\n",
            "[468,     4] loss: 0.645\n",
            "[469,     4] loss: 0.641\n",
            "[470,     4] loss: 0.635\n",
            "[471,     4] loss: 0.637\n",
            "[472,     4] loss: 0.626\n",
            "[473,     4] loss: 0.638\n",
            "[474,     4] loss: 0.646\n",
            "[475,     4] loss: 0.634\n",
            "[476,     4] loss: 0.636\n",
            "[477,     4] loss: 0.647\n",
            "[478,     4] loss: 0.639\n",
            "[479,     4] loss: 0.632\n",
            "[480,     4] loss: 0.644\n",
            "[481,     4] loss: 0.638\n",
            "[482,     4] loss: 0.633\n",
            "[483,     4] loss: 0.636\n",
            "[484,     4] loss: 0.640\n",
            "[485,     4] loss: 0.628\n",
            "[486,     4] loss: 0.638\n",
            "[487,     4] loss: 0.635\n",
            "[488,     4] loss: 0.624\n",
            "[489,     4] loss: 0.640\n",
            "[490,     4] loss: 0.624\n",
            "[491,     4] loss: 0.635\n",
            "[492,     4] loss: 0.649\n",
            "[493,     4] loss: 0.655\n",
            "[494,     4] loss: 0.635\n",
            "[495,     4] loss: 0.645\n",
            "[496,     4] loss: 0.643\n",
            "[497,     4] loss: 0.647\n",
            "[498,     4] loss: 0.640\n",
            "[499,     4] loss: 0.657\n",
            "[500,     4] loss: 0.641\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 66 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 65 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,     4] loss: 0.978\n",
            "[2,     4] loss: 0.670\n",
            "[3,     4] loss: 0.561\n",
            "[4,     4] loss: 0.547\n",
            "[5,     4] loss: 0.515\n",
            "[6,     4] loss: 0.543\n",
            "[7,     4] loss: 0.510\n",
            "[8,     4] loss: 0.514\n",
            "[9,     4] loss: 0.516\n",
            "[10,     4] loss: 0.493\n",
            "[11,     4] loss: 0.500\n",
            "[12,     4] loss: 0.505\n",
            "[13,     4] loss: 0.502\n",
            "[14,     4] loss: 0.511\n",
            "[15,     4] loss: 0.510\n",
            "[16,     4] loss: 0.515\n",
            "[17,     4] loss: 0.497\n",
            "[18,     4] loss: 0.516\n",
            "[19,     4] loss: 0.499\n",
            "[20,     4] loss: 0.497\n",
            "[21,     4] loss: 0.499\n",
            "[22,     4] loss: 0.501\n",
            "[23,     4] loss: 0.496\n",
            "[24,     4] loss: 0.500\n",
            "[25,     4] loss: 0.504\n",
            "[26,     4] loss: 0.497\n",
            "[27,     4] loss: 0.501\n",
            "[28,     4] loss: 0.512\n",
            "[29,     4] loss: 0.506\n",
            "[30,     4] loss: 0.504\n",
            "[31,     4] loss: 0.516\n",
            "[32,     4] loss: 0.506\n",
            "[33,     4] loss: 0.500\n",
            "[34,     4] loss: 0.498\n",
            "[35,     4] loss: 0.499\n",
            "[36,     4] loss: 0.504\n",
            "[37,     4] loss: 0.498\n",
            "[38,     4] loss: 0.495\n",
            "[39,     4] loss: 0.502\n",
            "[40,     4] loss: 0.488\n",
            "[41,     4] loss: 0.515\n",
            "[42,     4] loss: 0.506\n",
            "[43,     4] loss: 0.505\n",
            "[44,     4] loss: 0.490\n",
            "[45,     4] loss: 0.501\n",
            "[46,     4] loss: 0.503\n",
            "[47,     4] loss: 0.494\n",
            "[48,     4] loss: 0.490\n",
            "[49,     4] loss: 0.504\n",
            "[50,     4] loss: 0.503\n",
            "[51,     4] loss: 0.505\n",
            "[52,     4] loss: 0.492\n",
            "[53,     4] loss: 0.498\n",
            "[54,     4] loss: 0.503\n",
            "[55,     4] loss: 0.492\n",
            "[56,     4] loss: 0.489\n",
            "[57,     4] loss: 0.488\n",
            "[58,     4] loss: 0.509\n",
            "[59,     4] loss: 0.492\n",
            "[60,     4] loss: 0.490\n",
            "[61,     4] loss: 0.496\n",
            "[62,     4] loss: 0.498\n",
            "[63,     4] loss: 0.480\n",
            "[64,     4] loss: 0.501\n",
            "[65,     4] loss: 0.497\n",
            "[66,     4] loss: 0.506\n",
            "[67,     4] loss: 0.494\n",
            "[68,     4] loss: 0.496\n",
            "[69,     4] loss: 0.487\n",
            "[70,     4] loss: 0.487\n",
            "[71,     4] loss: 0.496\n",
            "[72,     4] loss: 0.507\n",
            "[73,     4] loss: 0.507\n",
            "[74,     4] loss: 0.491\n",
            "[75,     4] loss: 0.491\n",
            "[76,     4] loss: 0.491\n",
            "[77,     4] loss: 0.488\n",
            "[78,     4] loss: 0.502\n",
            "[79,     4] loss: 0.509\n",
            "[80,     4] loss: 0.491\n",
            "[81,     4] loss: 0.492\n",
            "[82,     4] loss: 0.490\n",
            "[83,     4] loss: 0.500\n",
            "[84,     4] loss: 0.499\n",
            "[85,     4] loss: 0.496\n",
            "[86,     4] loss: 0.501\n",
            "[87,     4] loss: 0.500\n",
            "[88,     4] loss: 0.500\n",
            "[89,     4] loss: 0.498\n",
            "[90,     4] loss: 0.499\n",
            "[91,     4] loss: 0.497\n",
            "[92,     4] loss: 0.490\n",
            "[93,     4] loss: 0.494\n",
            "[94,     4] loss: 0.509\n",
            "[95,     4] loss: 0.494\n",
            "[96,     4] loss: 0.498\n",
            "[97,     4] loss: 0.484\n",
            "[98,     4] loss: 0.499\n",
            "[99,     4] loss: 0.494\n",
            "[100,     4] loss: 0.495\n",
            "[101,     4] loss: 0.485\n",
            "[102,     4] loss: 0.494\n",
            "[103,     4] loss: 0.493\n",
            "[104,     4] loss: 0.493\n",
            "[105,     4] loss: 0.479\n",
            "[106,     4] loss: 0.485\n",
            "[107,     4] loss: 0.491\n",
            "[108,     4] loss: 0.493\n",
            "[109,     4] loss: 0.489\n",
            "[110,     4] loss: 0.491\n",
            "[111,     4] loss: 0.488\n",
            "[112,     4] loss: 0.516\n",
            "[113,     4] loss: 0.495\n",
            "[114,     4] loss: 0.508\n",
            "[115,     4] loss: 0.490\n",
            "[116,     4] loss: 0.494\n",
            "[117,     4] loss: 0.485\n",
            "[118,     4] loss: 0.496\n",
            "[119,     4] loss: 0.479\n",
            "[120,     4] loss: 0.487\n",
            "[121,     4] loss: 0.494\n",
            "[122,     4] loss: 0.495\n",
            "[123,     4] loss: 0.494\n",
            "[124,     4] loss: 0.487\n",
            "[125,     4] loss: 0.496\n",
            "[126,     4] loss: 0.487\n",
            "[127,     4] loss: 0.504\n",
            "[128,     4] loss: 0.497\n",
            "[129,     4] loss: 0.495\n",
            "[130,     4] loss: 0.498\n",
            "[131,     4] loss: 0.485\n",
            "[132,     4] loss: 0.483\n",
            "[133,     4] loss: 0.497\n",
            "[134,     4] loss: 0.487\n",
            "[135,     4] loss: 0.475\n",
            "[136,     4] loss: 0.498\n",
            "[137,     4] loss: 0.495\n",
            "[138,     4] loss: 0.507\n",
            "[139,     4] loss: 0.487\n",
            "[140,     4] loss: 0.481\n",
            "[141,     4] loss: 0.490\n",
            "[142,     4] loss: 0.471\n",
            "[143,     4] loss: 0.489\n",
            "[144,     4] loss: 0.496\n",
            "[145,     4] loss: 0.485\n",
            "[146,     4] loss: 0.496\n",
            "[147,     4] loss: 0.489\n",
            "[148,     4] loss: 0.494\n",
            "[149,     4] loss: 0.488\n",
            "[150,     4] loss: 0.501\n",
            "[151,     4] loss: 0.486\n",
            "[152,     4] loss: 0.497\n",
            "[153,     4] loss: 0.493\n",
            "[154,     4] loss: 0.493\n",
            "[155,     4] loss: 0.490\n",
            "[156,     4] loss: 0.480\n",
            "[157,     4] loss: 0.493\n",
            "[158,     4] loss: 0.498\n",
            "[159,     4] loss: 0.496\n",
            "[160,     4] loss: 0.487\n",
            "[161,     4] loss: 0.489\n",
            "[162,     4] loss: 0.477\n",
            "[163,     4] loss: 0.490\n",
            "[164,     4] loss: 0.490\n",
            "[165,     4] loss: 0.490\n",
            "[166,     4] loss: 0.499\n",
            "[167,     4] loss: 0.500\n",
            "[168,     4] loss: 0.484\n",
            "[169,     4] loss: 0.491\n",
            "[170,     4] loss: 0.487\n",
            "[171,     4] loss: 0.488\n",
            "[172,     4] loss: 0.503\n",
            "[173,     4] loss: 0.493\n",
            "[174,     4] loss: 0.473\n",
            "[175,     4] loss: 0.489\n",
            "[176,     4] loss: 0.495\n",
            "[177,     4] loss: 0.485\n",
            "[178,     4] loss: 0.502\n",
            "[179,     4] loss: 0.495\n",
            "[180,     4] loss: 0.491\n",
            "[181,     4] loss: 0.478\n",
            "[182,     4] loss: 0.481\n",
            "[183,     4] loss: 0.484\n",
            "[184,     4] loss: 0.495\n",
            "[185,     4] loss: 0.488\n",
            "[186,     4] loss: 0.492\n",
            "[187,     4] loss: 0.487\n",
            "[188,     4] loss: 0.501\n",
            "[189,     4] loss: 0.501\n",
            "[190,     4] loss: 0.489\n",
            "[191,     4] loss: 0.493\n",
            "[192,     4] loss: 0.483\n",
            "[193,     4] loss: 0.494\n",
            "[194,     4] loss: 0.491\n",
            "[195,     4] loss: 0.490\n",
            "[196,     4] loss: 0.493\n",
            "[197,     4] loss: 0.486\n",
            "[198,     4] loss: 0.485\n",
            "[199,     4] loss: 0.496\n",
            "[200,     4] loss: 0.491\n",
            "[201,     4] loss: 0.485\n",
            "[202,     4] loss: 0.494\n",
            "[203,     4] loss: 0.489\n",
            "[204,     4] loss: 0.493\n",
            "[205,     4] loss: 0.485\n",
            "[206,     4] loss: 0.513\n",
            "[207,     4] loss: 0.490\n",
            "[208,     4] loss: 0.493\n",
            "[209,     4] loss: 0.496\n",
            "[210,     4] loss: 0.487\n",
            "[211,     4] loss: 0.493\n",
            "[212,     4] loss: 0.493\n",
            "[213,     4] loss: 0.494\n",
            "[214,     4] loss: 0.489\n",
            "[215,     4] loss: 0.496\n",
            "[216,     4] loss: 0.502\n",
            "[217,     4] loss: 0.507\n",
            "[218,     4] loss: 0.487\n",
            "[219,     4] loss: 0.495\n",
            "[220,     4] loss: 0.485\n",
            "[221,     4] loss: 0.476\n",
            "[222,     4] loss: 0.483\n",
            "[223,     4] loss: 0.487\n",
            "[224,     4] loss: 0.496\n",
            "[225,     4] loss: 0.502\n",
            "[226,     4] loss: 0.479\n",
            "[227,     4] loss: 0.489\n",
            "[228,     4] loss: 0.493\n",
            "[229,     4] loss: 0.489\n",
            "[230,     4] loss: 0.492\n",
            "[231,     4] loss: 0.499\n",
            "[232,     4] loss: 0.477\n",
            "[233,     4] loss: 0.493\n",
            "[234,     4] loss: 0.498\n",
            "[235,     4] loss: 0.487\n",
            "[236,     4] loss: 0.487\n",
            "[237,     4] loss: 0.492\n",
            "[238,     4] loss: 0.488\n",
            "[239,     4] loss: 0.488\n",
            "[240,     4] loss: 0.503\n",
            "[241,     4] loss: 0.481\n",
            "[242,     4] loss: 0.498\n",
            "[243,     4] loss: 0.478\n",
            "[244,     4] loss: 0.477\n",
            "[245,     4] loss: 0.494\n",
            "[246,     4] loss: 0.486\n",
            "[247,     4] loss: 0.491\n",
            "[248,     4] loss: 0.509\n",
            "[249,     4] loss: 0.485\n",
            "[250,     4] loss: 0.493\n",
            "[251,     4] loss: 0.490\n",
            "[252,     4] loss: 0.483\n",
            "[253,     4] loss: 0.485\n",
            "[254,     4] loss: 0.493\n",
            "[255,     4] loss: 0.491\n",
            "[256,     4] loss: 0.492\n",
            "[257,     4] loss: 0.483\n",
            "[258,     4] loss: 0.500\n",
            "[259,     4] loss: 0.481\n",
            "[260,     4] loss: 0.489\n",
            "[261,     4] loss: 0.489\n",
            "[262,     4] loss: 0.503\n",
            "[263,     4] loss: 0.491\n",
            "[264,     4] loss: 0.481\n",
            "[265,     4] loss: 0.489\n",
            "[266,     4] loss: 0.498\n",
            "[267,     4] loss: 0.493\n",
            "[268,     4] loss: 0.495\n",
            "[269,     4] loss: 0.491\n",
            "[270,     4] loss: 0.495\n",
            "[271,     4] loss: 0.512\n",
            "[272,     4] loss: 0.489\n",
            "[273,     4] loss: 0.503\n",
            "[274,     4] loss: 0.495\n",
            "[275,     4] loss: 0.496\n",
            "[276,     4] loss: 0.492\n",
            "[277,     4] loss: 0.483\n",
            "[278,     4] loss: 0.496\n",
            "[279,     4] loss: 0.484\n",
            "[280,     4] loss: 0.495\n",
            "[281,     4] loss: 0.495\n",
            "[282,     4] loss: 0.481\n",
            "[283,     4] loss: 0.483\n",
            "[284,     4] loss: 0.479\n",
            "[285,     4] loss: 0.486\n",
            "[286,     4] loss: 0.483\n",
            "[287,     4] loss: 0.483\n",
            "[288,     4] loss: 0.488\n",
            "[289,     4] loss: 0.490\n",
            "[290,     4] loss: 0.490\n",
            "[291,     4] loss: 0.472\n",
            "[292,     4] loss: 0.458\n",
            "[293,     4] loss: 0.493\n",
            "[294,     4] loss: 0.490\n",
            "[295,     4] loss: 0.491\n",
            "[296,     4] loss: 0.501\n",
            "[297,     4] loss: 0.500\n",
            "[298,     4] loss: 0.499\n",
            "[299,     4] loss: 0.498\n",
            "[300,     4] loss: 0.490\n",
            "[301,     4] loss: 0.492\n",
            "[302,     4] loss: 0.486\n",
            "[303,     4] loss: 0.495\n",
            "[304,     4] loss: 0.489\n",
            "[305,     4] loss: 0.489\n",
            "[306,     4] loss: 0.490\n",
            "[307,     4] loss: 0.482\n",
            "[308,     4] loss: 0.478\n",
            "[309,     4] loss: 0.504\n",
            "[310,     4] loss: 0.492\n",
            "[311,     4] loss: 0.496\n",
            "[312,     4] loss: 0.490\n",
            "[313,     4] loss: 0.470\n",
            "[314,     4] loss: 0.494\n",
            "[315,     4] loss: 0.494\n",
            "[316,     4] loss: 0.484\n",
            "[317,     4] loss: 0.490\n",
            "[318,     4] loss: 0.503\n",
            "[319,     4] loss: 0.491\n",
            "[320,     4] loss: 0.479\n",
            "[321,     4] loss: 0.507\n",
            "[322,     4] loss: 0.494\n",
            "[323,     4] loss: 0.501\n",
            "[324,     4] loss: 0.495\n",
            "[325,     4] loss: 0.508\n",
            "[326,     4] loss: 0.487\n",
            "[327,     4] loss: 0.489\n",
            "[328,     4] loss: 0.485\n",
            "[329,     4] loss: 0.483\n",
            "[330,     4] loss: 0.488\n",
            "[331,     4] loss: 0.492\n",
            "[332,     4] loss: 0.480\n",
            "[333,     4] loss: 0.487\n",
            "[334,     4] loss: 0.483\n",
            "[335,     4] loss: 0.480\n",
            "[336,     4] loss: 0.487\n",
            "[337,     4] loss: 0.482\n",
            "[338,     4] loss: 0.484\n",
            "[339,     4] loss: 0.482\n",
            "[340,     4] loss: 0.477\n",
            "[341,     4] loss: 0.481\n",
            "[342,     4] loss: 0.484\n",
            "[343,     4] loss: 0.495\n",
            "[344,     4] loss: 0.495\n",
            "[345,     4] loss: 0.489\n",
            "[346,     4] loss: 0.492\n",
            "[347,     4] loss: 0.480\n",
            "[348,     4] loss: 0.470\n",
            "[349,     4] loss: 0.485\n",
            "[350,     4] loss: 0.485\n",
            "[351,     4] loss: 0.488\n",
            "[352,     4] loss: 0.471\n",
            "[353,     4] loss: 0.480\n",
            "[354,     4] loss: 0.479\n",
            "[355,     4] loss: 0.489\n",
            "[356,     4] loss: 0.496\n",
            "[357,     4] loss: 0.489\n",
            "[358,     4] loss: 0.482\n",
            "[359,     4] loss: 0.494\n",
            "[360,     4] loss: 0.484\n",
            "[361,     4] loss: 0.493\n",
            "[362,     4] loss: 0.503\n",
            "[363,     4] loss: 0.494\n",
            "[364,     4] loss: 0.495\n",
            "[365,     4] loss: 0.491\n",
            "[366,     4] loss: 0.495\n",
            "[367,     4] loss: 0.483\n",
            "[368,     4] loss: 0.493\n",
            "[369,     4] loss: 0.489\n",
            "[370,     4] loss: 0.479\n",
            "[371,     4] loss: 0.486\n",
            "[372,     4] loss: 0.487\n",
            "[373,     4] loss: 0.489\n",
            "[374,     4] loss: 0.499\n",
            "[375,     4] loss: 0.492\n",
            "[376,     4] loss: 0.486\n",
            "[377,     4] loss: 0.484\n",
            "[378,     4] loss: 0.479\n",
            "[379,     4] loss: 0.494\n",
            "[380,     4] loss: 0.479\n",
            "[381,     4] loss: 0.489\n",
            "[382,     4] loss: 0.498\n",
            "[383,     4] loss: 0.500\n",
            "[384,     4] loss: 0.495\n",
            "[385,     4] loss: 0.502\n",
            "[386,     4] loss: 0.501\n",
            "[387,     4] loss: 0.480\n",
            "[388,     4] loss: 0.489\n",
            "[389,     4] loss: 0.500\n",
            "[390,     4] loss: 0.478\n",
            "[391,     4] loss: 0.502\n",
            "[392,     4] loss: 0.485\n",
            "[393,     4] loss: 0.491\n",
            "[394,     4] loss: 0.488\n",
            "[395,     4] loss: 0.495\n",
            "[396,     4] loss: 0.479\n",
            "[397,     4] loss: 0.487\n",
            "[398,     4] loss: 0.484\n",
            "[399,     4] loss: 0.470\n",
            "[400,     4] loss: 0.486\n",
            "[401,     4] loss: 0.490\n",
            "[402,     4] loss: 0.493\n",
            "[403,     4] loss: 0.468\n",
            "[404,     4] loss: 0.486\n",
            "[405,     4] loss: 0.485\n",
            "[406,     4] loss: 0.482\n",
            "[407,     4] loss: 0.497\n",
            "[408,     4] loss: 0.485\n",
            "[409,     4] loss: 0.481\n",
            "[410,     4] loss: 0.489\n",
            "[411,     4] loss: 0.489\n",
            "[412,     4] loss: 0.483\n",
            "[413,     4] loss: 0.492\n",
            "[414,     4] loss: 0.466\n",
            "[415,     4] loss: 0.485\n",
            "[416,     4] loss: 0.481\n",
            "[417,     4] loss: 0.489\n",
            "[418,     4] loss: 0.493\n",
            "[419,     4] loss: 0.486\n",
            "[420,     4] loss: 0.489\n",
            "[421,     4] loss: 0.497\n",
            "[422,     4] loss: 0.473\n",
            "[423,     4] loss: 0.488\n",
            "[424,     4] loss: 0.497\n",
            "[425,     4] loss: 0.471\n",
            "[426,     4] loss: 0.492\n",
            "[427,     4] loss: 0.490\n",
            "[428,     4] loss: 0.500\n",
            "[429,     4] loss: 0.486\n",
            "[430,     4] loss: 0.493\n",
            "[431,     4] loss: 0.487\n",
            "[432,     4] loss: 0.494\n",
            "[433,     4] loss: 0.479\n",
            "[434,     4] loss: 0.486\n",
            "[435,     4] loss: 0.490\n",
            "[436,     4] loss: 0.482\n",
            "[437,     4] loss: 0.482\n",
            "[438,     4] loss: 0.491\n",
            "[439,     4] loss: 0.484\n",
            "[440,     4] loss: 0.481\n",
            "[441,     4] loss: 0.493\n",
            "[442,     4] loss: 0.494\n",
            "[443,     4] loss: 0.476\n",
            "[444,     4] loss: 0.487\n",
            "[445,     4] loss: 0.475\n",
            "[446,     4] loss: 0.493\n",
            "[447,     4] loss: 0.489\n",
            "[448,     4] loss: 0.472\n",
            "[449,     4] loss: 0.475\n",
            "[450,     4] loss: 0.485\n",
            "[451,     4] loss: 0.473\n",
            "[452,     4] loss: 0.495\n",
            "[453,     4] loss: 0.492\n",
            "[454,     4] loss: 0.480\n",
            "[455,     4] loss: 0.494\n",
            "[456,     4] loss: 0.480\n",
            "[457,     4] loss: 0.469\n",
            "[458,     4] loss: 0.473\n",
            "[459,     4] loss: 0.471\n",
            "[460,     4] loss: 0.479\n",
            "[461,     4] loss: 0.487\n",
            "[462,     4] loss: 0.486\n",
            "[463,     4] loss: 0.474\n",
            "[464,     4] loss: 0.490\n",
            "[465,     4] loss: 0.484\n",
            "[466,     4] loss: 0.484\n",
            "[467,     4] loss: 0.478\n",
            "[468,     4] loss: 0.484\n",
            "[469,     4] loss: 0.491\n",
            "[470,     4] loss: 0.479\n",
            "[471,     4] loss: 0.487\n",
            "[472,     4] loss: 0.483\n",
            "[473,     4] loss: 0.494\n",
            "[474,     4] loss: 0.492\n",
            "[475,     4] loss: 0.482\n",
            "[476,     4] loss: 0.490\n",
            "[477,     4] loss: 0.495\n",
            "[478,     4] loss: 0.494\n",
            "[479,     4] loss: 0.493\n",
            "[480,     4] loss: 0.479\n",
            "[481,     4] loss: 0.486\n",
            "[482,     4] loss: 0.478\n",
            "[483,     4] loss: 0.484\n",
            "[484,     4] loss: 0.492\n",
            "[485,     4] loss: 0.487\n",
            "[486,     4] loss: 0.484\n",
            "[487,     4] loss: 0.487\n",
            "[488,     4] loss: 0.484\n",
            "[489,     4] loss: 0.492\n",
            "[490,     4] loss: 0.485\n",
            "[491,     4] loss: 0.491\n",
            "[492,     4] loss: 0.491\n",
            "[493,     4] loss: 0.482\n",
            "[494,     4] loss: 0.491\n",
            "[495,     4] loss: 0.499\n",
            "[496,     4] loss: 0.488\n",
            "[497,     4] loss: 0.487\n",
            "[498,     4] loss: 0.500\n",
            "[499,     4] loss: 0.486\n",
            "[500,     4] loss: 0.501\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 55 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 63 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 66 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,     4] loss: 0.874\n",
            "[2,     4] loss: 0.660\n",
            "[3,     4] loss: 0.514\n",
            "[4,     4] loss: 0.489\n",
            "[5,     4] loss: 0.463\n",
            "[6,     4] loss: 0.496\n",
            "[7,     4] loss: 0.470\n",
            "[8,     4] loss: 0.479\n",
            "[9,     4] loss: 0.464\n",
            "[10,     4] loss: 0.460\n",
            "[11,     4] loss: 0.465\n",
            "[12,     4] loss: 0.462\n",
            "[13,     4] loss: 0.464\n",
            "[14,     4] loss: 0.462\n",
            "[15,     4] loss: 0.469\n",
            "[16,     4] loss: 0.455\n",
            "[17,     4] loss: 0.459\n",
            "[18,     4] loss: 0.465\n",
            "[19,     4] loss: 0.455\n",
            "[20,     4] loss: 0.455\n",
            "[21,     4] loss: 0.464\n",
            "[22,     4] loss: 0.454\n",
            "[23,     4] loss: 0.455\n",
            "[24,     4] loss: 0.461\n",
            "[25,     4] loss: 0.462\n",
            "[26,     4] loss: 0.454\n",
            "[27,     4] loss: 0.457\n",
            "[28,     4] loss: 0.458\n",
            "[29,     4] loss: 0.458\n",
            "[30,     4] loss: 0.462\n",
            "[31,     4] loss: 0.456\n",
            "[32,     4] loss: 0.459\n",
            "[33,     4] loss: 0.451\n",
            "[34,     4] loss: 0.454\n",
            "[35,     4] loss: 0.452\n",
            "[36,     4] loss: 0.457\n",
            "[37,     4] loss: 0.462\n",
            "[38,     4] loss: 0.462\n",
            "[39,     4] loss: 0.453\n",
            "[40,     4] loss: 0.453\n",
            "[41,     4] loss: 0.453\n",
            "[42,     4] loss: 0.456\n",
            "[43,     4] loss: 0.457\n",
            "[44,     4] loss: 0.458\n",
            "[45,     4] loss: 0.466\n",
            "[46,     4] loss: 0.458\n",
            "[47,     4] loss: 0.458\n",
            "[48,     4] loss: 0.459\n",
            "[49,     4] loss: 0.460\n",
            "[50,     4] loss: 0.468\n",
            "[51,     4] loss: 0.450\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 63 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 64 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,     4] loss: 0.795\n",
            "[2,     4] loss: 0.587\n",
            "[3,     4] loss: 0.935\n",
            "[4,     4] loss: 0.594\n",
            "[5,     4] loss: 0.769\n",
            "[6,     4] loss: 0.623\n",
            "[7,     4] loss: 0.478\n",
            "[8,     4] loss: 0.497\n",
            "[9,     4] loss: 0.456\n",
            "[10,     4] loss: 0.464\n",
            "[11,     4] loss: 0.481\n",
            "[12,     4] loss: 0.465\n",
            "[13,     4] loss: 0.457\n",
            "[14,     4] loss: 0.451\n",
            "[15,     4] loss: 0.461\n",
            "[16,     4] loss: 0.457\n",
            "[17,     4] loss: 0.465\n",
            "[18,     4] loss: 0.454\n",
            "[19,     4] loss: 0.451\n",
            "[20,     4] loss: 0.453\n",
            "[21,     4] loss: 0.451\n",
            "[22,     4] loss: 0.447\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 56 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 63 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 66 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,     4] loss: 0.744\n",
            "[2,     4] loss: 0.483\n",
            "[3,     4] loss: 0.486\n",
            "[4,     4] loss: 0.459\n",
            "[5,     4] loss: 0.451\n",
            "[6,     4] loss: 0.456\n",
            "[7,     4] loss: 0.447\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 52 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 56 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 60 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 64 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 69 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,     4] loss: 0.702\n",
            "[2,     4] loss: 0.529\n",
            "[3,     4] loss: 0.492\n",
            "[4,     4] loss: 0.473\n",
            "[5,     4] loss: 0.466\n",
            "[6,     4] loss: 0.462\n",
            "[7,     4] loss: 0.457\n",
            "[8,     4] loss: 0.463\n",
            "[9,     4] loss: 0.465\n",
            "[10,     4] loss: 0.456\n",
            "[11,     4] loss: 0.462\n",
            "[12,     4] loss: 0.453\n",
            "[13,     4] loss: 0.460\n",
            "[14,     4] loss: 0.457\n",
            "[15,     4] loss: 0.451\n",
            "[16,     4] loss: 0.461\n",
            "[17,     4] loss: 0.449\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 57 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 62 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 67 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 69 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,     4] loss: 0.758\n",
            "[2,     4] loss: 0.497\n",
            "[3,     4] loss: 0.453\n",
            "[4,     4] loss: 0.467\n",
            "[5,     4] loss: 0.462\n",
            "[6,     4] loss: 0.461\n",
            "[7,     4] loss: 0.460\n",
            "[8,     4] loss: 0.457\n",
            "[9,     4] loss: 0.455\n",
            "[10,     4] loss: 0.459\n",
            "[11,     4] loss: 0.460\n",
            "[12,     4] loss: 0.463\n",
            "[13,     4] loss: 0.467\n",
            "[14,     4] loss: 0.461\n",
            "[15,     4] loss: 0.455\n",
            "[16,     4] loss: 0.460\n",
            "[17,     4] loss: 0.460\n",
            "[18,     4] loss: 0.460\n",
            "[19,     4] loss: 0.457\n",
            "[20,     4] loss: 0.460\n",
            "[21,     4] loss: 0.454\n",
            "[22,     4] loss: 0.463\n",
            "[23,     4] loss: 0.468\n",
            "[24,     4] loss: 0.461\n",
            "[25,     4] loss: 0.464\n",
            "[26,     4] loss: 0.459\n",
            "[27,     4] loss: 0.459\n",
            "[28,     4] loss: 0.455\n",
            "[29,     4] loss: 0.459\n",
            "[30,     4] loss: 0.464\n",
            "[31,     4] loss: 0.461\n",
            "[32,     4] loss: 0.460\n",
            "[33,     4] loss: 0.465\n",
            "[34,     4] loss: 0.464\n",
            "[35,     4] loss: 0.454\n",
            "[36,     4] loss: 0.452\n",
            "[37,     4] loss: 0.454\n",
            "[38,     4] loss: 0.452\n",
            "[39,     4] loss: 0.467\n",
            "[40,     4] loss: 0.463\n",
            "[41,     4] loss: 0.465\n",
            "[42,     4] loss: 0.455\n",
            "[43,     4] loss: 0.463\n",
            "[44,     4] loss: 0.458\n",
            "[45,     4] loss: 0.459\n",
            "[46,     4] loss: 0.460\n",
            "[47,     4] loss: 0.456\n",
            "[48,     4] loss: 0.468\n",
            "[49,     4] loss: 0.459\n",
            "[50,     4] loss: 0.465\n",
            "[51,     4] loss: 0.455\n",
            "[52,     4] loss: 0.455\n",
            "[53,     4] loss: 0.453\n",
            "[54,     4] loss: 0.463\n",
            "[55,     4] loss: 0.461\n",
            "[56,     4] loss: 0.466\n",
            "[57,     4] loss: 0.467\n",
            "[58,     4] loss: 0.467\n",
            "[59,     4] loss: 0.446\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 64 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 66 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 68 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 69 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In76SYH_zZHV"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4HtOHEzZ0E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "596082fc-d455-4a23-93ce-b683c65f3d35"
      },
      "source": [
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe9caac1f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS5D_SpOzj9n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va4L2IZ1zj6o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}