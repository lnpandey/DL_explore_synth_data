{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR zero percentage corrupted.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olq-MKcdmOnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = EfficientNetB0()\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlZDWDZm6ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "term_width = 80\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ubk0aCmSD7",
        "colab_type": "code",
        "outputId": "59aeeaba-7d13-4ce5-fabd-b242e74be25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train_acc =[]\n",
        "test_acc=[]\n",
        "epoch_list=[]\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"train accuracy \", correct/total ,train_loss)\n",
        "    train_acc.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"test accuracy \", correct/total , test_loss)\n",
        "    test_acc.append(test_loss)\n",
        "\n",
        "    # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    epoch_list.append(epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 27499137.57it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "train accuracy  0.38554 656.5246020555496\n",
            "test accuracy  0.4892 140.98082602024078\n",
            "\n",
            "Epoch: 1\n",
            "train accuracy  0.5263 513.7535817623138\n",
            "test accuracy  0.568 120.42589664459229\n",
            "\n",
            "Epoch: 2\n",
            "train accuracy  0.58874 447.461889564991\n",
            "test accuracy  0.6214 104.83154392242432\n",
            "\n",
            "Epoch: 3\n",
            "train accuracy  0.63082 403.41305416822433\n",
            "test accuracy  0.6508 96.67630106210709\n",
            "\n",
            "Epoch: 4\n",
            "train accuracy  0.65732 375.5825608968735\n",
            "test accuracy  0.6752 90.41735810041428\n",
            "\n",
            "Epoch: 5\n",
            "train accuracy  0.68284 350.07959789037704\n",
            "test accuracy  0.6918 88.0678545832634\n",
            "\n",
            "Epoch: 6\n",
            "train accuracy  0.70132 327.93559193611145\n",
            "test accuracy  0.7063 82.19421231746674\n",
            "\n",
            "Epoch: 7\n",
            "train accuracy  0.71894 309.616293489933\n",
            "test accuracy  0.7316 76.17340767383575\n",
            "\n",
            "Epoch: 8\n",
            "train accuracy  0.73398 294.0949584841728\n",
            "test accuracy  0.7531 70.39981245994568\n",
            "\n",
            "Epoch: 9\n",
            "train accuracy  0.7491 277.92842707037926\n",
            "test accuracy  0.7478 72.93433034420013\n",
            "\n",
            "Epoch: 10\n",
            "train accuracy  0.76236 261.37405693531036\n",
            "test accuracy  0.7663 68.1382308602333\n",
            "\n",
            "Epoch: 11\n",
            "train accuracy  0.77542 249.6954048871994\n",
            "test accuracy  0.7807 61.692744225263596\n",
            "\n",
            "Epoch: 12\n",
            "train accuracy  0.7828 239.5005148947239\n",
            "test accuracy  0.7802 64.18733477592468\n",
            "\n",
            "Epoch: 13\n",
            "train accuracy  0.79152 230.49845999479294\n",
            "test accuracy  0.7953 61.21184891462326\n",
            "\n",
            "Epoch: 14\n",
            "train accuracy  0.80124 220.999538064003\n",
            "test accuracy  0.7928 60.55641311407089\n",
            "\n",
            "Epoch: 15\n",
            "train accuracy  0.80706 215.02863931655884\n",
            "test accuracy  0.8046 57.352575838565826\n",
            "\n",
            "Epoch: 16\n",
            "train accuracy  0.81272 207.5327976346016\n",
            "test accuracy  0.7985 58.83781906962395\n",
            "\n",
            "Epoch: 17\n",
            "train accuracy  0.81938 200.04391372203827\n",
            "test accuracy  0.8016 57.7275604903698\n",
            "\n",
            "Epoch: 18\n",
            "train accuracy  0.82602 194.50896418094635\n",
            "test accuracy  0.8222 54.63524678349495\n",
            "\n",
            "Epoch: 19\n",
            "train accuracy  0.83104 187.6017094552517\n",
            "test accuracy  0.8202 53.66879591345787\n",
            "\n",
            "Epoch: 20\n",
            "train accuracy  0.83508 183.7781077027321\n",
            "test accuracy  0.8201 52.83194863796234\n",
            "\n",
            "Epoch: 21\n",
            "train accuracy  0.83852 179.15162506699562\n",
            "test accuracy  0.8119 55.38211312890053\n",
            "\n",
            "Epoch: 22\n",
            "train accuracy  0.83898 176.7667896449566\n",
            "test accuracy  0.8265 50.202002972364426\n",
            "\n",
            "Epoch: 23\n",
            "train accuracy  0.84648 170.66035649180412\n",
            "test accuracy  0.8279 50.82900634407997\n",
            "\n",
            "Epoch: 24\n",
            "train accuracy  0.85004 166.3797078281641\n",
            "test accuracy  0.8321 49.35857507586479\n",
            "\n",
            "Epoch: 25\n",
            "train accuracy  0.8518 165.20453774929047\n",
            "test accuracy  0.8242 52.19617182016373\n",
            "\n",
            "Epoch: 26\n",
            "train accuracy  0.85352 161.84488566219807\n",
            "test accuracy  0.8368 48.171951949596405\n",
            "\n",
            "Epoch: 27\n",
            "train accuracy  0.85816 158.19839414954185\n",
            "test accuracy  0.8278 50.01987197995186\n",
            "\n",
            "Epoch: 28\n",
            "train accuracy  0.86278 154.4009674191475\n",
            "test accuracy  0.8409 46.206819623708725\n",
            "\n",
            "Epoch: 29\n",
            "train accuracy  0.863 152.00514282286167\n",
            "test accuracy  0.8383 46.958861351013184\n",
            "\n",
            "Epoch: 30\n",
            "train accuracy  0.86624 148.60551676154137\n",
            "test accuracy  0.8395 45.65679505467415\n",
            "\n",
            "Epoch: 31\n",
            "train accuracy  0.86628 147.5244238525629\n",
            "test accuracy  0.8397 47.34363481402397\n",
            "\n",
            "Epoch: 32\n",
            "train accuracy  0.86932 144.5054616779089\n",
            "test accuracy  0.8465 46.17263962328434\n",
            "\n",
            "Epoch: 33\n",
            "train accuracy  0.87214 141.88723735511303\n",
            "test accuracy  0.8481 45.372320741415024\n",
            "\n",
            "Epoch: 34\n",
            "train accuracy  0.87378 139.42998333275318\n",
            "test accuracy  0.8449 46.54096323251724\n",
            "\n",
            "Epoch: 35\n",
            "train accuracy  0.87748 136.6262212395668\n",
            "test accuracy  0.8457 46.16631856560707\n",
            "\n",
            "Epoch: 36\n",
            "train accuracy  0.87836 135.11338156461716\n",
            "test accuracy  0.853 43.94162766635418\n",
            "\n",
            "Epoch: 37\n",
            "train accuracy  0.87974 133.61227652430534\n",
            "test accuracy  0.8485 45.28431072831154\n",
            "\n",
            "Epoch: 38\n",
            "train accuracy  0.88184 130.97144976258278\n",
            "test accuracy  0.849 45.3407538831234\n",
            "\n",
            "Epoch: 39\n",
            "train accuracy  0.8803 131.5615707486868\n",
            "test accuracy  0.8405 47.22931022942066\n",
            "\n",
            "Epoch: 40\n",
            "train accuracy  0.88466 127.51606760919094\n",
            "test accuracy  0.8543 44.14876736700535\n",
            "\n",
            "Epoch: 41\n",
            "train accuracy  0.88406 127.10725855827332\n",
            "test accuracy  0.8526 43.7539022564888\n",
            "\n",
            "Epoch: 42\n",
            "train accuracy  0.88708 125.06281992793083\n",
            "test accuracy  0.8484 45.09496533870697\n",
            "\n",
            "Epoch: 43\n",
            "train accuracy  0.88688 124.14449539780617\n",
            "test accuracy  0.8508 45.21156957000494\n",
            "\n",
            "Epoch: 44\n",
            "train accuracy  0.88914 122.30427214503288\n",
            "test accuracy  0.8494 45.932239308953285\n",
            "\n",
            "Epoch: 45\n",
            "train accuracy  0.89098 120.25887994468212\n",
            "test accuracy  0.858 44.9088966101408\n",
            "\n",
            "Epoch: 46\n",
            "train accuracy  0.89132 119.36199693381786\n",
            "test accuracy  0.8457 47.05587485432625\n",
            "\n",
            "Epoch: 47\n",
            "train accuracy  0.89206 118.62305857241154\n",
            "test accuracy  0.855 43.42198495566845\n",
            "\n",
            "Epoch: 48\n",
            "train accuracy  0.89488 114.53350728750229\n",
            "test accuracy  0.8436 47.08754849433899\n",
            "\n",
            "Epoch: 49\n",
            "train accuracy  0.89354 117.2603505551815\n",
            "test accuracy  0.858 44.10934694111347\n",
            "\n",
            "Epoch: 50\n",
            "train accuracy  0.89774 113.2776212990284\n",
            "test accuracy  0.8561 43.688889652490616\n",
            "\n",
            "Epoch: 51\n",
            "train accuracy  0.89602 113.31311206519604\n",
            "test accuracy  0.8584 42.777146369218826\n",
            "\n",
            "Epoch: 52\n",
            "train accuracy  0.89852 112.15843315422535\n",
            "test accuracy  0.8631 41.92259283363819\n",
            "\n",
            "Epoch: 53\n",
            "train accuracy  0.89856 110.11021688580513\n",
            "test accuracy  0.8563 43.661401867866516\n",
            "\n",
            "Epoch: 54\n",
            "train accuracy  0.89946 110.10062563419342\n",
            "test accuracy  0.851 45.18706850707531\n",
            "\n",
            "Epoch: 55\n",
            "train accuracy  0.9014 107.54549251496792\n",
            "test accuracy  0.8541 45.45486083626747\n",
            "\n",
            "Epoch: 56\n",
            "train accuracy  0.89958 108.06852175295353\n",
            "test accuracy  0.86 44.26766085624695\n",
            "\n",
            "Epoch: 57\n",
            "train accuracy  0.89984 109.502515129745\n",
            "test accuracy  0.862 42.040751695632935\n",
            "\n",
            "Epoch: 58\n",
            "train accuracy  0.90634 102.53551588952541\n",
            "test accuracy  0.8627 44.42396095395088\n",
            "\n",
            "Epoch: 59\n",
            "train accuracy  0.9061 105.08703546226025\n",
            "test accuracy  0.8614 43.02260881662369\n",
            "\n",
            "Epoch: 60\n",
            "train accuracy  0.90608 103.10442265868187\n",
            "test accuracy  0.8627 43.233519434928894\n",
            "\n",
            "Epoch: 61\n",
            "train accuracy  0.90902 101.55442370474339\n",
            "test accuracy  0.86 44.623530104756355\n",
            "\n",
            "Epoch: 62\n",
            "train accuracy  0.90844 101.95536331832409\n",
            "test accuracy  0.8644 41.16019496321678\n",
            "\n",
            "Epoch: 63\n",
            "train accuracy  0.9078 100.8928779438138\n",
            "test accuracy  0.8682 41.641105115413666\n",
            "\n",
            "Epoch: 64\n",
            "train accuracy  0.90896 101.12185578793287\n",
            "test accuracy  0.8528 45.72738081216812\n",
            "\n",
            "Epoch: 65\n",
            "train accuracy  0.91144 97.93362874537706\n",
            "test accuracy  0.8643 42.096865087747574\n",
            "\n",
            "Epoch: 66\n",
            "train accuracy  0.90996 99.21303388476372\n",
            "test accuracy  0.8713 40.9863096922636\n",
            "\n",
            "Epoch: 67\n",
            "train accuracy  0.91134 98.19357287883759\n",
            "test accuracy  0.8638 42.645773723721504\n",
            "\n",
            "Epoch: 68\n",
            "train accuracy  0.91162 96.83505104482174\n",
            "test accuracy  0.8673 42.44315084815025\n",
            "\n",
            "Epoch: 69\n",
            "train accuracy  0.9126 96.03203324973583\n",
            "test accuracy  0.8634 42.71247926354408\n",
            "\n",
            "Epoch: 70\n",
            "train accuracy  0.9124 96.3466075733304\n",
            "test accuracy  0.8617 43.134120389819145\n",
            "\n",
            "Epoch: 71\n",
            "train accuracy  0.91294 95.3705113530159\n",
            "test accuracy  0.8602 43.324324905872345\n",
            "\n",
            "Epoch: 72\n",
            "train accuracy  0.91554 93.7198831140995\n",
            "test accuracy  0.8678 41.47783623635769\n",
            "\n",
            "Epoch: 73\n",
            "train accuracy  0.91294 93.87889979034662\n",
            "test accuracy  0.8589 45.614090755581856\n",
            "\n",
            "Epoch: 74\n",
            "train accuracy  0.91536 93.48585633188486\n",
            "test accuracy  0.8585 44.50811141729355\n",
            "\n",
            "Epoch: 75\n",
            "train accuracy  0.91634 92.66585500538349\n",
            "test accuracy  0.8538 46.562871515750885\n",
            "\n",
            "Epoch: 76\n",
            "train accuracy  0.91524 93.9355626180768\n",
            "test accuracy  0.8593 43.49831961095333\n",
            "\n",
            "Epoch: 77\n",
            "train accuracy  0.91794 90.49653249233961\n",
            "test accuracy  0.8733 41.21180172264576\n",
            "\n",
            "Epoch: 78\n",
            "train accuracy  0.91886 89.96544758975506\n",
            "test accuracy  0.8551 46.11314895749092\n",
            "\n",
            "Epoch: 79\n",
            "train accuracy  0.91982 88.40359140187502\n",
            "test accuracy  0.868 42.2308469414711\n",
            "\n",
            "Epoch: 80\n",
            "train accuracy  0.91654 89.74283274263144\n",
            "test accuracy  0.868 41.7348899692297\n",
            "\n",
            "Epoch: 81\n",
            "train accuracy  0.91822 90.49280057102442\n",
            "test accuracy  0.8706 41.897844567894936\n",
            "\n",
            "Epoch: 82\n",
            "train accuracy  0.92008 88.69186042249203\n",
            "test accuracy  0.8587 44.141117721796036\n",
            "\n",
            "Epoch: 83\n",
            "train accuracy  0.91778 90.77668888121843\n",
            "test accuracy  0.8604 45.904811054468155\n",
            "\n",
            "Epoch: 84\n",
            "train accuracy  0.92208 85.3472695723176\n",
            "test accuracy  0.8661 43.34400691092014\n",
            "\n",
            "Epoch: 85\n",
            "train accuracy  0.92024 87.60181630402803\n",
            "test accuracy  0.8656 42.23932434618473\n",
            "\n",
            "Epoch: 86\n",
            "train accuracy  0.91952 87.20628488063812\n",
            "test accuracy  0.8691 41.67847742140293\n",
            "\n",
            "Epoch: 87\n",
            "train accuracy  0.92338 84.37141493707895\n",
            "test accuracy  0.8707 40.185061395168304\n",
            "\n",
            "Epoch: 88\n",
            "train accuracy  0.92018 87.04630842804909\n",
            "test accuracy  0.8742 40.43032348155975\n",
            "\n",
            "Epoch: 89\n",
            "train accuracy  0.92298 84.03293691575527\n",
            "test accuracy  0.8663 43.44573551416397\n",
            "\n",
            "Epoch: 90\n",
            "train accuracy  0.92374 82.75459483265877\n",
            "test accuracy  0.8586 43.56867630779743\n",
            "\n",
            "Epoch: 91\n",
            "train accuracy  0.92166 86.04539898782969\n",
            "test accuracy  0.8706 41.749605253338814\n",
            "\n",
            "Epoch: 92\n",
            "train accuracy  0.92332 84.04895227402449\n",
            "test accuracy  0.8674 41.877177104353905\n",
            "\n",
            "Epoch: 93\n",
            "train accuracy  0.92302 84.06671793758869\n",
            "test accuracy  0.8735 39.92744283378124\n",
            "\n",
            "Epoch: 94\n",
            "train accuracy  0.92454 82.80753052979708\n",
            "test accuracy  0.8732 40.62697333097458\n",
            "\n",
            "Epoch: 95\n",
            "train accuracy  0.92518 81.83166299760342\n",
            "test accuracy  0.8569 46.2450954914093\n",
            "\n",
            "Epoch: 96\n",
            "train accuracy  0.926 81.2143903374672\n",
            "test accuracy  0.8677 42.779220059514046\n",
            "\n",
            "Epoch: 97\n",
            "train accuracy  0.9251 83.18326126784086\n",
            "test accuracy  0.8647 43.158575132489204\n",
            "\n",
            "Epoch: 98\n",
            "train accuracy  0.9266 80.47130987048149\n",
            "test accuracy  0.8685 42.936415046453476\n",
            "\n",
            "Epoch: 99\n",
            "train accuracy  0.92538 81.09152809530497\n",
            "test accuracy  0.8675 43.16490153968334\n",
            "\n",
            "Epoch: 100\n",
            "train accuracy  0.92812 79.29313135147095\n",
            "test accuracy  0.8734 40.12474046647549\n",
            "\n",
            "Epoch: 101\n",
            "train accuracy  0.92666 80.46253888309002\n",
            "test accuracy  0.8561 46.017534494400024\n",
            "\n",
            "Epoch: 102\n",
            "train accuracy  0.92596 80.73987990617752\n",
            "test accuracy  0.8734 40.55310845375061\n",
            "\n",
            "Epoch: 103\n",
            "train accuracy  0.92856 78.01770629733801\n",
            "test accuracy  0.8741 41.23138815164566\n",
            "\n",
            "Epoch: 104\n",
            "train accuracy  0.92794 80.15473701059818\n",
            "test accuracy  0.8698 42.68841856718063\n",
            "\n",
            "Epoch: 105\n",
            "train accuracy  0.92944 77.48830135166645\n",
            "test accuracy  0.8656 42.86988224089146\n",
            "\n",
            "Epoch: 106\n",
            "train accuracy  0.9313 76.50673008710146\n",
            "test accuracy  0.8781 39.662767201662064\n",
            "\n",
            "Epoch: 107\n",
            "train accuracy  0.93124 75.82673036307096\n",
            "test accuracy  0.8658 45.032288789749146\n",
            "\n",
            "Epoch: 108\n",
            "train accuracy  0.93128 75.24437046051025\n",
            "test accuracy  0.8762 40.60737420618534\n",
            "\n",
            "Epoch: 109\n",
            "train accuracy  0.9294 77.9710154980421\n",
            "test accuracy  0.8765 40.5107007920742\n",
            "\n",
            "Epoch: 110\n",
            "train accuracy  0.93026 77.89551945030689\n",
            "test accuracy  0.8789 38.960854917764664\n",
            "\n",
            "Epoch: 111\n",
            "train accuracy  0.93136 75.72203364223242\n",
            "test accuracy  0.868 42.48577970266342\n",
            "\n",
            "Epoch: 112\n",
            "train accuracy  0.93036 77.36748193204403\n",
            "test accuracy  0.8756 41.283645644783974\n",
            "\n",
            "Epoch: 113\n",
            "train accuracy  0.93258 73.45275528728962\n",
            "test accuracy  0.8671 44.54267616569996\n",
            "\n",
            "Epoch: 114\n",
            "train accuracy  0.93172 74.94233170896769\n",
            "test accuracy  0.8673 43.44410653412342\n",
            "\n",
            "Epoch: 115\n",
            "train accuracy  0.93014 76.00707956030965\n",
            "test accuracy  0.8755 41.29559852182865\n",
            "\n",
            "Epoch: 116\n",
            "train accuracy  0.93222 75.89769474416971\n",
            "test accuracy  0.8724 41.255130633711815\n",
            "\n",
            "Epoch: 117\n",
            "train accuracy  0.93266 73.54379109293222\n",
            "test accuracy  0.8812 41.237939581274986\n",
            "\n",
            "Epoch: 118\n",
            "train accuracy  0.93206 74.17839776724577\n",
            "test accuracy  0.8803 38.25934647023678\n",
            "\n",
            "Epoch: 119\n",
            "train accuracy  0.93514 71.66737908124924\n",
            "test accuracy  0.8722 42.61872382462025\n",
            "\n",
            "Epoch: 120\n",
            "train accuracy  0.933 72.50794547051191\n",
            "test accuracy  0.8681 44.323486000299454\n",
            "\n",
            "Epoch: 121\n",
            "train accuracy  0.93252 74.28054744005203\n",
            "test accuracy  0.8694 42.70571797341108\n",
            "\n",
            "Epoch: 122\n",
            "train accuracy  0.93328 72.91284474730492\n",
            "test accuracy  0.8718 43.77181150019169\n",
            "\n",
            "Epoch: 123\n",
            "train accuracy  0.93108 75.14698146283627\n",
            "test accuracy  0.8768 41.11073741316795\n",
            "\n",
            "Epoch: 124\n",
            "train accuracy  0.93356 72.71107334643602\n",
            "test accuracy  0.8729 41.20547491312027\n",
            "\n",
            "Epoch: 125\n",
            "train accuracy  0.93446 71.3850641772151\n",
            "test accuracy  0.8766 40.793938025832176\n",
            "\n",
            "Epoch: 126\n",
            "train accuracy  0.93496 70.5307189822197\n",
            "test accuracy  0.8697 42.48939247429371\n",
            "\n",
            "Epoch: 127\n",
            "train accuracy  0.93582 70.06771206855774\n",
            "test accuracy  0.8802 39.6594322770834\n",
            "\n",
            "Epoch: 128\n",
            "train accuracy  0.93514 70.79996530339122\n",
            "test accuracy  0.8735 41.710534147918224\n",
            "\n",
            "Epoch: 129\n",
            "train accuracy  0.93502 71.18736788630486\n",
            "test accuracy  0.8763 40.936993569135666\n",
            "\n",
            "Epoch: 130\n",
            "train accuracy  0.93612 69.63498098403215\n",
            "test accuracy  0.8765 39.269802659749985\n",
            "\n",
            "Epoch: 131\n",
            "train accuracy  0.93512 70.79665090888739\n",
            "test accuracy  0.8777 39.64044106006622\n",
            "\n",
            "Epoch: 132\n",
            "train accuracy  0.93608 70.73536955565214\n",
            "test accuracy  0.8785 41.94966562092304\n",
            "\n",
            "Epoch: 133\n",
            "train accuracy  0.93694 69.6542286798358\n",
            "test accuracy  0.868 43.0778873115778\n",
            "\n",
            "Epoch: 134\n",
            "train accuracy  0.936 70.30730187892914\n",
            "test accuracy  0.8683 43.95453657209873\n",
            "\n",
            "Epoch: 135\n",
            "train accuracy  0.936 70.63436990231276\n",
            "test accuracy  0.8756 40.42991130053997\n",
            "\n",
            "Epoch: 136\n",
            "train accuracy  0.93776 67.82719185948372\n",
            "test accuracy  0.8837 38.01013006269932\n",
            "\n",
            "Epoch: 137\n",
            "train accuracy  0.93624 69.40615555644035\n",
            "test accuracy  0.8774 40.84740388393402\n",
            "\n",
            "Epoch: 138\n",
            "train accuracy  0.93376 70.96203637868166\n",
            "test accuracy  0.8729 40.89668744802475\n",
            "\n",
            "Epoch: 139\n",
            "train accuracy  0.93988 66.9263916015625\n",
            "test accuracy  0.8725 41.201067730784416\n",
            "\n",
            "Epoch: 140\n",
            "train accuracy  0.93616 69.71791888773441\n",
            "test accuracy  0.8734 40.63268834352493\n",
            "\n",
            "Epoch: 141\n",
            "train accuracy  0.9367 68.19971761107445\n",
            "test accuracy  0.8793 38.78949952125549\n",
            "\n",
            "Epoch: 142\n",
            "train accuracy  0.9405 64.70995021983981\n",
            "test accuracy  0.8723 41.965615928173065\n",
            "\n",
            "Epoch: 143\n",
            "train accuracy  0.93872 67.88557010143995\n",
            "test accuracy  0.8675 44.629400074481964\n",
            "\n",
            "Epoch: 144\n",
            "train accuracy  0.93624 69.72138104587793\n",
            "test accuracy  0.8707 42.61896330118179\n",
            "\n",
            "Epoch: 145\n",
            "train accuracy  0.94072 65.38735843449831\n",
            "test accuracy  0.8779 40.37871818244457\n",
            "\n",
            "Epoch: 146\n",
            "train accuracy  0.93878 67.03071231022477\n",
            "test accuracy  0.8809 38.64846624433994\n",
            "\n",
            "Epoch: 147\n",
            "train accuracy  0.93886 67.08930867910385\n",
            "test accuracy  0.8777 40.9599322527647\n",
            "\n",
            "Epoch: 148\n",
            "train accuracy  0.93822 68.13386321440339\n",
            "test accuracy  0.8825 38.89636144042015\n",
            "\n",
            "Epoch: 149\n",
            "train accuracy  0.93926 65.71446140110493\n",
            "test accuracy  0.8708 44.58385728299618\n",
            "\n",
            "Epoch: 150\n",
            "train accuracy  0.93934 66.71206850931048\n",
            "test accuracy  0.8752 43.69221816956997\n",
            "\n",
            "Epoch: 151\n",
            "train accuracy  0.93976 65.34792172163725\n",
            "test accuracy  0.8864 37.815142169594765\n",
            "\n",
            "Epoch: 152\n",
            "train accuracy  0.94408 62.083930507302284\n",
            "test accuracy  0.8794 40.273697182536125\n",
            "\n",
            "Epoch: 153\n",
            "train accuracy  0.94142 64.93229518458247\n",
            "test accuracy  0.8807 39.56603163480759\n",
            "\n",
            "Epoch: 154\n",
            "train accuracy  0.94036 65.69984458386898\n",
            "test accuracy  0.8831 38.787676841020584\n",
            "\n",
            "Epoch: 155\n",
            "train accuracy  0.94284 62.98937602713704\n",
            "test accuracy  0.8752 42.040306985378265\n",
            "\n",
            "Epoch: 156\n",
            "train accuracy  0.94236 64.6706706956029\n",
            "test accuracy  0.8725 42.40268026292324\n",
            "\n",
            "Epoch: 157\n",
            "train accuracy  0.9391 66.11605085805058\n",
            "test accuracy  0.8744 40.77340418100357\n",
            "\n",
            "Epoch: 158\n",
            "train accuracy  0.94226 63.878481309860945\n",
            "test accuracy  0.88 42.06954425573349\n",
            "\n",
            "Epoch: 159\n",
            "train accuracy  0.94084 65.00281211733818\n",
            "test accuracy  0.8645 45.63064767420292\n",
            "\n",
            "Epoch: 160\n",
            "train accuracy  0.9381 67.36145693436265\n",
            "test accuracy  0.8785 41.139209270477295\n",
            "\n",
            "Epoch: 161\n",
            "train accuracy  0.94214 64.4137449041009\n",
            "test accuracy  0.8696 43.964358538389206\n",
            "\n",
            "Epoch: 162\n",
            "train accuracy  0.94162 64.19941280782223\n",
            "test accuracy  0.8703 42.06188279390335\n",
            "\n",
            "Epoch: 163\n",
            "train accuracy  0.94098 64.28262332081795\n",
            "test accuracy  0.8802 41.39276476204395\n",
            "\n",
            "Epoch: 164\n",
            "train accuracy  0.94166 62.2688824981451\n",
            "test accuracy  0.8788 41.70468567311764\n",
            "\n",
            "Epoch: 165\n",
            "train accuracy  0.94082 63.87880379706621\n",
            "test accuracy  0.887 39.360730051994324\n",
            "\n",
            "Epoch: 166\n",
            "train accuracy  0.94476 61.023670233786106\n",
            "test accuracy  0.8678 44.671384423971176\n",
            "\n",
            "Epoch: 167\n",
            "train accuracy  0.94218 63.52778284624219\n",
            "test accuracy  0.8806 39.292154371738434\n",
            "\n",
            "Epoch: 168\n",
            "train accuracy  0.94036 65.72641743347049\n",
            "test accuracy  0.8716 43.9334374666214\n",
            "\n",
            "Epoch: 169\n",
            "train accuracy  0.94328 62.04982773959637\n",
            "test accuracy  0.8703 43.279535070061684\n",
            "\n",
            "Epoch: 170\n",
            "train accuracy  0.94388 60.7429448440671\n",
            "test accuracy  0.8824 40.319766744971275\n",
            "\n",
            "Epoch: 171\n",
            "train accuracy  0.9441 61.37087258696556\n",
            "test accuracy  0.8818 39.72284137457609\n",
            "\n",
            "Epoch: 172\n",
            "train accuracy  0.94142 62.939611323177814\n",
            "test accuracy  0.8782 41.22550454735756\n",
            "\n",
            "Epoch: 173\n",
            "train accuracy  0.94344 62.23377554118633\n",
            "test accuracy  0.8694 44.2636376619339\n",
            "\n",
            "Epoch: 174\n",
            "train accuracy  0.94612 59.71328330785036\n",
            "test accuracy  0.8699 44.88324452936649\n",
            "\n",
            "Epoch: 175\n",
            "train accuracy  0.9439 60.51026914641261\n",
            "test accuracy  0.8799 40.437990099191666\n",
            "\n",
            "Epoch: 176\n",
            "train accuracy  0.94548 59.69557758793235\n",
            "test accuracy  0.87 44.87040174007416\n",
            "\n",
            "Epoch: 177\n",
            "train accuracy  0.9444 61.11244412511587\n",
            "test accuracy  0.8579 48.40640562772751\n",
            "\n",
            "Epoch: 178\n",
            "train accuracy  0.94484 61.086924627423286\n",
            "test accuracy  0.8775 42.108037158846855\n",
            "\n",
            "Epoch: 179\n",
            "train accuracy  0.94514 60.17786144465208\n",
            "test accuracy  0.8657 43.41190479695797\n",
            "\n",
            "Epoch: 180\n",
            "train accuracy  0.94406 61.56519853323698\n",
            "test accuracy  0.8749 41.42637576162815\n",
            "\n",
            "Epoch: 181\n",
            "train accuracy  0.94412 61.63529822975397\n",
            "test accuracy  0.8813 40.59256035089493\n",
            "\n",
            "Epoch: 182\n",
            "train accuracy  0.94804 58.25960851460695\n",
            "test accuracy  0.876 41.98841109871864\n",
            "\n",
            "Epoch: 183\n",
            "train accuracy  0.94686 58.27389585971832\n",
            "test accuracy  0.8727 44.44570742547512\n",
            "\n",
            "Epoch: 184\n",
            "train accuracy  0.9467 59.115109346807\n",
            "test accuracy  0.8711 43.52525585889816\n",
            "\n",
            "Epoch: 185\n",
            "train accuracy  0.9456 59.90610358864069\n",
            "test accuracy  0.8722 43.64542077481747\n",
            "\n",
            "Epoch: 186\n",
            "train accuracy  0.94634 58.44562269747257\n",
            "test accuracy  0.8714 43.632949247956276\n",
            "\n",
            "Epoch: 187\n",
            "train accuracy  0.94398 60.318418219685555\n",
            "test accuracy  0.8743 44.1426427513361\n",
            "\n",
            "Epoch: 188\n",
            "train accuracy  0.94282 63.145559933036566\n",
            "test accuracy  0.8788 40.697940960526466\n",
            "\n",
            "Epoch: 189\n",
            "train accuracy  0.94598 59.2719765342772\n",
            "test accuracy  0.8759 42.39969524741173\n",
            "\n",
            "Epoch: 190\n",
            "train accuracy  0.94612 59.139248855412006\n",
            "test accuracy  0.8701 43.147782266139984\n",
            "\n",
            "Epoch: 191\n",
            "train accuracy  0.9459 59.857438534498215\n",
            "test accuracy  0.8798 39.743752256035805\n",
            "\n",
            "Epoch: 192\n",
            "train accuracy  0.94516 59.665934424847364\n",
            "test accuracy  0.8744 42.56227898597717\n",
            "\n",
            "Epoch: 193\n",
            "train accuracy  0.94836 58.07297045364976\n",
            "test accuracy  0.8794 41.54774408042431\n",
            "\n",
            "Epoch: 194\n",
            "train accuracy  0.94782 58.807964503765106\n",
            "test accuracy  0.8774 42.29321540892124\n",
            "\n",
            "Epoch: 195\n",
            "train accuracy  0.94688 58.259676706045866\n",
            "test accuracy  0.8859 39.084198996424675\n",
            "\n",
            "Epoch: 196\n",
            "train accuracy  0.947 57.58280476555228\n",
            "test accuracy  0.8748 43.211957171559334\n",
            "\n",
            "Epoch: 197\n",
            "train accuracy  0.94694 58.02828120067716\n",
            "test accuracy  0.8805 40.64839465916157\n",
            "\n",
            "Epoch: 198\n",
            "train accuracy  0.94712 58.2038190625608\n",
            "test accuracy  0.8698 43.76663237810135\n",
            "\n",
            "Epoch: 199\n",
            "train accuracy  0.94768 58.26040348038077\n",
            "test accuracy  0.8812 40.60991194844246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMfVvXNunQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ee0f44ba-d53d-4830-8a2b-e61035b1072f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.plot(epoch_list, train_acc, 'g-' , label='train_acc')\n",
        "plt.plot(epoch_list, test_acc, 'b-', label = 'test_acc')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0QshhJ37f3lbYEGn/mfTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnAtDMtWYm2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "06b0f340-db87-44e2-b60b-7e2dc0f940b8"
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "import random\n",
        "length = len(testset.targets)\n",
        "percentage_corruption = 10\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "print(corrupt_idx)\n",
        "print(len(corrupt_classes))\n",
        "\n",
        "st1 = testset.targets\n",
        "a = np.array(testset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "testset.targets = list(a)\n",
        "st2 = testset.targets\n",
        "\n",
        "print(st1 == st2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "1000 1000\n",
            "[3204, 1561, 1742, 5257, 8878, 5249, 1798, 2633, 4335, 5494, 5091, 4008, 6871, 9, 8653, 3229, 669, 7683, 91, 5637, 8285, 3769, 9551, 9352, 393, 8011, 5873, 2681, 6421, 4267, 6632, 6755, 8009, 8337, 6609, 6359, 4012, 4067, 5564, 6100, 5737, 4447, 3580, 813, 2542, 9370, 5492, 3076, 8782, 2266, 6381, 2777, 4866, 2900, 2619, 6357, 442, 307, 2284, 4194, 1415, 5056, 9736, 7197, 658, 232, 2462, 3655, 6242, 4950, 6710, 7963, 7233, 1530, 1352, 4286, 2484, 2060, 1041, 8369, 2755, 5246, 12, 9961, 9180, 2285, 6721, 8713, 2137, 7841, 891, 3195, 1133, 4756, 4769, 2797, 690, 9085, 5401, 2058, 8406, 1384, 5767, 3773, 9498, 8460, 7576, 6224, 8629, 5857, 3094, 1648, 1802, 6137, 8180, 6365, 9032, 492, 3532, 6905, 153, 1587, 9776, 5335, 9953, 3899, 9432, 6843, 293, 8844, 2554, 9397, 6408, 8636, 7480, 9237, 9389, 290, 7815, 6060, 8238, 225, 889, 9089, 5374, 3092, 8293, 5626, 5941, 7790, 8477, 6946, 2763, 5187, 8818, 9140, 2654, 9098, 6160, 1722, 2338, 8400, 4061, 3636, 3708, 7612, 386, 289, 2702, 790, 6515, 8239, 2309, 5802, 523, 7277, 6411, 5028, 8930, 1629, 8792, 4548, 5588, 4954, 8368, 9406, 3100, 8270, 1881, 2527, 5134, 5677, 5523, 5016, 6350, 575, 7192, 8552, 1213, 9020, 7550, 4988, 2522, 9271, 6246, 1795, 9236, 6615, 8314, 9957, 5762, 2586, 1124, 1924, 2214, 4007, 603, 5119, 1707, 5542, 1833, 1685, 3479, 1773, 2001, 1489, 8199, 2705, 8806, 5110, 7927, 8082, 5061, 385, 7547, 5505, 344, 4526, 1365, 2168, 2141, 5208, 5547, 6102, 2973, 1148, 5965, 9449, 4169, 6219, 7007, 7875, 4734, 3410, 9962, 6980, 727, 9413, 6489, 5782, 7839, 6660, 4743, 6256, 422, 26, 5054, 2183, 8611, 9668, 8939, 9917, 2502, 4948, 1586, 1228, 281, 2279, 9529, 5021, 1235, 2321, 2222, 3660, 9965, 6610, 1839, 3280, 7035, 2666, 8737, 8167, 4646, 1947, 3742, 2334, 13, 5529, 8028, 801, 4836, 8426, 7455, 4591, 3218, 5994, 1914, 3236, 1567, 4813, 9735, 9365, 3144, 5745, 257, 9466, 2578, 24, 1038, 2516, 9256, 1893, 6169, 6310, 5470, 5252, 2256, 4927, 2305, 4814, 5213, 1354, 2717, 1968, 4434, 2365, 4287, 7142, 251, 6455, 2941, 5251, 1925, 1894, 3070, 6338, 4420, 1593, 5485, 7298, 6594, 3440, 59, 14, 1312, 3375, 5760, 213, 2870, 8120, 8536, 5752, 9500, 1778, 9781, 2811, 9686, 6787, 6643, 8063, 4139, 5594, 1545, 8995, 174, 4493, 2387, 2700, 9827, 2972, 3078, 459, 9940, 8483, 2771, 9445, 3734, 9555, 7632, 7205, 2821, 1776, 9549, 9257, 6956, 1195, 6393, 1103, 4990, 3765, 2378, 5685, 4459, 1897, 6879, 3937, 239, 7645, 7460, 3332, 4543, 7579, 7361, 6896, 1209, 5812, 1605, 6994, 2558, 9319, 392, 141, 4303, 7097, 5567, 7115, 4676, 9773, 3888, 6674, 2638, 4793, 5343, 1151, 1843, 4336, 5522, 3641, 3283, 4760, 9188, 7483, 2461, 6765, 6996, 5669, 6351, 1551, 9448, 5886, 8632, 3526, 4787, 810, 3024, 4474, 1184, 1080, 2273, 3562, 3688, 6208, 7406, 6320, 5146, 1963, 3923, 9394, 5825, 590, 3632, 3520, 1178, 2848, 1571, 1123, 6774, 4559, 3267, 4133, 7259, 7920, 8797, 6949, 8948, 478, 9964, 5041, 454, 2538, 8118, 6170, 6768, 4043, 8237, 6306, 2944, 2210, 3012, 2805, 2575, 6260, 9347, 241, 705, 6486, 6584, 3775, 249, 592, 5642, 4703, 9651, 4220, 5693, 7828, 2368, 1799, 6120, 8912, 2954, 115, 4545, 6694, 5936, 5430, 3082, 5438, 157, 6075, 3539, 9017, 5175, 9907, 4821, 639, 1486, 1533, 5196, 8284, 6152, 5193, 1657, 9308, 3098, 4309, 2216, 8032, 957, 4779, 9527, 9729, 7098, 5986, 2901, 9508, 9716, 5608, 427, 1392, 9995, 3739, 3345, 6204, 1357, 9715, 9404, 4747, 1169, 868, 7385, 9169, 3208, 4200, 5060, 1023, 7421, 6199, 3639, 1154, 1811, 8381, 8978, 7030, 2792, 4522, 5751, 9791, 1780, 5597, 4250, 5479, 2243, 583, 1012, 4360, 8699, 6567, 9835, 8308, 8617, 5189, 747, 5846, 3175, 4720, 7438, 2328, 2852, 5026, 9553, 9495, 6546, 1712, 7309, 9010, 9787, 8393, 31, 2508, 7995, 4204, 2290, 8468, 2306, 4672, 6458, 7294, 9091, 7337, 472, 4353, 6502, 9636, 9493, 5022, 1330, 8694, 3671, 2146, 1016, 3768, 8257, 8297, 1161, 6406, 3391, 9607, 274, 8760, 9617, 6346, 104, 8967, 6927, 9794, 4365, 5190, 4619, 170, 4799, 9059, 3686, 298, 6624, 3982, 7286, 873, 6753, 894, 5937, 5408, 7070, 983, 5019, 4925, 49, 7540, 2172, 7100, 403, 2957, 6834, 260, 4739, 6856, 6828, 5130, 1205, 4966, 6989, 9167, 1425, 2600, 5582, 3514, 194, 4489, 6488, 3941, 9821, 6098, 3789, 2136, 6580, 7502, 9550, 9159, 1572, 6405, 2401, 4389, 5915, 5178, 6428, 7347, 123, 9823, 6398, 1125, 7300, 181, 1857, 7590, 8738, 9910, 7635, 9970, 1873, 8977, 7836, 717, 4999, 2579, 6791, 7570, 4938, 3795, 8129, 5832, 4738, 2264, 176, 457, 1951, 4355, 6923, 3478, 1480, 1943, 7196, 5415, 7032, 6213, 5258, 4678, 9894, 9653, 8783, 9695, 1698, 9765, 7721, 3312, 5884, 4777, 9107, 3953, 3228, 6539, 1625, 9749, 5201, 5271, 9757, 893, 688, 3969, 782, 675, 1834, 5990, 7022, 5444, 8531, 3599, 7688, 1333, 4373, 1399, 5902, 4750, 6110, 7727, 152, 5273, 6823, 783, 5728, 6371, 3401, 8545, 355, 7849, 1794, 7628, 5776, 1703, 974, 275, 3759, 7940, 3892, 3199, 4979, 9101, 3958, 3827, 4050, 5407, 6176, 7340, 5101, 8604, 5346, 1768, 6725, 6440, 1469, 1225, 4871, 4767, 5993, 7978, 8251, 8233, 8016, 4421, 5894, 6589, 396, 9435, 1507, 9680, 5938, 7882, 1705, 9594, 7880, 1592, 4462, 3891, 5173, 1462, 4049, 2526, 8712, 1882, 9809, 9460, 6191, 3606, 8161, 7255, 6015, 3458, 2218, 1146, 329, 2987, 6119, 9866, 6289, 7075, 7270, 4381, 6245, 7423, 7360, 6795, 3206, 2039, 6034, 7122, 8336, 3015, 6321, 6596, 5839, 1553, 1646, 4811, 9560, 5471, 1329, 1081, 4831, 2658, 4025, 8512, 3362, 8594, 7246, 3983, 8924, 2535, 9410, 1084, 8610, 784, 3252, 2388, 8333, 3879, 5163, 524, 3404, 6395, 1407, 6464, 4805, 5666, 9226, 7892, 3384, 7351, 6342, 605, 1233, 8509, 8791, 1056, 1967, 5319, 723, 566, 4497, 887, 4751, 640, 6914, 8168, 106, 1249, 2443, 5967, 8970, 9511, 3342, 6052, 719, 4757, 9036, 1117, 5987, 7592, 2525, 5787, 4453, 406, 8165, 8445, 2918, 554, 9335, 882, 5230, 8500, 2500, 4634, 4590, 2747, 3979, 6136, 9268, 5575, 8183, 6815, 7728, 8577, 6479, 2760, 3446, 9443, 2157, 3910, 9061, 8021, 4481, 902, 3341, 1305, 3651, 7719, 6341, 246, 5689, 428, 4004, 1220]\n",
            "1000\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnU19L9s7x63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "982680a4-5272-4ab8-c34d-bdbfb3ce6ab4"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "global best_acc\n",
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "print(\"test accuracy \", correct/total , test_loss)\n",
        "test_acc.append(test_loss)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.8023 142.66202354431152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OywLNGc707x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}