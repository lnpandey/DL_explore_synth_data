{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR 20 percentage corrupted.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olq-MKcdmOnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = EfficientNetB0()\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlZDWDZm6ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "term_width = 80\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ubk0aCmSD7",
        "colab_type": "code",
        "outputId": "de921648-9f76-4091-b522-f49a7d80c2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "# trainset.targets***************************************************************\n",
        "import random\n",
        "length = len(trainset.targets)\n",
        "percentage_corruption = 20\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "# print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "# print(corrupt_idx)\n",
        "# print(len(corrupt_classes))\n",
        "a = np.array(trainset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "trainset.targets = list(a)\n",
        "#**********************************************************************************\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train_acc =[]\n",
        "test_acc=[]\n",
        "epoch_list=[]\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"train accuracy \", correct/total ,train_loss)\n",
        "    train_acc.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"test accuracy \", correct/total , test_loss)\n",
        "    test_acc.append(test_loss)\n",
        "\n",
        "    # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+220):\n",
        "    epoch_list.append(epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "train accuracy  0.29802 778.6562894582748\n",
            "test accuracy  0.4448 156.5143746137619\n",
            "\n",
            "Epoch: 1\n",
            "train accuracy  0.38938 706.4438925981522\n",
            "test accuracy  0.5032 142.58235275745392\n",
            "\n",
            "Epoch: 2\n",
            "train accuracy  0.43278 672.9813531637192\n",
            "test accuracy  0.5426 134.4097055196762\n",
            "\n",
            "Epoch: 3\n",
            "train accuracy  0.46724 645.5228077173233\n",
            "test accuracy  0.5926 121.41951024532318\n",
            "\n",
            "Epoch: 4\n",
            "train accuracy  0.49562 622.58396089077\n",
            "test accuracy  0.6145 116.21778452396393\n",
            "\n",
            "Epoch: 5\n",
            "train accuracy  0.51644 604.7235660552979\n",
            "test accuracy  0.6479 110.45844340324402\n",
            "\n",
            "Epoch: 6\n",
            "train accuracy  0.53446 587.3899703025818\n",
            "test accuracy  0.6531 107.57328015565872\n",
            "\n",
            "Epoch: 7\n",
            "train accuracy  0.55288 572.6322139501572\n",
            "test accuracy  0.6961 95.78554481267929\n",
            "\n",
            "Epoch: 8\n",
            "train accuracy  0.56544 559.1094213724136\n",
            "test accuracy  0.703 96.04043424129486\n",
            "\n",
            "Epoch: 9\n",
            "train accuracy  0.58036 547.2590819001198\n",
            "test accuracy  0.7048 93.03565561771393\n",
            "\n",
            "Epoch: 10\n",
            "train accuracy  0.59096 539.7793312072754\n",
            "test accuracy  0.7226 91.29488772153854\n",
            "\n",
            "Epoch: 11\n",
            "train accuracy  0.6011 530.4256780743599\n",
            "test accuracy  0.7347 87.83632618188858\n",
            "\n",
            "Epoch: 12\n",
            "train accuracy  0.60992 522.5238180160522\n",
            "test accuracy  0.7494 84.56604212522507\n",
            "\n",
            "Epoch: 13\n",
            "train accuracy  0.61712 514.8602415323257\n",
            "test accuracy  0.7315 88.46434813737869\n",
            "\n",
            "Epoch: 14\n",
            "train accuracy  0.6224 509.3577325940132\n",
            "test accuracy  0.756 83.6775974035263\n",
            "\n",
            "Epoch: 15\n",
            "train accuracy  0.6302 504.1766158938408\n",
            "test accuracy  0.7593 80.21845555305481\n",
            "\n",
            "Epoch: 16\n",
            "train accuracy  0.6333 499.16984325647354\n",
            "test accuracy  0.7645 83.01352393627167\n",
            "\n",
            "Epoch: 17\n",
            "train accuracy  0.64048 494.30007642507553\n",
            "test accuracy  0.7696 80.24676275253296\n",
            "\n",
            "Epoch: 18\n",
            "train accuracy  0.64486 490.3043910264969\n",
            "test accuracy  0.7774 75.8379168510437\n",
            "\n",
            "Epoch: 19\n",
            "train accuracy  0.646 486.19268625974655\n",
            "test accuracy  0.7677 77.89303350448608\n",
            "\n",
            "Epoch: 20\n",
            "train accuracy  0.65152 482.1317953467369\n",
            "test accuracy  0.7879 76.17561829090118\n",
            "\n",
            "Epoch: 21\n",
            "train accuracy  0.656 478.164230465889\n",
            "test accuracy  0.7844 76.48763984441757\n",
            "\n",
            "Epoch: 22\n",
            "train accuracy  0.65762 476.33158779144287\n",
            "test accuracy  0.792 73.75963151454926\n",
            "\n",
            "Epoch: 23\n",
            "train accuracy  0.66298 470.35050761699677\n",
            "test accuracy  0.7904 74.53856140375137\n",
            "\n",
            "Epoch: 24\n",
            "train accuracy  0.66454 469.2679722905159\n",
            "test accuracy  0.7921 72.62526178359985\n",
            "\n",
            "Epoch: 25\n",
            "train accuracy  0.66564 466.8661931157112\n",
            "test accuracy  0.7813 75.60397517681122\n",
            "\n",
            "Epoch: 26\n",
            "train accuracy  0.66882 463.8874571919441\n",
            "test accuracy  0.7997 74.54924511909485\n",
            "\n",
            "Epoch: 27\n",
            "train accuracy  0.67256 459.166029214859\n",
            "test accuracy  0.7836 75.25187623500824\n",
            "\n",
            "Epoch: 28\n",
            "train accuracy  0.6751 456.96075171232224\n",
            "test accuracy  0.7955 74.58751040697098\n",
            "\n",
            "Epoch: 29\n",
            "train accuracy  0.67732 454.87355703115463\n",
            "test accuracy  0.7987 68.54861164093018\n",
            "\n",
            "Epoch: 30\n",
            "train accuracy  0.67956 452.9459538459778\n",
            "test accuracy  0.8043 69.4001213312149\n",
            "\n",
            "Epoch: 31\n",
            "train accuracy  0.68614 449.0033676624298\n",
            "test accuracy  0.8031 70.59446412324905\n",
            "\n",
            "Epoch: 32\n",
            "train accuracy  0.68552 447.60417461395264\n",
            "test accuracy  0.7966 70.52842110395432\n",
            "\n",
            "Epoch: 33\n",
            "train accuracy  0.68646 445.10316067934036\n",
            "test accuracy  0.8078 69.92742186784744\n",
            "\n",
            "Epoch: 34\n",
            "train accuracy  0.68912 442.82712519168854\n",
            "test accuracy  0.8022 71.46816152334213\n",
            "\n",
            "Epoch: 35\n",
            "train accuracy  0.69392 440.41718006134033\n",
            "test accuracy  0.8024 71.54620933532715\n",
            "\n",
            "Epoch: 36\n",
            "train accuracy  0.69094 440.4926144480705\n",
            "test accuracy  0.8155 67.21753746271133\n",
            "\n",
            "Epoch: 37\n",
            "train accuracy  0.69512 436.18565142154694\n",
            "test accuracy  0.8153 67.62625658512115\n",
            "\n",
            "Epoch: 38\n",
            "train accuracy  0.69576 434.7899919748306\n",
            "test accuracy  0.8039 71.60483169555664\n",
            "\n",
            "Epoch: 39\n",
            "train accuracy  0.69776 433.3501699566841\n",
            "test accuracy  0.8117 68.41419205069542\n",
            "\n",
            "Epoch: 40\n",
            "train accuracy  0.69774 431.60523664951324\n",
            "test accuracy  0.8161 68.48592603206635\n",
            "\n",
            "Epoch: 41\n",
            "train accuracy  0.70194 428.55744421482086\n",
            "test accuracy  0.8169 67.01393872499466\n",
            "\n",
            "Epoch: 42\n",
            "train accuracy  0.70288 427.4444872736931\n",
            "test accuracy  0.8159 68.96101808547974\n",
            "\n",
            "Epoch: 43\n",
            "train accuracy  0.70078 427.02274030447006\n",
            "test accuracy  0.819 65.34351286292076\n",
            "\n",
            "Epoch: 44\n",
            "train accuracy  0.70322 424.544577896595\n",
            "test accuracy  0.8161 66.6164960861206\n",
            "\n",
            "Epoch: 45\n",
            "train accuracy  0.70572 422.1130726337433\n",
            "test accuracy  0.8229 64.89068275690079\n",
            "\n",
            "Epoch: 46\n",
            "train accuracy  0.70714 421.95495641231537\n",
            "test accuracy  0.8183 65.42648515105247\n",
            "\n",
            "Epoch: 47\n",
            "train accuracy  0.70908 418.2369210124016\n",
            "test accuracy  0.813 68.47120481729507\n",
            "\n",
            "Epoch: 48\n",
            "train accuracy  0.70854 418.08500587940216\n",
            "test accuracy  0.8242 66.15390306711197\n",
            "\n",
            "Epoch: 49\n",
            "train accuracy  0.71276 414.0825420618057\n",
            "test accuracy  0.811 70.16692554950714\n",
            "\n",
            "Epoch: 50\n",
            "train accuracy  0.71186 413.9606971144676\n",
            "test accuracy  0.8085 67.79968303442001\n",
            "\n",
            "Epoch: 51\n",
            "train accuracy  0.71192 412.7453622817993\n",
            "test accuracy  0.8152 67.01140433549881\n",
            "\n",
            "Epoch: 52\n",
            "train accuracy  0.71358 411.42951464653015\n",
            "test accuracy  0.8194 65.39503020048141\n",
            "\n",
            "Epoch: 53\n",
            "train accuracy  0.71496 409.28004479408264\n",
            "test accuracy  0.8151 66.95828306674957\n",
            "\n",
            "Epoch: 54\n",
            "train accuracy  0.7167 407.76013565063477\n",
            "test accuracy  0.8218 65.00307938456535\n",
            "\n",
            "Epoch: 55\n",
            "train accuracy  0.71994 404.93677043914795\n",
            "test accuracy  0.8074 71.40485340356827\n",
            "\n",
            "Epoch: 56\n",
            "train accuracy  0.71764 404.99446201324463\n",
            "test accuracy  0.8272 65.41395342350006\n",
            "\n",
            "Epoch: 57\n",
            "train accuracy  0.71896 404.4507058262825\n",
            "test accuracy  0.8187 67.16117906570435\n",
            "\n",
            "Epoch: 58\n",
            "train accuracy  0.71842 403.9534286260605\n",
            "test accuracy  0.8188 64.31370505690575\n",
            "\n",
            "Epoch: 59\n",
            "train accuracy  0.71848 402.80891728401184\n",
            "test accuracy  0.8223 65.62698617577553\n",
            "\n",
            "Epoch: 60\n",
            "train accuracy  0.71932 401.6234230399132\n",
            "test accuracy  0.8122 68.73757743835449\n",
            "\n",
            "Epoch: 61\n",
            "train accuracy  0.72226 397.6125935316086\n",
            "test accuracy  0.8208 65.46079578995705\n",
            "\n",
            "Epoch: 62\n",
            "train accuracy  0.72268 396.2412380576134\n",
            "test accuracy  0.8165 67.81698769330978\n",
            "\n",
            "Epoch: 63\n",
            "train accuracy  0.72366 394.90790861845016\n",
            "test accuracy  0.8213 67.19718518853188\n",
            "\n",
            "Epoch: 64\n",
            "train accuracy  0.72384 393.522352039814\n",
            "test accuracy  0.8066 69.11355397105217\n",
            "\n",
            "Epoch: 65\n",
            "train accuracy  0.72438 392.7067556977272\n",
            "test accuracy  0.8189 66.8518700003624\n",
            "\n",
            "Epoch: 66\n",
            "train accuracy  0.72508 392.12933617830276\n",
            "test accuracy  0.8237 65.32360807061195\n",
            "\n",
            "Epoch: 67\n",
            "train accuracy  0.72674 389.85971879959106\n",
            "test accuracy  0.8226 66.21746012568474\n",
            "\n",
            "Epoch: 68\n",
            "train accuracy  0.72632 388.8396971821785\n",
            "test accuracy  0.8237 66.56695392727852\n",
            "\n",
            "Epoch: 69\n",
            "train accuracy  0.72986 386.8627910017967\n",
            "test accuracy  0.8088 68.17513784766197\n",
            "\n",
            "Epoch: 70\n",
            "train accuracy  0.72728 386.2577243447304\n",
            "test accuracy  0.8202 66.64603465795517\n",
            "\n",
            "Epoch: 71\n",
            "train accuracy  0.72892 385.77435398101807\n",
            "test accuracy  0.8142 68.94063287973404\n",
            "\n",
            "Epoch: 72\n",
            "train accuracy  0.72922 384.28753370046616\n",
            "test accuracy  0.8209 66.67550253868103\n",
            "\n",
            "Epoch: 73\n",
            "train accuracy  0.73174 380.866434276104\n",
            "test accuracy  0.8189 64.84444570541382\n",
            "\n",
            "Epoch: 74\n",
            "train accuracy  0.73024 382.0601934194565\n",
            "test accuracy  0.8104 70.06449148058891\n",
            "\n",
            "Epoch: 75\n",
            "train accuracy  0.73222 379.55851179361343\n",
            "test accuracy  0.814 69.34120693802834\n",
            "\n",
            "Epoch: 76\n",
            "train accuracy  0.73214 378.27883994579315\n",
            "test accuracy  0.819 65.29421743750572\n",
            "\n",
            "Epoch: 77\n",
            "train accuracy  0.73402 376.3273535370827\n",
            "test accuracy  0.8222 65.12685415148735\n",
            "\n",
            "Epoch: 78\n",
            "train accuracy  0.73382 375.17290049791336\n",
            "test accuracy  0.8227 66.85037499666214\n",
            "\n",
            "Epoch: 79\n",
            "train accuracy  0.7357 371.9911140203476\n",
            "test accuracy  0.8239 65.17278468608856\n",
            "\n",
            "Epoch: 80\n",
            "train accuracy  0.73598 371.87006241083145\n",
            "test accuracy  0.8167 68.80711570382118\n",
            "\n",
            "Epoch: 81\n",
            "train accuracy  0.7358 371.457067489624\n",
            "test accuracy  0.8227 66.18035718798637\n",
            "\n",
            "Epoch: 82\n",
            "train accuracy  0.736 368.83639401197433\n",
            "test accuracy  0.8207 66.26846906542778\n",
            "\n",
            "Epoch: 83\n",
            "train accuracy  0.73658 367.3265435099602\n",
            "test accuracy  0.8207 66.22827517986298\n",
            "\n",
            "Epoch: 84\n",
            "train accuracy  0.73796 366.6370720267296\n",
            "test accuracy  0.8138 66.64517524838448\n",
            "\n",
            "Epoch: 85\n",
            "train accuracy  0.73828 365.4203015565872\n",
            "test accuracy  0.8125 70.4511561691761\n",
            "\n",
            "Epoch: 86\n",
            "train accuracy  0.74024 362.2283263206482\n",
            "test accuracy  0.8194 67.67780822515488\n",
            "\n",
            "Epoch: 87\n",
            "train accuracy  0.74072 362.7185416817665\n",
            "test accuracy  0.8075 70.42701852321625\n",
            "\n",
            "Epoch: 88\n",
            "train accuracy  0.74098 359.64633679389954\n",
            "test accuracy  0.8119 68.27848097681999\n",
            "\n",
            "Epoch: 89\n",
            "train accuracy  0.73946 359.7561529278755\n",
            "test accuracy  0.8222 66.27492848038673\n",
            "\n",
            "Epoch: 90\n",
            "train accuracy  0.74118 356.2181535959244\n",
            "test accuracy  0.8129 68.67285695672035\n",
            "\n",
            "Epoch: 91\n",
            "train accuracy  0.74208 354.70747566223145\n",
            "test accuracy  0.8047 70.36966827511787\n",
            "\n",
            "Epoch: 92\n",
            "train accuracy  0.74296 353.3547149300575\n",
            "test accuracy  0.8092 69.61800187826157\n",
            "\n",
            "Epoch: 93\n",
            "train accuracy  0.74456 351.68205094337463\n",
            "test accuracy  0.82 65.64945185184479\n",
            "\n",
            "Epoch: 94\n",
            "train accuracy  0.74618 349.2698109149933\n",
            "test accuracy  0.8084 69.7996201813221\n",
            "\n",
            "Epoch: 95\n",
            "train accuracy  0.7456 346.67433243989944\n",
            "test accuracy  0.8039 71.12026345729828\n",
            "\n",
            "Epoch: 96\n",
            "train accuracy  0.74576 346.6731659770012\n",
            "test accuracy  0.8048 70.8192731142044\n",
            "\n",
            "Epoch: 97\n",
            "train accuracy  0.74762 345.04105776548386\n",
            "test accuracy  0.8001 71.65470957756042\n",
            "\n",
            "Epoch: 98\n",
            "train accuracy  0.74684 344.12895810604095\n",
            "test accuracy  0.8043 70.33237388730049\n",
            "\n",
            "Epoch: 99\n",
            "train accuracy  0.74892 342.5512171983719\n",
            "test accuracy  0.7921 75.0169529914856\n",
            "\n",
            "Epoch: 100\n",
            "train accuracy  0.75146 338.84480822086334\n",
            "test accuracy  0.8028 69.68004587292671\n",
            "\n",
            "Epoch: 101\n",
            "train accuracy  0.75136 337.55179607868195\n",
            "test accuracy  0.8013 70.53384608030319\n",
            "\n",
            "Epoch: 102\n",
            "train accuracy  0.75226 334.5724457502365\n",
            "test accuracy  0.8047 72.26329991221428\n",
            "\n",
            "Epoch: 103\n",
            "train accuracy  0.7526 334.76715564727783\n",
            "test accuracy  0.7868 74.32245248556137\n",
            "\n",
            "Epoch: 104\n",
            "train accuracy  0.75334 331.0221940279007\n",
            "test accuracy  0.7973 71.54667150974274\n",
            "\n",
            "Epoch: 105\n",
            "train accuracy  0.75362 331.91384959220886\n",
            "test accuracy  0.7963 73.0965167582035\n",
            "\n",
            "Epoch: 106\n",
            "train accuracy  0.75324 329.57310819625854\n",
            "test accuracy  0.782 77.377186357975\n",
            "\n",
            "Epoch: 107\n",
            "train accuracy  0.7549 326.8891277909279\n",
            "test accuracy  0.7866 74.32859736680984\n",
            "\n",
            "Epoch: 108\n",
            "train accuracy  0.7545 327.5688281953335\n",
            "test accuracy  0.799 71.11249443888664\n",
            "\n",
            "Epoch: 109\n",
            "train accuracy  0.75494 324.1075238585472\n",
            "test accuracy  0.7941 71.66332557797432\n",
            "\n",
            "Epoch: 110\n",
            "train accuracy  0.75728 322.58254408836365\n",
            "test accuracy  0.8089 67.5798369050026\n",
            "\n",
            "Epoch: 111\n",
            "train accuracy  0.75684 319.12879550457\n",
            "test accuracy  0.7972 71.30588293075562\n",
            "\n",
            "Epoch: 112\n",
            "train accuracy  0.75652 319.2783297300339\n",
            "test accuracy  0.7933 72.18759968876839\n",
            "\n",
            "Epoch: 113\n",
            "train accuracy  0.75872 317.662496984005\n",
            "test accuracy  0.7935 72.1792197227478\n",
            "\n",
            "Epoch: 114\n",
            "train accuracy  0.75994 314.8229987323284\n",
            "test accuracy  0.7884 74.34605306386948\n",
            "\n",
            "Epoch: 115\n",
            "train accuracy  0.7583 314.4041151404381\n",
            "test accuracy  0.7919 72.40985155105591\n",
            "\n",
            "Epoch: 116\n",
            "train accuracy  0.76266 311.247906178236\n",
            "test accuracy  0.7813 75.14942273497581\n",
            "\n",
            "Epoch: 117\n",
            "train accuracy  0.7645 309.5762879252434\n",
            "test accuracy  0.786 74.47192642092705\n",
            "\n",
            "Epoch: 118\n",
            "train accuracy  0.76468 306.40500593185425\n",
            "test accuracy  0.782 76.14331337809563\n",
            "\n",
            "Epoch: 119\n",
            "train accuracy  0.76468 306.05850833654404\n",
            "test accuracy  0.8022 69.64225471019745\n",
            "\n",
            "Epoch: 120\n",
            "train accuracy  0.76548 304.3611422777176\n",
            "test accuracy  0.7879 72.84985798597336\n",
            "\n",
            "Epoch: 121\n",
            "train accuracy  0.76542 301.41856199502945\n",
            "test accuracy  0.7945 71.99183359742165\n",
            "\n",
            "Epoch: 122\n",
            "train accuracy  0.77102 298.4121292233467\n",
            "test accuracy  0.7841 75.56390333175659\n",
            "\n",
            "Epoch: 123\n",
            "train accuracy  0.7712 296.1831633448601\n",
            "test accuracy  0.7764 76.52483934164047\n",
            "\n",
            "Epoch: 124\n",
            "train accuracy  0.77026 295.4997074306011\n",
            "test accuracy  0.7835 76.86658176779747\n",
            "\n",
            "Epoch: 125\n",
            "train accuracy  0.77086 293.1711460649967\n",
            "test accuracy  0.7675 80.35780876874924\n",
            "\n",
            "Epoch: 126\n",
            "train accuracy  0.77446 290.20497331023216\n",
            "test accuracy  0.7819 75.01847341656685\n",
            "\n",
            "Epoch: 127\n",
            "train accuracy  0.7708 289.78416138887405\n",
            "test accuracy  0.7901 72.6350309252739\n",
            "\n",
            "Epoch: 128\n",
            "train accuracy  0.77424 288.36460959911346\n",
            "test accuracy  0.7987 70.0526134967804\n",
            "\n",
            "Epoch: 129\n",
            "train accuracy  0.77686 283.26457500457764\n",
            "test accuracy  0.7698 79.74870190024376\n",
            "\n",
            "Epoch: 130\n",
            "train accuracy  0.77548 284.9476455152035\n",
            "test accuracy  0.775 77.17965948581696\n",
            "\n",
            "Epoch: 131\n",
            "train accuracy  0.77978 281.4541380107403\n",
            "test accuracy  0.774 77.21638143062592\n",
            "\n",
            "Epoch: 132\n",
            "train accuracy  0.77978 279.8884055316448\n",
            "test accuracy  0.7847 74.63254883885384\n",
            "\n",
            "Epoch: 133\n",
            "train accuracy  0.78068 277.94839495420456\n",
            "test accuracy  0.7726 77.22688093781471\n",
            "\n",
            "Epoch: 134\n",
            "train accuracy  0.78166 275.2723536789417\n",
            "test accuracy  0.7816 75.47728881239891\n",
            "\n",
            "Epoch: 135\n",
            "train accuracy  0.7827 273.03280422091484\n",
            "test accuracy  0.7592 81.9325602054596\n",
            "\n",
            "Epoch: 136\n",
            "train accuracy  0.7827 272.3811445236206\n",
            "test accuracy  0.7731 78.65946012735367\n",
            "\n",
            "Epoch: 137\n",
            "train accuracy  0.78458 270.92742109298706\n",
            "test accuracy  0.7714 78.60224714875221\n",
            "\n",
            "Epoch: 138\n",
            "train accuracy  0.78456 269.583031475544\n",
            "test accuracy  0.7602 81.2576555609703\n",
            "\n",
            "Epoch: 139\n",
            "train accuracy  0.78604 266.31946793198586\n",
            "test accuracy  0.7644 80.19610357284546\n",
            "\n",
            "Epoch: 140\n",
            "train accuracy  0.78594 266.0073719918728\n",
            "test accuracy  0.7757 76.43707671761513\n",
            "\n",
            "Epoch: 141\n",
            "train accuracy  0.78848 262.64024981856346\n",
            "test accuracy  0.7744 77.07962381839752\n",
            "\n",
            "Epoch: 142\n",
            "train accuracy  0.78746 262.75667706131935\n",
            "test accuracy  0.7771 78.4014565050602\n",
            "\n",
            "Epoch: 143\n",
            "train accuracy  0.78964 259.897552549839\n",
            "test accuracy  0.7653 80.42675960063934\n",
            "\n",
            "Epoch: 144\n",
            "train accuracy  0.79104 257.94989854097366\n",
            "test accuracy  0.7674 78.4167833328247\n",
            "\n",
            "Epoch: 145\n",
            "train accuracy  0.79332 256.356776535511\n",
            "test accuracy  0.7611 80.6713604927063\n",
            "\n",
            "Epoch: 146\n",
            "train accuracy  0.7924 254.50612410902977\n",
            "test accuracy  0.7384 89.72108268737793\n",
            "\n",
            "Epoch: 147\n",
            "train accuracy  0.79582 252.0588555932045\n",
            "test accuracy  0.7492 84.23486667871475\n",
            "\n",
            "Epoch: 148\n",
            "train accuracy  0.7949 253.42237436771393\n",
            "test accuracy  0.7624 80.6657722890377\n",
            "\n",
            "Epoch: 149\n",
            "train accuracy  0.79862 248.89030507206917\n",
            "test accuracy  0.761 81.0154909491539\n",
            "\n",
            "Epoch: 150\n",
            "train accuracy  0.79874 246.74764615297318\n",
            "test accuracy  0.7485 85.6967601776123\n",
            "\n",
            "Epoch: 151\n",
            "train accuracy  0.79916 245.25975611805916\n",
            "test accuracy  0.7561 84.99146372079849\n",
            "\n",
            "Epoch: 152\n",
            "train accuracy  0.80212 242.50504544377327\n",
            "test accuracy  0.7504 84.6148412823677\n",
            "\n",
            "Epoch: 153\n",
            "train accuracy  0.80106 243.03796273469925\n",
            "test accuracy  0.747 86.45181092619896\n",
            "\n",
            "Epoch: 154\n",
            "train accuracy  0.80264 239.53200390934944\n",
            "test accuracy  0.7541 82.88863152265549\n",
            "\n",
            "Epoch: 155\n",
            "train accuracy  0.80254 237.92891851067543\n",
            "test accuracy  0.7315 90.81070613861084\n",
            "\n",
            "Epoch: 156\n",
            "train accuracy  0.80286 239.25641018152237\n",
            "test accuracy  0.7595 81.60615974664688\n",
            "\n",
            "Epoch: 157\n",
            "train accuracy  0.80372 237.69546815752983\n",
            "test accuracy  0.7473 82.98818558454514\n",
            "\n",
            "Epoch: 158\n",
            "train accuracy  0.80824 233.0853211581707\n",
            "test accuracy  0.7602 82.19331860542297\n",
            "\n",
            "Epoch: 159\n",
            "train accuracy  0.80824 230.48664090037346\n",
            "test accuracy  0.7462 84.72195833921432\n",
            "\n",
            "Epoch: 160\n",
            "train accuracy  0.80802 231.45088294148445\n",
            "test accuracy  0.7489 84.68123179674149\n",
            "\n",
            "Epoch: 161\n",
            "train accuracy  0.80948 229.9052148759365\n",
            "test accuracy  0.7482 85.04760637879372\n",
            "\n",
            "Epoch: 162\n",
            "train accuracy  0.81182 227.68315118551254\n",
            "test accuracy  0.732 88.29917645454407\n",
            "\n",
            "Epoch: 163\n",
            "train accuracy  0.8121 225.25211158394814\n",
            "test accuracy  0.7389 88.72089397907257\n",
            "\n",
            "Epoch: 164\n",
            "train accuracy  0.81464 225.08022129535675\n",
            "test accuracy  0.7386 88.57019853591919\n",
            "\n",
            "Epoch: 165\n",
            "train accuracy  0.81352 222.8088993728161\n",
            "test accuracy  0.7309 90.59056901931763\n",
            "\n",
            "Epoch: 166\n",
            "train accuracy  0.81294 224.09727144241333\n",
            "test accuracy  0.7594 82.2293598651886\n",
            "\n",
            "Epoch: 167\n",
            "train accuracy  0.81696 221.16961872577667\n",
            "test accuracy  0.7492 84.97895735502243\n",
            "\n",
            "Epoch: 168\n",
            "train accuracy  0.819 217.07341495156288\n",
            "test accuracy  0.7336 92.14894556999207\n",
            "\n",
            "Epoch: 169\n",
            "train accuracy  0.82034 215.2522310614586\n",
            "test accuracy  0.748 86.31762331724167\n",
            "\n",
            "Epoch: 170\n",
            "train accuracy  0.82004 215.44488191604614\n",
            "test accuracy  0.7418 86.69406098127365\n",
            "\n",
            "Epoch: 171\n",
            "train accuracy  0.822 210.12166115641594\n",
            "test accuracy  0.7224 94.69111746549606\n",
            "\n",
            "Epoch: 172\n",
            "train accuracy  0.81878 213.69357883930206\n",
            "test accuracy  0.7316 91.41525596380234\n",
            "\n",
            "Epoch: 173\n",
            "train accuracy  0.8216 211.79566103219986\n",
            "test accuracy  0.7564 84.6537778377533\n",
            "\n",
            "Epoch: 174\n",
            "train accuracy  0.82498 207.73921993374825\n",
            "test accuracy  0.7612 84.02707928419113\n",
            "\n",
            "Epoch: 175\n",
            "train accuracy  0.82754 203.4808398783207\n",
            "test accuracy  0.7352 91.16958916187286\n",
            "\n",
            "Epoch: 176\n",
            "train accuracy  0.82432 209.12020733952522\n",
            "test accuracy  0.7311 91.7948642373085\n",
            "\n",
            "Epoch: 177\n",
            "train accuracy  0.82594 207.499641507864\n",
            "test accuracy  0.7393 87.80405157804489\n",
            "\n",
            "Epoch: 178\n",
            "train accuracy  0.82822 203.87753638625145\n",
            "test accuracy  0.7379 89.22351008653641\n",
            "\n",
            "Epoch: 179\n",
            "train accuracy  0.82806 204.3866290152073\n",
            "test accuracy  0.7319 91.92791301012039\n",
            "\n",
            "Epoch: 180\n",
            "train accuracy  0.8264 204.30852594971657\n",
            "test accuracy  0.7264 93.42262947559357\n",
            "\n",
            "Epoch: 181\n",
            "train accuracy  0.8299 201.3577375113964\n",
            "test accuracy  0.7309 93.45387083292007\n",
            "\n",
            "Epoch: 182\n",
            "train accuracy  0.82982 201.03703129291534\n",
            "test accuracy  0.7494 86.652867436409\n",
            "\n",
            "Epoch: 183\n",
            "train accuracy  0.82618 202.9174783229828\n",
            "test accuracy  0.7518 84.36485084891319\n",
            "\n",
            "Epoch: 184\n",
            "train accuracy  0.83126 199.9145781993866\n",
            "test accuracy  0.74 88.88675808906555\n",
            "\n",
            "Epoch: 185\n",
            "train accuracy  0.82814 202.04173469543457\n",
            "test accuracy  0.7315 90.61159837245941\n",
            "\n",
            "Epoch: 186\n",
            "train accuracy  0.83182 198.73295277357101\n",
            "test accuracy  0.7393 89.64948052167892\n",
            "\n",
            "Epoch: 187\n",
            "train accuracy  0.83342 198.56297752261162\n",
            "test accuracy  0.7356 89.84218555688858\n",
            "\n",
            "Epoch: 188\n",
            "train accuracy  0.83308 195.64476469159126\n",
            "test accuracy  0.7317 92.01971578598022\n",
            "\n",
            "Epoch: 189\n",
            "train accuracy  0.8336 195.7503261268139\n",
            "test accuracy  0.7381 90.11478728055954\n",
            "\n",
            "Epoch: 190\n",
            "train accuracy  0.83492 194.4444842338562\n",
            "test accuracy  0.7356 90.53113251924515\n",
            "\n",
            "Epoch: 191\n",
            "train accuracy  0.83562 192.75277563929558\n",
            "test accuracy  0.7269 93.86405646800995\n",
            "\n",
            "Epoch: 192\n",
            "train accuracy  0.83642 193.9746235013008\n",
            "test accuracy  0.7307 93.68935132026672\n",
            "\n",
            "Epoch: 193\n",
            "train accuracy  0.84014 187.9957551062107\n",
            "test accuracy  0.7376 92.0421856045723\n",
            "\n",
            "Epoch: 194\n",
            "train accuracy  0.83346 193.40179538726807\n",
            "test accuracy  0.7256 93.42519688606262\n",
            "\n",
            "Epoch: 195\n",
            "train accuracy  0.83846 189.13447995483875\n",
            "test accuracy  0.7325 93.54857057332993\n",
            "\n",
            "Epoch: 196\n",
            "train accuracy  0.83808 190.82458212971687\n",
            "test accuracy  0.7289 92.57454490661621\n",
            "\n",
            "Epoch: 197\n",
            "train accuracy  0.8407 187.23734058439732\n",
            "test accuracy  0.7202 98.09717029333115\n",
            "\n",
            "Epoch: 198\n",
            "train accuracy  0.83904 189.49299378693104\n",
            "test accuracy  0.7107 99.3170702457428\n",
            "\n",
            "Epoch: 199\n",
            "train accuracy  0.83648 192.54538103938103\n",
            "test accuracy  0.7242 94.72566550970078\n",
            "\n",
            "Epoch: 200\n",
            "train accuracy  0.84266 184.77300590276718\n",
            "test accuracy  0.7265 93.28273493051529\n",
            "\n",
            "Epoch: 201\n",
            "train accuracy  0.83884 189.75651554763317\n",
            "test accuracy  0.7374 90.18695515394211\n",
            "\n",
            "Epoch: 202\n",
            "train accuracy  0.84146 184.55372014641762\n",
            "test accuracy  0.7298 93.84474056959152\n",
            "\n",
            "Epoch: 203\n",
            "train accuracy  0.84388 183.94294519722462\n",
            "test accuracy  0.7218 95.46523302793503\n",
            "\n",
            "Epoch: 204\n",
            "train accuracy  0.84662 181.08589677512646\n",
            "test accuracy  0.7314 94.32481199502945\n",
            "\n",
            "Epoch: 205\n",
            "train accuracy  0.8422 184.34090122580528\n",
            "test accuracy  0.7432 89.11839359998703\n",
            "\n",
            "Epoch: 206\n",
            "train accuracy  0.84488 182.37133184075356\n",
            "test accuracy  0.7307 94.07683670520782\n",
            "\n",
            "Epoch: 207\n",
            "train accuracy  0.84334 184.0673442184925\n",
            "test accuracy  0.7245 93.70579075813293\n",
            "\n",
            "Epoch: 208\n",
            "train accuracy  0.8447 182.9342276751995\n",
            "test accuracy  0.7341 93.10863262414932\n",
            "\n",
            "Epoch: 209\n",
            "train accuracy  0.84858 177.4622901082039\n",
            "test accuracy  0.7219 96.48555141687393\n",
            "\n",
            "Epoch: 210\n",
            "train accuracy  0.84378 182.15246018767357\n",
            "test accuracy  0.7264 93.9035724401474\n",
            "\n",
            "Epoch: 211\n",
            "train accuracy  0.84872 178.7110127210617\n",
            "test accuracy  0.7318 91.79556733369827\n",
            "\n",
            "Epoch: 212\n",
            "train accuracy  0.84796 178.63508063554764\n",
            "test accuracy  0.7324 94.58674770593643\n",
            "\n",
            "Epoch: 213\n",
            "train accuracy  0.8469 179.38919016718864\n",
            "test accuracy  0.7437 89.05005949735641\n",
            "\n",
            "Epoch: 214\n",
            "train accuracy  0.84172 184.7693802267313\n",
            "test accuracy  0.7234 92.21145379543304\n",
            "\n",
            "Epoch: 215\n",
            "train accuracy  0.8493 176.3396236896515\n",
            "test accuracy  0.7151 97.78682351112366\n",
            "\n",
            "Epoch: 216\n",
            "train accuracy  0.8471 177.54572866857052\n",
            "test accuracy  0.733 94.16543745994568\n",
            "\n",
            "Epoch: 217\n",
            "train accuracy  0.85118 174.0618640333414\n",
            "test accuracy  0.7215 96.2218970656395\n",
            "\n",
            "Epoch: 218\n",
            "train accuracy  0.84704 179.39344331622124\n",
            "test accuracy  0.6998 104.44717597961426\n",
            "\n",
            "Epoch: 219\n",
            "train accuracy  0.849 177.34869721531868\n",
            "test accuracy  0.7348 91.74752020835876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMfVvXNunQw",
        "colab_type": "code",
        "outputId": "8e91a525-df4a-40fd-a227-8bcdaa848869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.plot(epoch_list, train_acc, 'g--' , label='train_acc')\n",
        "plt.plot(epoch_list, test_acc, 'bo--', label = 'test_acc')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzi-Yr4gvye3",
        "colab_type": "code",
        "outputId": "d82116fa-df9e-43dc-eeca-16c5018c1182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "import random\n",
        "length = len(testset.targets)\n",
        "percentage_corruption = 20\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "print(corrupt_idx)\n",
        "print(len(corrupt_classes))\n",
        "\n",
        "st1 = testset.targets\n",
        "a = np.array(testset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "testset.targets = list(a)\n",
        "st2 = testset.targets\n",
        "\n",
        "print(st1 == st2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "2000 2000\n",
            "[6728, 9743, 4955, 9413, 4407, 5704, 2322, 6262, 3524, 8998, 5318, 6421, 5195, 5018, 3932, 2903, 2624, 4353, 5302, 3492, 2130, 8636, 1393, 5029, 9930, 3641, 432, 9950, 6061, 1603, 7139, 777, 4750, 4369, 9199, 2383, 8397, 2985, 3087, 3877, 4657, 2942, 5268, 5811, 6589, 3393, 3835, 59, 2870, 3196, 9680, 22, 5612, 9517, 3356, 1581, 7489, 1062, 4495, 7536, 7668, 9751, 6776, 873, 7855, 1921, 5343, 7438, 5406, 9508, 6002, 5322, 2243, 745, 5190, 5460, 5367, 9471, 9881, 3778, 9431, 6478, 1666, 8222, 3820, 6784, 8035, 8355, 2813, 1340, 3150, 2023, 4027, 8977, 5173, 8427, 6322, 1412, 5321, 2990, 9183, 5326, 4180, 7210, 1108, 7581, 6370, 7918, 8443, 5111, 21, 8975, 283, 2760, 5036, 6351, 5265, 1149, 7470, 2213, 4436, 8053, 4625, 370, 5847, 3340, 2788, 2152, 2655, 8595, 9054, 7620, 5547, 1883, 2125, 4928, 5632, 534, 3780, 8052, 2273, 9193, 819, 3493, 9966, 1869, 986, 457, 816, 919, 9395, 9896, 2792, 8607, 2420, 6360, 5167, 5915, 5629, 1994, 704, 2644, 2348, 1528, 9771, 526, 5648, 5912, 3686, 8588, 6694, 4977, 8828, 9685, 8186, 7477, 9363, 3593, 8150, 7519, 4427, 1207, 3983, 8422, 9175, 7813, 7300, 8668, 2587, 1773, 4534, 5726, 7725, 1509, 2045, 6462, 7600, 7337, 4310, 9527, 7711, 2021, 3947, 8626, 220, 1065, 7111, 5537, 3666, 4153, 8897, 1200, 9825, 7023, 2807, 4660, 3783, 6429, 6679, 4699, 2592, 9707, 1549, 8058, 9425, 7926, 559, 376, 7485, 9796, 50, 7054, 7808, 1621, 2768, 6373, 7947, 6130, 3991, 595, 6648, 5123, 228, 8195, 8217, 5079, 520, 9310, 3986, 3851, 2868, 3208, 4022, 2268, 6616, 3942, 4145, 2153, 8274, 2464, 5791, 8765, 4740, 489, 9982, 2277, 5843, 6735, 1107, 7785, 794, 7266, 9488, 8293, 2400, 3553, 7299, 9099, 6255, 8667, 1877, 3321, 7214, 2085, 6111, 3075, 232, 1449, 308, 9117, 4043, 1514, 6046, 2579, 1683, 2301, 4632, 5798, 1842, 333, 4241, 3259, 9924, 6292, 386, 764, 7244, 1280, 6265, 6055, 7039, 6296, 9180, 1088, 9120, 5809, 5017, 240, 1256, 4753, 5059, 2467, 4082, 5670, 706, 1648, 5671, 2737, 1950, 5498, 7686, 6024, 4485, 1596, 1161, 3691, 5551, 1048, 1984, 3329, 8049, 1816, 7845, 4635, 3516, 2366, 4602, 6861, 4960, 6690, 9234, 527, 3166, 4540, 2481, 2233, 4159, 9647, 9933, 4392, 2332, 6098, 1512, 6774, 9529, 2876, 642, 277, 9724, 9135, 1035, 1939, 7408, 4854, 8349, 1797, 7483, 5761, 9343, 7287, 5964, 8684, 5266, 2637, 1211, 90, 8458, 6643, 4318, 7963, 2631, 1928, 4666, 4754, 9692, 2577, 5395, 6194, 7672, 12, 7678, 9034, 7082, 6980, 498, 4377, 5593, 8110, 4303, 4488, 8936, 4276, 9076, 7860, 3934, 9031, 7441, 3948, 6455, 7016, 2641, 3316, 3370, 9355, 5861, 9806, 5442, 580, 628, 7161, 7528, 832, 3889, 7262, 6402, 7442, 2911, 2951, 1036, 6904, 7181, 3432, 924, 985, 4795, 7136, 7669, 71, 9241, 3629, 1306, 4199, 1440, 4160, 5978, 1246, 7977, 7104, 9321, 2918, 688, 7869, 6580, 4549, 9156, 531, 223, 8286, 2663, 9079, 5572, 8160, 9380, 4690, 6788, 1716, 646, 1322, 2191, 8016, 3369, 4058, 7275, 4828, 9365, 5412, 1902, 7812, 6864, 7345, 7873, 8450, 8534, 9993, 7510, 1547, 9816, 7385, 1843, 1208, 1513, 6246, 2136, 8508, 1277, 4599, 4356, 7127, 8327, 734, 7730, 6230, 8120, 5688, 4167, 9603, 8790, 2583, 6629, 2938, 3929, 2003, 8489, 1610, 8695, 2771, 3002, 7628, 7631, 1575, 3054, 7089, 915, 1473, 7603, 9011, 9812, 7940, 1755, 604, 5140, 574, 2122, 2190, 1777, 9118, 6254, 3589, 5635, 3439, 505, 7549, 2557, 900, 4288, 5331, 8681, 7694, 3791, 1564, 5944, 3576, 1857, 3679, 1477, 8789, 8040, 5914, 2747, 2939, 5089, 1573, 7273, 3487, 1205, 2200, 6998, 7311, 2084, 9979, 7705, 1232, 4364, 6983, 9608, 8871, 5817, 2604, 4190, 4946, 37, 5758, 5275, 681, 4945, 9886, 9613, 8232, 8893, 4500, 4230, 8745, 3712, 2062, 644, 5441, 5832, 6251, 8725, 1234, 1269, 9121, 4102, 6527, 5770, 3074, 5923, 7383, 5695, 9850, 8574, 6633, 755, 5044, 9763, 1658, 7592, 65, 649, 2032, 7417, 9136, 4692, 1491, 9351, 4719, 9083, 736, 1975, 4476, 3021, 2873, 4178, 7500, 1486, 9611, 1248, 2948, 5531, 8951, 7236, 8825, 7135, 9463, 2323, 6739, 791, 9863, 1708, 8582, 3460, 3940, 6368, 8605, 1748, 7588, 6367, 5264, 1019, 8490, 9849, 6372, 2869, 2012, 9841, 2490, 9793, 6557, 8100, 2639, 5148, 2423, 3941, 4112, 2020, 795, 6297, 5562, 2937, 9437, 9937, 5587, 7425, 269, 1936, 7219, 1414, 175, 1407, 64, 2508, 519, 7036, 7914, 5362, 5984, 5365, 1366, 4148, 6160, 1712, 6875, 3232, 683, 1566, 100, 3599, 6549, 2488, 1243, 8390, 1124, 2753, 2946, 9222, 8001, 2295, 1825, 8291, 9399, 1353, 8644, 9826, 4267, 5237, 5361, 4044, 4598, 3823, 1056, 655, 9061, 7090, 4416, 3610, 5737, 6434, 1563, 9690, 7556, 4394, 8410, 3169, 6560, 1709, 5716, 1862, 6208, 8254, 5041, 9516, 371, 4363, 4249, 7014, 880, 1228, 2353, 6722, 5075, 7049, 6146, 1865, 5987, 7817, 360, 492, 2009, 398, 339, 9273, 7768, 3084, 6381, 4873, 6978, 6095, 5290, 936, 9729, 3832, 4816, 9303, 987, 2140, 2914, 2896, 8448, 2131, 346, 3507, 8309, 8192, 4408, 1993, 9656, 4026, 4277, 7443, 4572, 1265, 5348, 3124, 9438, 9392, 8608, 3212, 9469, 6159, 1471, 1519, 6413, 4012, 7830, 4881, 2805, 1820, 4477, 5525, 1321, 9605, 6607, 5997, 4406, 6729, 9626, 8099, 9406, 1038, 5765, 3849, 6071, 4286, 4570, 3592, 7075, 7702, 4360, 2549, 1052, 6687, 7656, 7688, 1634, 4106, 7840, 2290, 3394, 25, 2888, 5439, 7854, 9052, 8314, 6399, 9990, 4474, 5609, 5181, 6279, 3841, 1730, 9103, 3097, 7763, 2208, 528, 1913, 6331, 4936, 2403, 8914, 4916, 3307, 4588, 8132, 814, 6713, 8386, 5388, 9641, 2221, 6961, 7777, 8203, 181, 372, 9918, 1982, 6661, 2611, 6553, 8972, 1855, 7506, 2494, 3005, 6594, 8084, 5354, 9101, 8938, 3871, 7222, 5357, 7509, 6732, 6624, 6526, 7015, 6860, 7829, 4389, 7053, 147, 3550, 9762, 9997, 8004, 5039, 6561, 7815, 8241, 8015, 138, 1279, 8106, 2725, 5119, 8814, 7739, 4151, 3815, 2531, 9899, 6312, 909, 9367, 2103, 6964, 2982, 3781, 4797, 5132, 3591, 8066, 8130, 7956, 5502, 1236, 9953, 6403, 8089, 5902, 2029, 3268, 2338, 2959, 8902, 7029, 9496, 5469, 8633, 914, 5426, 4775, 9757, 3315, 1897, 2341, 733, 7108, 4057, 4619, 1782, 8453, 131, 8446, 4263, 6310, 8777, 7643, 5834, 9382, 9869, 5948, 3132, 3997, 4840, 9081, 1100, 3632, 8078, 5775, 9891, 7149, 2001, 6938, 3620, 1342, 9722, 3392, 410, 3564, 1309, 6170, 4927, 3192, 2783, 1874, 769, 7866, 908, 2522, 3137, 5518, 5927, 9210, 1031, 4894, 7900, 8194, 7898, 5633, 4, 5252, 9278, 3085, 8265, 9161, 2436, 2141, 1320, 572, 7283, 7623, 8842, 2580, 6283, 2458, 8425, 4001, 7851, 929, 9458, 9844, 4067, 9190, 3910, 7318, 3364, 4111, 7816, 6172, 7138, 9398, 1776, 4335, 2924, 1391, 2258, 6591, 7661, 8176, 1590, 167, 2865, 3965, 8782, 5251, 4433, 3506, 1646, 7426, 153, 5019, 8136, 9819, 6054, 3344, 6016, 8798, 9602, 2606, 931, 6879, 9978, 4498, 393, 8236, 9020, 5073, 1608, 6646, 6065, 4947, 5214, 1738, 7675, 5153, 7151, 5379, 3608, 8032, 7775, 3893, 6063, 1711, 9338, 4132, 2361, 2963, 9097, 9506, 877, 7954, 9987, 3863, 6329, 1347, 1522, 2408, 1076, 8673, 1397, 4751, 1951, 4614, 9407, 9992, 9568, 922, 701, 6630, 5823, 6922, 324, 1364, 8905, 7713, 1239, 9277, 7756, 3292, 3657, 46, 784, 2965, 1212, 5182, 5605, 3973, 5862, 1918, 4142, 3327, 1584, 1783, 2195, 2686, 561, 9494, 1675, 1952, 9509, 3935, 5474, 9720, 7382, 616, 9457, 3808, 4553, 7933, 679, 8793, 4462, 6161, 4847, 1671, 6887, 4417, 4481, 2435, 2178, 6154, 3753, 1148, 4700, 6897, 53, 6496, 2547, 5792, 6419, 6053, 1809, 3170, 4921, 9893, 6456, 7591, 4805, 5434, 2722, 3557, 3704, 9069, 3457, 8732, 8172, 2977, 8201, 3509, 3379, 3093, 347, 2450, 7517, 8867, 3816, 5517, 2321, 6516, 796, 2692, 2999, 2067, 7498, 9445, 9129, 84, 8378, 3053, 9372, 4841, 7387, 536, 5127, 2173, 887, 7800, 7086, 7323, 6459, 5415, 7625, 2554, 185, 7027, 4621, 9718, 3649, 5645, 8085, 5700, 3548, 1620, 8675, 6829, 6632, 1898, 439, 8368, 8436, 8704, 4387, 7335, 3916, 974, 8550, 2784, 9293, 4721, 9049, 329, 7103, 6424, 7610, 2841, 9176, 6986, 3902, 9287, 2407, 437, 9945, 4906, 690, 7369, 7238, 6394, 6772, 2182, 3756, 1332, 5977, 1354, 5138, 1808, 8188, 8677, 2077, 767, 5169, 7890, 2451, 2998, 5877, 6235, 8430, 6028, 9513, 1417, 9419, 1699, 837, 6838, 1821, 689, 933, 5796, 9349, 1319, 6900, 2667, 9397, 8352, 1468, 6466, 184, 7079, 4243, 6855, 8014, 264, 6970, 6477, 6957, 5806, 5890, 7670, 5171, 7950, 3127, 8767, 7312, 1147, 4637, 6225, 127, 8820, 7301, 4579, 6839, 9832, 6761, 3475, 258, 3633, 6636, 1432, 4583, 3461, 4818, 4244, 7693, 2218, 1068, 231, 2864, 5835, 1169, 322, 9739, 6908, 7360, 3234, 5404, 8786, 5158, 4730, 6363, 943, 981, 2166, 2707, 7512, 4401, 7891, 5666, 8276, 4343, 7129, 6840, 8783, 6108, 7295, 7257, 4028, 2794, 9257, 6258, 4329, 4125, 4576, 9558, 5879, 125, 7040, 4796, 9975, 5781, 1191, 9155, 1004, 9616, 9473, 8474, 4991, 4958, 8556, 3501, 5369, 6820, 4496, 3112, 6708, 7857, 2850, 6391, 7729, 2054, 1202, 1817, 9683, 8026, 32, 8302, 4236, 6349, 2337, 60, 3383, 2819, 2382, 6041, 3482, 3892, 5962, 2680, 6947, 1122, 939, 8917, 9301, 7132, 4819, 5661, 6266, 7325, 1371, 48, 3933, 8174, 2743, 8711, 2756, 7885, 8018, 2291, 2413, 5674, 3187, 3607, 6398, 6781, 5627, 7167, 438, 2614, 1028, 2415, 6995, 101, 5067, 7726, 2621, 2260, 6625, 1469, 3044, 9805, 115, 7480, 9123, 7220, 910, 4323, 158, 7484, 8122, 3813, 2609, 7788, 552, 2555, 1793, 6397, 3007, 5352, 4714, 1924, 7203, 3339, 7362, 3401, 891, 9166, 7050, 1960, 9885, 7163, 7329, 1752, 3464, 4656, 1492, 204, 8348, 1802, 8920, 8048, 9615, 1156, 6665, 5391, 2088, 2932, 656, 4264, 7880, 7184, 2824, 5221, 3722, 8000, 553, 9892, 2700, 1959, 137, 7676, 4307, 6067, 129, 4933, 5061, 4098, 1568, 8319, 7432, 6502, 6384, 7967, 6583, 7017, 5213, 55, 1771, 1189, 3842, 2274, 4787, 3952, 3036, 7712, 9075, 7334, 5506, 4306, 6544, 2426, 4806, 364, 170, 4059, 3286, 6748, 7835, 3724, 7527, 5721, 2307, 6164, 8963, 9536, 730, 1995, 395, 1687, 4636, 1077, 4158, 2940, 16, 380, 3216, 9717, 3577, 4793, 4164, 5768, 5168, 4649, 6848, 2475, 6684, 9567, 7871, 1744, 4559, 5491, 824, 5416, 6972, 8322, 7177, 7116, 4756, 9467, 9843, 9932, 4956, 3730, 5128, 2468, 2114, 3894, 3855, 9328, 591, 3681, 9009, 9561, 5315, 6009, 6340, 2240, 3057, 1370, 4283, 9916, 8533, 1510, 1400, 7942, 4451, 8320, 2472, 4593, 6333, 4217, 1569, 1273, 2589, 8478, 7680, 4959, 4575, 6834, 6269, 9691, 9737, 3570, 5812, 7804, 4047, 6514, 1387, 7737, 6792, 7279, 9361, 9548, 5393, 5916, 9769, 5712, 1377, 3619, 3311, 3449, 4696, 8008, 8696, 7837, 4205, 5254, 1980, 182, 2787, 2133, 720, 1007, 2678, 1018, 5013, 2506, 8654, 7478, 4503, 260, 2697, 8239, 1800, 4482, 341, 9598, 7745, 3253, 2300, 5679, 3125, 5457, 6287, 5144, 3792, 9304, 666, 434, 2630, 1764, 3695, 7991, 7494, 2154, 3293, 7363, 9533, 3840, 8505, 5270, 7330, 3702, 6109, 8911, 6216, 1287, 202, 6482, 1069, 1160, 4040, 5274, 6289, 510, 9005, 4527, 5006, 8723, 4334, 8487, 8183, 9531, 4423, 2234, 6174, 6426, 2207, 8606, 6378, 1446, 5729, 9744, 9290, 3685, 8303, 9514, 7645, 8719, 2105, 9986, 7966, 1524, 2406, 8843, 1880, 8154, 6943, 4918, 3675, 4455, 5961, 4702, 5875, 842, 624, 5983, 8273, 4388, 735, 1444, 4065, 9027, 2198, 217, 5596, 7212, 4171, 5738, 3237, 6485, 9309, 2113, 3749, 5371, 340, 6414, 7749, 6769, 9142, 2600, 4577, 3416, 2769, 9623, 4051, 8050, 6851, 3763, 9159, 4409, 2922, 3257, 1063, 7487, 8835, 751, 858, 7452, 6113, 4723, 8663, 3655, 1260, 6597, 9664, 576, 7710, 4253, 3318, 6524, 3552, 6821, 7032, 142, 5452, 7081, 6525, 7122, 3652, 6928, 2826, 1307, 935, 1355, 839, 6828, 2744, 6260, 9128, 1534, 3077, 6190, 1193, 3012, 4452, 1841, 5260, 3282, 9653, 3361, 4296, 4603, 9, 2427, 2418, 3904, 560, 9074, 2137, 5116, 6899, 761, 8561, 3314, 3515, 7065, 1289, 282, 5250, 3193, 13, 3972, 9614, 8164, 1823, 1762, 8509, 1286, 3830, 6032, 4113, 2028, 7263, 6612, 5532, 3052, 1599, 1008, 5253, 3134, 4299, 9526, 893, 133, 176, 8630, 2298, 6179, 4133, 515, 8888, 6043, 9485, 7248, 9269, 5179, 7304, 7044, 7974, 81, 5880, 8463, 2957, 8272, 8009, 5340, 4976, 8065, 8923, 5223, 6405, 6662, 9767, 5789, 5479, 9376, 9346, 3076, 4629, 7613, 3365, 4780, 1333, 9552, 5970, 5188, 3398, 1589, 9160, 5751, 7255, 1552, 8498, 5330, 6858, 3797, 1241, 5615, 6193, 3787, 5217, 7793, 6144, 2269, 1379, 6941, 4161]\n",
            "2000\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWLG2Hmx6gTM",
        "colab_type": "code",
        "outputId": "3a513eaf-fa0d-464f-d485-9169d0a909f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "global best_acc\n",
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "print(\"test accuracy \", correct/total , test_loss)\n",
        "test_acc.append(test_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.6107 190.9880999326706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhIpnf-6xyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}