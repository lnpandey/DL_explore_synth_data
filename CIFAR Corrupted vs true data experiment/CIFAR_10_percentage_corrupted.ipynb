{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR 10 percentage corrupted.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olq-MKcdmOnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = EfficientNetB0()\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlZDWDZm6ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "term_width = 80\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ubk0aCmSD7",
        "colab_type": "code",
        "outputId": "2323d52f-97bb-4105-d23f-25c95e743952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "# trainset.targets***************************************************************\n",
        "import random\n",
        "length = len(trainset.targets)\n",
        "percentage_corruption = 10\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "# print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "# print(corrupt_idx)\n",
        "# print(len(corrupt_classes))\n",
        "a = np.array(trainset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "trainset.targets = list(a)\n",
        "#**********************************************************************************\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train_acc =[]\n",
        "test_acc=[]\n",
        "epoch_list=[]\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"train accuracy \", correct/total ,train_loss)\n",
        "    train_acc.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"test accuracy \", correct/total , test_loss)\n",
        "    test_acc.append(test_loss)\n",
        "\n",
        "    # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+200):\n",
        "    epoch_list.append(epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "train accuracy  0.33968 723.2110443115234\n",
            "test accuracy  0.4712 145.29924499988556\n",
            "\n",
            "Epoch: 1\n",
            "train accuracy  0.4556 622.2004497051239\n",
            "test accuracy  0.5367 130.2173843383789\n",
            "\n",
            "Epoch: 2\n",
            "train accuracy  0.51732 567.9947154521942\n",
            "test accuracy  0.5952 114.44167774915695\n",
            "\n",
            "Epoch: 3\n",
            "train accuracy  0.55902 528.9328490495682\n",
            "test accuracy  0.6361 105.87462598085403\n",
            "\n",
            "Epoch: 4\n",
            "train accuracy  0.59138 502.53112065792084\n",
            "test accuracy  0.6782 95.0349914431572\n",
            "\n",
            "Epoch: 5\n",
            "train accuracy  0.6148 477.75171959400177\n",
            "test accuracy  0.697 90.50753456354141\n",
            "\n",
            "Epoch: 6\n",
            "train accuracy  0.63304 461.46702939271927\n",
            "test accuracy  0.7092 86.82290917634964\n",
            "\n",
            "Epoch: 7\n",
            "train accuracy  0.65052 446.32660734653473\n",
            "test accuracy  0.6997 89.70420157909393\n",
            "\n",
            "Epoch: 8\n",
            "train accuracy  0.66208 434.52560490369797\n",
            "test accuracy  0.7261 82.91941583156586\n",
            "\n",
            "Epoch: 9\n",
            "train accuracy  0.67266 423.6209987401962\n",
            "test accuracy  0.7411 79.16496193408966\n",
            "\n",
            "Epoch: 10\n",
            "train accuracy  0.6858 412.3508099913597\n",
            "test accuracy  0.7624 75.22064936161041\n",
            "\n",
            "Epoch: 11\n",
            "train accuracy  0.69588 402.55343651771545\n",
            "test accuracy  0.7562 74.12300550937653\n",
            "\n",
            "Epoch: 12\n",
            "train accuracy  0.70042 396.97251212596893\n",
            "test accuracy  0.7581 72.80743551254272\n",
            "\n",
            "Epoch: 13\n",
            "train accuracy  0.71012 389.5306012034416\n",
            "test accuracy  0.7696 71.49878185987473\n",
            "\n",
            "Epoch: 14\n",
            "train accuracy  0.71638 383.25333857536316\n",
            "test accuracy  0.78 68.743372797966\n",
            "\n",
            "Epoch: 15\n",
            "train accuracy  0.71986 378.33534878492355\n",
            "test accuracy  0.776 69.67609232664108\n",
            "\n",
            "Epoch: 16\n",
            "train accuracy  0.72462 372.48358541727066\n",
            "test accuracy  0.7882 65.55136436223984\n",
            "\n",
            "Epoch: 17\n",
            "train accuracy  0.72958 367.19941341876984\n",
            "test accuracy  0.7822 67.27681767940521\n",
            "\n",
            "Epoch: 18\n",
            "train accuracy  0.73384 363.42506045103073\n",
            "test accuracy  0.785 67.88131242990494\n",
            "\n",
            "Epoch: 19\n",
            "train accuracy  0.73772 359.19831866025925\n",
            "test accuracy  0.8102 60.04903802275658\n",
            "\n",
            "Epoch: 20\n",
            "train accuracy  0.7433 354.2411230802536\n",
            "test accuracy  0.8008 64.06075435876846\n",
            "\n",
            "Epoch: 21\n",
            "train accuracy  0.74356 351.13533943891525\n",
            "test accuracy  0.8026 62.77009290456772\n",
            "\n",
            "Epoch: 22\n",
            "train accuracy  0.74892 347.645775616169\n",
            "test accuracy  0.8068 60.91710415482521\n",
            "\n",
            "Epoch: 23\n",
            "train accuracy  0.7539 342.54931765794754\n",
            "test accuracy  0.8147 60.23846212029457\n",
            "\n",
            "Epoch: 24\n",
            "train accuracy  0.75682 340.1804731488228\n",
            "test accuracy  0.8145 61.8061460852623\n",
            "\n",
            "Epoch: 25\n",
            "train accuracy  0.7579 337.5849405527115\n",
            "test accuracy  0.812 60.79790812730789\n",
            "\n",
            "Epoch: 26\n",
            "train accuracy  0.76218 333.98947060108185\n",
            "test accuracy  0.8136 60.14155840873718\n",
            "\n",
            "Epoch: 27\n",
            "train accuracy  0.76336 331.3227510154247\n",
            "test accuracy  0.8125 58.646297454833984\n",
            "\n",
            "Epoch: 28\n",
            "train accuracy  0.76568 329.43806862831116\n",
            "test accuracy  0.8251 58.681714951992035\n",
            "\n",
            "Epoch: 29\n",
            "train accuracy  0.76942 327.1133047938347\n",
            "test accuracy  0.8188 58.692932575941086\n",
            "\n",
            "Epoch: 30\n",
            "train accuracy  0.77186 322.5007546842098\n",
            "test accuracy  0.8216 57.54264497756958\n",
            "\n",
            "Epoch: 31\n",
            "train accuracy  0.77244 319.69967836141586\n",
            "test accuracy  0.8206 57.83956202864647\n",
            "\n",
            "Epoch: 32\n",
            "train accuracy  0.77538 318.63041830062866\n",
            "test accuracy  0.8221 58.055295288562775\n",
            "\n",
            "Epoch: 33\n",
            "train accuracy  0.77678 316.4293954372406\n",
            "test accuracy  0.8084 62.14222076535225\n",
            "\n",
            "Epoch: 34\n",
            "train accuracy  0.7806 313.9738443493843\n",
            "test accuracy  0.8241 57.83059984445572\n",
            "\n",
            "Epoch: 35\n",
            "train accuracy  0.78186 310.66987332701683\n",
            "test accuracy  0.8185 59.28270798921585\n",
            "\n",
            "Epoch: 36\n",
            "train accuracy  0.78154 309.6358935236931\n",
            "test accuracy  0.8274 55.59095871448517\n",
            "\n",
            "Epoch: 37\n",
            "train accuracy  0.7832 307.7242643237114\n",
            "test accuracy  0.8288 56.923005163669586\n",
            "\n",
            "Epoch: 38\n",
            "train accuracy  0.78394 306.53942912817\n",
            "test accuracy  0.8296 56.83981838822365\n",
            "\n",
            "Epoch: 39\n",
            "train accuracy  0.7842 304.99138909578323\n",
            "test accuracy  0.8375 54.32325837016106\n",
            "\n",
            "Epoch: 40\n",
            "train accuracy  0.78884 302.0974459052086\n",
            "test accuracy  0.8377 54.88322886824608\n",
            "\n",
            "Epoch: 41\n",
            "train accuracy  0.7907 299.15860876441\n",
            "test accuracy  0.8319 56.69034489989281\n",
            "\n",
            "Epoch: 42\n",
            "train accuracy  0.7906 299.69944339990616\n",
            "test accuracy  0.8285 57.31425791978836\n",
            "\n",
            "Epoch: 43\n",
            "train accuracy  0.7933 297.01893866062164\n",
            "test accuracy  0.8305 56.12197530269623\n",
            "\n",
            "Epoch: 44\n",
            "train accuracy  0.7943 295.37677958607674\n",
            "test accuracy  0.8299 56.81094115972519\n",
            "\n",
            "Epoch: 45\n",
            "train accuracy  0.79388 294.9689472615719\n",
            "test accuracy  0.8294 57.366946280002594\n",
            "\n",
            "Epoch: 46\n",
            "train accuracy  0.79658 292.3660413324833\n",
            "test accuracy  0.8273 56.54763653874397\n",
            "\n",
            "Epoch: 47\n",
            "train accuracy  0.79602 291.4871116578579\n",
            "test accuracy  0.8318 55.25943210721016\n",
            "\n",
            "Epoch: 48\n",
            "train accuracy  0.79878 289.1601836681366\n",
            "test accuracy  0.8331 54.54065811634064\n",
            "\n",
            "Epoch: 49\n",
            "train accuracy  0.79994 288.1487617492676\n",
            "test accuracy  0.836 55.31574419140816\n",
            "\n",
            "Epoch: 50\n",
            "train accuracy  0.80138 285.31652784347534\n",
            "test accuracy  0.8444 51.911877393722534\n",
            "\n",
            "Epoch: 51\n",
            "train accuracy  0.80228 283.60603255033493\n",
            "test accuracy  0.8392 52.75482168793678\n",
            "\n",
            "Epoch: 52\n",
            "train accuracy  0.80192 284.0730311870575\n",
            "test accuracy  0.8404 54.277779161930084\n",
            "\n",
            "Epoch: 53\n",
            "train accuracy  0.8014 283.08960896730423\n",
            "test accuracy  0.8397 53.05461260676384\n",
            "\n",
            "Epoch: 54\n",
            "train accuracy  0.80238 280.2739953696728\n",
            "test accuracy  0.8441 52.88217905163765\n",
            "\n",
            "Epoch: 55\n",
            "train accuracy  0.80404 279.54762530326843\n",
            "test accuracy  0.8419 53.40041309595108\n",
            "\n",
            "Epoch: 56\n",
            "train accuracy  0.80516 277.4332620203495\n",
            "test accuracy  0.8333 55.60633826255798\n",
            "\n",
            "Epoch: 57\n",
            "train accuracy  0.80642 276.46187475323677\n",
            "test accuracy  0.8385 53.630682080984116\n",
            "\n",
            "Epoch: 58\n",
            "train accuracy  0.80488 277.5528885424137\n",
            "test accuracy  0.8389 54.05839931964874\n",
            "\n",
            "Epoch: 59\n",
            "train accuracy  0.80966 273.30378979444504\n",
            "test accuracy  0.8379 55.05518773198128\n",
            "\n",
            "Epoch: 60\n",
            "train accuracy  0.808 273.7232147157192\n",
            "test accuracy  0.8438 52.459106743335724\n",
            "\n",
            "Epoch: 61\n",
            "train accuracy  0.8095 272.07324558496475\n",
            "test accuracy  0.8423 53.16674366593361\n",
            "\n",
            "Epoch: 62\n",
            "train accuracy  0.8109 269.9433436989784\n",
            "test accuracy  0.8454 52.839366525411606\n",
            "\n",
            "Epoch: 63\n",
            "train accuracy  0.80998 270.24312722682953\n",
            "test accuracy  0.8449 53.20853379368782\n",
            "\n",
            "Epoch: 64\n",
            "train accuracy  0.81428 267.20694267749786\n",
            "test accuracy  0.8438 52.87857684493065\n",
            "\n",
            "Epoch: 65\n",
            "train accuracy  0.81224 265.94317814707756\n",
            "test accuracy  0.8465 53.45273178815842\n",
            "\n",
            "Epoch: 66\n",
            "train accuracy  0.81586 264.64601054787636\n",
            "test accuracy  0.8433 53.65276777744293\n",
            "\n",
            "Epoch: 67\n",
            "train accuracy  0.81392 263.304657548666\n",
            "test accuracy  0.8534 51.09708049893379\n",
            "\n",
            "Epoch: 68\n",
            "train accuracy  0.81518 262.33685383200645\n",
            "test accuracy  0.8469 53.14064282178879\n",
            "\n",
            "Epoch: 69\n",
            "train accuracy  0.81508 262.68591836094856\n",
            "test accuracy  0.8444 53.1199226975441\n",
            "\n",
            "Epoch: 70\n",
            "train accuracy  0.81592 260.7392089664936\n",
            "test accuracy  0.832 57.68059051036835\n",
            "\n",
            "Epoch: 71\n",
            "train accuracy  0.81688 258.94582411646843\n",
            "test accuracy  0.845 52.00617003440857\n",
            "\n",
            "Epoch: 72\n",
            "train accuracy  0.81946 257.78371730446815\n",
            "test accuracy  0.8342 56.2697736620903\n",
            "\n",
            "Epoch: 73\n",
            "train accuracy  0.8168 259.236394315958\n",
            "test accuracy  0.8472 51.52322480082512\n",
            "\n",
            "Epoch: 74\n",
            "train accuracy  0.81904 254.8582157790661\n",
            "test accuracy  0.8433 52.89515841007233\n",
            "\n",
            "Epoch: 75\n",
            "train accuracy  0.81804 254.21597120165825\n",
            "test accuracy  0.8412 53.72860497236252\n",
            "\n",
            "Epoch: 76\n",
            "train accuracy  0.82024 253.1001652777195\n",
            "test accuracy  0.8301 57.36271199584007\n",
            "\n",
            "Epoch: 77\n",
            "train accuracy  0.81974 253.59853649139404\n",
            "test accuracy  0.8366 55.86310076713562\n",
            "\n",
            "Epoch: 78\n",
            "train accuracy  0.82162 250.33565551042557\n",
            "test accuracy  0.8384 53.989292949438095\n",
            "\n",
            "Epoch: 79\n",
            "train accuracy  0.82192 250.51586937904358\n",
            "test accuracy  0.8275 58.126990497112274\n",
            "\n",
            "Epoch: 80\n",
            "train accuracy  0.8239 247.87647566199303\n",
            "test accuracy  0.8415 54.656129121780396\n",
            "\n",
            "Epoch: 81\n",
            "train accuracy  0.82416 246.70727545022964\n",
            "test accuracy  0.8488 50.86904725432396\n",
            "\n",
            "Epoch: 82\n",
            "train accuracy  0.82314 246.20604422688484\n",
            "test accuracy  0.8302 56.37147590517998\n",
            "\n",
            "Epoch: 83\n",
            "train accuracy  0.82502 244.71088734269142\n",
            "test accuracy  0.8339 56.05357167124748\n",
            "\n",
            "Epoch: 84\n",
            "train accuracy  0.82572 243.7434811592102\n",
            "test accuracy  0.8315 56.8783954679966\n",
            "\n",
            "Epoch: 85\n",
            "train accuracy  0.8239 244.1768980026245\n",
            "test accuracy  0.8253 60.090090692043304\n",
            "\n",
            "Epoch: 86\n",
            "train accuracy  0.82612 242.4949008524418\n",
            "test accuracy  0.8396 56.026100277900696\n",
            "\n",
            "Epoch: 87\n",
            "train accuracy  0.8261 240.8730685710907\n",
            "test accuracy  0.8339 56.247521072626114\n",
            "\n",
            "Epoch: 88\n",
            "train accuracy  0.8276 239.2661473751068\n",
            "test accuracy  0.8386 54.91303476691246\n",
            "\n",
            "Epoch: 89\n",
            "train accuracy  0.82592 239.5068645477295\n",
            "test accuracy  0.8397 53.54690185189247\n",
            "\n",
            "Epoch: 90\n",
            "train accuracy  0.83116 235.09776496887207\n",
            "test accuracy  0.8377 56.63678482174873\n",
            "\n",
            "Epoch: 91\n",
            "train accuracy  0.83026 234.889211922884\n",
            "test accuracy  0.8421 52.68873605132103\n",
            "\n",
            "Epoch: 92\n",
            "train accuracy  0.82794 235.91412302851677\n",
            "test accuracy  0.8365 55.847929298877716\n",
            "\n",
            "Epoch: 93\n",
            "train accuracy  0.82926 233.8979248404503\n",
            "test accuracy  0.8307 57.33294561505318\n",
            "\n",
            "Epoch: 94\n",
            "train accuracy  0.831 233.39229196310043\n",
            "test accuracy  0.8385 53.69379782676697\n",
            "\n",
            "Epoch: 95\n",
            "train accuracy  0.82984 231.95966663956642\n",
            "test accuracy  0.8433 54.71215412020683\n",
            "\n",
            "Epoch: 96\n",
            "train accuracy  0.83244 228.9359184205532\n",
            "test accuracy  0.8301 58.56812596321106\n",
            "\n",
            "Epoch: 97\n",
            "train accuracy  0.83066 230.25613677501678\n",
            "test accuracy  0.8285 58.17714503407478\n",
            "\n",
            "Epoch: 98\n",
            "train accuracy  0.83396 225.78647503256798\n",
            "test accuracy  0.8425 54.75722476840019\n",
            "\n",
            "Epoch: 99\n",
            "train accuracy  0.83454 225.02278196811676\n",
            "test accuracy  0.8435 54.417366564273834\n",
            "\n",
            "Epoch: 100\n",
            "train accuracy  0.83528 223.70171797275543\n",
            "test accuracy  0.8306 58.15430501103401\n",
            "\n",
            "Epoch: 101\n",
            "train accuracy  0.83642 221.8390882909298\n",
            "test accuracy  0.8321 57.6101775765419\n",
            "\n",
            "Epoch: 102\n",
            "train accuracy  0.8366 220.99991157650948\n",
            "test accuracy  0.8366 54.828553915023804\n",
            "\n",
            "Epoch: 103\n",
            "train accuracy  0.8348 222.11136877536774\n",
            "test accuracy  0.8218 60.96650317311287\n",
            "\n",
            "Epoch: 104\n",
            "train accuracy  0.8343 222.5822008252144\n",
            "test accuracy  0.8207 60.00682884454727\n",
            "\n",
            "Epoch: 105\n",
            "train accuracy  0.83686 218.17414858937263\n",
            "test accuracy  0.8159 61.972267508506775\n",
            "\n",
            "Epoch: 106\n",
            "train accuracy  0.8378 215.96588790416718\n",
            "test accuracy  0.8397 54.54043051600456\n",
            "\n",
            "Epoch: 107\n",
            "train accuracy  0.83772 215.27322612702847\n",
            "test accuracy  0.8315 56.724413603544235\n",
            "\n",
            "Epoch: 108\n",
            "train accuracy  0.83776 214.9324297606945\n",
            "test accuracy  0.8368 56.600628554821014\n",
            "\n",
            "Epoch: 109\n",
            "train accuracy  0.83852 214.56584733724594\n",
            "test accuracy  0.8318 57.78499016165733\n",
            "\n",
            "Epoch: 110\n",
            "train accuracy  0.8432 210.00953477621078\n",
            "test accuracy  0.836 57.56217649579048\n",
            "\n",
            "Epoch: 111\n",
            "train accuracy  0.83974 212.0675332248211\n",
            "test accuracy  0.8223 60.853956043720245\n",
            "\n",
            "Epoch: 112\n",
            "train accuracy  0.8431 208.95213213562965\n",
            "test accuracy  0.8321 56.84577152132988\n",
            "\n",
            "Epoch: 113\n",
            "train accuracy  0.84072 207.88453868031502\n",
            "test accuracy  0.8337 58.44291168451309\n",
            "\n",
            "Epoch: 114\n",
            "train accuracy  0.84032 209.28163075447083\n",
            "test accuracy  0.8177 60.74319264292717\n",
            "\n",
            "Epoch: 115\n",
            "train accuracy  0.84328 204.54387015104294\n",
            "test accuracy  0.821 60.17998665571213\n",
            "\n",
            "Epoch: 116\n",
            "train accuracy  0.84474 203.0304515361786\n",
            "test accuracy  0.8291 56.77225714921951\n",
            "\n",
            "Epoch: 117\n",
            "train accuracy  0.84516 202.85625833272934\n",
            "test accuracy  0.8338 57.58560910820961\n",
            "\n",
            "Epoch: 118\n",
            "train accuracy  0.8482 201.65092411637306\n",
            "test accuracy  0.8288 57.460528790950775\n",
            "\n",
            "Epoch: 119\n",
            "train accuracy  0.84598 198.88937282562256\n",
            "test accuracy  0.8198 60.81570062041283\n",
            "\n",
            "Epoch: 120\n",
            "train accuracy  0.84788 196.7374266386032\n",
            "test accuracy  0.831 57.748853623867035\n",
            "\n",
            "Epoch: 121\n",
            "train accuracy  0.84662 197.70990297198296\n",
            "test accuracy  0.8291 57.22137567400932\n",
            "\n",
            "Epoch: 122\n",
            "train accuracy  0.84828 195.81048014760017\n",
            "test accuracy  0.8316 58.18591848015785\n",
            "\n",
            "Epoch: 123\n",
            "train accuracy  0.84872 194.12086620926857\n",
            "test accuracy  0.8127 63.22132843732834\n",
            "\n",
            "Epoch: 124\n",
            "train accuracy  0.84916 193.8426312506199\n",
            "test accuracy  0.8191 62.3215474486351\n",
            "\n",
            "Epoch: 125\n",
            "train accuracy  0.8505 191.92006534337997\n",
            "test accuracy  0.8197 60.84576192498207\n",
            "\n",
            "Epoch: 126\n",
            "train accuracy  0.85104 191.22176498174667\n",
            "test accuracy  0.8147 62.932822078466415\n",
            "\n",
            "Epoch: 127\n",
            "train accuracy  0.8523 188.85292553901672\n",
            "test accuracy  0.808 65.99584457278252\n",
            "\n",
            "Epoch: 128\n",
            "train accuracy  0.85132 188.37974441051483\n",
            "test accuracy  0.8097 63.727861523628235\n",
            "\n",
            "Epoch: 129\n",
            "train accuracy  0.85508 185.48728969693184\n",
            "test accuracy  0.8022 65.86317685246468\n",
            "\n",
            "Epoch: 130\n",
            "train accuracy  0.8547 183.75931468605995\n",
            "test accuracy  0.8207 59.718653947114944\n",
            "\n",
            "Epoch: 131\n",
            "train accuracy  0.85582 182.88835749030113\n",
            "test accuracy  0.8317 56.74583584070206\n",
            "\n",
            "Epoch: 132\n",
            "train accuracy  0.85654 181.7809717953205\n",
            "test accuracy  0.8221 60.99618351459503\n",
            "\n",
            "Epoch: 133\n",
            "train accuracy  0.85688 180.23178361356258\n",
            "test accuracy  0.8136 62.63860526680946\n",
            "\n",
            "Epoch: 134\n",
            "train accuracy  0.85792 179.59527707099915\n",
            "test accuracy  0.8209 61.17371121048927\n",
            "\n",
            "Epoch: 135\n",
            "train accuracy  0.85638 178.88960555195808\n",
            "test accuracy  0.82 61.942041754722595\n",
            "\n",
            "Epoch: 136\n",
            "train accuracy  0.85654 180.1660772562027\n",
            "test accuracy  0.8116 63.13557142019272\n",
            "\n",
            "Epoch: 137\n",
            "train accuracy  0.86182 175.98113006353378\n",
            "test accuracy  0.8033 65.70567905902863\n",
            "\n",
            "Epoch: 138\n",
            "train accuracy  0.85852 175.18289421498775\n",
            "test accuracy  0.8126 63.08607241511345\n",
            "\n",
            "Epoch: 139\n",
            "train accuracy  0.85958 174.6881042420864\n",
            "test accuracy  0.825 59.813624173402786\n",
            "\n",
            "Epoch: 140\n",
            "train accuracy  0.86158 172.71084178984165\n",
            "test accuracy  0.8097 65.32749184966087\n",
            "\n",
            "Epoch: 141\n",
            "train accuracy  0.86052 172.3997607678175\n",
            "test accuracy  0.8138 61.81658101081848\n",
            "\n",
            "Epoch: 142\n",
            "train accuracy  0.86238 172.07087557017803\n",
            "test accuracy  0.8058 65.5866830945015\n",
            "\n",
            "Epoch: 143\n",
            "train accuracy  0.8629 171.21608053147793\n",
            "test accuracy  0.8184 61.66596603393555\n",
            "\n",
            "Epoch: 144\n",
            "train accuracy  0.86308 170.99905700981617\n",
            "test accuracy  0.8111 63.62751293182373\n",
            "\n",
            "Epoch: 145\n",
            "train accuracy  0.86432 168.3528887629509\n",
            "test accuracy  0.8061 67.58974814414978\n",
            "\n",
            "Epoch: 146\n",
            "train accuracy  0.86464 168.049675360322\n",
            "test accuracy  0.8109 62.49551784992218\n",
            "\n",
            "Epoch: 147\n",
            "train accuracy  0.86638 166.33554539084435\n",
            "test accuracy  0.7996 67.6696566939354\n",
            "\n",
            "Epoch: 148\n",
            "train accuracy  0.8671 164.93716579675674\n",
            "test accuracy  0.8219 60.68379206955433\n",
            "\n",
            "Epoch: 149\n",
            "train accuracy  0.86838 162.45357567071915\n",
            "test accuracy  0.8143 64.36064398288727\n",
            "\n",
            "Epoch: 150\n",
            "train accuracy  0.86758 161.76023118197918\n",
            "test accuracy  0.8108 64.81683921813965\n",
            "\n",
            "Epoch: 151\n",
            "train accuracy  0.86746 162.3501590192318\n",
            "test accuracy  0.8139 62.756285548210144\n",
            "\n",
            "Epoch: 152\n",
            "train accuracy  0.87138 158.4678072631359\n",
            "test accuracy  0.8084 66.58014646172523\n",
            "\n",
            "Epoch: 153\n",
            "train accuracy  0.86946 159.53426414728165\n",
            "test accuracy  0.8114 65.0061009824276\n",
            "\n",
            "Epoch: 154\n",
            "train accuracy  0.86968 159.9129429757595\n",
            "test accuracy  0.8075 64.93652746081352\n",
            "\n",
            "Epoch: 155\n",
            "train accuracy  0.87344 156.8704592883587\n",
            "test accuracy  0.8112 63.93002426624298\n",
            "\n",
            "Epoch: 156\n",
            "train accuracy  0.86982 157.52057845890522\n",
            "test accuracy  0.8065 64.50501689314842\n",
            "\n",
            "Epoch: 157\n",
            "train accuracy  0.86818 158.26995661854744\n",
            "test accuracy  0.8116 64.18570083379745\n",
            "\n",
            "Epoch: 158\n",
            "train accuracy  0.8707 158.3248147815466\n",
            "test accuracy  0.798 67.680524289608\n",
            "\n",
            "Epoch: 159\n",
            "train accuracy  0.87344 152.25829312205315\n",
            "test accuracy  0.8139 64.61457815766335\n",
            "\n",
            "Epoch: 160\n",
            "train accuracy  0.87774 150.76624271273613\n",
            "test accuracy  0.7984 67.24303874373436\n",
            "\n",
            "Epoch: 161\n",
            "train accuracy  0.87336 153.98296128213406\n",
            "test accuracy  0.8128 63.43458968400955\n",
            "\n",
            "Epoch: 162\n",
            "train accuracy  0.87518 150.73193728923798\n",
            "test accuracy  0.8218 62.3444938659668\n",
            "\n",
            "Epoch: 163\n",
            "train accuracy  0.87424 152.9209577292204\n",
            "test accuracy  0.8027 66.76880037784576\n",
            "\n",
            "Epoch: 164\n",
            "train accuracy  0.87304 152.44011910259724\n",
            "test accuracy  0.7925 70.4966692328453\n",
            "\n",
            "Epoch: 165\n",
            "train accuracy  0.87546 149.94102178514004\n",
            "test accuracy  0.8116 62.995829075574875\n",
            "\n",
            "Epoch: 166\n",
            "train accuracy  0.87656 149.03507955372334\n",
            "test accuracy  0.8082 66.72369840741158\n",
            "\n",
            "Epoch: 167\n",
            "train accuracy  0.879 145.79187907278538\n",
            "test accuracy  0.8187 61.217128962278366\n",
            "\n",
            "Epoch: 168\n",
            "train accuracy  0.87632 149.89404276013374\n",
            "test accuracy  0.7948 69.73856461048126\n",
            "\n",
            "Epoch: 169\n",
            "train accuracy  0.87672 146.49679844081402\n",
            "test accuracy  0.808 65.96722453832626\n",
            "\n",
            "Epoch: 170\n",
            "train accuracy  0.87888 146.44215585291386\n",
            "test accuracy  0.8121 64.83605256676674\n",
            "\n",
            "Epoch: 171\n",
            "train accuracy  0.87842 145.58540964126587\n",
            "test accuracy  0.7766 76.40108793973923\n",
            "\n",
            "Epoch: 172\n",
            "train accuracy  0.88006 144.74613478779793\n",
            "test accuracy  0.7803 72.59219866991043\n",
            "\n",
            "Epoch: 173\n",
            "train accuracy  0.87984 144.0187257528305\n",
            "test accuracy  0.7917 72.18901175260544\n",
            "\n",
            "Epoch: 174\n",
            "train accuracy  0.88002 142.48036590218544\n",
            "test accuracy  0.8107 64.67659029364586\n",
            "\n",
            "Epoch: 175\n",
            "train accuracy  0.87902 143.60161167383194\n",
            "test accuracy  0.8115 65.25254708528519\n",
            "\n",
            "Epoch: 176\n",
            "train accuracy  0.88134 141.06219331920147\n",
            "test accuracy  0.8043 68.00282827019691\n",
            "\n",
            "Epoch: 177\n",
            "train accuracy  0.87978 143.2487100660801\n",
            "test accuracy  0.8129 64.70923560857773\n",
            "\n",
            "Epoch: 178\n",
            "train accuracy  0.883 139.88301262259483\n",
            "test accuracy  0.808 65.52607247233391\n",
            "\n",
            "Epoch: 179\n",
            "train accuracy  0.88512 138.15669044852257\n",
            "test accuracy  0.8087 64.80186457931995\n",
            "\n",
            "Epoch: 180\n",
            "train accuracy  0.88284 140.61285969614983\n",
            "test accuracy  0.796 71.26162651181221\n",
            "\n",
            "Epoch: 181\n",
            "train accuracy  0.88506 137.71156752109528\n",
            "test accuracy  0.8089 68.02439507842064\n",
            "\n",
            "Epoch: 182\n",
            "train accuracy  0.88224 140.62483629584312\n",
            "test accuracy  0.7818 75.0390804708004\n",
            "\n",
            "Epoch: 183\n",
            "train accuracy  0.88558 136.01196919381618\n",
            "test accuracy  0.785 73.4690260887146\n",
            "\n",
            "Epoch: 184\n",
            "train accuracy  0.88444 137.8136166781187\n",
            "test accuracy  0.8083 66.30385875701904\n",
            "\n",
            "Epoch: 185\n",
            "train accuracy  0.88478 135.55753754079342\n",
            "test accuracy  0.7953 70.42690145969391\n",
            "\n",
            "Epoch: 186\n",
            "train accuracy  0.88698 135.15882042050362\n",
            "test accuracy  0.7895 71.3526081442833\n",
            "\n",
            "Epoch: 187\n",
            "train accuracy  0.88702 133.227335318923\n",
            "test accuracy  0.7949 69.49506187438965\n",
            "\n",
            "Epoch: 188\n",
            "train accuracy  0.8862 136.4104122966528\n",
            "test accuracy  0.8128 65.21174323558807\n",
            "\n",
            "Epoch: 189\n",
            "train accuracy  0.88906 131.09813843667507\n",
            "test accuracy  0.7992 68.71456250548363\n",
            "\n",
            "Epoch: 190\n",
            "train accuracy  0.88982 132.39657939970493\n",
            "test accuracy  0.781 74.15282535552979\n",
            "\n",
            "Epoch: 191\n",
            "train accuracy  0.8876 132.86439238488674\n",
            "test accuracy  0.8048 66.01298928260803\n",
            "\n",
            "Epoch: 192\n",
            "train accuracy  0.8888 131.9300186187029\n",
            "test accuracy  0.8056 66.16433802247047\n",
            "\n",
            "Epoch: 193\n",
            "train accuracy  0.88876 130.59291103482246\n",
            "test accuracy  0.7978 69.46061617136002\n",
            "\n",
            "Epoch: 194\n",
            "train accuracy  0.88656 133.9025570601225\n",
            "test accuracy  0.8085 65.30799788236618\n",
            "\n",
            "Epoch: 195\n",
            "train accuracy  0.8924 126.59233364462852\n",
            "test accuracy  0.7998 71.06457674503326\n",
            "\n",
            "Epoch: 196\n",
            "train accuracy  0.89146 128.56151469051838\n",
            "test accuracy  0.8027 66.43996897339821\n",
            "\n",
            "Epoch: 197\n",
            "train accuracy  0.88794 133.05735978484154\n",
            "test accuracy  0.8023 68.25538840889931\n",
            "\n",
            "Epoch: 198\n",
            "train accuracy  0.89312 126.68947078287601\n",
            "test accuracy  0.8164 64.69224679470062\n",
            "\n",
            "Epoch: 199\n",
            "train accuracy  0.88934 130.55380715429783\n",
            "test accuracy  0.7996 68.63900861144066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMfVvXNunQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "131e4242-2ce3-4c68-8d9d-0ed7de93482e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.plot(epoch_list, train_acc, 'g--' , label='train_acc')\n",
        "plt.plot(epoch_list, test_acc, 'bo--', label = 'test_acc')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzi-Yr4gvye3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1f86b969-5ad6-4037-dcd3-943da2e8e64a"
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "import random\n",
        "length = len(testset.targets)\n",
        "percentage_corruption = 10\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "print(corrupt_idx)\n",
        "print(len(corrupt_classes))\n",
        "\n",
        "st1 = testset.targets\n",
        "a = np.array(testset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "testset.targets = list(a)\n",
        "st2 = testset.targets\n",
        "\n",
        "print(st1 == st2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "1000 1000\n",
            "[7039, 2506, 8798, 9389, 674, 7760, 9205, 8662, 2464, 4633, 4487, 1881, 5840, 7544, 8836, 7067, 8029, 8786, 1668, 4935, 1222, 7296, 4634, 5624, 9438, 2412, 2888, 7922, 511, 3984, 3935, 7184, 8501, 2437, 6322, 3896, 3398, 7980, 7228, 4745, 894, 833, 336, 7848, 5012, 9605, 4899, 1207, 8730, 8283, 7349, 3192, 5901, 2853, 2557, 4044, 3451, 69, 5470, 8835, 9096, 6949, 1611, 3695, 4729, 5477, 5977, 3003, 7354, 5104, 8825, 1929, 7394, 4584, 7695, 974, 3792, 2448, 9601, 9273, 212, 9284, 2477, 6954, 457, 6129, 9924, 895, 4536, 6152, 2996, 1044, 3538, 8125, 2358, 9071, 4653, 310, 7616, 662, 9145, 2645, 6513, 4337, 6041, 4655, 2564, 8014, 5883, 2667, 7240, 1886, 3556, 5535, 5547, 9821, 9444, 5672, 1872, 6169, 70, 658, 6344, 6701, 2603, 3479, 7399, 5132, 4269, 9929, 9072, 3445, 4145, 6098, 8156, 332, 601, 2146, 4784, 5976, 7880, 4407, 4197, 3921, 4992, 9348, 4009, 2539, 5735, 964, 4769, 9260, 4839, 513, 3135, 6885, 1283, 768, 1338, 3594, 2393, 8169, 765, 1817, 5716, 1310, 1302, 5835, 2651, 653, 5406, 9231, 422, 6564, 1004, 3379, 1705, 1652, 6551, 3455, 7009, 4201, 2505, 7188, 6776, 1029, 772, 5075, 7415, 986, 1603, 6211, 2781, 4163, 6307, 9422, 7536, 7290, 373, 1687, 5828, 7882, 1485, 797, 5625, 139, 4858, 2716, 2686, 4643, 6533, 6449, 8172, 255, 6874, 8976, 7321, 4120, 6094, 8315, 7962, 6724, 504, 1388, 9360, 8654, 8644, 1936, 57, 3910, 2209, 2290, 2291, 775, 6004, 3291, 9878, 3024, 7435, 5454, 4039, 4663, 5229, 7952, 9913, 7833, 4682, 6187, 6242, 8979, 1658, 8182, 3315, 2583, 7280, 9670, 905, 982, 2298, 2407, 3797, 1824, 1063, 4275, 7893, 969, 3178, 6202, 196, 3716, 9477, 787, 3642, 3933, 6289, 3201, 4068, 3793, 5223, 6739, 7029, 5778, 7569, 7070, 4681, 9902, 8110, 8345, 4501, 4484, 3837, 5925, 7214, 1711, 2962, 8140, 9316, 7657, 5529, 1262, 4511, 2838, 7278, 1987, 742, 9769, 1098, 4647, 622, 1132, 6396, 5952, 6623, 5802, 8785, 3133, 2944, 784, 4342, 1563, 747, 9039, 5665, 7379, 3220, 1469, 3503, 9252, 3968, 4383, 4042, 6850, 5923, 5002, 1122, 3619, 7822, 8874, 3184, 4972, 4404, 5042, 6987, 7783, 2022, 4005, 3239, 6694, 6575, 5666, 5566, 5793, 1116, 4045, 4415, 9070, 6203, 1710, 688, 4225, 217, 3114, 5746, 6145, 7477, 5428, 5114, 5701, 2313, 9309, 1358, 65, 90, 4785, 245, 9008, 227, 7436, 9475, 7664, 2334, 2303, 4348, 762, 4693, 1252, 5036, 7426, 8525, 3841, 1491, 6937, 9393, 5197, 9753, 3087, 5065, 676, 778, 4646, 9581, 1841, 8566, 9553, 9332, 6462, 3861, 8541, 8307, 8718, 5023, 4807, 7781, 7687, 8725, 2617, 7140, 4185, 4388, 5700, 1930, 6315, 7229, 5660, 9068, 5166, 6000, 7949, 6807, 491, 8392, 6514, 2586, 5296, 8227, 4576, 2370, 6993, 2338, 1185, 9945, 6364, 8819, 86, 8515, 1794, 6618, 4855, 2033, 8170, 5832, 2908, 1523, 9767, 5372, 8166, 4325, 8171, 9854, 3983, 4463, 9063, 3354, 7072, 8321, 5679, 1962, 6435, 4306, 5965, 6360, 6532, 929, 4735, 222, 5481, 4614, 7001, 1823, 2929, 4711, 6628, 3154, 9993, 3063, 76, 2217, 4656, 5572, 4049, 208, 3205, 8160, 4480, 6997, 7328, 6825, 5564, 1819, 2561, 9532, 7955, 4881, 3176, 6236, 3338, 3740, 9770, 1224, 8989, 1363, 9421, 8248, 8663, 7396, 7984, 9225, 3537, 4952, 2486, 3225, 5858, 3560, 3554, 6111, 7235, 2602, 5429, 96, 1034, 510, 8585, 8469, 1755, 7268, 66, 9365, 5212, 7818, 2476, 2177, 390, 1622, 5649, 921, 1975, 1617, 3901, 5100, 4236, 9681, 516, 4309, 28, 6692, 3508, 5402, 2952, 945, 3919, 8037, 8055, 7602, 1428, 4854, 380, 572, 671, 727, 3064, 7708, 8138, 6910, 6404, 9177, 1134, 6371, 5713, 1339, 7983, 9723, 7403, 5810, 8998, 4917, 4036, 5297, 9195, 497, 2244, 4133, 5642, 5490, 3251, 3602, 9567, 2972, 6797, 3949, 9607, 3336, 593, 8150, 2809, 1996, 1885, 7127, 240, 3221, 9980, 4759, 543, 4695, 6906, 2892, 1323, 1449, 8956, 7017, 1894, 4699, 6558, 8077, 3325, 9161, 4365, 3016, 1261, 5581, 9373, 5804, 2070, 2815, 181, 5690, 699, 3427, 202, 2189, 1951, 2420, 2627, 2805, 2265, 2946, 670, 2999, 216, 9712, 8320, 7174, 9406, 5505, 2600, 5131, 6442, 3806, 3838, 5772, 2176, 505, 3939, 9703, 2889, 2352, 6218, 8412, 2598, 8025, 639, 4959, 5929, 6633, 6518, 9319, 6482, 5386, 8027, 8588, 4426, 4766, 4268, 3564, 2158, 3198, 1750, 6820, 1635, 5763, 3865, 1352, 9211, 8905, 995, 4249, 6006, 5395, 9977, 1420, 6729, 4901, 4928, 9023, 9294, 777, 7276, 2129, 4544, 1001, 1555, 8042, 6854, 5825, 9285, 395, 1113, 8581, 9206, 2174, 6323, 5882, 54, 7688, 1258, 366, 4808, 9347, 1429, 4710, 2383, 4821, 5130, 7550, 7712, 2821, 9053, 7624, 4767, 7380, 9378, 4613, 3982, 1833, 525, 97, 8462, 7742, 5091, 7036, 6782, 6766, 7087, 8343, 5323, 1707, 5967, 7311, 3150, 9953, 6565, 1790, 83, 6340, 474, 7437, 1992, 683, 9283, 815, 6930, 4824, 645, 8259, 4875, 220, 9396, 6777, 6123, 1402, 1143, 6521, 8568, 5461, 9470, 8670, 3928, 7432, 6814, 4700, 408, 9445, 5501, 6971, 9756, 862, 3685, 2790, 3527, 5000, 1861, 1121, 9689, 1779, 9019, 2680, 7666, 4859, 7118, 4586, 2547, 8734, 4800, 2834, 5891, 3836, 6105, 9350, 751, 6643, 5176, 4230, 919, 1854, 611, 3046, 5390, 1086, 1802, 5149, 1239, 9097, 7789, 5139, 2002, 4038, 3293, 8495, 4671, 2422, 1364, 7598, 5958, 6059, 9094, 437, 3871, 6941, 4828, 4282, 5369, 6327, 940, 7293, 799, 9585, 1159, 111, 235, 32, 5346, 2130, 1724, 1308, 1851, 241, 2285, 6278, 3581, 878, 3831, 3352, 585, 4983, 9835, 2983, 8838, 9222, 7546, 3351, 3166, 4363, 3817, 6044, 9083, 2818, 8862, 7747, 5105, 8814, 5240, 5340, 6732, 7356, 6791, 3149, 9530, 5807, 3913, 786, 1309, 1581, 6727, 5961, 146, 383, 1561, 9744, 239, 7604, 926, 8964, 1598, 5601, 5931, 8299, 1616, 317, 1450, 3499, 2015, 3294, 7457, 1716, 9588, 2634, 8401, 7086, 4437, 3805, 8164, 9535, 8177, 3028, 6545, 7333, 2154, 2104, 8224, 558, 7935, 5013, 6616, 6338, 3599, 578, 1160, 5482, 6093, 4826, 8482, 3726, 9895, 3266, 3168, 5754, 4110, 7011, 2956, 1229, 7316, 2231, 3569, 4435, 4223, 5282, 6005, 8458, 194, 7048, 3271, 7093, 536, 3457, 8771, 9591, 8876, 3387, 1243, 101, 5085, 8173, 5866, 1386, 5193, 375, 8679, 4343, 4183, 9098, 3197, 4496, 7856, 8180, 4688, 224, 6622, 4287, 9771, 51, 6305]\n",
            "1000\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWLG2Hmx6gTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5538a6a0-7fa6-4fdb-97e5-6da98cc16e34"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "global best_acc\n",
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "print(\"test accuracy \", correct/total , test_loss)\n",
        "test_acc.append(test_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.7288 127.47102218866348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhIpnf-6xyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}