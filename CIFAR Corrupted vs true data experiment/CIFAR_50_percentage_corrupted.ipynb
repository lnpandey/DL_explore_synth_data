{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_50_percentage_corrupted.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olq-MKcdmOnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = EfficientNetB0()\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlZDWDZm6ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "term_width = 80\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ubk0aCmSD7",
        "colab_type": "code",
        "outputId": "af07c0ad-eeb3-44af-f959-f7c6de447d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "# trainset.targets***************************************************************\n",
        "import random\n",
        "length = len(trainset.targets)\n",
        "percentage_corruption = 50\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "# print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "# print(corrupt_idx)\n",
        "# print(len(corrupt_classes))\n",
        "a = np.array(trainset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "trainset.targets = list(a)\n",
        "#**********************************************************************************\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train_acc =[]\n",
        "test_acc=[]\n",
        "epoch_list=[]\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"train accuracy \", correct/total ,train_loss)\n",
        "    train_acc.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"test accuracy \", correct/total , test_loss)\n",
        "    test_acc.append(test_loss)\n",
        "\n",
        "    # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+280):\n",
        "    epoch_list.append(epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▊| 168181760/170498071 [00:11<00:00, 15989393.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r170500096it [00:30, 15989393.77it/s]                               "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train accuracy  0.18426 880.7980136871338\n",
            "test accuracy  0.3703 183.23696386814117\n",
            "\n",
            "Epoch: 1\n",
            "train accuracy  0.24072 849.2772092819214\n",
            "test accuracy  0.4446 170.4612889289856\n",
            "\n",
            "Epoch: 2\n",
            "train accuracy  0.266 834.2947088479996\n",
            "test accuracy  0.4758 167.137531042099\n",
            "\n",
            "Epoch: 3\n",
            "train accuracy  0.28586 823.3860310316086\n",
            "test accuracy  0.5045 164.32257282733917\n",
            "\n",
            "Epoch: 4\n",
            "train accuracy  0.30076 812.872777223587\n",
            "test accuracy  0.5126 164.90105640888214\n",
            "\n",
            "Epoch: 5\n",
            "train accuracy  0.31312 805.896599650383\n",
            "test accuracy  0.568 150.38087439537048\n",
            "\n",
            "Epoch: 6\n",
            "train accuracy  0.32306 799.2352303266525\n",
            "test accuracy  0.5614 147.9873046875\n",
            "\n",
            "Epoch: 7\n",
            "train accuracy  0.33072 793.8853240013123\n",
            "test accuracy  0.5855 148.0572990179062\n",
            "\n",
            "Epoch: 8\n",
            "train accuracy  0.34146 788.7425773143768\n",
            "test accuracy  0.6023 139.63469171524048\n",
            "\n",
            "Epoch: 9\n",
            "train accuracy  0.3459 784.3352944850922\n",
            "test accuracy  0.6224 138.53489911556244\n",
            "\n",
            "Epoch: 10\n",
            "train accuracy  0.3545 779.2702071666718\n",
            "test accuracy  0.6155 141.86988079547882\n",
            "\n",
            "Epoch: 11\n",
            "train accuracy  0.3581 776.6989023685455\n",
            "test accuracy  0.6446 136.7088395357132\n",
            "\n",
            "Epoch: 12\n",
            "train accuracy  0.36628 770.5583788156509\n",
            "test accuracy  0.6395 133.1139476299286\n",
            "\n",
            "Epoch: 13\n",
            "train accuracy  0.3705 769.0997524261475\n",
            "test accuracy  0.6426 137.62536919116974\n",
            "\n",
            "Epoch: 14\n",
            "train accuracy  0.375 765.7481714487076\n",
            "test accuracy  0.6511 131.37892961502075\n",
            "\n",
            "Epoch: 15\n",
            "train accuracy  0.37768 762.4444131851196\n",
            "test accuracy  0.6539 131.4640885591507\n",
            "\n",
            "Epoch: 16\n",
            "train accuracy  0.38358 760.874328494072\n",
            "test accuracy  0.6682 130.0742782354355\n",
            "\n",
            "Epoch: 17\n",
            "train accuracy  0.38726 757.2026623487473\n",
            "test accuracy  0.6689 126.47319650650024\n",
            "\n",
            "Epoch: 18\n",
            "train accuracy  0.39304 752.9371166229248\n",
            "test accuracy  0.6773 129.17072904109955\n",
            "\n",
            "Epoch: 19\n",
            "train accuracy  0.39432 751.2700604200363\n",
            "test accuracy  0.6838 129.87459111213684\n",
            "\n",
            "Epoch: 20\n",
            "train accuracy  0.39776 748.5166480541229\n",
            "test accuracy  0.6966 119.97625839710236\n",
            "\n",
            "Epoch: 21\n",
            "train accuracy  0.40386 745.7899758815765\n",
            "test accuracy  0.6881 124.69362020492554\n",
            "\n",
            "Epoch: 22\n",
            "train accuracy  0.40656 742.1000509262085\n",
            "test accuracy  0.695 126.31630492210388\n",
            "\n",
            "Epoch: 23\n",
            "train accuracy  0.40936 740.53073823452\n",
            "test accuracy  0.6989 121.25456595420837\n",
            "\n",
            "Epoch: 24\n",
            "train accuracy  0.41318 737.8262979984283\n",
            "test accuracy  0.7023 121.95503234863281\n",
            "\n",
            "Epoch: 25\n",
            "train accuracy  0.41454 736.9331374168396\n",
            "test accuracy  0.7143 120.54212236404419\n",
            "\n",
            "Epoch: 26\n",
            "train accuracy  0.41874 733.8537957668304\n",
            "test accuracy  0.7145 119.43252623081207\n",
            "\n",
            "Epoch: 27\n",
            "train accuracy  0.42044 732.3694298267365\n",
            "test accuracy  0.7187 121.34824573993683\n",
            "\n",
            "Epoch: 28\n",
            "train accuracy  0.42458 729.816800236702\n",
            "test accuracy  0.7089 118.26252484321594\n",
            "\n",
            "Epoch: 29\n",
            "train accuracy  0.42524 727.9363371133804\n",
            "test accuracy  0.7188 117.59929084777832\n",
            "\n",
            "Epoch: 30\n",
            "train accuracy  0.42816 726.7050037384033\n",
            "test accuracy  0.7276 118.83739709854126\n",
            "\n",
            "Epoch: 31\n",
            "train accuracy  0.43062 724.0219616889954\n",
            "test accuracy  0.7362 117.00708758831024\n",
            "\n",
            "Epoch: 32\n",
            "train accuracy  0.43184 723.3603131771088\n",
            "test accuracy  0.7345 114.96240234375\n",
            "\n",
            "Epoch: 33\n",
            "train accuracy  0.43262 721.4166610240936\n",
            "test accuracy  0.745 114.01677799224854\n",
            "\n",
            "Epoch: 34\n",
            "train accuracy  0.43614 719.4548127651215\n",
            "test accuracy  0.709 119.38244557380676\n",
            "\n",
            "Epoch: 35\n",
            "train accuracy  0.43578 718.5551497936249\n",
            "test accuracy  0.7183 115.03138464689255\n",
            "\n",
            "Epoch: 36\n",
            "train accuracy  0.43724 717.1225296258926\n",
            "test accuracy  0.7414 112.41357064247131\n",
            "\n",
            "Epoch: 37\n",
            "train accuracy  0.44086 714.7488123178482\n",
            "test accuracy  0.7317 115.35929799079895\n",
            "\n",
            "Epoch: 38\n",
            "train accuracy  0.4403 714.5664422512054\n",
            "test accuracy  0.738 115.72592866420746\n",
            "\n",
            "Epoch: 39\n",
            "train accuracy  0.44276 712.5739535093307\n",
            "test accuracy  0.7423 111.76280093193054\n",
            "\n",
            "Epoch: 40\n",
            "train accuracy  0.44228 712.8847205638885\n",
            "test accuracy  0.7406 113.8179058432579\n",
            "\n",
            "Epoch: 41\n",
            "train accuracy  0.44406 710.3403356075287\n",
            "test accuracy  0.7517 111.16722249984741\n",
            "\n",
            "Epoch: 42\n",
            "train accuracy  0.44798 708.7903507947922\n",
            "test accuracy  0.7503 110.38559073209763\n",
            "\n",
            "Epoch: 43\n",
            "train accuracy  0.4486 706.0915722846985\n",
            "test accuracy  0.7492 113.62045550346375\n",
            "\n",
            "Epoch: 44\n",
            "train accuracy  0.44752 705.8651187419891\n",
            "test accuracy  0.7481 112.51013761758804\n",
            "\n",
            "Epoch: 45\n",
            "train accuracy  0.45164 704.1556043624878\n",
            "test accuracy  0.7455 111.43931311368942\n",
            "\n",
            "Epoch: 46\n",
            "train accuracy  0.45094 703.4317750930786\n",
            "test accuracy  0.7435 113.54797613620758\n",
            "\n",
            "Epoch: 47\n",
            "train accuracy  0.44996 703.358691573143\n",
            "test accuracy  0.7449 112.62363362312317\n",
            "\n",
            "Epoch: 48\n",
            "train accuracy  0.45114 701.603434085846\n",
            "test accuracy  0.737 114.5928966999054\n",
            "\n",
            "Epoch: 49\n",
            "train accuracy  0.45424 699.9368370771408\n",
            "test accuracy  0.7442 110.60635083913803\n",
            "\n",
            "Epoch: 50\n",
            "train accuracy  0.45462 699.2331755161285\n",
            "test accuracy  0.7563 109.4595490694046\n",
            "\n",
            "Epoch: 51\n",
            "train accuracy  0.45568 697.1143217086792\n",
            "test accuracy  0.7342 112.2925980091095\n",
            "\n",
            "Epoch: 52\n",
            "train accuracy  0.4551 696.6232155561447\n",
            "test accuracy  0.7545 107.1408828496933\n",
            "\n",
            "Epoch: 53\n",
            "train accuracy  0.45752 694.2037626504898\n",
            "test accuracy  0.7505 112.1374471783638\n",
            "\n",
            "Epoch: 54\n",
            "train accuracy  0.4585 694.2229976654053\n",
            "test accuracy  0.7508 113.38590210676193\n",
            "\n",
            "Epoch: 55\n",
            "train accuracy  0.45992 692.5506796836853\n",
            "test accuracy  0.7449 110.85078310966492\n",
            "\n",
            "Epoch: 56\n",
            "train accuracy  0.46006 690.9959614276886\n",
            "test accuracy  0.7459 110.33451360464096\n",
            "\n",
            "Epoch: 57\n",
            "train accuracy  0.46026 690.1210414171219\n",
            "test accuracy  0.7436 112.08955442905426\n",
            "\n",
            "Epoch: 58\n",
            "train accuracy  0.46204 689.2635242938995\n",
            "test accuracy  0.7545 109.60768610239029\n",
            "\n",
            "Epoch: 59\n",
            "train accuracy  0.46366 687.5929424762726\n",
            "test accuracy  0.7337 112.98419445753098\n",
            "\n",
            "Epoch: 60\n",
            "train accuracy  0.46258 686.4183069467545\n",
            "test accuracy  0.7369 112.65886044502258\n",
            "\n",
            "Epoch: 61\n",
            "train accuracy  0.46502 684.3548856973648\n",
            "test accuracy  0.7397 114.22135692834854\n",
            "\n",
            "Epoch: 62\n",
            "train accuracy  0.46476 685.2541953325272\n",
            "test accuracy  0.757 108.57397645711899\n",
            "\n",
            "Epoch: 63\n",
            "train accuracy  0.46614 683.2174836397171\n",
            "test accuracy  0.7538 111.8043383359909\n",
            "\n",
            "Epoch: 64\n",
            "train accuracy  0.46858 680.1746277809143\n",
            "test accuracy  0.7328 114.16144496202469\n",
            "\n",
            "Epoch: 65\n",
            "train accuracy  0.46614 680.5526746511459\n",
            "test accuracy  0.7582 109.39605748653412\n",
            "\n",
            "Epoch: 66\n",
            "train accuracy  0.4675 678.7528680562973\n",
            "test accuracy  0.7582 109.16807931661606\n",
            "\n",
            "Epoch: 67\n",
            "train accuracy  0.46888 678.4931527376175\n",
            "test accuracy  0.7429 109.60338920354843\n",
            "\n",
            "Epoch: 68\n",
            "train accuracy  0.4693 676.2976920604706\n",
            "test accuracy  0.7481 109.60656678676605\n",
            "\n",
            "Epoch: 69\n",
            "train accuracy  0.4699 674.7484856843948\n",
            "test accuracy  0.7435 113.7309273481369\n",
            "\n",
            "Epoch: 70\n",
            "train accuracy  0.47244 673.9673265218735\n",
            "test accuracy  0.7533 105.37157732248306\n",
            "\n",
            "Epoch: 71\n",
            "train accuracy  0.47034 673.2152925729752\n",
            "test accuracy  0.7451 109.6526535153389\n",
            "\n",
            "Epoch: 72\n",
            "train accuracy  0.47216 670.9550483226776\n",
            "test accuracy  0.7476 109.14161795377731\n",
            "\n",
            "Epoch: 73\n",
            "train accuracy  0.47484 670.09723508358\n",
            "test accuracy  0.7475 107.6524069905281\n",
            "\n",
            "Epoch: 74\n",
            "train accuracy  0.47372 667.91559445858\n",
            "test accuracy  0.7408 109.15275943279266\n",
            "\n",
            "Epoch: 75\n",
            "train accuracy  0.47328 667.872763633728\n",
            "test accuracy  0.7327 110.71466892957687\n",
            "\n",
            "Epoch: 76\n",
            "train accuracy  0.4754 666.65214574337\n",
            "test accuracy  0.746 109.12733125686646\n",
            "\n",
            "Epoch: 77\n",
            "train accuracy  0.47604 664.5043042898178\n",
            "test accuracy  0.7304 111.61840611696243\n",
            "\n",
            "Epoch: 78\n",
            "train accuracy  0.4754 662.8907531499863\n",
            "test accuracy  0.7399 110.61299777030945\n",
            "\n",
            "Epoch: 79\n",
            "train accuracy  0.47586 660.9025889635086\n",
            "test accuracy  0.7275 112.74141311645508\n",
            "\n",
            "Epoch: 80\n",
            "train accuracy  0.47756 660.0476176738739\n",
            "test accuracy  0.7283 115.36331582069397\n",
            "\n",
            "Epoch: 81\n",
            "train accuracy  0.4765 659.222904920578\n",
            "test accuracy  0.727 115.49756395816803\n",
            "\n",
            "Epoch: 82\n",
            "train accuracy  0.47686 657.873584985733\n",
            "test accuracy  0.7296 113.35067451000214\n",
            "\n",
            "Epoch: 83\n",
            "train accuracy  0.47948 655.5407148599625\n",
            "test accuracy  0.7158 115.57300889492035\n",
            "\n",
            "Epoch: 84\n",
            "train accuracy  0.47906 655.5588145256042\n",
            "test accuracy  0.7084 115.31654238700867\n",
            "\n",
            "Epoch: 85\n",
            "train accuracy  0.48254 652.590993642807\n",
            "test accuracy  0.7258 114.36684733629227\n",
            "\n",
            "Epoch: 86\n",
            "train accuracy  0.48104 651.1626545190811\n",
            "test accuracy  0.7159 114.34121417999268\n",
            "\n",
            "Epoch: 87\n",
            "train accuracy  0.48182 650.2437877655029\n",
            "test accuracy  0.7383 109.50643306970596\n",
            "\n",
            "Epoch: 88\n",
            "train accuracy  0.4829 647.82275390625\n",
            "test accuracy  0.7207 114.49751591682434\n",
            "\n",
            "Epoch: 89\n",
            "train accuracy  0.48588 645.219654917717\n",
            "test accuracy  0.7208 112.98576265573502\n",
            "\n",
            "Epoch: 90\n",
            "train accuracy  0.48578 644.4910111427307\n",
            "test accuracy  0.7247 110.83813065290451\n",
            "\n",
            "Epoch: 91\n",
            "train accuracy  0.48296 642.9219442605972\n",
            "test accuracy  0.7047 115.18135643005371\n",
            "\n",
            "Epoch: 92\n",
            "train accuracy  0.48696 640.2868897914886\n",
            "test accuracy  0.7129 113.35520297288895\n",
            "\n",
            "Epoch: 93\n",
            "train accuracy  0.48804 638.4671137332916\n",
            "test accuracy  0.7211 112.05495482683182\n",
            "\n",
            "Epoch: 94\n",
            "train accuracy  0.48832 636.4767221212387\n",
            "test accuracy  0.7132 114.20478707551956\n",
            "\n",
            "Epoch: 95\n",
            "train accuracy  0.49138 634.0139153003693\n",
            "test accuracy  0.7118 113.785983979702\n",
            "\n",
            "Epoch: 96\n",
            "train accuracy  0.49166 631.7499670982361\n",
            "test accuracy  0.6966 115.36047828197479\n",
            "\n",
            "Epoch: 97\n",
            "train accuracy  0.4943 628.8701037168503\n",
            "test accuracy  0.7092 113.0176791548729\n",
            "\n",
            "Epoch: 98\n",
            "train accuracy  0.49602 626.2211997509003\n",
            "test accuracy  0.6956 118.96733546257019\n",
            "\n",
            "Epoch: 99\n",
            "train accuracy  0.49474 625.336171746254\n",
            "test accuracy  0.6601 123.77964735031128\n",
            "\n",
            "Epoch: 100\n",
            "train accuracy  0.49934 622.3048788309097\n",
            "test accuracy  0.6862 119.56047129631042\n",
            "\n",
            "Epoch: 101\n",
            "train accuracy  0.49836 621.2593059539795\n",
            "test accuracy  0.6932 116.83722031116486\n",
            "\n",
            "Epoch: 102\n",
            "train accuracy  0.49862 619.0522881746292\n",
            "test accuracy  0.686 117.82874810695648\n",
            "\n",
            "Epoch: 103\n",
            "train accuracy  0.50002 615.5876684188843\n",
            "test accuracy  0.6565 124.79269778728485\n",
            "\n",
            "Epoch: 104\n",
            "train accuracy  0.49954 614.1605294942856\n",
            "test accuracy  0.671 122.4634782075882\n",
            "\n",
            "Epoch: 105\n",
            "train accuracy  0.50312 611.4744073152542\n",
            "test accuracy  0.6823 119.20482808351517\n",
            "\n",
            "Epoch: 106\n",
            "train accuracy  0.50506 608.2461137771606\n",
            "test accuracy  0.653 124.68247401714325\n",
            "\n",
            "Epoch: 107\n",
            "train accuracy  0.50504 609.3096702098846\n",
            "test accuracy  0.677 118.46592438220978\n",
            "\n",
            "Epoch: 108\n",
            "train accuracy  0.5071 603.0775743722916\n",
            "test accuracy  0.6696 122.80689442157745\n",
            "\n",
            "Epoch: 109\n",
            "train accuracy  0.50868 602.0343691110611\n",
            "test accuracy  0.6757 115.93602478504181\n",
            "\n",
            "Epoch: 110\n",
            "train accuracy  0.51086 598.3903660774231\n",
            "test accuracy  0.6629 122.34684181213379\n",
            "\n",
            "Epoch: 111\n",
            "train accuracy  0.50856 596.6631178855896\n",
            "test accuracy  0.6583 120.58253693580627\n",
            "\n",
            "Epoch: 112\n",
            "train accuracy  0.51364 592.2861857414246\n",
            "test accuracy  0.6383 125.41183227300644\n",
            "\n",
            "Epoch: 113\n",
            "train accuracy  0.51554 592.0073853731155\n",
            "test accuracy  0.6519 123.9802725315094\n",
            "\n",
            "Epoch: 114\n",
            "train accuracy  0.51982 584.8691749572754\n",
            "test accuracy  0.6713 118.27699935436249\n",
            "\n",
            "Epoch: 115\n",
            "train accuracy  0.5169 585.0451114177704\n",
            "test accuracy  0.6518 123.03542983531952\n",
            "\n",
            "Epoch: 116\n",
            "train accuracy  0.52016 582.4343862533569\n",
            "test accuracy  0.6412 125.44624918699265\n",
            "\n",
            "Epoch: 117\n",
            "train accuracy  0.5228 579.3323115110397\n",
            "test accuracy  0.6271 128.6551092863083\n",
            "\n",
            "Epoch: 118\n",
            "train accuracy  0.52496 574.7648519277573\n",
            "test accuracy  0.6315 125.79077899456024\n",
            "\n",
            "Epoch: 119\n",
            "train accuracy  0.52404 572.7781298160553\n",
            "test accuracy  0.622 127.56431078910828\n",
            "\n",
            "Epoch: 120\n",
            "train accuracy  0.52456 569.1509441137314\n",
            "test accuracy  0.6255 126.87791323661804\n",
            "\n",
            "Epoch: 121\n",
            "train accuracy  0.5304 565.9886087179184\n",
            "test accuracy  0.6385 125.04916697740555\n",
            "\n",
            "Epoch: 122\n",
            "train accuracy  0.53296 563.0966765880585\n",
            "test accuracy  0.6187 130.1306574344635\n",
            "\n",
            "Epoch: 123\n",
            "train accuracy  0.5327 560.2627575397491\n",
            "test accuracy  0.6022 131.59986698627472\n",
            "\n",
            "Epoch: 124\n",
            "train accuracy  0.53568 557.3388668298721\n",
            "test accuracy  0.5951 132.8725016117096\n",
            "\n",
            "Epoch: 125\n",
            "train accuracy  0.53542 554.869637966156\n",
            "test accuracy  0.6094 129.9078049659729\n",
            "\n",
            "Epoch: 126\n",
            "train accuracy  0.54048 550.1763789653778\n",
            "test accuracy  0.5887 135.3813967704773\n",
            "\n",
            "Epoch: 127\n",
            "train accuracy  0.54012 549.0019025802612\n",
            "test accuracy  0.5896 135.32721889019012\n",
            "\n",
            "Epoch: 128\n",
            "train accuracy  0.5439 543.5672810077667\n",
            "test accuracy  0.593 132.02740800380707\n",
            "\n",
            "Epoch: 129\n",
            "train accuracy  0.54762 539.7014348506927\n",
            "test accuracy  0.6135 128.58781158924103\n",
            "\n",
            "Epoch: 130\n",
            "train accuracy  0.54884 538.5166229009628\n",
            "test accuracy  0.6227 128.34393632411957\n",
            "\n",
            "Epoch: 131\n",
            "train accuracy  0.55194 534.4878519773483\n",
            "test accuracy  0.6138 128.48679649829865\n",
            "\n",
            "Epoch: 132\n",
            "train accuracy  0.55582 529.3517588376999\n",
            "test accuracy  0.6042 132.84217047691345\n",
            "\n",
            "Epoch: 133\n",
            "train accuracy  0.55694 526.1296164393425\n",
            "test accuracy  0.5547 144.49114298820496\n",
            "\n",
            "Epoch: 134\n",
            "train accuracy  0.56088 522.6852380037308\n",
            "test accuracy  0.5764 138.4531456232071\n",
            "\n",
            "Epoch: 135\n",
            "train accuracy  0.56288 518.7199564576149\n",
            "test accuracy  0.5834 136.28137814998627\n",
            "\n",
            "Epoch: 136\n",
            "train accuracy  0.56774 513.2637296915054\n",
            "test accuracy  0.5858 136.35025095939636\n",
            "\n",
            "Epoch: 137\n",
            "train accuracy  0.56764 509.7949869632721\n",
            "test accuracy  0.5738 140.20974051952362\n",
            "\n",
            "Epoch: 138\n",
            "train accuracy  0.56884 508.5605068206787\n",
            "test accuracy  0.5785 138.087877035141\n",
            "\n",
            "Epoch: 139\n",
            "train accuracy  0.57116 505.33338862657547\n",
            "test accuracy  0.5597 140.40841472148895\n",
            "\n",
            "Epoch: 140\n",
            "train accuracy  0.57436 501.7944330573082\n",
            "test accuracy  0.5833 133.46036624908447\n",
            "\n",
            "Epoch: 141\n",
            "train accuracy  0.57912 496.9416280388832\n",
            "test accuracy  0.5809 135.99105048179626\n",
            "\n",
            "Epoch: 142\n",
            "train accuracy  0.5803 495.51049560308456\n",
            "test accuracy  0.5569 143.15184772014618\n",
            "\n",
            "Epoch: 143\n",
            "train accuracy  0.58434 490.23542338609695\n",
            "test accuracy  0.5637 142.04882419109344\n",
            "\n",
            "Epoch: 144\n",
            "train accuracy  0.58544 486.08028119802475\n",
            "test accuracy  0.5591 140.66433882713318\n",
            "\n",
            "Epoch: 145\n",
            "train accuracy  0.5842 484.807486474514\n",
            "test accuracy  0.5529 144.31740045547485\n",
            "\n",
            "Epoch: 146\n",
            "train accuracy  0.58964 480.30461168289185\n",
            "test accuracy  0.5525 146.40367662906647\n",
            "\n",
            "Epoch: 147\n",
            "train accuracy  0.59302 476.7133472561836\n",
            "test accuracy  0.5657 140.38613510131836\n",
            "\n",
            "Epoch: 148\n",
            "train accuracy  0.5935 473.8553696870804\n",
            "test accuracy  0.5474 146.62283265590668\n",
            "\n",
            "Epoch: 149\n",
            "train accuracy  0.59708 470.9522736668587\n",
            "test accuracy  0.5723 138.73421013355255\n",
            "\n",
            "Epoch: 150\n",
            "train accuracy  0.59946 467.53878849744797\n",
            "test accuracy  0.5494 146.56622421741486\n",
            "\n",
            "Epoch: 151\n",
            "train accuracy  0.60096 464.84807658195496\n",
            "test accuracy  0.5605 142.93056440353394\n",
            "\n",
            "Epoch: 152\n",
            "train accuracy  0.60478 459.8880733847618\n",
            "test accuracy  0.5566 143.2948019504547\n",
            "\n",
            "Epoch: 153\n",
            "train accuracy  0.60868 456.7010260820389\n",
            "test accuracy  0.5395 146.55310535430908\n",
            "\n",
            "Epoch: 154\n",
            "train accuracy  0.61438 450.04095083475113\n",
            "test accuracy  0.5497 145.54489958286285\n",
            "\n",
            "Epoch: 155\n",
            "train accuracy  0.61376 447.95384269952774\n",
            "test accuracy  0.5442 149.14411234855652\n",
            "\n",
            "Epoch: 156\n",
            "train accuracy  0.61718 446.69132417440414\n",
            "test accuracy  0.5524 146.32596170902252\n",
            "\n",
            "Epoch: 157\n",
            "train accuracy  0.62104 441.85463947057724\n",
            "test accuracy  0.5589 142.64263939857483\n",
            "\n",
            "Epoch: 158\n",
            "train accuracy  0.6212 439.1504056453705\n",
            "test accuracy  0.5266 152.6011859178543\n",
            "\n",
            "Epoch: 159\n",
            "train accuracy  0.62558 435.98820984363556\n",
            "test accuracy  0.5381 154.5006002187729\n",
            "\n",
            "Epoch: 160\n",
            "train accuracy  0.62674 431.9212281703949\n",
            "test accuracy  0.5503 149.00756907463074\n",
            "\n",
            "Epoch: 161\n",
            "train accuracy  0.63238 425.7920909523964\n",
            "test accuracy  0.5114 159.24906432628632\n",
            "\n",
            "Epoch: 162\n",
            "train accuracy  0.63246 424.6423867344856\n",
            "test accuracy  0.5175 158.16046142578125\n",
            "\n",
            "Epoch: 163\n",
            "train accuracy  0.6347 421.7480311989784\n",
            "test accuracy  0.5298 154.50215351581573\n",
            "\n",
            "Epoch: 164\n",
            "train accuracy  0.64132 414.37786239385605\n",
            "test accuracy  0.5049 160.25841093063354\n",
            "\n",
            "Epoch: 165\n",
            "train accuracy  0.64338 414.30558383464813\n",
            "test accuracy  0.5294 154.17670738697052\n",
            "\n",
            "Epoch: 166\n",
            "train accuracy  0.64298 413.3182207942009\n",
            "test accuracy  0.5004 165.1259390115738\n",
            "\n",
            "Epoch: 167\n",
            "train accuracy  0.64304 412.80940425395966\n",
            "test accuracy  0.5343 155.22457575798035\n",
            "\n",
            "Epoch: 168\n",
            "train accuracy  0.64998 404.31740963459015\n",
            "test accuracy  0.5209 161.48646187782288\n",
            "\n",
            "Epoch: 169\n",
            "train accuracy  0.64996 402.872290790081\n",
            "test accuracy  0.4681 177.04205107688904\n",
            "\n",
            "Epoch: 170\n",
            "train accuracy  0.65484 398.86395835876465\n",
            "test accuracy  0.5098 160.0728200674057\n",
            "\n",
            "Epoch: 171\n",
            "train accuracy  0.65992 394.13079619407654\n",
            "test accuracy  0.5236 161.3833932876587\n",
            "\n",
            "Epoch: 172\n",
            "train accuracy  0.6559 394.40591472387314\n",
            "test accuracy  0.5095 160.87127435207367\n",
            "\n",
            "Epoch: 173\n",
            "train accuracy  0.6612 392.17547821998596\n",
            "test accuracy  0.4991 165.67534506320953\n",
            "\n",
            "Epoch: 174\n",
            "train accuracy  0.66396 386.21146804094315\n",
            "test accuracy  0.509 164.11348724365234\n",
            "\n",
            "Epoch: 175\n",
            "train accuracy  0.6664 385.04757076501846\n",
            "test accuracy  0.5333 155.74306559562683\n",
            "\n",
            "Epoch: 176\n",
            "train accuracy  0.66682 382.39946031570435\n",
            "test accuracy  0.5127 163.1110851764679\n",
            "\n",
            "Epoch: 177\n",
            "train accuracy  0.66842 381.4644972085953\n",
            "test accuracy  0.5224 162.15590620040894\n",
            "\n",
            "Epoch: 178\n",
            "train accuracy  0.67274 375.13956040143967\n",
            "test accuracy  0.4943 168.3094174861908\n",
            "\n",
            "Epoch: 179\n",
            "train accuracy  0.67298 376.2965251803398\n",
            "test accuracy  0.5114 163.5089681148529\n",
            "\n",
            "Epoch: 180\n",
            "train accuracy  0.67502 372.9128091931343\n",
            "test accuracy  0.4893 170.59436869621277\n",
            "\n",
            "Epoch: 181\n",
            "train accuracy  0.67614 371.7270525097847\n",
            "test accuracy  0.5287 158.27891516685486\n",
            "\n",
            "Epoch: 182\n",
            "train accuracy  0.6801 366.5900653600693\n",
            "test accuracy  0.5145 161.6043689250946\n",
            "\n",
            "Epoch: 183\n",
            "train accuracy  0.68002 367.19054359197617\n",
            "test accuracy  0.5186 160.22553277015686\n",
            "\n",
            "Epoch: 184\n",
            "train accuracy  0.6873 360.6186343431473\n",
            "test accuracy  0.4956 170.79590511322021\n",
            "\n",
            "Epoch: 185\n",
            "train accuracy  0.68742 360.2389267683029\n",
            "test accuracy  0.5266 160.24453210830688\n",
            "\n",
            "Epoch: 186\n",
            "train accuracy  0.6873 358.2351248860359\n",
            "test accuracy  0.4835 171.29560804367065\n",
            "\n",
            "Epoch: 187\n",
            "train accuracy  0.68782 358.11156764626503\n",
            "test accuracy  0.5154 162.1564689874649\n",
            "\n",
            "Epoch: 188\n",
            "train accuracy  0.69078 355.3002591729164\n",
            "test accuracy  0.5191 163.48529660701752\n",
            "\n",
            "Epoch: 189\n",
            "train accuracy  0.69592 347.752234518528\n",
            "test accuracy  0.5124 164.51410102844238\n",
            "\n",
            "Epoch: 190\n",
            "train accuracy  0.69088 352.08569407463074\n",
            "test accuracy  0.522 160.10379099845886\n",
            "\n",
            "Epoch: 191\n",
            "train accuracy  0.6979 347.089291036129\n",
            "test accuracy  0.496 169.92682027816772\n",
            "\n",
            "Epoch: 192\n",
            "train accuracy  0.69866 345.63008695840836\n",
            "test accuracy  0.4901 171.35135996341705\n",
            "\n",
            "Epoch: 193\n",
            "train accuracy  0.69888 343.4499153494835\n",
            "test accuracy  0.496 171.79379415512085\n",
            "\n",
            "Epoch: 194\n",
            "train accuracy  0.69638 344.9553509950638\n",
            "test accuracy  0.5103 167.1924111843109\n",
            "\n",
            "Epoch: 195\n",
            "train accuracy  0.6974 343.8866058290005\n",
            "test accuracy  0.4822 175.30724608898163\n",
            "\n",
            "Epoch: 196\n",
            "train accuracy  0.70526 338.43515688180923\n",
            "test accuracy  0.4817 178.3014611005783\n",
            "\n",
            "Epoch: 197\n",
            "train accuracy  0.70628 335.69502514600754\n",
            "test accuracy  0.4497 188.10805559158325\n",
            "\n",
            "Epoch: 198\n",
            "train accuracy  0.7038 339.2057236433029\n",
            "test accuracy  0.4825 178.57718646526337\n",
            "\n",
            "Epoch: 199\n",
            "train accuracy  0.71228 331.9219055175781\n",
            "test accuracy  0.5019 172.18519473075867\n",
            "\n",
            "Epoch: 200\n",
            "train accuracy  0.70836 333.35984110832214\n",
            "test accuracy  0.5053 169.0989509820938\n",
            "\n",
            "Epoch: 201\n",
            "train accuracy  0.71302 328.26773977279663\n",
            "test accuracy  0.4839 179.70004284381866\n",
            "\n",
            "Epoch: 202\n",
            "train accuracy  0.71054 331.22752583026886\n",
            "test accuracy  0.4794 180.71682357788086\n",
            "\n",
            "Epoch: 203\n",
            "train accuracy  0.71592 324.4999867081642\n",
            "test accuracy  0.4745 180.23002350330353\n",
            "\n",
            "Epoch: 204\n",
            "train accuracy  0.71274 327.68875324726105\n",
            "test accuracy  0.4935 177.13416945934296\n",
            "\n",
            "Epoch: 205\n",
            "train accuracy  0.7189 321.5320182442665\n",
            "test accuracy  0.4723 181.6559977531433\n",
            "\n",
            "Epoch: 206\n",
            "train accuracy  0.71898 322.7401542067528\n",
            "test accuracy  0.4938 175.21741139888763\n",
            "\n",
            "Epoch: 207\n",
            "train accuracy  0.7207 319.86190533638\n",
            "test accuracy  0.4845 176.58243465423584\n",
            "\n",
            "Epoch: 208\n",
            "train accuracy  0.72394 317.98509281873703\n",
            "test accuracy  0.4957 175.84752547740936\n",
            "\n",
            "Epoch: 209\n",
            "train accuracy  0.72538 314.5004706978798\n",
            "test accuracy  0.4786 182.56930196285248\n",
            "\n",
            "Epoch: 210\n",
            "train accuracy  0.71992 320.3167698085308\n",
            "test accuracy  0.4752 181.60072529315948\n",
            "\n",
            "Epoch: 211\n",
            "train accuracy  0.72782 313.0195871591568\n",
            "test accuracy  0.4839 177.19658482074738\n",
            "\n",
            "Epoch: 212\n",
            "train accuracy  0.72222 318.27567222714424\n",
            "test accuracy  0.4706 179.64395678043365\n",
            "\n",
            "Epoch: 213\n",
            "train accuracy  0.7261 312.24942296743393\n",
            "test accuracy  0.4844 177.56434309482574\n",
            "\n",
            "Epoch: 214\n",
            "train accuracy  0.72958 312.0870244204998\n",
            "test accuracy  0.481 181.92829954624176\n",
            "\n",
            "Epoch: 215\n",
            "train accuracy  0.73136 309.395075827837\n",
            "test accuracy  0.4868 177.87322473526\n",
            "\n",
            "Epoch: 216\n",
            "train accuracy  0.73254 305.43968090415\n",
            "test accuracy  0.4799 188.37552046775818\n",
            "\n",
            "Epoch: 217\n",
            "train accuracy  0.7289 312.21811014413834\n",
            "test accuracy  0.4705 184.753364443779\n",
            "\n",
            "Epoch: 218\n",
            "train accuracy  0.73166 305.46056470274925\n",
            "test accuracy  0.5013 178.0813431739807\n",
            "\n",
            "Epoch: 219\n",
            "train accuracy  0.73102 307.88953801989555\n",
            "test accuracy  0.4999 172.34579372406006\n",
            "\n",
            "Epoch: 220\n",
            "train accuracy  0.73676 301.7530315518379\n",
            "test accuracy  0.479 183.03449606895447\n",
            "\n",
            "Epoch: 221\n",
            "train accuracy  0.73522 302.17767947912216\n",
            "test accuracy  0.4679 186.05900585651398\n",
            "\n",
            "Epoch: 222\n",
            "train accuracy  0.73722 302.5916271209717\n",
            "test accuracy  0.4851 179.7994692325592\n",
            "\n",
            "Epoch: 223\n",
            "train accuracy  0.73702 300.9323360323906\n",
            "test accuracy  0.4742 184.6425997018814\n",
            "\n",
            "Epoch: 224\n",
            "train accuracy  0.7434 292.8794416487217\n",
            "test accuracy  0.4861 181.38720226287842\n",
            "\n",
            "Epoch: 225\n",
            "train accuracy  0.73758 298.33736476302147\n",
            "test accuracy  0.4681 184.97742116451263\n",
            "\n",
            "Epoch: 226\n",
            "train accuracy  0.73886 299.8431586921215\n",
            "test accuracy  0.4802 178.22399592399597\n",
            "\n",
            "Epoch: 227\n",
            "train accuracy  0.74058 295.4519287645817\n",
            "test accuracy  0.465 189.86597204208374\n",
            "\n",
            "Epoch: 228\n",
            "train accuracy  0.74074 295.2314415872097\n",
            "test accuracy  0.4742 187.89441871643066\n",
            "\n",
            "Epoch: 229\n",
            "train accuracy  0.7442 293.93921586871147\n",
            "test accuracy  0.4749 184.94703578948975\n",
            "\n",
            "Epoch: 230\n",
            "train accuracy  0.74152 295.69908556342125\n",
            "test accuracy  0.4707 182.84601652622223\n",
            "\n",
            "Epoch: 231\n",
            "train accuracy  0.74276 292.26794520020485\n",
            "test accuracy  0.4645 188.93236482143402\n",
            "\n",
            "Epoch: 232\n",
            "train accuracy  0.73936 295.43930369615555\n",
            "test accuracy  0.4641 188.85322558879852\n",
            "\n",
            "Epoch: 233\n",
            "train accuracy  0.74666 291.8303074836731\n",
            "test accuracy  0.4795 183.52994883060455\n",
            "\n",
            "Epoch: 234\n",
            "train accuracy  0.74652 288.10738158226013\n",
            "test accuracy  0.4682 185.2149029970169\n",
            "\n",
            "Epoch: 235\n",
            "train accuracy  0.7431 292.88552647829056\n",
            "test accuracy  0.4747 185.49380731582642\n",
            "\n",
            "Epoch: 236\n",
            "train accuracy  0.74832 289.2615680396557\n",
            "test accuracy  0.4714 180.31787753105164\n",
            "\n",
            "Epoch: 237\n",
            "train accuracy  0.75334 284.0221266448498\n",
            "test accuracy  0.4613 186.11209654808044\n",
            "\n",
            "Epoch: 238\n",
            "train accuracy  0.75034 284.81763342022896\n",
            "test accuracy  0.4798 184.40300571918488\n",
            "\n",
            "Epoch: 239\n",
            "train accuracy  0.75366 283.36861354112625\n",
            "test accuracy  0.4746 187.34534776210785\n",
            "\n",
            "Epoch: 240\n",
            "train accuracy  0.74984 285.88667047023773\n",
            "test accuracy  0.4751 183.3467562198639\n",
            "\n",
            "Epoch: 241\n",
            "train accuracy  0.7511 283.2566412091255\n",
            "test accuracy  0.4753 184.4755996465683\n",
            "\n",
            "Epoch: 242\n",
            "train accuracy  0.75478 281.8654838502407\n",
            "test accuracy  0.449 198.6824643611908\n",
            "\n",
            "Epoch: 243\n",
            "train accuracy  0.74868 287.1666027903557\n",
            "test accuracy  0.4622 191.52780103683472\n",
            "\n",
            "Epoch: 244\n",
            "train accuracy  0.7563 279.43614611029625\n",
            "test accuracy  0.4536 198.24337792396545\n",
            "\n",
            "Epoch: 245\n",
            "train accuracy  0.75272 282.45621398091316\n",
            "test accuracy  0.4899 183.79080986976624\n",
            "\n",
            "Epoch: 246\n",
            "train accuracy  0.75064 285.0353207588196\n",
            "test accuracy  0.4748 188.47984993457794\n",
            "\n",
            "Epoch: 247\n",
            "train accuracy  0.75628 278.13924103975296\n",
            "test accuracy  0.469 194.87192499637604\n",
            "\n",
            "Epoch: 248\n",
            "train accuracy  0.75242 284.7402311563492\n",
            "test accuracy  0.4684 188.16574215888977\n",
            "\n",
            "Epoch: 249\n",
            "train accuracy  0.75562 279.5036309659481\n",
            "test accuracy  0.4935 182.1290488243103\n",
            "\n",
            "Epoch: 250\n",
            "train accuracy  0.7581 277.3246927559376\n",
            "test accuracy  0.5034 178.10909116268158\n",
            "\n",
            "Epoch: 251\n",
            "train accuracy  0.75476 280.2830409705639\n",
            "test accuracy  0.4804 182.61256980895996\n",
            "\n",
            "Epoch: 252\n",
            "train accuracy  0.75724 276.60735461115837\n",
            "test accuracy  0.4724 187.08196187019348\n",
            "\n",
            "Epoch: 253\n",
            "train accuracy  0.75766 276.9533261060715\n",
            "test accuracy  0.458 198.07799458503723\n",
            "\n",
            "Epoch: 254\n",
            "train accuracy  0.7561 278.5686476826668\n",
            "test accuracy  0.4755 187.08642387390137\n",
            "\n",
            "Epoch: 255\n",
            "train accuracy  0.75546 279.3011410832405\n",
            "test accuracy  0.4652 189.85825610160828\n",
            "\n",
            "Epoch: 256\n",
            "train accuracy  0.75648 278.950999379158\n",
            "test accuracy  0.4651 196.15628945827484\n",
            "\n",
            "Epoch: 257\n",
            "train accuracy  0.75646 278.53173837065697\n",
            "test accuracy  0.4754 190.91041767597198\n",
            "\n",
            "Epoch: 258\n",
            "train accuracy  0.76118 275.1764151453972\n",
            "test accuracy  0.491 183.4213662147522\n",
            "\n",
            "Epoch: 259\n",
            "train accuracy  0.76012 274.07978132367134\n",
            "test accuracy  0.4756 189.95962285995483\n",
            "\n",
            "Epoch: 260\n",
            "train accuracy  0.7558 277.3612621128559\n",
            "test accuracy  0.4473 202.54112708568573\n",
            "\n",
            "Epoch: 261\n",
            "train accuracy  0.75798 277.3719172179699\n",
            "test accuracy  0.4683 192.2026927471161\n",
            "\n",
            "Epoch: 262\n",
            "train accuracy  0.76208 272.7541442513466\n",
            "test accuracy  0.4586 193.09655940532684\n",
            "\n",
            "Epoch: 263\n",
            "train accuracy  0.75998 273.48962301015854\n",
            "test accuracy  0.4664 195.0947072505951\n",
            "\n",
            "Epoch: 264\n",
            "train accuracy  0.75846 277.15941363573074\n",
            "test accuracy  0.4793 186.96149444580078\n",
            "\n",
            "Epoch: 265\n",
            "train accuracy  0.76152 273.04405814409256\n",
            "test accuracy  0.4629 197.42001163959503\n",
            "\n",
            "Epoch: 266\n",
            "train accuracy  0.7629 271.6040196418762\n",
            "test accuracy  0.4506 203.384397149086\n",
            "\n",
            "Epoch: 267\n",
            "train accuracy  0.76422 270.37501069903374\n",
            "test accuracy  0.4882 184.07297790050507\n",
            "\n",
            "Epoch: 268\n",
            "train accuracy  0.76326 270.7155672311783\n",
            "test accuracy  0.4629 196.5640765428543\n",
            "\n",
            "Epoch: 269\n",
            "train accuracy  0.7644 272.1296715736389\n",
            "test accuracy  0.4697 191.63264000415802\n",
            "\n",
            "Epoch: 270\n",
            "train accuracy  0.76064 272.77461165189743\n",
            "test accuracy  0.4676 192.07038640975952\n",
            "\n",
            "Epoch: 271\n",
            "train accuracy  0.76014 272.3087166547775\n",
            "test accuracy  0.4609 197.4937446117401\n",
            "\n",
            "Epoch: 272\n",
            "train accuracy  0.763 271.3431994318962\n",
            "test accuracy  0.4689 189.26691341400146\n",
            "\n",
            "Epoch: 273\n",
            "train accuracy  0.76564 268.99452140927315\n",
            "test accuracy  0.4875 183.06551361083984\n",
            "\n",
            "Epoch: 274\n",
            "train accuracy  0.76014 273.9510472714901\n",
            "test accuracy  0.4688 194.56135773658752\n",
            "\n",
            "Epoch: 275\n",
            "train accuracy  0.76466 268.60820785164833\n",
            "test accuracy  0.4516 204.70310580730438\n",
            "\n",
            "Epoch: 276\n",
            "train accuracy  0.76026 273.7092439830303\n",
            "test accuracy  0.4815 183.91188621520996\n",
            "\n",
            "Epoch: 277\n",
            "train accuracy  0.76306 271.05712711811066\n",
            "test accuracy  0.4787 188.72097444534302\n",
            "\n",
            "Epoch: 278\n",
            "train accuracy  0.76658 268.73681727051735\n",
            "test accuracy  0.4638 199.5466421842575\n",
            "\n",
            "Epoch: 279\n",
            "train accuracy  0.76226 270.3872118592262\n",
            "test accuracy  0.447 205.1243920326233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMfVvXNunQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "127aebde-0629-4ccc-c962-32caef310814"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.plot(epoch_list, train_acc, 'g--' , label='train_acc')\n",
        "plt.plot(epoch_list, test_acc, 'bo--', label = 'test_acc')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzi-Yr4gvye3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "74fbba5b-1af6-4eef-beb5-84616fea9f9d"
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "import random\n",
        "length = len(testset.targets)\n",
        "percentage_corruption = 40\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "print(corrupt_idx)\n",
        "print(len(corrupt_classes))\n",
        "\n",
        "st1 = testset.targets\n",
        "a = np.array(testset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "testset.targets = list(a)\n",
        "st2 = testset.targets\n",
        "\n",
        "print(st1 == st2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "4000 4000\n",
            "[9296, 6623, 986, 6730, 4058, 769, 9371, 4072, 4819, 5888, 6885, 8625, 1280, 1600, 4901, 1262, 7454, 5738, 9196, 1819, 6539, 7673, 715, 3031, 9607, 7489, 8686, 5491, 3140, 3943, 4907, 3096, 6456, 1942, 6077, 3765, 2136, 2678, 5200, 6978, 9720, 8978, 7609, 802, 1461, 4524, 99, 314, 223, 7073, 5669, 9089, 8099, 5694, 3373, 8071, 1140, 5015, 6404, 6346, 5308, 3170, 6258, 8152, 8869, 2882, 9669, 2757, 696, 4728, 3462, 4268, 9185, 4535, 1360, 947, 4127, 5806, 951, 7526, 8706, 1588, 2521, 8196, 3695, 5113, 4876, 6066, 5427, 385, 9341, 4973, 9477, 6185, 1531, 6621, 1625, 4730, 6038, 759, 4789, 1871, 4105, 9965, 8056, 6041, 8746, 7155, 4774, 7532, 8729, 2393, 1977, 7929, 1721, 5857, 5842, 9427, 3435, 7877, 3730, 4319, 4537, 7253, 9830, 9564, 4158, 9687, 4746, 6687, 8642, 752, 4418, 6112, 7430, 8313, 8095, 5051, 2309, 3889, 3589, 5154, 9586, 8673, 1840, 2733, 9717, 2780, 3198, 2651, 3747, 7431, 2777, 3533, 8370, 2423, 125, 3806, 4632, 1848, 4814, 1293, 5289, 8248, 1900, 5664, 1885, 2866, 6553, 930, 7791, 8194, 1133, 8428, 8451, 3970, 6379, 8572, 4027, 7096, 906, 7005, 9028, 1068, 9247, 4778, 8031, 2988, 4499, 7480, 7683, 5106, 3583, 1430, 6705, 4886, 2117, 9835, 5648, 4224, 2238, 7930, 3897, 7636, 2273, 92, 7684, 3762, 6367, 2530, 6483, 857, 8419, 3522, 936, 3865, 6303, 7756, 3918, 3663, 9987, 653, 1877, 6007, 4482, 636, 2683, 1676, 1055, 8899, 2013, 6955, 5914, 2321, 7650, 2591, 8303, 9321, 1638, 1286, 7653, 4262, 2282, 3307, 4515, 3200, 6486, 2323, 2361, 1019, 7439, 5925, 7619, 276, 7737, 5006, 7215, 6577, 6216, 4068, 101, 743, 3684, 6168, 589, 7692, 5724, 5177, 2058, 4660, 225, 7500, 8840, 7004, 8453, 7773, 7284, 5222, 7448, 7889, 2313, 2467, 6891, 42, 2210, 6916, 2406, 8454, 9640, 4147, 7310, 7979, 8841, 5879, 1165, 8986, 1273, 6405, 8902, 8671, 8732, 5317, 4172, 9893, 3527, 3472, 8931, 5452, 6828, 9471, 1359, 8121, 5757, 5781, 7997, 6248, 7507, 723, 2021, 7799, 8676, 3829, 1654, 1538, 186, 9212, 5689, 3818, 7475, 2738, 3251, 6762, 4273, 6323, 6138, 4235, 306, 7584, 408, 9821, 6166, 5510, 8221, 0, 5217, 5023, 6642, 2568, 6923, 558, 9533, 3030, 2219, 5503, 8581, 2962, 5612, 6215, 9856, 403, 6333, 5625, 4741, 4996, 8362, 3557, 1830, 8439, 4334, 1219, 8247, 3562, 4579, 644, 9969, 422, 8601, 7783, 5529, 1309, 9318, 5841, 4278, 2829, 381, 5245, 4960, 8946, 7451, 8970, 1568, 6073, 156, 3479, 4528, 6599, 901, 5736, 5849, 2833, 5751, 8443, 5886, 9470, 1616, 1245, 1772, 6125, 2331, 5133, 1855, 3658, 7983, 2285, 9284, 640, 5949, 2589, 7933, 7695, 761, 7323, 700, 5497, 1334, 1993, 2311, 5431, 8220, 2814, 366, 7922, 3412, 2701, 9741, 9248, 165, 8163, 3003, 7704, 2717, 2547, 2207, 4532, 2320, 2088, 7661, 8011, 6341, 5302, 9777, 6659, 3986, 1357, 9686, 707, 9575, 66, 9343, 8669, 9067, 804, 6087, 2668, 3427, 6014, 3100, 7871, 8139, 7175, 5721, 9818, 8582, 2769, 6411, 3089, 9301, 5844, 9005, 4161, 642, 7708, 8073, 1989, 7239, 2167, 5966, 7261, 9844, 4171, 1828, 6084, 1302, 91, 7662, 6564, 2271, 7702, 1508, 8632, 8887, 4940, 8265, 4519, 7245, 3231, 2303, 5443, 9023, 1694, 1545, 8651, 3171, 506, 4683, 3143, 7418, 2938, 352, 4244, 8159, 2170, 2449, 3937, 4304, 794, 4402, 2438, 7383, 9283, 1417, 6783, 635, 2665, 3340, 9537, 3082, 8032, 2915, 6465, 2344, 2277, 7116, 3053, 6024, 7890, 848, 9353, 8296, 3490, 1496, 4836, 9579, 8345, 1980, 2446, 9578, 78, 5573, 5025, 7125, 6535, 8162, 9043, 8034, 9675, 6649, 807, 6508, 6691, 3312, 7727, 8434, 4216, 1672, 3466, 6635, 3768, 4404, 645, 4141, 3597, 1415, 3964, 6596, 9625, 142, 4596, 6627, 2812, 4614, 3718, 1241, 734, 1706, 2830, 510, 2892, 4834, 8018, 2912, 6670, 798, 4634, 208, 1603, 2096, 6025, 6416, 298, 9361, 9381, 9544, 7019, 3049, 5444, 7535, 4963, 153, 7606, 1013, 4950, 8341, 7398, 6830, 8934, 5354, 9874, 9327, 8155, 1046, 8379, 7902, 3630, 3536, 4908, 5064, 3310, 5679, 7867, 7178, 1507, 3233, 6088, 4714, 5178, 1511, 787, 6421, 4447, 8993, 449, 8343, 6605, 7291, 7040, 3839, 8166, 3180, 3953, 772, 6715, 7250, 763, 828, 8059, 7027, 3384, 7244, 8081, 4711, 8222, 9330, 432, 705, 4355, 6337, 7167, 3962, 9308, 8561, 1864, 9652, 6233, 8174, 8080, 17, 3560, 13, 4039, 4867, 1315, 9509, 4271, 5792, 5228, 1844, 7474, 7001, 307, 8960, 910, 8774, 6467, 1184, 9967, 3660, 7509, 9898, 4767, 867, 8867, 4769, 3867, 5712, 7520, 8403, 4688, 6818, 5141, 6958, 192, 3035, 4793, 827, 6217, 8160, 5412, 4377, 984, 1529, 967, 3914, 8758, 7209, 895, 3616, 9368, 7025, 3223, 2819, 5333, 9234, 7279, 1114, 7666, 9157, 5963, 1255, 7745, 4679, 3563, 1076, 2762, 7634, 8925, 9733, 1060, 6019, 2226, 1017, 9570, 1107, 2778, 6263, 8799, 1850, 8482, 7122, 1129, 953, 2976, 7113, 2877, 3062, 5630, 8577, 8028, 205, 4422, 3537, 2995, 565, 1937, 8135, 2567, 9129, 3335, 3051, 5451, 1814, 9258, 8891, 8678, 6857, 8186, 304, 2612, 6361, 3378, 610, 9117, 8963, 5501, 8014, 3592, 2608, 788, 4259, 5035, 612, 6059, 9136, 5170, 2675, 6559, 4764, 9481, 8994, 2173, 1608, 6681, 6169, 3429, 5187, 3080, 4130, 3174, 1153, 2875, 1484, 9605, 6124, 1077, 2017, 90, 6252, 5163, 1257, 4798, 3477, 8147, 9457, 6934, 8094, 1040, 1525, 7392, 351, 9696, 4758, 6137, 4500, 5765, 500, 2903, 8044, 8728, 3359, 428, 3819, 8766, 5415, 4449, 6866, 9133, 4800, 9678, 2461, 2194, 6719, 9036, 6529, 6135, 5506, 376, 2177, 6388, 2553, 9561, 3303, 2653, 6447, 6824, 2496, 8459, 8677, 176, 2799, 7647, 922, 1328, 3574, 1742, 1224, 9850, 4091, 6595, 4809, 1649, 3814, 797, 5428, 5085, 4597, 3258, 7161, 3076, 1164, 7985, 7844, 7544, 9046, 1757, 4685, 9838, 5594, 7432, 7735, 2457, 3678, 4136, 1834, 1455, 2693, 7876, 3830, 3941, 9039, 275, 5526, 6226, 655, 5309, 9190, 3153, 9265, 3262, 5685, 261, 2312, 213, 7592, 4736, 3802, 5057, 9197, 3050, 2533, 116, 6037, 4215, 75, 6115, 3817, 836, 3725, 8849, 7754, 3794, 9145, 4692, 3271, 4831, 1144, 2216, 3868, 5700, 9920, 7693, 6841, 7792, 4988, 4283, 914, 8324, 7766, 2673, 7068, 5723, 6371, 7910, 9682, 9083, 4094, 1862, 2085, 5622, 7137, 9504, 4826, 4307, 6944, 2248, 8061, 5726, 1073, 2605, 7952, 9599, 5137, 6612, 1483, 8530, 6450, 9350, 3318, 9, 3492, 7378, 3751, 5953, 8665, 5644, 8785, 1899, 7344, 122, 6162, 3640, 2656, 6990, 3362, 342, 7626, 6122, 6973, 4324, 1469, 4291, 517, 4975, 6654, 9723, 6192, 6637, 8264, 6451, 1951, 561, 1066, 604, 1121, 7879, 8537, 6875, 7780, 8761, 3387, 8045, 877, 9689, 8063, 514, 4029, 2363, 6886, 1102, 913, 6265, 144, 1759, 3975, 3295, 8263, 2859, 6326, 693, 7747, 210, 3238, 3414, 8352, 6877, 146, 3475, 1777, 5384, 7895, 3641, 678, 2027, 5461, 7857, 3325, 8692, 1861, 6622, 1713, 2139, 6873, 2996, 2429, 3367, 5662, 2132, 3463, 1811, 8091, 3093, 1333, 4696, 8097, 3280, 7888, 8445, 3330, 5251, 7272, 7502, 4725, 8068, 9905, 3857, 1497, 9156, 6729, 3803, 8057, 5968, 7596, 2782, 330, 8683, 4813, 7944, 4488, 9977, 580, 4656, 8697, 2598, 9320, 979, 8258, 7617, 74, 2754, 3343, 917, 2352, 3166, 4507, 8384, 6976, 7774, 5747, 6777, 28, 490, 1074, 3437, 667, 1646, 4139, 3702, 1117, 7762, 3495, 7115, 8897, 6570, 5202, 2453, 6895, 5413, 5311, 9114, 1130, 3549, 1854, 9469, 9069, 7171, 7576, 5952, 6442, 5981, 8723, 7696, 6104, 1247, 9228, 8598, 3090, 4460, 4159, 1271, 6836, 3355, 1669, 716, 1709, 5264, 6264, 7492, 1639, 3001, 8816, 2702, 938, 9238, 407, 1240, 845, 4841, 7742, 9209, 1476, 324, 1895, 1718, 6438, 5429, 7668, 3423, 2867, 1376, 4512, 5833, 5989, 3642, 1710, 5525, 8657, 7967, 7860, 886, 4196, 3229, 7050, 6060, 2623, 3497, 8256, 6238, 7909, 9311, 1145, 7320, 6667, 6491, 9363, 3647, 7811, 8694, 157, 9080, 379, 2078, 1254, 4652, 706, 8383, 1572, 9748, 9515, 6738, 9223, 9910, 5554, 9797, 8790, 8185, 6751, 3934, 1745, 3541, 3012, 738, 48, 2600, 4745, 2532, 3619, 4490, 337, 995, 6827, 4657, 6899, 812, 8365, 5508, 9325, 2561, 4694, 5069, 6690, 8875, 9188, 5761, 4240, 9193, 447, 8726, 7445, 3491, 8006, 7866, 6940, 2169, 1386, 3828, 7370, 2886, 5538, 2362, 9549, 8974, 1306, 4025, 6189, 9573, 9524, 220, 5000, 4102, 9113, 9191, 4904, 2648, 6985, 2140, 3078, 6415, 5417, 6558, 4367, 7265, 3738, 1234, 2861, 2718, 2578, 2157, 9344, 4294, 211, 5543, 2181, 9355, 5616, 3188, 9983, 2739, 6454, 2641, 8862, 6106, 4432, 5819, 272, 943, 3265, 8670, 7940, 8639, 7093, 9583, 7427, 3816, 8649, 619, 5755, 2353, 1433, 2432, 501, 2985, 6452, 6834, 4677, 3399, 4155, 3893, 7759, 1981, 2262, 4999, 7249, 7082, 4928, 6801, 4828, 8587, 5710, 7079, 7061, 1808, 4400, 1251, 9298, 1418, 9824, 3999, 8870, 9380, 596, 2462, 8492, 9654, 1450, 3485, 458, 183, 6487, 5306, 4892, 5339, 3042, 2255, 4619, 4536, 1416, 3045, 2116, 3559, 4544, 4435, 935, 88, 745, 4341, 2551, 8516, 6292, 548, 5552, 4277, 3523, 8204, 6054, 7064, 4918, 5330, 6240, 614, 5692, 9791, 6103, 8578, 8655, 2497, 5421, 2685, 4397, 5205, 9623, 7946, 2971, 3398, 740, 6393, 273, 7868, 1549, 1770, 4717, 8803, 7346, 1530, 1047, 5927, 1126, 2660, 5295, 4970, 3874, 3008, 8388, 7908, 5562, 9894, 8719, 2635, 2947, 4188, 6012, 1424, 9401, 1671, 2298, 6898, 3944, 8414, 8989, 9877, 2614, 6907, 2290, 5052, 7672, 6469, 5811, 3240, 3334, 4342, 7973, 3061, 6961, 7420, 2187, 1870, 4035, 654, 4829, 8182, 7381, 7257, 9531, 6218, 8260, 3521, 3617, 3932, 6542, 6260, 733, 6032, 9855, 5214, 5515, 4935, 8907, 7589, 9907, 8580, 9848, 1253, 8742, 3746, 6748, 6116, 5184, 9972, 8460, 5853, 5220, 2680, 1847, 117, 9138, 9429, 2133, 968, 9164, 5468, 7179, 3894, 248, 4558, 9979, 5682, 1707, 8753, 2148, 4585, 9494, 1351, 5101, 4784, 1339, 4036, 8916, 9888, 2083, 6369, 7088, 7030, 2720, 3402, 8210, 3596, 2531, 2171, 5974, 5290, 8998, 6922, 8594, 8560, 6427, 3758, 9300, 7466, 8497, 7390, 1377, 3151, 4693, 8807, 7521, 2888, 2215, 865, 897, 285, 2792, 6754, 3926, 3389, 9645, 5149, 5498, 7488, 9589, 6548, 6680, 350, 103, 2022, 5588, 3296, 1837, 4075, 9099, 6838, 9447, 6310, 5683, 3323, 1149, 1592, 5911, 7941, 3815, 9451, 2217, 3591, 4356, 5435, 6924, 389, 1922, 5287, 1087, 9727, 5535, 7527, 4434, 1322, 2809, 3129, 6717, 6554, 2415, 7751, 4903, 3465, 834, 6000, 8276, 3657, 8938, 3734, 8636, 7371, 2074, 4374, 4866, 8007, 622, 8778, 4065, 2772, 9933, 9086, 9200, 7391, 1699, 2787, 1422, 7205, 1296, 5447, 4588, 83, 2597, 1196, 9384, 1551, 8273, 2121, 3782, 6846, 7719, 3025, 7172, 8928, 9108, 1544, 4674, 120, 5705, 1788, 8664, 9731, 1220, 454, 8773, 5396, 3531, 6689, 3529, 5456, 167, 720, 5476, 1284, 2204, 9141, 6302, 5851, 60, 8301, 5301, 8559, 4655, 3784, 9054, 4258, 3216, 7977, 4134, 5584, 1436, 4485, 2883, 9637, 8634, 8395, 4727, 8130, 2923, 5299, 2059, 9592, 2258, 7105, 5794, 6584, 9459, 7949, 1860, 3480, 8252, 5826, 4797, 7870, 1370, 4644, 5446, 8002, 9672, 7947, 970, 8674, 6921, 3779, 6282, 3776, 536, 1348, 7306, 5780, 4664, 6081, 3202, 5499, 5521, 908, 3998, 5596, 3637, 343, 1624, 4707, 8399, 1173, 1513, 8945, 5589, 2658, 8703, 2634, 3219, 3241, 1578, 9843, 9183, 1936, 7375, 6167, 6408, 2727, 6874, 5349, 8066, 5242, 8648, 9514, 9498, 5480, 6745, 5493, 9594, 9149, 9502, 1212, 8682, 2305, 5774, 5675, 9391, 3957, 6702, 7567, 6639, 9317, 3440, 8223, 2326, 3789, 8646, 6732, 1571, 8864, 5378, 9488, 9692, 2719, 1061, 2155, 7149, 7127, 3864, 4790, 4522, 7287, 4219, 9724, 7009, 6142, 3720, 5272, 3234, 7771, 6618, 8430, 7487, 4464, 3770, 9523, 9786, 545, 9799, 394, 8029, 7490, 1248, 4436, 6840, 7713, 1968, 8149, 8576, 3368, 3844, 7311, 5782, 3450, 4336, 3144, 697, 4012, 3696, 2773, 6822, 168, 890, 664, 3488, 8407, 4231, 5512, 915, 3107, 5623, 4073, 1881, 6743, 6424, 8904, 6276, 8351, 4529, 484, 2585, 4473, 1941, 9912, 2749, 5690, 5460, 6013, 5343, 5291, 9541, 1440, 7202, 4827, 983, 6849, 4393, 6655, 3273, 5905, 4238, 7966, 8321, 3454, 9378, 5353, 3624, 57, 1631, 7538, 8336, 3692, 6378, 9065, 5420, 1519, 3786, 6251, 8126, 5950, 650, 5346, 7288, 7028, 7458, 3629, 9903, 3024, 5173, 9199, 4612, 8769, 5088, 3626, 242, 2400, 8900, 334, 5146, 3013, 7086, 896, 9590, 689, 3418, 1475, 3627, 5816, 254, 887, 4266, 5096, 3430, 3506, 9084, 5470, 5437, 7907, 6867, 803, 1301, 3719, 8845, 3268, 9815, 216, 5800, 3033, 8377, 7748, 1044, 3385, 8881, 5388, 2050, 2603, 105, 1097, 7092, 4431, 9070, 6057, 568, 9804, 7680, 616, 1361, 9813, 1700, 3113, 5696, 9657, 7154, 2901, 1781, 6151, 708, 3112, 5553, 8950, 9680, 8144, 3195, 5068, 6509, 1896, 7586, 658, 2067, 2064, 7174, 5868, 1246, 9180, 9650, 7660, 4066, 9863, 5527, 1026, 6935, 4328, 2559, 3774, 7157, 5013, 7164, 9224, 6152, 526, 3388, 1849, 9591, 751, 2485, 8627, 525, 244, 2460, 3044, 2765, 9516, 6224, 4704, 586, 7639, 1190, 7467, 1737, 5920, 2304, 4875, 1841, 6823, 5797, 495, 278, 67, 822, 1621, 2647, 8131, 9148, 1796, 9921, 9834, 1629, 2502, 5672, 4279, 444, 3032, 5560, 600, 2164, 6342, 712, 4175, 4491, 8566, 6198, 7543, 3065, 3735, 464, 9232, 1139, 9867, 8759, 9665, 1010, 937, 3925, 4115, 6938, 1402, 418, 898, 3175, 8981, 8832, 9310, 3766, 2143, 8747, 9870, 7218, 178, 9240, 2811, 4223, 9760, 2349, 982, 808, 4386, 5627, 9462, 2036, 5946, 2855, 3704, 1505, 9092, 5729, 8624, 5605, 3555, 2394, 801, 5668, 4919, 460, 489, 3593, 6317, 1389, 8307, 9184, 6882, 5813, 3882, 2725, 3588, 360, 8855, 9702, 6718, 6716, 7112, 7710, 6298, 9889, 6177, 1748, 7181, 4335, 3711, 3499, 7074, 4041, 9827, 7833, 9492, 4587, 2151, 888, 4479, 7031, 1917, 1846, 9642, 775, 1965, 776, 6488, 7208, 6602, 1720, 2272, 4169, 9424, 4622, 8078, 5352, 2152, 1118, 1628, 575, 4501, 5305, 9257, 4325, 9806, 8213, 9428, 3952, 1999, 4667, 6279, 1349, 4426, 3652, 3611, 9004, 2934, 9026, 4654, 1685, 6660, 8311, 4469, 3656, 5735, 8932, 3607, 2700, 1839, 7219, 4366, 4276, 4329, 9612, 6273, 9174, 4160, 3474, 4257, 9790, 4899, 8504, 6489, 4480, 2543, 2639, 7405, 1554, 7026, 5942, 7529, 7640, 4078, 284, 6134, 4540, 2146, 796, 6466, 9747, 8455, 1468, 8571, 2451, 7656, 1789, 4052, 3246, 8145, 9151, 2963, 5569, 9764, 3494, 7816, 6075, 4712, 8179, 6228, 1975, 3221, 5097, 549, 9032, 7094, 5206, 8316, 6768, 8104, 8933, 9563, 7580, 9003, 1517, 5223, 5059, 4781, 2093, 9866, 3091, 5028, 7839, 7097, 6155, 486, 9501, 8367, 9176, 4293, 8053, 6994, 7326, 5181, 1655, 2137, 8709, 7574, 4019, 2389, 5915, 3587, 4466, 8244, 524, 5754, 9100, 1833, 6031, 5646, 2347, 5344, 9626, 1550, 9272, 8957, 5790, 4217, 2153, 1230, 3397, 7085, 1423, 2545, 4247, 7111, 6755, 1119, 8812, 2906, 1585, 6757, 7522, 7038, 7147, 5847, 8570, 8251, 6433, 3158, 420, 3880, 3908, 8261, 5226, 9131, 4931, 95, 1852, 9293, 2870, 1753, 5577, 7275, 7187, 5398, 1159, 7974, 1371, 7388, 6578, 9755, 8165, 3085, 6675, 7916, 559, 3955, 2655, 8813, 2374, 9552, 1580, 2199, 2114, 5958, 2704, 4691, 1080, 4737, 9305, 2159, 2960, 3360, 5607, 9510, 2356, 7414, 958, 1659, 6035, 860, 6808, 8558, 557, 5959, 5567, 2737, 9339, 7217, 8052, 6339, 9489, 7349, 632, 2732, 2381, 8920, 8239, 5248, 3924, 5635, 8076, 3269, 5753, 1926, 2391, 7834, 3264, 7581, 1112, 3783, 8936, 5983, 6199, 2149, 6957, 1387, 7937, 1606, 1875, 3290, 9770, 8675, 7810, 4274, 3573, 5140, 4122, 496, 3610, 5018, 1983, 4878, 4665, 6933, 6835, 2505, 5231, 4811, 3907, 4752, 3277, 209, 4270, 4898, 8951, 7302, 3098, 7330, 5014, 371, 8330, 8442, 6128, 3147, 8843, 8294, 9781, 1732, 3372, 3712, 19, 5324, 5718, 7635, 2889, 485, 6461, 6497, 3919, 2663, 6904, 9122, 520, 9400, 2987, 6950, 6357, 6145, 3741, 134, 2249, 8608, 5933, 5902, 1903, 7228, 7604, 6114, 6778, 6708, 1111, 7772, 4553, 3215, 1277, 7204, 7601, 2336, 639, 4060, 2126, 7233, 8141, 5516, 2100, 6799, 3902, 553, 8972, 5197, 3773, 7364, 6174, 1171, 7725, 2672, 8020, 6787, 7315, 6624, 4850, 8718, 3994, 9449, 7452, 1874, 4207, 4780, 3255, 233, 7689, 2899, 4602, 1667, 3791, 6620, 9450, 5135, 1099, 3284, 6656, 3558, 8979, 1612, 8475, 8962, 6440, 7034, 4364, 5261, 7935, 2840, 4013, 2523, 46, 3546, 8912, 7348, 6384, 4399, 4659, 7984, 5778, 1948, 218, 3401, 7286, 8049, 2494, 764, 3105, 8550, 2941, 6640, 8295, 9057, 6941, 8129, 5787, 6284, 8730, 7190, 9061, 6123, 5347, 7734, 1091, 5628, 1665, 2265, 3190, 8569, 8259, 1490, 2527, 1185, 2492, 3128, 1480, 2337, 1904, 3212, 5549, 4823, 6431, 6453, 7368, 8502, 5453, 4234, 8794, 6345, 508, 5167, 1782, 4502, 6603, 5327, 5706, 3365, 5180, 1832, 182, 4609, 3419, 3242, 3405, 8695, 6129, 6845, 6140, 7118, 690, 9208, 4944, 1868, 5391, 6314, 4952, 7188, 3767, 8051, 1756, 2066, 9307, 2918, 4557, 49, 2055, 2479, 4076, 7444, 3988, 1062, 2989, 2348, 8888, 4411, 3020, 4977, 9557, 5537, 5654, 6062, 9461, 2629, 6213, 9126, 7992, 1244, 8894, 214, 2405, 5731, 4032, 9550, 9441, 4009, 7380, 9329, 4181, 2280, 7269, 749, 6354, 1193, 6851, 480, 3609, 9194, 1176, 9448, 3604, 4445, 7357, 7534, 9869, 7883, 2147, 8480, 9503, 6040, 6046, 9373, 5665, 962, 9618, 6259, 7630, 9872, 4237, 6666, 4603, 5439, 7296, 4387, 1082, 5164, 9580, 7165, 9414, 8456, 4412, 5482, 771, 7109, 4403, 3017, 2569, 3164, 6744, 6880, 8411, 7939, 8911, 8953, 6870, 4643, 5746, 3799, 429, 8297, 873, 2, 2638, 9737, 9040, 3286, 5579, 5274, 6723, 5741, 5776, 3842, 139, 1437, 1790, 8488, 4738, 9304, 14, 5307, 1792, 3347, 119, 6400, 230, 5186, 9677, 6752, 1292, 2633, 9698, 9402, 6588, 8637, 6503, 990, 3048, 5120, 6951, 9044, 911, 9035, 93, 4323, 4635, 4957, 5907, 3823, 8731, 5071, 2681, 5878, 7893, 3489, 3222, 240, 9405, 9718, 7295, 1763, 2357, 1304, 374, 8153, 6423, 8977, 578, 2783, 9882, 3904, 2417, 2104, 6455, 387, 9719, 8128, 1477, 1137, 1677, 2345, 5450, 3276, 9956, 4184, 6927, 1835, 3500, 267, 7486, 6391, 8744, 9846, 1668, 2026, 8035, 236, 7044, 7260, 5323, 996, 6772, 498, 1749, 2196, 2299, 6462, 3951, 6505, 9097, 3411, 9547, 893, 9646, 2057, 9560, 9849, 7206, 6959, 5332, 2390, 1407, 4617, 1104, 4203, 1276, 7746, 3681, 372, 7360, 3666, 5275, 6473, 7443, 754, 7726, 3087, 1911, 4574, 2520, 8781, 9808, 5182, 4859, 3081, 4894, 4851, 8987, 5637, 5901, 7891, 6630, 6261, 8096, 5392, 5227, 9397, 9740, 6876, 9829, 692, 1035, 1320, 191, 6626, 7674, 8662, 6580, 1098, 4893, 378, 4197, 1495, 5003, 669, 1923, 9281, 1776, 543, 3046, 3416, 6485, 6913, 6831, 4484, 1978, 4818, 1515, 1734, 2556, 8199, 7554, 2729, 6628, 9387, 8143, 5455, 7313, 2890, 6844, 2741, 7194, 2244, 864, 1264, 6178, 1146, 9571, 7216, 4280, 2002, 779, 9915, 7211, 566, 5561, 1038, 2905, 4953, 7156, 676, 4110, 4776, 8836, 7828, 7768, 1827, 9168, 5783, 7043, 6163, 2108, 8026, 8910, 5080, 6953, 4296, 3470, 9826, 7363, 4763, 9166, 8553, 1534, 2789, 6471, 5647, 2613, 5078, 1693, 6056, 5620, 3279, 6343, 2481, 959, 691, 9897, 2292, 5993, 4613, 5414, 6563, 6018, 7976, 5708, 3232, 9231, 3872, 563, 4921, 6086, 5634, 4458, 9908, 6589, 891, 8525, 7607, 5139, 634, 9499, 9593, 6811, 8617, 9493, 3584, 1760, 2221, 6444, 1330, 5253, 6662, 6693, 2884, 9971, 4964, 3655, 338, 1295, 7168, 4761, 219, 9333, 2879, 9558, 6900, 224, 7707, 3981, 7132, 6294, 8903, 4625, 5524, 1157, 4562, 4626, 5386, 2351, 7515, 8859, 541, 8077, 8762, 1601, 2536, 2625, 883, 5237, 3715, 727, 5788, 73, 1235, 7345, 5250, 2575, 322, 5318, 9170, 5153, 7553, 8346, 5176, 1946, 6490, 2511, 5211, 7379, 6458, 6110, 2549, 7341, 8117, 433, 1786, 6912, 6241, 5973, 7903, 1459, 8552, 6636, 3125, 1170, 5822, 8070, 9392, 4611, 6962, 6713, 8629, 4729, 9555, 2851, 266, 9512, 6931, 5464, 7593, 7052, 7263, 8440, 1961, 2473, 8329, 657, 7677, 1447, 3625, 395, 8827, 6646, 5744, 2459, 2766, 4548, 7920, 5086, 5977, 1103, 674, 2706, 5862, 1312, 2392, 3879, 5483, 2468, 6821, 6097, 5485, 6735, 118, 3886, 8545, 9490, 4081, 8309, 9602, 3178, 2528, 287, 5766, 3858, 3027, 497, 9073, 3293, 3016, 5172, 8573, 4748, 2235, 6547, 7855, 8432, 1952, 1167, 7177, 3146, 9315, 8278, 5899, 1703, 9513, 2657, 8319, 1385, 8513, 2198, 5293, 2071, 9249, 5373, 2483, 7361, 1227, 661, 2162, 4856, 4372, 4354, 7676, 6855, 4288, 2781, 1182, 2154, 7643, 7801, 4079, 2684, 1444, 2048, 7655, 2692, 2012, 1075, 7008, 7530, 1083, 6847, 3132, 5348, 5613, 7579, 9663, 3749, 3956, 3228, 147, 2986, 6389, 8512, 3302, 2842, 8003, 2619, 9774, 7927, 4913, 9525, 7899, 7896, 7227, 6692, 6355, 7355, 1902, 1000, 8217, 8422, 4927, 6536, 1341, 1485, 662, 5998, 1909, 7195, 8046, 5909, 4762, 4206, 3136, 583, 5555, 5873, 1278, 9112, 6566, 2853, 5509, 6894, 5855, 1915, 3832, 4421, 4246, 1547, 7758, 6157, 3348, 1092, 4965, 5249, 2411, 4349, 1382, 7343, 6590, 9532, 3400, 1664, 948, 2848, 246, 7986, 5361, 6733, 7321, 4956, 4565, 8892, 3138, 3706, 3850, 3877, 8866, 7394, 4864, 358, 1912, 9303, 3116, 482, 3707, 4100, 6848, 1392, 6805, 7399, 2534, 9847, 2768, 7557, 9700, 6173, 6448, 1237, 862, 7029, 6149, 9842, 9609, 3442, 7472, 4782, 5152, 2953, 2839, 6022, 8880, 1582, 5660, 8596, 9526, 7658, 2671, 9710, 1574, 7464, 4398, 5875, 7183, 8922, 7781, 1332, 3037, 1217, 215, 9395, 6707, 7285, 4332, 1150, 7565, 912, 9291, 6527, 8113, 8701, 4420, 1052, 7741, 9766, 5448, 9624, 247, 200, 1405, 5049, 6220, 5771, 7872, 7698, 5100, 7119, 5403, 9216, 3071, 355, 7731, 5948, 4344, 4363, 6176, 8805, 2610, 9614, 5930, 2339, 9711, 4369, 3915, 593, 7266, 5645, 5514, 8996, 6769, 4718, 6975, 9440, 839, 929, 4254, 1829, 9925, 7519, 2376, 4720, 9964, 7037, 2237, 8708, 2015, 3566, 2539, 4221, 2713, 4048, 713, 819, 5282, 602, 6063, 7222, 5852, 3209, 5666, 5603, 7408, 9217, 4862, 4941, 1635, 3507, 5939, 8666, 2697, 3703, 1375, 6872, 9116, 8975, 3006, 255, 2841, 8788, 6525, 3826, 5471, 7736, 3301, 5424, 6583, 9024, 6515, 4192, 8837, 5513, 8521, 9992, 2767, 6353, 1888, 2184, 6742, 3797, 9954, 3748, 8750, 202, 8906, 4895, 5495, 2894, 198, 4835, 3270, 9227, 9900, 7051, 8527, 9337, 317, 2360, 113, 8079, 7645, 7314, 718, 9735, 576, 5695, 3438, 3029, 3690, 6150, 8387, 4148, 2831, 2588, 6441, 7548, 8510, 8106, 7541, 9519, 1243, 7675, 5937, 9123, 5840, 4453, 9012, 8873, 132, 7447, 4568, 6161, 7959, 4010, 2041, 7144, 7191, 3394, 7642, 6925, 4910, 1042, 3448, 2328, 8376, 2631, 2033, 905, 2094, 2872, 3976, 5767, 3854, 463, 8724, 2562, 3849, 5725, 5871, 5276, 9567, 5558, 8824, 1135, 2046, 2134, 9152, 8663, 3896, 6287, 8842, 9998, 9312, 5408, 6043, 766, 8605, 6906, 5608, 5580, 7632, 9369, 7831, 4306, 9214, 3883, 7794, 3809, 7062, 8372, 9757, 3294, 426, 7016, 2175, 3701, 5860, 2135, 8010, 6653, 8333, 5375, 8433, 7433, 6351, 8317, 963, 5568, 1223, 627, 1401, 6, 2705, 4225, 7786, 2854, 3540, 457, 3709, 9430, 1039, 7417, 8751, 5442, 2087, 9922, 8088, 6321, 6956, 150, 1291, 2218, 7226, 8280, 6382, 2788, 1201, 1463, 3572, 169, 7555, 9027, 4106, 3931, 8465, 4064, 4716, 6146, 4992, 2488, 2419, 3086, 7809, 1180, 6963, 2472, 7808, 876, 8590, 6144, 1804, 1682, 2225, 721, 6362, 6714, 1528, 730, 2435, 1543, 8416, 7926, 4616, 7514, 3040, 3639, 9150, 5532, 6765, 1203, 123, 9875, 4560, 4213, 6615, 9739, 8195, 333, 3403, 3947, 4454, 8326, 1458, 4971, 325, 2969, 8681, 8142, 9929, 3781, 3759, 7338, 1953, 6383, 6800, 789, 5257, 9182, 8305, 3124, 7705, 4441, 4822, 8883, 6021, 7280, 4750, 4423, 8000, 7060, 6227, 2464, 6683, 643, 9419, 9437, 6756, 1532, 9949, 6319, 1653, 1723, 3654, 2664, 5733, 7065, 9202, 2407, 438, 9187, 2599, 3079, 5699, 6143, 5582, 4150, 8565, 104, 7331, 6494, 8306, 8396, 3614, 5969, 3968, 6010, 663, 8450, 94, 4330, 2798, 9037, 2747, 7090, 648, 6492, 2764, 9403, 2878, 969, 1579, 3623, 4666, 9540, 840, 4384, 5119, 6413, 6781, 679, 3825, 1591, 1258, 830, 1457, 5083, 1843, 7703, 4689, 6089, 5583, 1256, 1670, 6245, 5256, 4810, 4195, 8588, 2208, 3822, 3054, 5472, 3259, 5652, 7862, 5551, 760, 5701, 1002, 1619, 2977, 8791, 80, 4889, 1172, 8948, 7162, 868, 6201, 9864, 8420, 9030, 6482, 9902, 1815, 8808, 6387, 8325, 1932, 2816, 7603, 7797, 8178, 8917, 5893, 4088, 6311, 9119, 375, 1123, 5796, 3599, 1462, 2626, 5572, 2587, 7353, 3043, 64, 4301, 3253, 2227, 7106, 6911, 2714, 8092, 5075, 9775, 4001, 4038, 8441, 1643, 4510, 249, 1086, 550, 4465, 5255, 7775, 2498, 3077, 3381, 1187, 129, 5008, 3836, 3141, 9053, 9009, 7955, 6184, 4744, 9520, 2857, 885, 5758, 4739, 6704, 9613, 7585, 9644, 201, 5055, 3653, 5926, 8146, 1120, 6942, 560, 9059, 189, 4863, 3070, 3638, 425, 4241, 9497, 1893, 9901, 3088, 3911, 8997, 599, 3930, 5001, 1784, 5863, 9455, 5903, 552, 362, 9773, 8198, 6806, 1481, 9676, 6295, 8171, 8515, 3778, 398, 3120, 4253, 8959, 9963, 8232, 2436, 3109, 6574, 6747, 5614, 8187, 5876, 505, 1427, 4648, 8161, 2165, 8659, 6980, 5095, 1988, 9385, 6869, 1290, 2925, 8687, 8597, 8205]\n",
            "4000\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWLG2Hmx6gTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08c62f2e-d1e6-4352-e4b2-1185b271b97b"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "global best_acc\n",
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "print(\"test accuracy \", correct/total , test_loss)\n",
        "test_acc.append(test_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.3091 325.89594292640686\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}