{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR 30 percentage corrupted.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olq-MKcdmOnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = EfficientNetB0()\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlZDWDZm6ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "term_width = 80\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ubk0aCmSD7",
        "colab_type": "code",
        "outputId": "47a49b5b-77c8-4bc1-974c-09d3b0c7db25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "# trainset.targets***************************************************************\n",
        "import random\n",
        "length = len(trainset.targets)\n",
        "percentage_corruption = 30\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "# print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "# print(corrupt_idx)\n",
        "# print(len(corrupt_classes))\n",
        "a = np.array(trainset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "trainset.targets = list(a)\n",
        "#**********************************************************************************\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train_acc =[]\n",
        "test_acc=[]\n",
        "epoch_list=[]\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"train accuracy \", correct/total ,train_loss)\n",
        "    train_acc.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"test accuracy \", correct/total , test_loss)\n",
        "    test_acc.append(test_loss)\n",
        "\n",
        "    # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+240):\n",
        "    epoch_list.append(epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "train accuracy  0.26618 812.8559906482697\n",
            "test accuracy  0.4336 163.60094785690308\n",
            "\n",
            "Epoch: 1\n",
            "train accuracy  0.34022 760.8285477161407\n",
            "test accuracy  0.485 148.72398579120636\n",
            "\n",
            "Epoch: 2\n",
            "train accuracy  0.37938 734.0937960147858\n",
            "test accuracy  0.5223 143.6996786594391\n",
            "\n",
            "Epoch: 3\n",
            "train accuracy  0.40692 713.9332104921341\n",
            "test accuracy  0.5477 139.30944573879242\n",
            "\n",
            "Epoch: 4\n",
            "train accuracy  0.42756 697.4704223871231\n",
            "test accuracy  0.5776 130.6513135433197\n",
            "\n",
            "Epoch: 5\n",
            "train accuracy  0.44942 684.4938690662384\n",
            "test accuracy  0.609 126.86749505996704\n",
            "\n",
            "Epoch: 6\n",
            "train accuracy  0.46616 670.2822283506393\n",
            "test accuracy  0.6314 120.57060182094574\n",
            "\n",
            "Epoch: 7\n",
            "train accuracy  0.4807 658.954892039299\n",
            "test accuracy  0.6597 115.00230419635773\n",
            "\n",
            "Epoch: 8\n",
            "train accuracy  0.494 648.7512789964676\n",
            "test accuracy  0.6574 116.51172995567322\n",
            "\n",
            "Epoch: 9\n",
            "train accuracy  0.50218 641.2590769529343\n",
            "test accuracy  0.6743 108.55721098184586\n",
            "\n",
            "Epoch: 10\n",
            "train accuracy  0.50992 633.1936794519424\n",
            "test accuracy  0.6984 105.29998308420181\n",
            "\n",
            "Epoch: 11\n",
            "train accuracy  0.52098 626.3488886356354\n",
            "test accuracy  0.7042 102.56087028980255\n",
            "\n",
            "Epoch: 12\n",
            "train accuracy  0.52748 620.8199735879898\n",
            "test accuracy  0.6953 104.87909299135208\n",
            "\n",
            "Epoch: 13\n",
            "train accuracy  0.5355 613.6194854974747\n",
            "test accuracy  0.7186 98.83211237192154\n",
            "\n",
            "Epoch: 14\n",
            "train accuracy  0.53956 610.0105959177017\n",
            "test accuracy  0.7297 97.78834056854248\n",
            "\n",
            "Epoch: 15\n",
            "train accuracy  0.5447 604.5246616601944\n",
            "test accuracy  0.741 95.69294333457947\n",
            "\n",
            "Epoch: 16\n",
            "train accuracy  0.55188 599.5323677062988\n",
            "test accuracy  0.7406 94.35643750429153\n",
            "\n",
            "Epoch: 17\n",
            "train accuracy  0.55644 596.4442573785782\n",
            "test accuracy  0.7521 95.37805372476578\n",
            "\n",
            "Epoch: 18\n",
            "train accuracy  0.56328 591.1114122867584\n",
            "test accuracy  0.7365 92.49826771020889\n",
            "\n",
            "Epoch: 19\n",
            "train accuracy  0.5658 587.4867047071457\n",
            "test accuracy  0.7595 90.23105746507645\n",
            "\n",
            "Epoch: 20\n",
            "train accuracy  0.56768 583.8552573919296\n",
            "test accuracy  0.7581 91.24793148040771\n",
            "\n",
            "Epoch: 21\n",
            "train accuracy  0.57388 579.4216853380203\n",
            "test accuracy  0.7644 86.77043789625168\n",
            "\n",
            "Epoch: 22\n",
            "train accuracy  0.57866 576.4214658737183\n",
            "test accuracy  0.7691 86.8931599855423\n",
            "\n",
            "Epoch: 23\n",
            "train accuracy  0.57966 573.563835978508\n",
            "test accuracy  0.7833 84.78314590454102\n",
            "\n",
            "Epoch: 24\n",
            "train accuracy  0.58406 570.6232590675354\n",
            "test accuracy  0.7762 86.34651374816895\n",
            "\n",
            "Epoch: 25\n",
            "train accuracy  0.58738 568.5870983600616\n",
            "test accuracy  0.7712 89.64596259593964\n",
            "\n",
            "Epoch: 26\n",
            "train accuracy  0.58664 567.0395314693451\n",
            "test accuracy  0.7752 87.60059583187103\n",
            "\n",
            "Epoch: 27\n",
            "train accuracy  0.59296 563.3727196455002\n",
            "test accuracy  0.7923 81.29362261295319\n",
            "\n",
            "Epoch: 28\n",
            "train accuracy  0.59374 561.0595750808716\n",
            "test accuracy  0.775 87.03276038169861\n",
            "\n",
            "Epoch: 29\n",
            "train accuracy  0.5983 558.8434113264084\n",
            "test accuracy  0.7854 84.82740992307663\n",
            "\n",
            "Epoch: 30\n",
            "train accuracy  0.6005 556.4884951114655\n",
            "test accuracy  0.7816 85.50493651628494\n",
            "\n",
            "Epoch: 31\n",
            "train accuracy  0.60072 555.1149156093597\n",
            "test accuracy  0.773 87.49730408191681\n",
            "\n",
            "Epoch: 32\n",
            "train accuracy  0.60404 551.443127155304\n",
            "test accuracy  0.7931 81.9894203543663\n",
            "\n",
            "Epoch: 33\n",
            "train accuracy  0.60488 550.6980367898941\n",
            "test accuracy  0.8044 81.74820828437805\n",
            "\n",
            "Epoch: 34\n",
            "train accuracy  0.6064 548.2836949825287\n",
            "test accuracy  0.8034 80.3003488779068\n",
            "\n",
            "Epoch: 35\n",
            "train accuracy  0.6072 547.0855222940445\n",
            "test accuracy  0.7934 82.74269413948059\n",
            "\n",
            "Epoch: 36\n",
            "train accuracy  0.61054 543.5700695514679\n",
            "test accuracy  0.7927 81.42431950569153\n",
            "\n",
            "Epoch: 37\n",
            "train accuracy  0.6131 543.3925679922104\n",
            "test accuracy  0.796 81.83738678693771\n",
            "\n",
            "Epoch: 38\n",
            "train accuracy  0.611 542.1963583230972\n",
            "test accuracy  0.8016 82.5638799071312\n",
            "\n",
            "Epoch: 39\n",
            "train accuracy  0.61494 539.3434957265854\n",
            "test accuracy  0.7966 81.77127707004547\n",
            "\n",
            "Epoch: 40\n",
            "train accuracy  0.61712 538.1616940498352\n",
            "test accuracy  0.7965 79.86836516857147\n",
            "\n",
            "Epoch: 41\n",
            "train accuracy  0.61646 537.2544521093369\n",
            "test accuracy  0.7946 80.86022263765335\n",
            "\n",
            "Epoch: 42\n",
            "train accuracy  0.61942 533.6921502351761\n",
            "test accuracy  0.8079 78.01431119441986\n",
            "\n",
            "Epoch: 43\n",
            "train accuracy  0.62042 532.2781814336777\n",
            "test accuracy  0.8018 78.58379673957825\n",
            "\n",
            "Epoch: 44\n",
            "train accuracy  0.62184 531.324146270752\n",
            "test accuracy  0.8062 78.162064909935\n",
            "\n",
            "Epoch: 45\n",
            "train accuracy  0.6231 530.919887304306\n",
            "test accuracy  0.8052 78.53405267000198\n",
            "\n",
            "Epoch: 46\n",
            "train accuracy  0.6224 529.0098053216934\n",
            "test accuracy  0.8083 79.56794226169586\n",
            "\n",
            "Epoch: 47\n",
            "train accuracy  0.62586 527.4751372337341\n",
            "test accuracy  0.8062 79.26461100578308\n",
            "\n",
            "Epoch: 48\n",
            "train accuracy  0.62764 524.597277879715\n",
            "test accuracy  0.8053 78.90216040611267\n",
            "\n",
            "Epoch: 49\n",
            "train accuracy  0.6268 525.0584371089935\n",
            "test accuracy  0.7968 79.70564931631088\n",
            "\n",
            "Epoch: 50\n",
            "train accuracy  0.62764 523.3399212360382\n",
            "test accuracy  0.8065 79.03109538555145\n",
            "\n",
            "Epoch: 51\n",
            "train accuracy  0.62864 521.7939323186874\n",
            "test accuracy  0.8006 79.02380800247192\n",
            "\n",
            "Epoch: 52\n",
            "train accuracy  0.63024 520.8223551511765\n",
            "test accuracy  0.8102 76.11174643039703\n",
            "\n",
            "Epoch: 53\n",
            "train accuracy  0.63016 518.4595439434052\n",
            "test accuracy  0.809 77.8928462266922\n",
            "\n",
            "Epoch: 54\n",
            "train accuracy  0.6313 517.2714856863022\n",
            "test accuracy  0.8091 78.57664561271667\n",
            "\n",
            "Epoch: 55\n",
            "train accuracy  0.63304 515.5209648609161\n",
            "test accuracy  0.813 76.50863885879517\n",
            "\n",
            "Epoch: 56\n",
            "train accuracy  0.63386 514.6598349809647\n",
            "test accuracy  0.8064 79.5139793753624\n",
            "\n",
            "Epoch: 57\n",
            "train accuracy  0.63208 515.1574945449829\n",
            "test accuracy  0.7991 81.18667775392532\n",
            "\n",
            "Epoch: 58\n",
            "train accuracy  0.63402 512.9220597743988\n",
            "test accuracy  0.7959 79.82869118452072\n",
            "\n",
            "Epoch: 59\n",
            "train accuracy  0.63284 514.0735105276108\n",
            "test accuracy  0.8113 77.29106456041336\n",
            "\n",
            "Epoch: 60\n",
            "train accuracy  0.63616 510.6348613500595\n",
            "test accuracy  0.8138 76.84454536437988\n",
            "\n",
            "Epoch: 61\n",
            "train accuracy  0.63664 509.0057955980301\n",
            "test accuracy  0.8076 77.9173441529274\n",
            "\n",
            "Epoch: 62\n",
            "train accuracy  0.63726 507.82652872800827\n",
            "test accuracy  0.8039 80.20621359348297\n",
            "\n",
            "Epoch: 63\n",
            "train accuracy  0.6381 506.9279750585556\n",
            "test accuracy  0.8014 80.35127979516983\n",
            "\n",
            "Epoch: 64\n",
            "train accuracy  0.63886 505.91366189718246\n",
            "test accuracy  0.7987 81.01257401704788\n",
            "\n",
            "Epoch: 65\n",
            "train accuracy  0.64038 503.4271076321602\n",
            "test accuracy  0.8106 77.48792254924774\n",
            "\n",
            "Epoch: 66\n",
            "train accuracy  0.63946 502.8568615913391\n",
            "test accuracy  0.8073 79.95205771923065\n",
            "\n",
            "Epoch: 67\n",
            "train accuracy  0.63984 502.7634696364403\n",
            "test accuracy  0.8139 75.3112822175026\n",
            "\n",
            "Epoch: 68\n",
            "train accuracy  0.64084 501.3342463374138\n",
            "test accuracy  0.8116 77.93440037965775\n",
            "\n",
            "Epoch: 69\n",
            "train accuracy  0.64106 500.6138144135475\n",
            "test accuracy  0.7986 81.07704788446426\n",
            "\n",
            "Epoch: 70\n",
            "train accuracy  0.64282 497.9280608892441\n",
            "test accuracy  0.7997 79.26690244674683\n",
            "\n",
            "Epoch: 71\n",
            "train accuracy  0.64422 496.95891481637955\n",
            "test accuracy  0.8153 75.53462815284729\n",
            "\n",
            "Epoch: 72\n",
            "train accuracy  0.64366 495.3413813114166\n",
            "test accuracy  0.8059 76.57784050703049\n",
            "\n",
            "Epoch: 73\n",
            "train accuracy  0.64464 493.5482335090637\n",
            "test accuracy  0.7925 80.89980345964432\n",
            "\n",
            "Epoch: 74\n",
            "train accuracy  0.64444 493.3064399957657\n",
            "test accuracy  0.7977 81.96174186468124\n",
            "\n",
            "Epoch: 75\n",
            "train accuracy  0.64504 491.8615865111351\n",
            "test accuracy  0.8097 77.14908570051193\n",
            "\n",
            "Epoch: 76\n",
            "train accuracy  0.6488 488.68500876426697\n",
            "test accuracy  0.8031 81.11278694868088\n",
            "\n",
            "Epoch: 77\n",
            "train accuracy  0.64694 488.80865520238876\n",
            "test accuracy  0.8079 79.94513428211212\n",
            "\n",
            "Epoch: 78\n",
            "train accuracy  0.64792 486.99030566215515\n",
            "test accuracy  0.8099 77.5231169462204\n",
            "\n",
            "Epoch: 79\n",
            "train accuracy  0.6461 487.48852998018265\n",
            "test accuracy  0.8077 78.30331981182098\n",
            "\n",
            "Epoch: 80\n",
            "train accuracy  0.64764 486.854988694191\n",
            "test accuracy  0.7936 80.72698146104813\n",
            "\n",
            "Epoch: 81\n",
            "train accuracy  0.64932 483.7548608183861\n",
            "test accuracy  0.8101 77.15688735246658\n",
            "\n",
            "Epoch: 82\n",
            "train accuracy  0.6503 483.07145208120346\n",
            "test accuracy  0.814 76.9781841635704\n",
            "\n",
            "Epoch: 83\n",
            "train accuracy  0.64902 480.69610303640366\n",
            "test accuracy  0.7891 80.54066276550293\n",
            "\n",
            "Epoch: 84\n",
            "train accuracy  0.65132 480.82222986221313\n",
            "test accuracy  0.7815 84.28395634889603\n",
            "\n",
            "Epoch: 85\n",
            "train accuracy  0.64956 480.7163172364235\n",
            "test accuracy  0.7941 80.2657026052475\n",
            "\n",
            "Epoch: 86\n",
            "train accuracy  0.65272 476.8501899242401\n",
            "test accuracy  0.799 79.37802809476852\n",
            "\n",
            "Epoch: 87\n",
            "train accuracy  0.65178 475.6316629052162\n",
            "test accuracy  0.8022 79.81015229225159\n",
            "\n",
            "Epoch: 88\n",
            "train accuracy  0.653 474.43486112356186\n",
            "test accuracy  0.7967 79.79936027526855\n",
            "\n",
            "Epoch: 89\n",
            "train accuracy  0.65254 473.7950528860092\n",
            "test accuracy  0.7939 81.25718766450882\n",
            "\n",
            "Epoch: 90\n",
            "train accuracy  0.65352 471.63521188497543\n",
            "test accuracy  0.7875 82.4405328631401\n",
            "\n",
            "Epoch: 91\n",
            "train accuracy  0.65766 468.6978568434715\n",
            "test accuracy  0.7945 80.40636658668518\n",
            "\n",
            "Epoch: 92\n",
            "train accuracy  0.65602 467.1494207382202\n",
            "test accuracy  0.7964 79.58438450098038\n",
            "\n",
            "Epoch: 93\n",
            "train accuracy  0.65656 465.82685965299606\n",
            "test accuracy  0.7947 80.97736442089081\n",
            "\n",
            "Epoch: 94\n",
            "train accuracy  0.65692 463.90282624959946\n",
            "test accuracy  0.7951 79.6096076965332\n",
            "\n",
            "Epoch: 95\n",
            "train accuracy  0.65766 463.6311122775078\n",
            "test accuracy  0.7839 83.13146126270294\n",
            "\n",
            "Epoch: 96\n",
            "train accuracy  0.65726 461.6265382170677\n",
            "test accuracy  0.7862 82.9016090631485\n",
            "\n",
            "Epoch: 97\n",
            "train accuracy  0.65926 461.1894598007202\n",
            "test accuracy  0.7571 90.0069272518158\n",
            "\n",
            "Epoch: 98\n",
            "train accuracy  0.66094 456.96894431114197\n",
            "test accuracy  0.7759 84.32772296667099\n",
            "\n",
            "Epoch: 99\n",
            "train accuracy  0.6608 455.61304849386215\n",
            "test accuracy  0.7912 79.791679084301\n",
            "\n",
            "Epoch: 100\n",
            "train accuracy  0.66084 454.92697298526764\n",
            "test accuracy  0.7857 82.76000905036926\n",
            "\n",
            "Epoch: 101\n",
            "train accuracy  0.6628 452.9929840564728\n",
            "test accuracy  0.7864 82.39188086986542\n",
            "\n",
            "Epoch: 102\n",
            "train accuracy  0.66276 451.8445439338684\n",
            "test accuracy  0.7691 86.59204471111298\n",
            "\n",
            "Epoch: 103\n",
            "train accuracy  0.664 449.472487449646\n",
            "test accuracy  0.7565 90.55416756868362\n",
            "\n",
            "Epoch: 104\n",
            "train accuracy  0.66228 449.11048793792725\n",
            "test accuracy  0.7846 81.10982018709183\n",
            "\n",
            "Epoch: 105\n",
            "train accuracy  0.66524 445.5161312222481\n",
            "test accuracy  0.7837 82.87349760532379\n",
            "\n",
            "Epoch: 106\n",
            "train accuracy  0.66944 442.0804272890091\n",
            "test accuracy  0.7723 86.44333511590958\n",
            "\n",
            "Epoch: 107\n",
            "train accuracy  0.6685 442.06058210134506\n",
            "test accuracy  0.7763 84.8226233124733\n",
            "\n",
            "Epoch: 108\n",
            "train accuracy  0.66884 439.0503019094467\n",
            "test accuracy  0.7722 83.52214115858078\n",
            "\n",
            "Epoch: 109\n",
            "train accuracy  0.66786 438.393139064312\n",
            "test accuracy  0.7842 80.41483163833618\n",
            "\n",
            "Epoch: 110\n",
            "train accuracy  0.6696 435.3377048969269\n",
            "test accuracy  0.7719 86.44809252023697\n",
            "\n",
            "Epoch: 111\n",
            "train accuracy  0.67156 434.2779639363289\n",
            "test accuracy  0.7803 82.43817418813705\n",
            "\n",
            "Epoch: 112\n",
            "train accuracy  0.67132 432.4960342645645\n",
            "test accuracy  0.7516 91.322414457798\n",
            "\n",
            "Epoch: 113\n",
            "train accuracy  0.67146 432.2953976392746\n",
            "test accuracy  0.7681 85.08596467971802\n",
            "\n",
            "Epoch: 114\n",
            "train accuracy  0.67332 429.2836348414421\n",
            "test accuracy  0.7739 85.45691275596619\n",
            "\n",
            "Epoch: 115\n",
            "train accuracy  0.6756 425.64630591869354\n",
            "test accuracy  0.7649 86.4591451883316\n",
            "\n",
            "Epoch: 116\n",
            "train accuracy  0.67572 424.67949873209\n",
            "test accuracy  0.756 87.9304506778717\n",
            "\n",
            "Epoch: 117\n",
            "train accuracy  0.6738 422.7826348543167\n",
            "test accuracy  0.7785 81.35896444320679\n",
            "\n",
            "Epoch: 118\n",
            "train accuracy  0.67562 420.17392134666443\n",
            "test accuracy  0.7745 83.11285054683685\n",
            "\n",
            "Epoch: 119\n",
            "train accuracy  0.67672 416.53071957826614\n",
            "test accuracy  0.7665 85.46964293718338\n",
            "\n",
            "Epoch: 120\n",
            "train accuracy  0.67772 417.75330942869186\n",
            "test accuracy  0.7491 89.5197075009346\n",
            "\n",
            "Epoch: 121\n",
            "train accuracy  0.67926 415.85894906520844\n",
            "test accuracy  0.7654 87.45104223489761\n",
            "\n",
            "Epoch: 122\n",
            "train accuracy  0.68012 412.9355493783951\n",
            "test accuracy  0.7402 93.3753564953804\n",
            "\n",
            "Epoch: 123\n",
            "train accuracy  0.68378 409.3341618180275\n",
            "test accuracy  0.7533 90.93684428930283\n",
            "\n",
            "Epoch: 124\n",
            "train accuracy  0.68368 406.7199327945709\n",
            "test accuracy  0.7375 91.98510271310806\n",
            "\n",
            "Epoch: 125\n",
            "train accuracy  0.68328 405.7817113995552\n",
            "test accuracy  0.7432 90.53261995315552\n",
            "\n",
            "Epoch: 126\n",
            "train accuracy  0.68366 405.48163628578186\n",
            "test accuracy  0.7351 93.66630524396896\n",
            "\n",
            "Epoch: 127\n",
            "train accuracy  0.68948 399.64853698015213\n",
            "test accuracy  0.7442 91.45882713794708\n",
            "\n",
            "Epoch: 128\n",
            "train accuracy  0.68794 398.8697293996811\n",
            "test accuracy  0.7496 91.39906984567642\n",
            "\n",
            "Epoch: 129\n",
            "train accuracy  0.69102 394.81939220428467\n",
            "test accuracy  0.7442 88.47820460796356\n",
            "\n",
            "Epoch: 130\n",
            "train accuracy  0.69082 392.71193540096283\n",
            "test accuracy  0.7296 94.88938957452774\n",
            "\n",
            "Epoch: 131\n",
            "train accuracy  0.69396 389.3105950951576\n",
            "test accuracy  0.7549 86.86792731285095\n",
            "\n",
            "Epoch: 132\n",
            "train accuracy  0.6923 390.3065561056137\n",
            "test accuracy  0.7481 88.96449637413025\n",
            "\n",
            "Epoch: 133\n",
            "train accuracy  0.69506 385.46093678474426\n",
            "test accuracy  0.7298 95.37864762544632\n",
            "\n",
            "Epoch: 134\n",
            "train accuracy  0.69504 383.88302582502365\n",
            "test accuracy  0.7307 93.40797740221024\n",
            "\n",
            "Epoch: 135\n",
            "train accuracy  0.69474 381.81644266843796\n",
            "test accuracy  0.7358 92.31288224458694\n",
            "\n",
            "Epoch: 136\n",
            "train accuracy  0.69826 381.11145490407944\n",
            "test accuracy  0.7185 97.05732983350754\n",
            "\n",
            "Epoch: 137\n",
            "train accuracy  0.6994 378.04873156547546\n",
            "test accuracy  0.7262 94.10452610254288\n",
            "\n",
            "Epoch: 138\n",
            "train accuracy  0.70086 377.86615765094757\n",
            "test accuracy  0.7509 87.27836471796036\n",
            "\n",
            "Epoch: 139\n",
            "train accuracy  0.6997 373.3078188300133\n",
            "test accuracy  0.7328 94.28712499141693\n",
            "\n",
            "Epoch: 140\n",
            "train accuracy  0.70238 370.5151847600937\n",
            "test accuracy  0.7202 94.57260769605637\n",
            "\n",
            "Epoch: 141\n",
            "train accuracy  0.70174 369.7728476524353\n",
            "test accuracy  0.7181 98.44602185487747\n",
            "\n",
            "Epoch: 142\n",
            "train accuracy  0.7038 369.9158098101616\n",
            "test accuracy  0.715 97.7803447842598\n",
            "\n",
            "Epoch: 143\n",
            "train accuracy  0.71014 361.55447125434875\n",
            "test accuracy  0.7198 96.81142801046371\n",
            "\n",
            "Epoch: 144\n",
            "train accuracy  0.7091 360.1614050269127\n",
            "test accuracy  0.7283 94.2369259595871\n",
            "\n",
            "Epoch: 145\n",
            "train accuracy  0.7117 358.58521795272827\n",
            "test accuracy  0.7302 92.0173088312149\n",
            "\n",
            "Epoch: 146\n",
            "train accuracy  0.7124 356.4620985388756\n",
            "test accuracy  0.7145 97.34819680452347\n",
            "\n",
            "Epoch: 147\n",
            "train accuracy  0.71344 354.97516214847565\n",
            "test accuracy  0.7348 91.90711814165115\n",
            "\n",
            "Epoch: 148\n",
            "train accuracy  0.71596 352.7110124230385\n",
            "test accuracy  0.72 95.76101714372635\n",
            "\n",
            "Epoch: 149\n",
            "train accuracy  0.71512 350.7160676121712\n",
            "test accuracy  0.6944 103.61464297771454\n",
            "\n",
            "Epoch: 150\n",
            "train accuracy  0.71692 348.75494009256363\n",
            "test accuracy  0.6996 101.7655593752861\n",
            "\n",
            "Epoch: 151\n",
            "train accuracy  0.718 346.41817605495453\n",
            "test accuracy  0.7148 97.09713470935822\n",
            "\n",
            "Epoch: 152\n",
            "train accuracy  0.72016 343.30638843774796\n",
            "test accuracy  0.7183 94.6333532333374\n",
            "\n",
            "Epoch: 153\n",
            "train accuracy  0.72064 342.3570818901062\n",
            "test accuracy  0.7022 100.16912811994553\n",
            "\n",
            "Epoch: 154\n",
            "train accuracy  0.72544 336.5373736023903\n",
            "test accuracy  0.6869 107.04188603162766\n",
            "\n",
            "Epoch: 155\n",
            "train accuracy  0.72494 336.93052327632904\n",
            "test accuracy  0.7085 98.94261348247528\n",
            "\n",
            "Epoch: 156\n",
            "train accuracy  0.7253 335.10183250904083\n",
            "test accuracy  0.7001 102.76818883419037\n",
            "\n",
            "Epoch: 157\n",
            "train accuracy  0.7274 331.852487385273\n",
            "test accuracy  0.6802 108.11411881446838\n",
            "\n",
            "Epoch: 158\n",
            "train accuracy  0.73136 328.0381877422333\n",
            "test accuracy  0.703 98.92136931419373\n",
            "\n",
            "Epoch: 159\n",
            "train accuracy  0.72914 328.97800701856613\n",
            "test accuracy  0.6962 103.06618362665176\n",
            "\n",
            "Epoch: 160\n",
            "train accuracy  0.73212 325.1250753104687\n",
            "test accuracy  0.7005 101.89500868320465\n",
            "\n",
            "Epoch: 161\n",
            "train accuracy  0.7318 325.73488461971283\n",
            "test accuracy  0.7076 98.17968285083771\n",
            "\n",
            "Epoch: 162\n",
            "train accuracy  0.73516 321.7846228480339\n",
            "test accuracy  0.6872 106.61405336856842\n",
            "\n",
            "Epoch: 163\n",
            "train accuracy  0.73382 321.4750871658325\n",
            "test accuracy  0.7002 103.41605246067047\n",
            "\n",
            "Epoch: 164\n",
            "train accuracy  0.73914 314.9800493121147\n",
            "test accuracy  0.6857 105.69901353120804\n",
            "\n",
            "Epoch: 165\n",
            "train accuracy  0.73744 317.2511640191078\n",
            "test accuracy  0.6871 105.06840908527374\n",
            "\n",
            "Epoch: 166\n",
            "train accuracy  0.74046 314.35747241973877\n",
            "test accuracy  0.6736 109.6952496767044\n",
            "\n",
            "Epoch: 167\n",
            "train accuracy  0.7411 311.0383030474186\n",
            "test accuracy  0.7061 100.79471898078918\n",
            "\n",
            "Epoch: 168\n",
            "train accuracy  0.74118 310.434717297554\n",
            "test accuracy  0.6929 105.30954760313034\n",
            "\n",
            "Epoch: 169\n",
            "train accuracy  0.74352 307.1059639453888\n",
            "test accuracy  0.694 104.63220554590225\n",
            "\n",
            "Epoch: 170\n",
            "train accuracy  0.7454 306.03439098596573\n",
            "test accuracy  0.6879 106.87852561473846\n",
            "\n",
            "Epoch: 171\n",
            "train accuracy  0.74684 305.1759075820446\n",
            "test accuracy  0.6942 102.28048974275589\n",
            "\n",
            "Epoch: 172\n",
            "train accuracy  0.7515 297.6099880039692\n",
            "test accuracy  0.702 100.43165731430054\n",
            "\n",
            "Epoch: 173\n",
            "train accuracy  0.74794 300.67045360803604\n",
            "test accuracy  0.6591 112.82773077487946\n",
            "\n",
            "Epoch: 174\n",
            "train accuracy  0.74994 297.4180496931076\n",
            "test accuracy  0.6694 111.31668907403946\n",
            "\n",
            "Epoch: 175\n",
            "train accuracy  0.74826 298.5224955379963\n",
            "test accuracy  0.6828 109.02410691976547\n",
            "\n",
            "Epoch: 176\n",
            "train accuracy  0.75328 296.30410274863243\n",
            "test accuracy  0.6634 113.59051597118378\n",
            "\n",
            "Epoch: 177\n",
            "train accuracy  0.75514 293.49030700325966\n",
            "test accuracy  0.6689 112.61667311191559\n",
            "\n",
            "Epoch: 178\n",
            "train accuracy  0.75406 291.7707376778126\n",
            "test accuracy  0.6777 109.45732408761978\n",
            "\n",
            "Epoch: 179\n",
            "train accuracy  0.75804 289.3088067173958\n",
            "test accuracy  0.6942 103.25265139341354\n",
            "\n",
            "Epoch: 180\n",
            "train accuracy  0.75648 290.31739914417267\n",
            "test accuracy  0.6919 107.17429459095001\n",
            "\n",
            "Epoch: 181\n",
            "train accuracy  0.7561 289.36833184957504\n",
            "test accuracy  0.6792 108.90964215993881\n",
            "\n",
            "Epoch: 182\n",
            "train accuracy  0.76338 281.9616514444351\n",
            "test accuracy  0.6879 106.50763231515884\n",
            "\n",
            "Epoch: 183\n",
            "train accuracy  0.75736 285.5086852014065\n",
            "test accuracy  0.6773 110.65555715560913\n",
            "\n",
            "Epoch: 184\n",
            "train accuracy  0.7622 282.31569424271584\n",
            "test accuracy  0.6881 106.95877647399902\n",
            "\n",
            "Epoch: 185\n",
            "train accuracy  0.76502 280.9666364490986\n",
            "test accuracy  0.6733 110.17630565166473\n",
            "\n",
            "Epoch: 186\n",
            "train accuracy  0.76556 278.3187094926834\n",
            "test accuracy  0.6781 110.01991093158722\n",
            "\n",
            "Epoch: 187\n",
            "train accuracy  0.76946 274.5471333861351\n",
            "test accuracy  0.676 111.57222008705139\n",
            "\n",
            "Epoch: 188\n",
            "train accuracy  0.7676 277.7226737141609\n",
            "test accuracy  0.6672 116.18754196166992\n",
            "\n",
            "Epoch: 189\n",
            "train accuracy  0.76848 273.8980997800827\n",
            "test accuracy  0.6714 113.93189042806625\n",
            "\n",
            "Epoch: 190\n",
            "train accuracy  0.76754 273.5404304265976\n",
            "test accuracy  0.6885 107.88523328304291\n",
            "\n",
            "Epoch: 191\n",
            "train accuracy  0.77038 271.88137662410736\n",
            "test accuracy  0.6685 112.4232075214386\n",
            "\n",
            "Epoch: 192\n",
            "train accuracy  0.7675 270.82537919282913\n",
            "test accuracy  0.6801 109.87581145763397\n",
            "\n",
            "Epoch: 193\n",
            "train accuracy  0.7741 267.41217648983\n",
            "test accuracy  0.6569 117.18589121103287\n",
            "\n",
            "Epoch: 194\n",
            "train accuracy  0.76824 270.0982150733471\n",
            "test accuracy  0.6772 109.68662244081497\n",
            "\n",
            "Epoch: 195\n",
            "train accuracy  0.77536 264.0973431766033\n",
            "test accuracy  0.6834 106.31123894453049\n",
            "\n",
            "Epoch: 196\n",
            "train accuracy  0.77346 265.1918941438198\n",
            "test accuracy  0.6819 107.50060015916824\n",
            "\n",
            "Epoch: 197\n",
            "train accuracy  0.7742 263.51033318042755\n",
            "test accuracy  0.6681 111.60731995105743\n",
            "\n",
            "Epoch: 198\n",
            "train accuracy  0.77718 259.12828773260117\n",
            "test accuracy  0.6644 113.7156473994255\n",
            "\n",
            "Epoch: 199\n",
            "train accuracy  0.7788 261.78497022390366\n",
            "test accuracy  0.6477 121.44209969043732\n",
            "\n",
            "Epoch: 200\n",
            "train accuracy  0.77858 259.6009124815464\n",
            "test accuracy  0.678 110.93031066656113\n",
            "\n",
            "Epoch: 201\n",
            "train accuracy  0.7813 257.24407836794853\n",
            "test accuracy  0.6748 109.8570346236229\n",
            "\n",
            "Epoch: 202\n",
            "train accuracy  0.78194 256.0580902695656\n",
            "test accuracy  0.683 108.51858299970627\n",
            "\n",
            "Epoch: 203\n",
            "train accuracy  0.78218 254.47078201174736\n",
            "test accuracy  0.6796 110.13867098093033\n",
            "\n",
            "Epoch: 204\n",
            "train accuracy  0.78264 255.49143359065056\n",
            "test accuracy  0.6702 112.49080139398575\n",
            "\n",
            "Epoch: 205\n",
            "train accuracy  0.78 255.9588539302349\n",
            "test accuracy  0.66 114.6870254278183\n",
            "\n",
            "Epoch: 206\n",
            "train accuracy  0.78344 253.26018434762955\n",
            "test accuracy  0.6688 114.574247777462\n",
            "\n",
            "Epoch: 207\n",
            "train accuracy  0.7867 250.62484461069107\n",
            "test accuracy  0.6619 115.70328027009964\n",
            "\n",
            "Epoch: 208\n",
            "train accuracy  0.78282 254.02046319842339\n",
            "test accuracy  0.6938 104.88036978244781\n",
            "\n",
            "Epoch: 209\n",
            "train accuracy  0.78818 248.0738392174244\n",
            "test accuracy  0.6475 124.57171553373337\n",
            "\n",
            "Epoch: 210\n",
            "train accuracy  0.78254 252.00523352622986\n",
            "test accuracy  0.6619 116.0040517449379\n",
            "\n",
            "Epoch: 211\n",
            "train accuracy  0.78714 248.83509919047356\n",
            "test accuracy  0.6821 107.90187257528305\n",
            "\n",
            "Epoch: 212\n",
            "train accuracy  0.78994 245.59289520978928\n",
            "test accuracy  0.6563 117.12729865312576\n",
            "\n",
            "Epoch: 213\n",
            "train accuracy  0.7887 247.00915357470512\n",
            "test accuracy  0.6768 111.31580686569214\n",
            "\n",
            "Epoch: 214\n",
            "train accuracy  0.79008 243.4539460837841\n",
            "test accuracy  0.6474 120.44248080253601\n",
            "\n",
            "Epoch: 215\n",
            "train accuracy  0.7899 245.07906633615494\n",
            "test accuracy  0.6716 113.21678555011749\n",
            "\n",
            "Epoch: 216\n",
            "train accuracy  0.7926 242.0269857943058\n",
            "test accuracy  0.6542 119.64230859279633\n",
            "\n",
            "Epoch: 217\n",
            "train accuracy  0.79304 242.08704310655594\n",
            "test accuracy  0.6701 111.23015171289444\n",
            "\n",
            "Epoch: 218\n",
            "train accuracy  0.79424 239.45288333296776\n",
            "test accuracy  0.6452 123.1030580997467\n",
            "\n",
            "Epoch: 219\n",
            "train accuracy  0.79258 240.88403657078743\n",
            "test accuracy  0.6684 113.06775307655334\n",
            "\n",
            "Epoch: 220\n",
            "train accuracy  0.79388 240.82397761940956\n",
            "test accuracy  0.6656 113.17107635736465\n",
            "\n",
            "Epoch: 221\n",
            "train accuracy  0.79768 235.92926740646362\n",
            "test accuracy  0.6724 113.16852992773056\n",
            "\n",
            "Epoch: 222\n",
            "train accuracy  0.7961 236.5294930934906\n",
            "test accuracy  0.6632 116.90736198425293\n",
            "\n",
            "Epoch: 223\n",
            "train accuracy  0.79596 238.0460213124752\n",
            "test accuracy  0.6723 114.49793422222137\n",
            "\n",
            "Epoch: 224\n",
            "train accuracy  0.7954 237.71226933598518\n",
            "test accuracy  0.669 116.19345390796661\n",
            "\n",
            "Epoch: 225\n",
            "train accuracy  0.7994 233.72551706433296\n",
            "test accuracy  0.6621 118.55769211053848\n",
            "\n",
            "Epoch: 226\n",
            "train accuracy  0.79808 233.84837278723717\n",
            "test accuracy  0.6659 115.65685844421387\n",
            "\n",
            "Epoch: 227\n",
            "train accuracy  0.79838 234.36292207241058\n",
            "test accuracy  0.6661 117.99686336517334\n",
            "\n",
            "Epoch: 228\n",
            "train accuracy  0.79878 231.6866150200367\n",
            "test accuracy  0.6213 132.69650530815125\n",
            "\n",
            "Epoch: 229\n",
            "train accuracy  0.79744 233.41664081811905\n",
            "test accuracy  0.6571 115.52495378255844\n",
            "\n",
            "Epoch: 230\n",
            "train accuracy  0.80044 231.9199841916561\n",
            "test accuracy  0.6538 119.99855464696884\n",
            "\n",
            "Epoch: 231\n",
            "train accuracy  0.79972 230.76026505231857\n",
            "test accuracy  0.6513 124.23019099235535\n",
            "\n",
            "Epoch: 232\n",
            "train accuracy  0.79546 237.53725773096085\n",
            "test accuracy  0.6556 120.92520570755005\n",
            "\n",
            "Epoch: 233\n",
            "train accuracy  0.80148 231.9783158302307\n",
            "test accuracy  0.6819 112.92418891191483\n",
            "\n",
            "Epoch: 234\n",
            "train accuracy  0.80136 230.2299900352955\n",
            "test accuracy  0.6587 119.0914351940155\n",
            "\n",
            "Epoch: 235\n",
            "train accuracy  0.7982 231.83568930625916\n",
            "test accuracy  0.6472 123.94113063812256\n",
            "\n",
            "Epoch: 236\n",
            "train accuracy  0.79998 232.45787072181702\n",
            "test accuracy  0.6582 117.40501934289932\n",
            "\n",
            "Epoch: 237\n",
            "train accuracy  0.80472 227.0978230983019\n",
            "test accuracy  0.6448 123.7490718960762\n",
            "\n",
            "Epoch: 238\n",
            "train accuracy  0.80116 229.68573993444443\n",
            "test accuracy  0.6897 108.48803323507309\n",
            "\n",
            "Epoch: 239\n",
            "train accuracy  0.8057 227.5566866695881\n",
            "test accuracy  0.6492 122.94322437047958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMfVvXNunQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "ba28ed53-34d3-4f9b-8c8c-8276ee0a0a65"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.plot(epoch_list, train_acc, 'g--' , label='train_acc')\n",
        "plt.plot(epoch_list, test_acc, 'bo--', label = 'test_acc')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQJr8lVIqAGnyV0qpAPT/ARLjaHmeAcyQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzi-Yr4gvye3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f23dda31-74c4-4002-e68d-cde0166090b6"
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "import random\n",
        "length = len(testset.targets)\n",
        "percentage_corruption = 30\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "print(corrupt_idx)\n",
        "print(len(corrupt_classes))\n",
        "\n",
        "st1 = testset.targets\n",
        "a = np.array(testset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "testset.targets = list(a)\n",
        "st2 = testset.targets\n",
        "\n",
        "print(st1 == st2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "3000 3000\n",
            "[9293, 5933, 1312, 5810, 5295, 1212, 6902, 3217, 28, 7694, 3210, 1188, 4866, 8040, 4385, 7268, 5271, 7316, 2109, 391, 6828, 8532, 2937, 2083, 229, 1531, 9235, 3898, 1752, 3745, 4230, 7748, 9851, 4750, 4401, 8799, 8711, 3765, 9229, 5459, 8424, 7092, 8740, 7717, 7465, 9487, 2952, 8505, 3589, 162, 7674, 6359, 472, 1177, 6461, 8307, 8543, 4117, 6298, 1802, 1791, 539, 7548, 5725, 9317, 6336, 6713, 9559, 4998, 3374, 4645, 6284, 9518, 2209, 3223, 6984, 7184, 5591, 9353, 3914, 2784, 8481, 3161, 7504, 5146, 9500, 2246, 9886, 3150, 6255, 1597, 435, 5551, 3023, 5390, 5233, 67, 8220, 7948, 3025, 8065, 5856, 4064, 1820, 3313, 1832, 5480, 8780, 8612, 693, 6533, 2868, 4452, 3330, 5483, 6768, 5232, 2221, 8583, 4590, 9976, 2072, 2338, 1436, 9129, 2895, 9769, 2554, 8739, 8805, 2705, 2148, 3533, 6895, 3024, 3619, 6380, 4282, 6159, 8587, 1501, 9416, 8032, 7342, 7170, 1525, 8297, 4474, 1073, 9497, 6105, 2511, 6834, 190, 8323, 6032, 1632, 347, 7048, 951, 3009, 3305, 1398, 7585, 8482, 3518, 7137, 1504, 2490, 3987, 8651, 9365, 7605, 8677, 1382, 6138, 6438, 7469, 1940, 9688, 3454, 969, 1186, 2115, 386, 7002, 841, 1935, 6907, 3523, 6752, 6179, 8807, 556, 6654, 9858, 6368, 1600, 2372, 5176, 392, 6263, 2049, 3845, 3489, 1315, 6338, 2867, 5631, 2876, 1231, 3901, 8253, 3926, 2463, 3643, 7830, 2087, 9932, 3399, 9695, 8818, 9458, 7875, 7698, 4541, 9341, 2189, 8635, 8279, 8890, 4398, 6400, 6493, 9777, 5850, 3039, 2323, 8452, 3143, 3939, 8027, 2590, 6800, 1044, 3728, 7294, 8852, 6282, 7039, 6312, 2525, 332, 4026, 7999, 1448, 1397, 5961, 7106, 737, 2438, 2836, 2449, 8117, 5033, 753, 2425, 7815, 2871, 9552, 1747, 3500, 9176, 7503, 7214, 8467, 3919, 7648, 1214, 5775, 4843, 4602, 3072, 3473, 2897, 1789, 7266, 9667, 606, 1303, 311, 217, 3000, 7473, 3763, 4917, 6081, 4898, 6698, 7084, 8979, 9489, 5681, 5220, 9903, 8211, 5327, 2484, 7415, 2287, 8362, 8246, 932, 3736, 5026, 4436, 3989, 7477, 9604, 4199, 9057, 8147, 9626, 4875, 9372, 6038, 298, 9900, 470, 9177, 7512, 4770, 3227, 4819, 8384, 8219, 7606, 7229, 5901, 4002, 920, 9284, 6810, 1, 1595, 8726, 7739, 5899, 9495, 3735, 9986, 2585, 6914, 8869, 3342, 8430, 9685, 1246, 2198, 889, 5351, 4286, 1377, 8022, 5190, 1068, 553, 4104, 83, 8490, 9452, 3046, 8589, 8976, 1602, 592, 3080, 5645, 7104, 3099, 1084, 2548, 5786, 4457, 8099, 3575, 2444, 3803, 5011, 4855, 3141, 8568, 6397, 1877, 3081, 3285, 9484, 2477, 2274, 8899, 1127, 8373, 5607, 6780, 496, 9730, 9224, 6989, 6236, 3355, 4552, 1000, 3362, 971, 207, 266, 4049, 7403, 8880, 842, 9171, 4442, 1516, 3854, 9208, 1380, 1665, 1299, 538, 3635, 1540, 9898, 8687, 5427, 9194, 4517, 8637, 2297, 2019, 1561, 7355, 4957, 1980, 7207, 1438, 7790, 8319, 6684, 2745, 3320, 4478, 8724, 4796, 2035, 909, 119, 2250, 8067, 1252, 2245, 4914, 3409, 8115, 5070, 4532, 9263, 76, 1038, 573, 5652, 2074, 2931, 4867, 2513, 8600, 4147, 4659, 5147, 9670, 1499, 5484, 349, 5632, 7662, 4325, 7555, 4374, 7336, 2206, 7544, 4354, 5892, 6091, 533, 9554, 4713, 425, 2204, 7113, 8273, 8476, 1897, 6376, 9648, 6626, 6170, 468, 4225, 1809, 1570, 6646, 5215, 9852, 8847, 1670, 6599, 8713, 865, 382, 5687, 2626, 5845, 4744, 5938, 2028, 9318, 7988, 4939, 3582, 7888, 2953, 6152, 1326, 3656, 4658, 4462, 1526, 5837, 7551, 3160, 3361, 6345, 4007, 6644, 7876, 8187, 8867, 2840, 4334, 205, 2821, 8374, 8650, 2381, 4120, 9656, 9001, 5240, 955, 1744, 8909, 7912, 9363, 6351, 8271, 449, 7269, 3675, 2454, 5143, 6881, 7510, 4954, 9919, 873, 6175, 5227, 5800, 199, 9147, 9216, 6522, 5025, 8754, 6680, 9702, 4035, 7522, 2455, 4259, 3730, 93, 4508, 2726, 9833, 7300, 9781, 9881, 2958, 940, 7311, 8282, 9766, 8937, 3846, 2791, 9666, 7703, 3483, 5997, 6113, 7591, 8686, 5451, 3318, 3497, 5041, 204, 7663, 408, 4160, 9023, 667, 7825, 3430, 2919, 5886, 1355, 4558, 5310, 1804, 6444, 2275, 5829, 15, 7577, 7223, 4529, 6690, 5015, 9027, 7785, 7052, 7050, 4999, 7187, 2977, 2928, 3490, 2541, 3415, 1509, 1191, 8940, 7026, 7675, 4702, 1882, 6704, 9589, 8943, 6951, 2203, 603, 2341, 1035, 9537, 8398, 8232, 1693, 6137, 8102, 5281, 5998, 3855, 6694, 9361, 2050, 1407, 441, 2345, 4818, 9837, 3510, 8997, 3435, 9218, 7709, 2346, 1916, 807, 8763, 2018, 4156, 4406, 6341, 911, 413, 9810, 8736, 3522, 2143, 1024, 6362, 3773, 7265, 1125, 6112, 9035, 4681, 9722, 6050, 9515, 9234, 785, 1649, 8085, 7358, 7720, 2475, 3615, 3244, 1269, 2849, 8699, 5298, 2139, 6982, 8815, 6700, 7455, 8394, 4467, 3252, 1748, 4564, 3603, 9358, 5827, 1953, 1674, 2635, 8313, 239, 623, 4162, 6793, 3133, 5491, 2308, 5406, 4061, 7722, 1822, 9698, 746, 8012, 4435, 5311, 337, 6143, 2390, 1465, 9319, 3655, 137, 5488, 7030, 6379, 8031, 4101, 9916, 4824, 5981, 2387, 1399, 3055, 9928, 7204, 4060, 2731, 8694, 9347, 3769, 6565, 2239, 7611, 8436, 5485, 6831, 585, 2903, 7735, 3542, 5978, 8927, 8066, 5808, 1209, 5641, 7922, 5969, 515, 597, 5721, 2320, 5909, 7562, 4783, 4290, 4258, 8639, 8058, 4651, 9466, 2483, 2467, 7015, 2706, 512, 5260, 9890, 4892, 3740, 2997, 824, 4712, 6258, 9619, 6222, 9434, 7806, 3480, 6304, 9968, 5704, 2042, 5869, 3213, 7393, 8710, 4759, 6554, 6475, 1083, 7757, 2107, 2580, 7402, 3129, 6265, 7862, 3439, 3229, 3777, 3444, 5799, 7327, 6394, 3601, 4679, 9291, 2029, 7444, 8429, 8260, 660, 6232, 5362, 4946, 1585, 5363, 3766, 6422, 2759, 3560, 2559, 1484, 6374, 9508, 676, 8081, 7941, 4265, 7665, 1865, 9553, 9756, 4624, 6465, 2766, 4674, 7491, 7148, 4671, 1643, 4138, 55, 4601, 755, 3975, 1724, 8053, 1032, 3624, 3671, 3536, 7817, 6859, 8888, 7205, 7387, 8644, 2334, 2544, 1217, 3647, 6624, 1348, 1192, 1557, 8110, 2349, 482, 5735, 5140, 5643, 2689, 1223, 1929, 9563, 2355, 1550, 7273, 6027, 9179, 1837, 6784, 5555, 2106, 7459, 9616, 214, 9774, 6440, 5267, 4966, 184, 8407, 4228, 7233, 5243, 2421, 5932, 5392, 2884, 8001, 7421, 3897, 6191, 7546, 4450, 1464, 5985, 9211, 1054, 3562, 1770, 2707, 7777, 723, 1137, 6623, 33, 7450, 565, 9209, 5580, 78, 3800, 8499, 2329, 7727, 5833, 1972, 2818, 6283, 2059, 3552, 9259, 9130, 7547, 658, 618, 675, 6840, 9991, 1334, 6861, 504, 8681, 6506, 6423, 1566, 1146, 6188, 1735, 7211, 3005, 8269, 5325, 295, 8145, 8050, 8146, 6471, 8981, 2290, 4118, 6836, 5470, 4871, 1828, 6723, 4, 17, 3967, 8716, 6349, 2536, 8312, 2652, 3791, 730, 5512, 6285, 2260, 6583, 2235, 558, 759, 4093, 4849, 2908, 1930, 2104, 2861, 3960, 3132, 1705, 6592, 4483, 3861, 6837, 4974, 9364, 5316, 6094, 8920, 8222, 6959, 4288, 7165, 6211, 6132, 9551, 8733, 9806, 8827, 1241, 1174, 5774, 3032, 5486, 5481, 7956, 4716, 8832, 1419, 7524, 1861, 4870, 2761, 7128, 4332, 2787, 8562, 5963, 2625, 7507, 6759, 4548, 7647, 1968, 231, 8848, 5647, 5356, 1025, 9222, 9828, 2394, 290, 3352, 319, 7561, 169, 4295, 9757, 3558, 153, 561, 5361, 8437, 7164, 4430, 9322, 9285, 4953, 5495, 8231, 1294, 6393, 60, 2126, 6021, 9863, 1736, 9090, 1685, 9680, 8544, 5350, 3707, 5014, 8989, 731, 2524, 8517, 3583, 9897, 5811, 8009, 1576, 2692, 5343, 6971, 7644, 120, 6523, 3195, 9459, 3466, 1417, 8831, 7318, 7361, 665, 4192, 6299, 6433, 4869, 6963, 3095, 7085, 5196, 6416, 1033, 2450, 1523, 3753, 8931, 3002, 2675, 7275, 7568, 9069, 3927, 6220, 6131, 8256, 9379, 9577, 1394, 783, 2284, 7420, 4763, 6455, 6118, 1711, 7799, 2030, 4588, 6520, 2956, 2740, 6034, 5814, 6192, 1647, 3548, 2051, 1462, 1943, 5319, 9815, 5432, 919, 1750, 3993, 7713, 3555, 2983, 3070, 2824, 415, 6797, 3953, 6281, 3145, 9437, 4938, 2969, 1425, 4296, 4790, 5018, 2350, 1794, 828, 2188, 4839, 3420, 7495, 9539, 2146, 8642, 2419, 192, 662, 8939, 751, 8525, 1473, 8070, 6382, 5365, 5504, 3829, 6790, 4680, 9201, 9007, 6098, 8023, 748, 1449, 4380, 9310, 8216, 3588, 2506, 4348, 6013, 835, 5030, 4589, 9079, 3451, 795, 7538, 7660, 6958, 5101, 7924, 7390, 3890, 9691, 2618, 9550, 3103, 7321, 6388, 4400, 7508, 7569, 8448, 1799, 3478, 548, 7059, 2768, 1409, 7584, 1219, 7567, 8088, 8480, 6249, 7900, 8417, 9995, 6900, 2205, 6629, 2186, 7367, 2870, 7115, 1139, 9778, 2099, 3291, 9597, 3470, 8546, 419, 6399, 5492, 5836, 3614, 1422, 7447, 1043, 7855, 8910, 5994, 9026, 9187, 9055, 7976, 5020, 5113, 7586, 8771, 1194, 7226, 9490, 5345, 6148, 8970, 1402, 925, 8177, 5446, 838, 3874, 1198, 6952, 4444, 4206, 2660, 8531, 8675, 4143, 3075, 7173, 4507, 9267, 6086, 3459, 1723, 8861, 6594, 8515, 9469, 6233, 3883, 9103, 4757, 9020, 7056, 9649, 1441, 2922, 9138, 7724, 5989, 6451, 1955, 7013, 741, 694, 929, 4351, 8399, 7047, 9915, 1743, 4231, 3513, 5744, 3974, 3063, 9904, 4277, 8414, 7534, 404, 8475, 8693, 830, 5865, 7195, 1128, 894, 3208, 7468, 6187, 3696, 4287, 8097, 7462, 637, 901, 738, 2321, 9850, 2838, 3050, 1063, 8770, 2760, 8751, 8811, 3938, 1601, 6667, 7594, 3045, 4734, 3339, 8887, 6743, 8080, 6824, 8463, 7240, 7516, 9278, 6559, 5943, 8594, 7826, 9752, 7803, 6641, 2324, 1256, 9951, 8495, 8479, 2793, 2545, 9106, 5214, 5917, 8804, 3124, 9542, 342, 5241, 1731, 1719, 9700, 4952, 6808, 6332, 527, 8884, 4585, 522, 2120, 8886, 8960, 3739, 6225, 3908, 1682, 9146, 974, 2267, 6788, 3278, 6650, 8225, 2920, 3253, 1691, 3528, 9190, 6253, 1142, 3179, 9088, 8396, 6063, 7353, 871, 8915, 2013, 4274, 7730, 1625, 5695, 2739, 9110, 6628, 1904, 2839, 8270, 2326, 2835, 6056, 5189, 2926, 8284, 5398, 3083, 6477, 84, 5737, 6640, 5234, 6805, 3556, 1673, 6277, 9225, 9665, 2589, 2962, 448, 7927, 2743, 8538, 5600, 3283, 2708, 9354, 9277, 9220, 3591, 9993, 6363, 3468, 3286, 5054, 4096, 1657, 1827, 3847, 8640, 9933, 4956, 6065, 3561, 3300, 562, 7626, 3858, 8243, 8561, 663, 2259, 3066, 4094, 7740, 1108, 8248, 9252, 5237, 4568, 3271, 436, 7787, 2573, 5323, 954, 2750, 1905, 310, 4570, 3692, 7531, 8125, 6801, 2807, 1784, 2910, 2789, 3284, 5671, 1379, 5272, 1623, 797, 1963, 3168, 6108, 263, 9652, 820, 9896, 8994, 6205, 8264, 3657, 6025, 9488, 5907, 1642, 7089, 4200, 8603, 8808, 9332, 7493, 5677, 5229, 9197, 5289, 4512, 6155, 6169, 7117, 7333, 5964, 3389, 7992, 5100, 3202, 8881, 867, 7042, 6507, 8241, 63, 8092, 928, 7049, 9076, 2252, 9436, 3343, 8163, 9047, 4788, 4212, 6200, 6651, 9272, 5834, 5344, 6158, 9481, 7132, 9426, 5711, 9581, 8355, 1959, 6891, 3701, 4375, 1676, 9970, 5412, 9397, 2946, 4046, 6421, 8659, 9178, 5136, 6460, 6688, 3022, 6937, 1244, 1553, 9192, 77, 6062, 5654, 9701, 3809, 3438, 6314, 7994, 6, 9042, 1291, 1105, 1352, 4647, 201, 2717, 7405, 8274, 6185, 7180, 2581, 3585, 9926, 8853, 227, 7095, 256, 5463, 8845, 7151, 2213, 4009, 1614, 9253, 8862, 2101, 754, 2991, 3097, 6679, 8766, 2614, 8767, 4613, 1424, 1359, 241, 4338, 1338, 9095, 8657, 4051, 5761, 2619, 2936, 8302, 6002, 1487, 922, 5249, 5040, 3167, 5572, 7690, 9355, 5990, 9884, 2899, 1059, 6819, 5936, 4799, 4709, 2553, 9031, 2723, 1184, 5199, 6292, 7479, 980, 4190, 4778, 5529, 8718, 5754, 111, 1140, 2304, 5379, 7931, 3825, 995, 106, 200, 5370, 4812, 1769, 8701, 8189, 5913, 5934, 113, 5301, 4070, 6689, 8039, 2995, 6401, 1325, 6414, 1362, 7679, 5068, 4714, 7244, 211, 8649, 6923, 789, 9735, 3425, 3029, 8161, 6489, 4092, 4989, 4641, 6918, 3682, 7424, 281, 973, 4013, 687, 5181, 6878, 376, 4241, 3653, 5574, 5787, 7110, 9738, 7498, 3610, 3884, 4803, 467, 5950, 5726, 1782, 4768, 6975, 5324, 7058, 6820, 4726, 3862, 1821, 2241, 718, 3020, 1148, 86, 475, 2116, 2201, 9198, 3387, 1855, 4006, 9750, 5496, 7857, 6030, 2369, 3115, 6718, 4186, 5840, 6310, 59, 2832, 5342, 9119, 8474, 3537, 5187, 5508, 8964, 4736, 3535, 3778, 2225, 9408, 2935, 147, 3641, 3544, 6862, 5552, 9534, 9816, 6944, 9650, 3892, 8162, 5259, 8434, 7012, 7575, 3240, 315, 6692, 3293, 8800, 5194, 5693, 6510, 4116, 3346, 1454, 4610, 3233, 1759, 6435, 535, 1679, 8539, 8930, 7438, 7306, 6404, 4816, 2089, 1893, 2118, 7714, 7287, 5605, 9383, 8182, 3689, 4453, 9918, 1109, 3881, 655, 1190, 9856, 6919, 9930, 4876, 2865, 1654, 7518, 3264, 1662, 9746, 3356, 1350, 5991, 2428, 1400, 331, 4057, 2371, 2502, 1884, 9005, 532, 2439, 9664, 3888, 341, 7307, 5553, 1413, 9610, 9876, 2043, 4886, 6228, 983, 9475, 6194, 2852, 377, 4699, 9231, 4921, 2721, 8494, 4422, 9946, 5255, 6217, 3440, 7615, 1311, 5675, 7913, 6096, 4959, 2237, 5716, 9588, 2795, 2732, 1255, 2330, 6424, 1261, 8772, 4311, 8706, 9784, 2033, 4916, 4911, 1100, 9091, 1452, 7227, 2720, 42, 1834, 1571, 3543, 4745, 7536, 9199, 5638, 2565, 6004, 2088, 1734, 5059, 3138, 38, 9555, 293, 1477, 3079, 8438, 5407, 5521, 5106, 4731, 1013, 2939, 8378, 583, 9585, 2212, 7414, 9374, 500, 1384, 7323, 1180, 9118, 893, 7374, 4992, 9048, 9891, 9596, 3078, 170, 570, 3910, 5722, 3049, 7035, 7372, 9463, 7870, 3827, 32, 8902, 4082, 9849, 3157, 7076, 5945, 8986, 3130, 6807, 4089, 4534, 445, 6296, 9734, 1700, 5095, 1195, 5817, 5122, 5898, 8266, 66, 3151, 6725, 1952, 9855, 8826, 2495, 8728, 3705, 3442, 7808, 7621, 8106, 4675, 7916, 488, 2344, 9521, 7871, 5338, 2178, 7712, 6835, 7742, 2414, 4904, 6329, 3166, 9410, 7622, 8908, 8922, 1471, 9124, 4304, 947, 8077, 1014, 7310, 6701, 5873, 2363, 1091, 3390, 9393, 6848, 9149, 4247, 1085, 3546, 9305, 389, 9270, 855, 4328, 4066, 6731, 5159, 7758, 2989, 324, 9507, 5257, 306, 5620, 790, 1097, 2309, 2500, 4098, 4842, 7388, 4062, 9641, 2437, 7448, 7530, 5057, 62, 4551, 3996, 1880, 2462, 7145, 8390, 7027, 6176, 6162, 7433, 1755, 6908, 4480, 5879, 2094, 7446, 7736, 9309, 3148, 4525, 1045, 5974, 2339, 6058, 714, 2966, 8300, 5960, 393, 9283, 257, 3493, 811, 2354, 1488, 5085, 8921, 6483, 4018, 7152, 8456, 4730, 4080, 2866, 9765, 1727, 7962, 8359, 6046, 107, 7181, 2588, 2133, 3288, 5039, 890, 3877, 3207, 7869, 6933, 3576, 1725, 2258, 7783, 8245, 3051, 4817, 3963, 5415, 8184, 5262, 8905, 3232, 5444, 7824, 8217, 8506, 8553, 8619, 2711, 9072, 7527, 957, 3592, 702, 3721, 3738, 2234, 554, 4495, 9719, 2479, 1412, 1225, 9941, 787, 180, 4831, 1340, 4511, 2570, 1375, 4085, 6370, 7373, 4888, 4020, 3962, 5230, 1437, 5657, 1002, 912, 629, 8607, 276, 1160, 6215, 8227, 5940, 2498, 8295, 9927, 36, 8865, 6007, 4238, 6371, 6067, 41, 8061, 7745, 9956, 7576, 3040, 868, 5854, 8778, 5074, 8598, 4549, 4477, 1451, 3948, 6425, 9620, 1286, 3807, 9645, 9338, 9261, 9182, 8043, 6350, 7935, 3443, 326, 631, 3569, 3695, 5700, 4003, 1660, 5860, 2676, 4498, 1170, 5022, 6075, 8124, 6101, 2001, 8000, 4909, 3616, 9571, 4764, 2406, 2190, 8423, 4756, 7965, 501, 346, 8142, 8752, 9240, 3931, 443, 2004, 5307, 4823, 7019, 5942, 6804, 3134, 927, 8143, 6775, 4631, 3326, 9538, 1901, 5269, 159, 3764, 6671, 3191, 8900, 8170, 2113, 1544, 3985, 3534, 884, 9889, 9251, 6555, 8033, 8229, 6128, 5078, 9250, 3944, 2159, 9739, 719, 7485, 6154, 9913, 1816, 9145, 777, 6934, 5987, 5434, 6899, 5353, 5173, 8731, 8412, 9530, 1853, 6548, 3269, 1931, 3821, 6973, 1519, 7978, 2943, 4887, 4355, 2411, 1207, 6871, 2167, 3323, 1819, 82, 6316, 5587, 3144, 1099, 6412, 555, 4335, 3654, 5464, 5359, 6119, 442, 3262, 2182, 8405, 3663, 2561, 7849, 131, 1969, 2551, 4968, 7144, 5979, 9872, 762, 3595, 2912, 4148, 9987, 2054, 2310, 9232, 5983, 2446, 4071, 8315, 6271, 8179, 1545, 8386, 9859, 2307, 9831, 2123, 6925, 6749, 6431, 1942, 267, 4309, 4455, 9460, 4554, 9476, 6508, 8360, 2285, 7571, 5417, 1296, 6494, 5293, 8912, 8272, 8784, 4240, 9279, 4683, 4718, 4021, 8034, 7905, 2982, 5821, 8352, 4985, 1854, 7022, 7741, 5619, 2407, 1885, 5714, 7986, 4881, 716, 2764, 2039, 1236, 3859, 6055, 8550, 81, 360, 5403, 2658, 4521, 9716, 4091, 8907, 6950, 8308, 2313, 9996, 5649, 2128, 8275, 1276, 5399, 4396, 8111, 4454, 7249, 9758, 6186, 3950, 2277, 9255, 378, 9107, 735, 9290, 8095, 1235, 91, 7967, 6410, 7707, 9260, 115, 780, 2173, 3945, 2564, 6617, 6061, 4248, 9984, 3485, 4415, 7838, 2172, 9822, 8548, 5877, 6553, 8500, 308, 5012, 6616, 3146, 1423, 4331, 1737, 3118, 9207, 7710, 5825, 9848, 3076, 8643, 8514, 4786, 6204, 6089, 7481, 8822, 3077, 1308, 4048, 1926, 2014, 4356, 2079, 3204, 1925, 3729, 9128, 4249, 1694, 4527, 852, 7501, 2628, 57, 9382, 9832, 3190, 960, 703, 5767, 3280, 6325, 7686, 2070, 4242, 1535, 8075, 6634, 2873, 9609, 3396, 1886, 4524, 1912, 9733, 827, 5784, 9268, 8130, 1939, 2942, 1977, 53, 6243, 5530, 1947, 9464, 3102, 3878, 3265, 8415, 1435, 1864, 1580, 844, 8508, 806, 8149, 1381, 2376, 3612, 9510, 939, 1080, 6750, 5027, 3867, 1403, 8069, 4787, 4979, 3348, 9546, 1199, 3841, 1478, 7453, 6691, 6771, 4620, 7985, 5673, 1459, 3718, 6813, 8924, 9206, 4835, 9499, 3573, 9841, 6011, 1242, 6863, 4669, 4555, 3810, 6635, 4649, 2020, 1533, 689, 1009, 2315, 7267, 3033, 7755, 4181, 3008, 3256, 6241, 7553, 1532, 4935, 7078, 9413, 3254, 2443, 9377, 4643, 7723, 3487, 2800, 261, 517, 5047, 6476, 29, 3645, 7903, 6913, 6627, 5294, 1683, 2347, 705, 5930, 8369, 2384, 9017, 5314, 1961, 172, 1264, 8569, 3594, 9205, 6501, 9605, 6115, 9097, 8223, 8759, 6846, 3997, 6333, 3799, 7893, 396, 8929, 2651, 4618, 6765, 3238, 8178, 5278, 6582, 711, 4678, 3613, 5087, 958, 2210, 2427, 2181, 1030, 4068, 2420, 2157, 6270, 304, 3680, 3669, 1356, 5437, 6683, 8566, 8963, 2775, 1918, 4977, 701, 1349, 587, 129, 4448, 4255, 1320, 1563, 8166, 7256, 874, 3456, 8258, 6597, 8484, 5086, 5157, 4464, 5066, 7520, 3505, 9159, 6827, 5565, 2134, 3276, 1684, 7472, 6454, 2900, 7264, 6578, 6059, 5648, 910, 9175, 1938, 4503, 7765, 2988, 5174, 3574, 7320, 4742, 4603, 7751, 8782, 439, 7346, 9922, 434, 1275, 9658, 6948, 5335, 2111, 4246, 5804, 1838, 4266, 3831, 1220, 7525, 6709, 8977, 7480, 3524, 5374, 7136, 5546, 9732, 1064, 5707, 607, 4547, 9843, 9811, 2359, 2296, 1994, 1692, 4738, 5349, 7795, 2831, 7062, 8524, 1941, 1630, 3506, 7045, 3136, 249, 4634, 6983, 6430, 6814, 4652, 420, 2270, 1329, 9583, 9541, 8422, 849, 3446, 2228, 8662, 7580, 1292, 8621, 2423, 54, 1060, 7874, 9502, 8606, 7416, 7784, 9637, 5137, 1934, 4565, 2211, 2918, 6852, 4825, 8262, 975, 1283, 5115, 9974, 9657, 1687, 4107, 156, 7804, 962, 8556, 1017, 5263, 4307, 3502, 6845, 8322, 5739, 3922, 9659, 5776, 7997, 8327, 469, 1490, 6970, 6335, 4986, 4808, 4097, 6715, 7298, 3187, 9384, 3936, 6051, 4782, 1161, 7442, 2247, 3633, 2586, 193, 542, 9050, 9743, 8704, 3251, 2519, 627, 1986, 1179, 569, 5120, 7860, 4741, 1046, 8794, 4894, 1166, 5450, 8526, 5455, 4704, 7053, 5628, 8658, 3322, 1672, 4804, 4767, 7496, 4054, 5448, 3311, 7649, 9113, 9715, 460, 8670, 3152, 1671, 2762, 2992, 9642]\n",
            "3000\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWLG2Hmx6gTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "197764d7-0286-4d81-ab05-cd350d6aa007"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "global best_acc\n",
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "print(\"test accuracy \", correct/total , test_loss)\n",
        "test_acc.append(test_loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.4813 252.09153366088867\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}