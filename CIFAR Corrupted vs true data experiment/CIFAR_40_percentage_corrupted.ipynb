{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_40_percentage_corrupted.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olq-MKcdmOnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise + squeeze-excitation'''\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        planes = expansion * in_planes\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, groups=planes, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_planes != out_planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                          stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_planes),\n",
        "            )\n",
        "\n",
        "        # SE layers\n",
        "        self.fc1 = nn.Conv2d(out_planes, out_planes//16, kernel_size=1)\n",
        "        self.fc2 = nn.Conv2d(out_planes//16, out_planes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        shortcut = self.shortcut(x) if self.stride == 1 else out\n",
        "        # Squeeze-Excitation\n",
        "        w = F.avg_pool2d(out, out.size(2))\n",
        "        w = F.relu(self.fc1(w))\n",
        "        w = self.fc2(w).sigmoid()\n",
        "        out = out * w + shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(EfficientNet, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layers = self._make_layers(in_planes=32)\n",
        "        self.linear = nn.Linear(cfg[-1][1], num_classes)\n",
        "\n",
        "    def _make_layers(self, in_planes):\n",
        "        layers = []\n",
        "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            for stride in strides:\n",
        "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
        "                in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layers(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def EfficientNetB0():\n",
        "    # (expansion, out_planes, num_blocks, stride)\n",
        "    cfg = [(1,  16, 1, 2),\n",
        "           (6,  24, 2, 1),\n",
        "           (6,  40, 2, 2),\n",
        "           (6,  80, 3, 2),\n",
        "           (6, 112, 3, 1),\n",
        "           (6, 192, 4, 2),\n",
        "           (6, 320, 1, 2)]\n",
        "    return EfficientNet(cfg)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = EfficientNetB0()\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = net(x)\n",
        "    print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlZDWDZm6ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "    mean.div_(len(dataset))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "\n",
        "# _, term_width = os.popen('stty size', 'r').read().split()\n",
        "term_width = 80\n",
        "term_width = int(term_width)\n",
        "\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "def progress_bar(current, total, msg=None):\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()  # Reset for new bar.\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "\n",
        "    sys.stdout.write(' [')\n",
        "    for i in range(cur_len):\n",
        "        sys.stdout.write('=')\n",
        "    sys.stdout.write('>')\n",
        "    for i in range(rest_len):\n",
        "        sys.stdout.write('.')\n",
        "    sys.stdout.write(']')\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    L = []\n",
        "    L.append('  Step: %s' % format_time(step_time))\n",
        "    L.append(' | Tot: %s' % format_time(tot_time))\n",
        "    if msg:\n",
        "        L.append(' | ' + msg)\n",
        "\n",
        "    msg = ''.join(L)\n",
        "    sys.stdout.write(msg)\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
        "        sys.stdout.write(' ')\n",
        "\n",
        "    # Go back to the center of the bar.\n",
        "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
        "        sys.stdout.write('\\b')\n",
        "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
        "\n",
        "    if current < total-1:\n",
        "        sys.stdout.write('\\r')\n",
        "    else:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def format_time(seconds):\n",
        "    days = int(seconds / 3600/24)\n",
        "    seconds = seconds - days*3600*24\n",
        "    hours = int(seconds / 3600)\n",
        "    seconds = seconds - hours*3600\n",
        "    minutes = int(seconds / 60)\n",
        "    seconds = seconds - minutes*60\n",
        "    secondsf = int(seconds)\n",
        "    seconds = seconds - secondsf\n",
        "    millis = int(seconds*1000)\n",
        "\n",
        "    f = ''\n",
        "    i = 1\n",
        "    if days > 0:\n",
        "        f += str(days) + 'D'\n",
        "        i += 1\n",
        "    if hours > 0 and i <= 2:\n",
        "        f += str(hours) + 'h'\n",
        "        i += 1\n",
        "    if minutes > 0 and i <= 2:\n",
        "        f += str(minutes) + 'm'\n",
        "        i += 1\n",
        "    if secondsf > 0 and i <= 2:\n",
        "        f += str(secondsf) + 's'\n",
        "        i += 1\n",
        "    if millis > 0 and i <= 2:\n",
        "        f += str(millis) + 'ms'\n",
        "        i += 1\n",
        "    if f == '':\n",
        "        f = '0ms'\n",
        "    return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ubk0aCmSD7",
        "colab_type": "code",
        "outputId": "6f165795-ce78-492f-ec09-f36f0fa16bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "# from models import *\n",
        "# from utils import progress_bar\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "# parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "\n",
        "# trainset.targets***************************************************************\n",
        "import random\n",
        "length = len(trainset.targets)\n",
        "percentage_corruption = 40\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "# print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "# print(corrupt_idx)\n",
        "# print(len(corrupt_classes))\n",
        "a = np.array(trainset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "trainset.targets = list(a)\n",
        "#**********************************************************************************\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "# net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "net = EfficientNetB0()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "# if args.resume:\n",
        "#     # Load checkpoint.\n",
        "#     print('==> Resuming from checkpoint..')\n",
        "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "#     net.load_state_dict(checkpoint['net'])\n",
        "#     best_acc = checkpoint['acc']\n",
        "#     start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr, momentum=0.9, weight_decay=5e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train_acc =[]\n",
        "test_acc=[]\n",
        "epoch_list=[]\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"train accuracy \", correct/total ,train_loss)\n",
        "    train_acc.append(train_loss)\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    print(\"test accuracy \", correct/total , test_loss)\n",
        "    test_acc.append(test_loss)\n",
        "\n",
        "    # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+260):\n",
        "    epoch_list.append(epoch)\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:04, 41502863.07it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 0\n",
            "train accuracy  0.21976 853.9208344221115\n",
            "test accuracy  0.4142 172.4899492263794\n",
            "\n",
            "Epoch: 1\n",
            "train accuracy  0.28904 812.3917199373245\n",
            "test accuracy  0.4532 160.35817694664001\n",
            "\n",
            "Epoch: 2\n",
            "train accuracy  0.31078 794.7417081594467\n",
            "test accuracy  0.4979 157.6440750360489\n",
            "\n",
            "Epoch: 3\n",
            "train accuracy  0.3333 781.8654596805573\n",
            "test accuracy  0.5206 148.52984869480133\n",
            "\n",
            "Epoch: 4\n",
            "train accuracy  0.3515 770.0698857307434\n",
            "test accuracy  0.5434 143.1229349374771\n",
            "\n",
            "Epoch: 5\n",
            "train accuracy  0.3665 759.767192363739\n",
            "test accuracy  0.5559 141.44717502593994\n",
            "\n",
            "Epoch: 6\n",
            "train accuracy  0.37808 750.1626145839691\n",
            "test accuracy  0.596 135.73888909816742\n",
            "\n",
            "Epoch: 7\n",
            "train accuracy  0.39094 741.1490622758865\n",
            "test accuracy  0.6126 133.68984496593475\n",
            "\n",
            "Epoch: 8\n",
            "train accuracy  0.40378 733.7595299482346\n",
            "test accuracy  0.6222 131.43842494487762\n",
            "\n",
            "Epoch: 9\n",
            "train accuracy  0.41252 726.2257887125015\n",
            "test accuracy  0.6465 126.4460678100586\n",
            "\n",
            "Epoch: 10\n",
            "train accuracy  0.4235 720.4596339464188\n",
            "test accuracy  0.6408 124.03301966190338\n",
            "\n",
            "Epoch: 11\n",
            "train accuracy  0.42892 714.5618015527725\n",
            "test accuracy  0.6444 122.11992013454437\n",
            "\n",
            "Epoch: 12\n",
            "train accuracy  0.43736 708.5489665269852\n",
            "test accuracy  0.6744 119.90069824457169\n",
            "\n",
            "Epoch: 13\n",
            "train accuracy  0.44388 703.9573857784271\n",
            "test accuracy  0.6839 119.27156913280487\n",
            "\n",
            "Epoch: 14\n",
            "train accuracy  0.4486 698.7051249742508\n",
            "test accuracy  0.684 114.30278587341309\n",
            "\n",
            "Epoch: 15\n",
            "train accuracy  0.45878 693.5376884937286\n",
            "test accuracy  0.6953 112.44843286275864\n",
            "\n",
            "Epoch: 16\n",
            "train accuracy  0.4624 689.5602667331696\n",
            "test accuracy  0.693 113.04691690206528\n",
            "\n",
            "Epoch: 17\n",
            "train accuracy  0.46774 686.587831735611\n",
            "test accuracy  0.7057 112.7706087231636\n",
            "\n",
            "Epoch: 18\n",
            "train accuracy  0.47326 682.9047232866287\n",
            "test accuracy  0.7042 111.11604428291321\n",
            "\n",
            "Epoch: 19\n",
            "train accuracy  0.47662 679.1990803480148\n",
            "test accuracy  0.719 108.26317572593689\n",
            "\n",
            "Epoch: 20\n",
            "train accuracy  0.4794 675.3869558572769\n",
            "test accuracy  0.7182 107.49354612827301\n",
            "\n",
            "Epoch: 21\n",
            "train accuracy  0.4848 671.7898577451706\n",
            "test accuracy  0.7256 107.67380028963089\n",
            "\n",
            "Epoch: 22\n",
            "train accuracy  0.4874 669.5875871181488\n",
            "test accuracy  0.736 103.79584902524948\n",
            "\n",
            "Epoch: 23\n",
            "train accuracy  0.49104 666.9222681522369\n",
            "test accuracy  0.7318 107.23368787765503\n",
            "\n",
            "Epoch: 24\n",
            "train accuracy  0.49388 664.4557538032532\n",
            "test accuracy  0.7433 102.91027116775513\n",
            "\n",
            "Epoch: 25\n",
            "train accuracy  0.49976 660.0494303703308\n",
            "test accuracy  0.7254 105.44414329528809\n",
            "\n",
            "Epoch: 26\n",
            "train accuracy  0.50102 659.1149032115936\n",
            "test accuracy  0.7477 103.00361144542694\n",
            "\n",
            "Epoch: 27\n",
            "train accuracy  0.50276 656.3391704559326\n",
            "test accuracy  0.7476 104.45696824789047\n",
            "\n",
            "Epoch: 28\n",
            "train accuracy  0.50522 654.1352233886719\n",
            "test accuracy  0.7551 98.74354016780853\n",
            "\n",
            "Epoch: 29\n",
            "train accuracy  0.50992 652.0149946212769\n",
            "test accuracy  0.7528 101.9062072634697\n",
            "\n",
            "Epoch: 30\n",
            "train accuracy  0.50736 649.8612713813782\n",
            "test accuracy  0.7607 99.21260643005371\n",
            "\n",
            "Epoch: 31\n",
            "train accuracy  0.5142 646.5961954593658\n",
            "test accuracy  0.7563 100.7947912812233\n",
            "\n",
            "Epoch: 32\n",
            "train accuracy  0.51668 645.0363764762878\n",
            "test accuracy  0.7559 101.14073252677917\n",
            "\n",
            "Epoch: 33\n",
            "train accuracy  0.51882 642.8190233707428\n",
            "test accuracy  0.7572 100.28122234344482\n",
            "\n",
            "Epoch: 34\n",
            "train accuracy  0.51846 641.0959681272507\n",
            "test accuracy  0.7697 96.96220368146896\n",
            "\n",
            "Epoch: 35\n",
            "train accuracy  0.52028 639.2977890968323\n",
            "test accuracy  0.7756 96.25118166208267\n",
            "\n",
            "Epoch: 36\n",
            "train accuracy  0.52374 636.7118209600449\n",
            "test accuracy  0.7484 102.00702732801437\n",
            "\n",
            "Epoch: 37\n",
            "train accuracy  0.52562 635.7873042821884\n",
            "test accuracy  0.7706 97.69757920503616\n",
            "\n",
            "Epoch: 38\n",
            "train accuracy  0.52696 633.6391178369522\n",
            "test accuracy  0.7659 98.47374528646469\n",
            "\n",
            "Epoch: 39\n",
            "train accuracy  0.52788 631.2699882984161\n",
            "test accuracy  0.7725 95.31059050559998\n",
            "\n",
            "Epoch: 40\n",
            "train accuracy  0.53046 629.9924757480621\n",
            "test accuracy  0.7749 92.18824344873428\n",
            "\n",
            "Epoch: 41\n",
            "train accuracy  0.53134 627.8103693723679\n",
            "test accuracy  0.7714 97.60115641355515\n",
            "\n",
            "Epoch: 42\n",
            "train accuracy  0.53316 627.827268242836\n",
            "test accuracy  0.7781 97.07271701097488\n",
            "\n",
            "Epoch: 43\n",
            "train accuracy  0.53362 624.7914446592331\n",
            "test accuracy  0.7631 100.83671426773071\n",
            "\n",
            "Epoch: 44\n",
            "train accuracy  0.53582 623.3429366350174\n",
            "test accuracy  0.7758 93.88610577583313\n",
            "\n",
            "Epoch: 45\n",
            "train accuracy  0.53634 621.6220192909241\n",
            "test accuracy  0.779 94.68628531694412\n",
            "\n",
            "Epoch: 46\n",
            "train accuracy  0.53772 619.8656024932861\n",
            "test accuracy  0.7712 98.37223321199417\n",
            "\n",
            "Epoch: 47\n",
            "train accuracy  0.53776 618.9587675333023\n",
            "test accuracy  0.7742 95.07792866230011\n",
            "\n",
            "Epoch: 48\n",
            "train accuracy  0.53886 618.2054898738861\n",
            "test accuracy  0.7844 92.86845207214355\n",
            "\n",
            "Epoch: 49\n",
            "train accuracy  0.5429 615.689416050911\n",
            "test accuracy  0.7751 93.68491739034653\n",
            "\n",
            "Epoch: 50\n",
            "train accuracy  0.54142 614.694868683815\n",
            "test accuracy  0.7828 93.35202533006668\n",
            "\n",
            "Epoch: 51\n",
            "train accuracy  0.54278 612.0115647315979\n",
            "test accuracy  0.7727 95.03543174266815\n",
            "\n",
            "Epoch: 52\n",
            "train accuracy  0.54436 611.3439327478409\n",
            "test accuracy  0.7849 96.52983570098877\n",
            "\n",
            "Epoch: 53\n",
            "train accuracy  0.54652 609.0315976142883\n",
            "test accuracy  0.7873 95.15401792526245\n",
            "\n",
            "Epoch: 54\n",
            "train accuracy  0.54636 608.5438940525055\n",
            "test accuracy  0.789 93.94369864463806\n",
            "\n",
            "Epoch: 55\n",
            "train accuracy  0.5478 605.5182733535767\n",
            "test accuracy  0.7782 95.484878718853\n",
            "\n",
            "Epoch: 56\n",
            "train accuracy  0.54522 606.4888155460358\n",
            "test accuracy  0.7886 92.43462133407593\n",
            "\n",
            "Epoch: 57\n",
            "train accuracy  0.54958 604.8447340726852\n",
            "test accuracy  0.7867 92.92599594593048\n",
            "\n",
            "Epoch: 58\n",
            "train accuracy  0.55008 603.1185346841812\n",
            "test accuracy  0.7856 92.06247454881668\n",
            "\n",
            "Epoch: 59\n",
            "train accuracy  0.55192 599.8460427522659\n",
            "test accuracy  0.7831 94.03543961048126\n",
            "\n",
            "Epoch: 60\n",
            "train accuracy  0.55222 599.8754706382751\n",
            "test accuracy  0.7757 95.80163007974625\n",
            "\n",
            "Epoch: 61\n",
            "train accuracy  0.5532 598.4136213064194\n",
            "test accuracy  0.7777 95.83457082509995\n",
            "\n",
            "Epoch: 62\n",
            "train accuracy  0.55284 598.0130026340485\n",
            "test accuracy  0.7714 96.33326733112335\n",
            "\n",
            "Epoch: 63\n",
            "train accuracy  0.55224 594.8512824773788\n",
            "test accuracy  0.7675 94.28194040060043\n",
            "\n",
            "Epoch: 64\n",
            "train accuracy  0.5571 592.6908065080643\n",
            "test accuracy  0.7729 95.16277968883514\n",
            "\n",
            "Epoch: 65\n",
            "train accuracy  0.5549 592.5769035816193\n",
            "test accuracy  0.7863 93.35732936859131\n",
            "\n",
            "Epoch: 66\n",
            "train accuracy  0.55594 590.8449466228485\n",
            "test accuracy  0.7471 102.60719031095505\n",
            "\n",
            "Epoch: 67\n",
            "train accuracy  0.5581 589.2733820676804\n",
            "test accuracy  0.7708 93.7291042804718\n",
            "\n",
            "Epoch: 68\n",
            "train accuracy  0.55822 587.2863285541534\n",
            "test accuracy  0.7784 95.27303725481033\n",
            "\n",
            "Epoch: 69\n",
            "train accuracy  0.55926 585.8891942501068\n",
            "test accuracy  0.7824 90.05283236503601\n",
            "\n",
            "Epoch: 70\n",
            "train accuracy  0.55876 585.0247021913528\n",
            "test accuracy  0.769 96.83449453115463\n",
            "\n",
            "Epoch: 71\n",
            "train accuracy  0.55946 584.1610078811646\n",
            "test accuracy  0.7712 96.97729033231735\n",
            "\n",
            "Epoch: 72\n",
            "train accuracy  0.56074 582.5579904317856\n",
            "test accuracy  0.77 94.91563814878464\n",
            "\n",
            "Epoch: 73\n",
            "train accuracy  0.56056 580.79658639431\n",
            "test accuracy  0.7682 97.45169073343277\n",
            "\n",
            "Epoch: 74\n",
            "train accuracy  0.5592 578.3479679822922\n",
            "test accuracy  0.7652 97.60965746641159\n",
            "\n",
            "Epoch: 75\n",
            "train accuracy  0.56106 578.1203266382217\n",
            "test accuracy  0.7695 93.46073800325394\n",
            "\n",
            "Epoch: 76\n",
            "train accuracy  0.56292 575.2796876430511\n",
            "test accuracy  0.769 95.68121498823166\n",
            "\n",
            "Epoch: 77\n",
            "train accuracy  0.56356 573.6571751832962\n",
            "test accuracy  0.7692 96.72316217422485\n",
            "\n",
            "Epoch: 78\n",
            "train accuracy  0.56428 572.6441261768341\n",
            "test accuracy  0.7646 94.47518700361252\n",
            "\n",
            "Epoch: 79\n",
            "train accuracy  0.5651 571.1810655593872\n",
            "test accuracy  0.7534 98.23629552125931\n",
            "\n",
            "Epoch: 80\n",
            "train accuracy  0.56476 570.057367682457\n",
            "test accuracy  0.7612 96.16225618124008\n",
            "\n",
            "Epoch: 81\n",
            "train accuracy  0.56712 565.930322766304\n",
            "test accuracy  0.7713 93.46635895967484\n",
            "\n",
            "Epoch: 82\n",
            "train accuracy  0.56678 566.0979473590851\n",
            "test accuracy  0.7549 102.9340934753418\n",
            "\n",
            "Epoch: 83\n",
            "train accuracy  0.56834 564.4848464727402\n",
            "test accuracy  0.774 90.11536419391632\n",
            "\n",
            "Epoch: 84\n",
            "train accuracy  0.56716 562.8466564416885\n",
            "test accuracy  0.765 95.68901360034943\n",
            "\n",
            "Epoch: 85\n",
            "train accuracy  0.56878 560.7789994478226\n",
            "test accuracy  0.76 94.94196945428848\n",
            "\n",
            "Epoch: 86\n",
            "train accuracy  0.5714 557.1186974048615\n",
            "test accuracy  0.7367 103.54103833436966\n",
            "\n",
            "Epoch: 87\n",
            "train accuracy  0.5708 556.9167486429214\n",
            "test accuracy  0.7562 97.28725242614746\n",
            "\n",
            "Epoch: 88\n",
            "train accuracy  0.57442 554.4534630775452\n",
            "test accuracy  0.7466 100.71378248929977\n",
            "\n",
            "Epoch: 89\n",
            "train accuracy  0.5729 552.3903187513351\n",
            "test accuracy  0.7465 99.21485924720764\n",
            "\n",
            "Epoch: 90\n",
            "train accuracy  0.57162 552.4197220802307\n",
            "test accuracy  0.7626 93.81228339672089\n",
            "\n",
            "Epoch: 91\n",
            "train accuracy  0.57428 548.7529945373535\n",
            "test accuracy  0.7566 95.17546272277832\n",
            "\n",
            "Epoch: 92\n",
            "train accuracy  0.57654 546.889540553093\n",
            "test accuracy  0.7424 99.69083952903748\n",
            "\n",
            "Epoch: 93\n",
            "train accuracy  0.57734 544.5694861412048\n",
            "test accuracy  0.7462 97.51033127307892\n",
            "\n",
            "Epoch: 94\n",
            "train accuracy  0.57574 543.7855427265167\n",
            "test accuracy  0.744 97.66722965240479\n",
            "\n",
            "Epoch: 95\n",
            "train accuracy  0.57988 538.886464715004\n",
            "test accuracy  0.7552 96.30749815702438\n",
            "\n",
            "Epoch: 96\n",
            "train accuracy  0.58224 536.2193267345428\n",
            "test accuracy  0.7201 104.52914929389954\n",
            "\n",
            "Epoch: 97\n",
            "train accuracy  0.5819 535.3774419426918\n",
            "test accuracy  0.724 103.98903161287308\n",
            "\n",
            "Epoch: 98\n",
            "train accuracy  0.58398 531.6562439203262\n",
            "test accuracy  0.7489 97.92522710561752\n",
            "\n",
            "Epoch: 99\n",
            "train accuracy  0.583 531.3004623651505\n",
            "test accuracy  0.7258 103.20288330316544\n",
            "\n",
            "Epoch: 100\n",
            "train accuracy  0.58628 527.9958013296127\n",
            "test accuracy  0.743 98.00906306505203\n",
            "\n",
            "Epoch: 101\n",
            "train accuracy  0.58526 525.8778069019318\n",
            "test accuracy  0.7212 103.79449480772018\n",
            "\n",
            "Epoch: 102\n",
            "train accuracy  0.58844 522.1894985437393\n",
            "test accuracy  0.7219 101.89705616235733\n",
            "\n",
            "Epoch: 103\n",
            "train accuracy  0.58886 520.5786288976669\n",
            "test accuracy  0.7209 104.85642713308334\n",
            "\n",
            "Epoch: 104\n",
            "train accuracy  0.58698 518.2852076292038\n",
            "test accuracy  0.7352 98.74302798509598\n",
            "\n",
            "Epoch: 105\n",
            "train accuracy  0.59048 515.7232773900032\n",
            "test accuracy  0.7187 103.05715984106064\n",
            "\n",
            "Epoch: 106\n",
            "train accuracy  0.59232 511.9296610355377\n",
            "test accuracy  0.714 102.52156782150269\n",
            "\n",
            "Epoch: 107\n",
            "train accuracy  0.59318 510.29536974430084\n",
            "test accuracy  0.6923 111.19049018621445\n",
            "\n",
            "Epoch: 108\n",
            "train accuracy  0.5934 510.15703070163727\n",
            "test accuracy  0.7106 106.52252501249313\n",
            "\n",
            "Epoch: 109\n",
            "train accuracy  0.59534 505.86789590120316\n",
            "test accuracy  0.7234 99.91419887542725\n",
            "\n",
            "Epoch: 110\n",
            "train accuracy  0.59682 504.4280107021332\n",
            "test accuracy  0.6953 107.21846479177475\n",
            "\n",
            "Epoch: 111\n",
            "train accuracy  0.596 502.31759452819824\n",
            "test accuracy  0.7009 105.78023558855057\n",
            "\n",
            "Epoch: 112\n",
            "train accuracy  0.60004 496.5515142083168\n",
            "test accuracy  0.7047 104.51116371154785\n",
            "\n",
            "Epoch: 113\n",
            "train accuracy  0.60236 495.81118351221085\n",
            "test accuracy  0.7284 98.85237169265747\n",
            "\n",
            "Epoch: 114\n",
            "train accuracy  0.60162 493.2895027399063\n",
            "test accuracy  0.6991 106.06509011983871\n",
            "\n",
            "Epoch: 115\n",
            "train accuracy  0.60392 489.8570009469986\n",
            "test accuracy  0.6922 108.4393180012703\n",
            "\n",
            "Epoch: 116\n",
            "train accuracy  0.60616 486.3911890387535\n",
            "test accuracy  0.6739 113.4916319847107\n",
            "\n",
            "Epoch: 117\n",
            "train accuracy  0.60674 484.94508957862854\n",
            "test accuracy  0.6931 105.51389747858047\n",
            "\n",
            "Epoch: 118\n",
            "train accuracy  0.60914 482.06300377845764\n",
            "test accuracy  0.6977 105.74481016397476\n",
            "\n",
            "Epoch: 119\n",
            "train accuracy  0.61004 480.3950864672661\n",
            "test accuracy  0.6743 112.26913332939148\n",
            "\n",
            "Epoch: 120\n",
            "train accuracy  0.61464 475.83261209726334\n",
            "test accuracy  0.672 111.75316387414932\n",
            "\n",
            "Epoch: 121\n",
            "train accuracy  0.61474 472.9729005098343\n",
            "test accuracy  0.7029 105.54393577575684\n",
            "\n",
            "Epoch: 122\n",
            "train accuracy  0.61478 470.65147960186005\n",
            "test accuracy  0.6762 110.06979942321777\n",
            "\n",
            "Epoch: 123\n",
            "train accuracy  0.61968 465.4099877476692\n",
            "test accuracy  0.6936 106.55585825443268\n",
            "\n",
            "Epoch: 124\n",
            "train accuracy  0.62068 463.5555960536003\n",
            "test accuracy  0.6944 106.50667411088943\n",
            "\n",
            "Epoch: 125\n",
            "train accuracy  0.62172 461.0722427368164\n",
            "test accuracy  0.6524 116.0032462477684\n",
            "\n",
            "Epoch: 126\n",
            "train accuracy  0.62464 456.88875699043274\n",
            "test accuracy  0.6714 111.84344220161438\n",
            "\n",
            "Epoch: 127\n",
            "train accuracy  0.62716 452.7994395494461\n",
            "test accuracy  0.6532 115.72163152694702\n",
            "\n",
            "Epoch: 128\n",
            "train accuracy  0.62768 452.2755815386772\n",
            "test accuracy  0.6662 112.28741407394409\n",
            "\n",
            "Epoch: 129\n",
            "train accuracy  0.6294 449.0475147366524\n",
            "test accuracy  0.672 110.49107295274734\n",
            "\n",
            "Epoch: 130\n",
            "train accuracy  0.63266 444.42990642786026\n",
            "test accuracy  0.6411 118.58314365148544\n",
            "\n",
            "Epoch: 131\n",
            "train accuracy  0.63632 439.6202889084816\n",
            "test accuracy  0.6388 120.65688526630402\n",
            "\n",
            "Epoch: 132\n",
            "train accuracy  0.63678 438.3489585518837\n",
            "test accuracy  0.6523 114.9524490237236\n",
            "\n",
            "Epoch: 133\n",
            "train accuracy  0.63798 436.3922460079193\n",
            "test accuracy  0.6688 111.24420529603958\n",
            "\n",
            "Epoch: 134\n",
            "train accuracy  0.64062 432.5693919658661\n",
            "test accuracy  0.6547 115.22478395700455\n",
            "\n",
            "Epoch: 135\n",
            "train accuracy  0.64244 431.12391543388367\n",
            "test accuracy  0.6397 118.5514726638794\n",
            "\n",
            "Epoch: 136\n",
            "train accuracy  0.64358 426.7598443031311\n",
            "test accuracy  0.6515 116.18067020177841\n",
            "\n",
            "Epoch: 137\n",
            "train accuracy  0.64668 422.3796175122261\n",
            "test accuracy  0.6485 116.52197462320328\n",
            "\n",
            "Epoch: 138\n",
            "train accuracy  0.6491 420.0138927102089\n",
            "test accuracy  0.6455 118.36640936136246\n",
            "\n",
            "Epoch: 139\n",
            "train accuracy  0.65172 414.4103183746338\n",
            "test accuracy  0.6253 122.28354507684708\n",
            "\n",
            "Epoch: 140\n",
            "train accuracy  0.65404 413.51658004522324\n",
            "test accuracy  0.639 118.9559296965599\n",
            "\n",
            "Epoch: 141\n",
            "train accuracy  0.65716 409.5079079270363\n",
            "test accuracy  0.6574 115.44539999961853\n",
            "\n",
            "Epoch: 142\n",
            "train accuracy  0.65292 411.1409225463867\n",
            "test accuracy  0.6363 119.91207987070084\n",
            "\n",
            "Epoch: 143\n",
            "train accuracy  0.65858 406.13501822948456\n",
            "test accuracy  0.632 120.82131105661392\n",
            "\n",
            "Epoch: 144\n",
            "train accuracy  0.66262 400.0429603457451\n",
            "test accuracy  0.631 120.81302720308304\n",
            "\n",
            "Epoch: 145\n",
            "train accuracy  0.66066 401.4069480895996\n",
            "test accuracy  0.6249 120.90455257892609\n",
            "\n",
            "Epoch: 146\n",
            "train accuracy  0.66592 396.71636629104614\n",
            "test accuracy  0.6232 124.30667173862457\n",
            "\n",
            "Epoch: 147\n",
            "train accuracy  0.66602 394.0380820631981\n",
            "test accuracy  0.6077 129.28864586353302\n",
            "\n",
            "Epoch: 148\n",
            "train accuracy  0.67062 388.8707119822502\n",
            "test accuracy  0.6189 126.36148327589035\n",
            "\n",
            "Epoch: 149\n",
            "train accuracy  0.67088 388.40329003334045\n",
            "test accuracy  0.6266 122.79492253065109\n",
            "\n",
            "Epoch: 150\n",
            "train accuracy  0.67562 383.2485753297806\n",
            "test accuracy  0.6148 125.62245070934296\n",
            "\n",
            "Epoch: 151\n",
            "train accuracy  0.67976 381.3489083647728\n",
            "test accuracy  0.6445 117.22745472192764\n",
            "\n",
            "Epoch: 152\n",
            "train accuracy  0.67642 380.2713352441788\n",
            "test accuracy  0.6009 128.75109136104584\n",
            "\n",
            "Epoch: 153\n",
            "train accuracy  0.68024 375.54070669412613\n",
            "test accuracy  0.6069 129.51220703125\n",
            "\n",
            "Epoch: 154\n",
            "train accuracy  0.68316 370.4243152141571\n",
            "test accuracy  0.6038 130.66652572155\n",
            "\n",
            "Epoch: 155\n",
            "train accuracy  0.6838 370.78444826602936\n",
            "test accuracy  0.5964 131.58914130926132\n",
            "\n",
            "Epoch: 156\n",
            "train accuracy  0.6861 367.7727366089821\n",
            "test accuracy  0.6298 121.71733868122101\n",
            "\n",
            "Epoch: 157\n",
            "train accuracy  0.68886 364.3604038953781\n",
            "test accuracy  0.59 132.79257559776306\n",
            "\n",
            "Epoch: 158\n",
            "train accuracy  0.68888 362.95920169353485\n",
            "test accuracy  0.606 128.62549686431885\n",
            "\n",
            "Epoch: 159\n",
            "train accuracy  0.69274 360.0065378546715\n",
            "test accuracy  0.5968 132.0509813427925\n",
            "\n",
            "Epoch: 160\n",
            "train accuracy  0.6928 357.6204808950424\n",
            "test accuracy  0.6127 127.20776414871216\n",
            "\n",
            "Epoch: 161\n",
            "train accuracy  0.69912 353.2369443178177\n",
            "test accuracy  0.5858 136.0007786154747\n",
            "\n",
            "Epoch: 162\n",
            "train accuracy  0.69696 354.19532585144043\n",
            "test accuracy  0.575 138.54241621494293\n",
            "\n",
            "Epoch: 163\n",
            "train accuracy  0.697 352.0681366920471\n",
            "test accuracy  0.6059 130.48974066972733\n",
            "\n",
            "Epoch: 164\n",
            "train accuracy  0.69824 350.1397816538811\n",
            "test accuracy  0.5941 134.94009393453598\n",
            "\n",
            "Epoch: 165\n",
            "train accuracy  0.70276 346.35785365104675\n",
            "test accuracy  0.5917 137.17200255393982\n",
            "\n",
            "Epoch: 166\n",
            "train accuracy  0.70844 341.3541186451912\n",
            "test accuracy  0.5823 139.50813269615173\n",
            "\n",
            "Epoch: 167\n",
            "train accuracy  0.70618 342.43828493356705\n",
            "test accuracy  0.6021 130.9874011874199\n",
            "\n",
            "Epoch: 168\n",
            "train accuracy  0.71096 337.3034029006958\n",
            "test accuracy  0.6023 131.1278109550476\n",
            "\n",
            "Epoch: 169\n",
            "train accuracy  0.70964 336.4776540994644\n",
            "test accuracy  0.6173 127.8525208234787\n",
            "\n",
            "Epoch: 170\n",
            "train accuracy  0.71286 334.25668197870255\n",
            "test accuracy  0.6145 127.05691856145859\n",
            "\n",
            "Epoch: 171\n",
            "train accuracy  0.71328 331.7507665157318\n",
            "test accuracy  0.5853 137.22028362751007\n",
            "\n",
            "Epoch: 172\n",
            "train accuracy  0.71444 332.1012232899666\n",
            "test accuracy  0.5837 137.5965782403946\n",
            "\n",
            "Epoch: 173\n",
            "train accuracy  0.71566 328.9694816470146\n",
            "test accuracy  0.6193 127.76608628034592\n",
            "\n",
            "Epoch: 174\n",
            "train accuracy  0.71986 325.2773723602295\n",
            "test accuracy  0.5964 133.39204514026642\n",
            "\n",
            "Epoch: 175\n",
            "train accuracy  0.71924 324.77753978967667\n",
            "test accuracy  0.5999 131.6247067451477\n",
            "\n",
            "Epoch: 176\n",
            "train accuracy  0.72056 323.8643615245819\n",
            "test accuracy  0.5831 138.52692556381226\n",
            "\n",
            "Epoch: 177\n",
            "train accuracy  0.72272 321.20606404542923\n",
            "test accuracy  0.5953 135.1400886774063\n",
            "\n",
            "Epoch: 178\n",
            "train accuracy  0.72764 317.5309172272682\n",
            "test accuracy  0.5836 139.61386460065842\n",
            "\n",
            "Epoch: 179\n",
            "train accuracy  0.72392 317.0002208352089\n",
            "test accuracy  0.5837 138.58440327644348\n",
            "\n",
            "Epoch: 180\n",
            "train accuracy  0.72884 313.29828602075577\n",
            "test accuracy  0.5797 141.3975431919098\n",
            "\n",
            "Epoch: 181\n",
            "train accuracy  0.72474 316.57620280981064\n",
            "test accuracy  0.5789 138.01277577877045\n",
            "\n",
            "Epoch: 182\n",
            "train accuracy  0.73128 311.470558822155\n",
            "test accuracy  0.5873 136.8025177717209\n",
            "\n",
            "Epoch: 183\n",
            "train accuracy  0.73152 309.33190882205963\n",
            "test accuracy  0.5814 140.7732800245285\n",
            "\n",
            "Epoch: 184\n",
            "train accuracy  0.72978 309.98960596323013\n",
            "test accuracy  0.5952 134.58168411254883\n",
            "\n",
            "Epoch: 185\n",
            "train accuracy  0.73478 306.2928252518177\n",
            "test accuracy  0.5988 135.74434208869934\n",
            "\n",
            "Epoch: 186\n",
            "train accuracy  0.73786 302.205172508955\n",
            "test accuracy  0.5866 139.1727579832077\n",
            "\n",
            "Epoch: 187\n",
            "train accuracy  0.73914 300.5899181365967\n",
            "test accuracy  0.6017 133.7184695005417\n",
            "\n",
            "Epoch: 188\n",
            "train accuracy  0.73924 301.00298327207565\n",
            "test accuracy  0.571 144.72115850448608\n",
            "\n",
            "Epoch: 189\n",
            "train accuracy  0.7346 305.59332260489464\n",
            "test accuracy  0.5838 138.6640272140503\n",
            "\n",
            "Epoch: 190\n",
            "train accuracy  0.74048 299.94259026646614\n",
            "test accuracy  0.5703 145.62080872058868\n",
            "\n",
            "Epoch: 191\n",
            "train accuracy  0.74194 297.291424959898\n",
            "test accuracy  0.5774 140.52827942371368\n",
            "\n",
            "Epoch: 192\n",
            "train accuracy  0.74206 298.75662702322006\n",
            "test accuracy  0.5772 141.5677855014801\n",
            "\n",
            "Epoch: 193\n",
            "train accuracy  0.74054 296.56365129351616\n",
            "test accuracy  0.5634 147.15365278720856\n",
            "\n",
            "Epoch: 194\n",
            "train accuracy  0.74114 295.5448566675186\n",
            "test accuracy  0.5607 149.77166628837585\n",
            "\n",
            "Epoch: 195\n",
            "train accuracy  0.74712 293.22048860788345\n",
            "test accuracy  0.5685 147.15246522426605\n",
            "\n",
            "Epoch: 196\n",
            "train accuracy  0.7457 291.0577666461468\n",
            "test accuracy  0.5804 144.83754348754883\n",
            "\n",
            "Epoch: 197\n",
            "train accuracy  0.74834 289.08630415797234\n",
            "test accuracy  0.5641 148.1086933016777\n",
            "\n",
            "Epoch: 198\n",
            "train accuracy  0.7472 289.48972991108894\n",
            "test accuracy  0.5878 140.19658708572388\n",
            "\n",
            "Epoch: 199\n",
            "train accuracy  0.75264 285.54091078042984\n",
            "test accuracy  0.5734 145.23984968662262\n",
            "\n",
            "Epoch: 200\n",
            "train accuracy  0.75022 288.6093575656414\n",
            "test accuracy  0.5826 143.76802623271942\n",
            "\n",
            "Epoch: 201\n",
            "train accuracy  0.75194 282.1250945031643\n",
            "test accuracy  0.6044 135.55310821533203\n",
            "\n",
            "Epoch: 202\n",
            "train accuracy  0.75226 282.52603283524513\n",
            "test accuracy  0.5751 146.55617260932922\n",
            "\n",
            "Epoch: 203\n",
            "train accuracy  0.75424 282.62950924038887\n",
            "test accuracy  0.57 146.4538105726242\n",
            "\n",
            "Epoch: 204\n",
            "train accuracy  0.75912 277.2100004553795\n",
            "test accuracy  0.5916 141.08218777179718\n",
            "\n",
            "Epoch: 205\n",
            "train accuracy  0.75766 278.9562584757805\n",
            "test accuracy  0.5833 145.3252317905426\n",
            "\n",
            "Epoch: 206\n",
            "train accuracy  0.76012 275.1800280511379\n",
            "test accuracy  0.5704 144.16087198257446\n",
            "\n",
            "Epoch: 207\n",
            "train accuracy  0.75976 276.1851913034916\n",
            "test accuracy  0.5753 143.7592146396637\n",
            "\n",
            "Epoch: 208\n",
            "train accuracy  0.76198 275.8861089646816\n",
            "test accuracy  0.5764 144.02551794052124\n",
            "\n",
            "Epoch: 209\n",
            "train accuracy  0.76092 275.0827829539776\n",
            "test accuracy  0.574 144.27276611328125\n",
            "\n",
            "Epoch: 210\n",
            "train accuracy  0.75814 276.3814851343632\n",
            "test accuracy  0.5766 145.07356691360474\n",
            "\n",
            "Epoch: 211\n",
            "train accuracy  0.75932 274.59425845742226\n",
            "test accuracy  0.5864 140.3183616399765\n",
            "\n",
            "Epoch: 212\n",
            "train accuracy  0.7623 272.18122881650925\n",
            "test accuracy  0.5601 155.5581020116806\n",
            "\n",
            "Epoch: 213\n",
            "train accuracy  0.76636 267.83188593387604\n",
            "test accuracy  0.5443 157.74770593643188\n",
            "\n",
            "Epoch: 214\n",
            "train accuracy  0.76492 270.2853076159954\n",
            "test accuracy  0.5733 144.533500790596\n",
            "\n",
            "Epoch: 215\n",
            "train accuracy  0.76852 266.05752620100975\n",
            "test accuracy  0.5828 145.1688780784607\n",
            "\n",
            "Epoch: 216\n",
            "train accuracy  0.77086 264.0125240981579\n",
            "test accuracy  0.5588 154.3526360988617\n",
            "\n",
            "Epoch: 217\n",
            "train accuracy  0.7678 264.3337843120098\n",
            "test accuracy  0.5633 149.75632786750793\n",
            "\n",
            "Epoch: 218\n",
            "train accuracy  0.76872 265.6364147365093\n",
            "test accuracy  0.5801 144.51234340667725\n",
            "\n",
            "Epoch: 219\n",
            "train accuracy  0.76652 264.72920358181\n",
            "test accuracy  0.5713 148.0058138370514\n",
            "\n",
            "Epoch: 220\n",
            "train accuracy  0.77032 262.0930009186268\n",
            "test accuracy  0.5662 148.08996558189392\n",
            "\n",
            "Epoch: 221\n",
            "train accuracy  0.77162 263.2467995584011\n",
            "test accuracy  0.5589 153.56829357147217\n",
            "\n",
            "Epoch: 222\n",
            "train accuracy  0.76808 265.4724227488041\n",
            "test accuracy  0.578 144.52827870845795\n",
            "\n",
            "Epoch: 223\n",
            "train accuracy  0.77442 258.36920443177223\n",
            "test accuracy  0.571 145.27314162254333\n",
            "\n",
            "Epoch: 224\n",
            "train accuracy  0.77126 263.3439430594444\n",
            "test accuracy  0.5765 144.70452439785004\n",
            "\n",
            "Epoch: 225\n",
            "train accuracy  0.77334 259.2183323800564\n",
            "test accuracy  0.558 154.77935671806335\n",
            "\n",
            "Epoch: 226\n",
            "train accuracy  0.77236 262.5189263522625\n",
            "test accuracy  0.5922 138.84607553482056\n",
            "\n",
            "Epoch: 227\n",
            "train accuracy  0.77706 257.17860662937164\n",
            "test accuracy  0.577 148.17214554548264\n",
            "\n",
            "Epoch: 228\n",
            "train accuracy  0.77128 261.7117265164852\n",
            "test accuracy  0.5761 147.27245271205902\n",
            "\n",
            "Epoch: 229\n",
            "train accuracy  0.77012 261.3285692036152\n",
            "test accuracy  0.5538 155.15781152248383\n",
            "\n",
            "Epoch: 230\n",
            "train accuracy  0.7742 257.92126056551933\n",
            "test accuracy  0.555 155.61695659160614\n",
            "\n",
            "Epoch: 231\n",
            "train accuracy  0.77378 257.90492993593216\n",
            "test accuracy  0.5839 145.20143789052963\n",
            "\n",
            "Epoch: 232\n",
            "train accuracy  0.7766 255.7475364804268\n",
            "test accuracy  0.581 143.4179140329361\n",
            "\n",
            "Epoch: 233\n",
            "train accuracy  0.77742 254.2082343995571\n",
            "test accuracy  0.5817 144.69837057590485\n",
            "\n",
            "Epoch: 234\n",
            "train accuracy  0.78042 252.60070884227753\n",
            "test accuracy  0.5709 147.27646601200104\n",
            "\n",
            "Epoch: 235\n",
            "train accuracy  0.77788 255.00076073408127\n",
            "test accuracy  0.5778 147.25966054201126\n",
            "\n",
            "Epoch: 236\n",
            "train accuracy  0.7751 255.55797761678696\n",
            "test accuracy  0.5599 151.5057282447815\n",
            "\n",
            "Epoch: 237\n",
            "train accuracy  0.78344 248.55700996518135\n",
            "test accuracy  0.5675 152.06347489356995\n",
            "\n",
            "Epoch: 238\n",
            "train accuracy  0.77862 252.00241592526436\n",
            "test accuracy  0.5722 150.46482503414154\n",
            "\n",
            "Epoch: 239\n",
            "train accuracy  0.77986 253.76631870865822\n",
            "test accuracy  0.574 148.41076958179474\n",
            "\n",
            "Epoch: 240\n",
            "train accuracy  0.78304 251.49955087900162\n",
            "test accuracy  0.5498 159.20672965049744\n",
            "\n",
            "Epoch: 241\n",
            "train accuracy  0.77924 252.9506110548973\n",
            "test accuracy  0.5871 145.6095587015152\n",
            "\n",
            "Epoch: 242\n",
            "train accuracy  0.77954 252.26452511548996\n",
            "test accuracy  0.5626 148.60945963859558\n",
            "\n",
            "Epoch: 243\n",
            "train accuracy  0.78098 252.1709062755108\n",
            "test accuracy  0.562 152.98373007774353\n",
            "\n",
            "Epoch: 244\n",
            "train accuracy  0.78256 248.88437923789024\n",
            "test accuracy  0.5546 155.73638200759888\n",
            "\n",
            "Epoch: 245\n",
            "train accuracy  0.78104 251.29700192809105\n",
            "test accuracy  0.5489 156.25462436676025\n",
            "\n",
            "Epoch: 246\n",
            "train accuracy  0.78284 247.61367163062096\n",
            "test accuracy  0.5451 161.61252903938293\n",
            "\n",
            "Epoch: 247\n",
            "train accuracy  0.78218 248.63951030373573\n",
            "test accuracy  0.5478 158.73098003864288\n",
            "\n",
            "Epoch: 248\n",
            "train accuracy  0.78204 248.5504693686962\n",
            "test accuracy  0.5859 144.3254873752594\n",
            "\n",
            "Epoch: 249\n",
            "train accuracy  0.78098 250.16830596327782\n",
            "test accuracy  0.5506 156.11933875083923\n",
            "\n",
            "Epoch: 250\n",
            "train accuracy  0.7845 247.54055333137512\n",
            "test accuracy  0.5703 148.02301144599915\n",
            "\n",
            "Epoch: 251\n",
            "train accuracy  0.78336 248.17070174217224\n",
            "test accuracy  0.5835 144.66347587108612\n",
            "\n",
            "Epoch: 252\n",
            "train accuracy  0.7817 250.86324280500412\n",
            "test accuracy  0.59 140.09388238191605\n",
            "\n",
            "Epoch: 253\n",
            "train accuracy  0.78436 248.92637041211128\n",
            "test accuracy  0.5936 141.75331896543503\n",
            "\n",
            "Epoch: 254\n",
            "train accuracy  0.78298 249.3222179710865\n",
            "test accuracy  0.5705 148.90553331375122\n",
            "\n",
            "Epoch: 255\n",
            "train accuracy  0.7843 245.64574548602104\n",
            "test accuracy  0.5694 149.13081085681915\n",
            "\n",
            "Epoch: 256\n",
            "train accuracy  0.78728 244.91093748807907\n",
            "test accuracy  0.5787 147.1290796995163\n",
            "\n",
            "Epoch: 257\n",
            "train accuracy  0.78368 248.41036251187325\n",
            "test accuracy  0.5684 152.13389313220978\n",
            "\n",
            "Epoch: 258\n",
            "train accuracy  0.7864 245.78947123885155\n",
            "test accuracy  0.5876 144.98220098018646\n",
            "\n",
            "Epoch: 259\n",
            "train accuracy  0.7846 247.8261053264141\n",
            "test accuracy  0.5596 153.58048176765442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMfVvXNunQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a5845abc-3685-49ea-9448-7da188c86d37"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "plt.plot(epoch_list, train_acc, 'g--' , label='train_acc')\n",
        "plt.plot(epoch_list, test_acc, 'bo--', label = 'test_acc')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VOX1wPHvyZ4QSEKCAdkVRAkQ\nIMgiKgnI6gKtilhUcIsCohaxYrG2tfqrVqxbJWoF2Yu4IFRFQAguFZBFQHYCsgQQJRJCCGEJ7++P\nuQlDyDJJZnIzM+fzPPeZe9+7zDkzycnNezcxxqCUUsp3BdgdgFJKKc/SQq+UUj5OC71SSvk4LfRK\nKeXjtNArpZSP00KvlFI+Tgu9Ukr5OC30Sinl47TQK6WUjwuyOwCAuLg406xZs0qte/z4cWrVquXe\ngGoozdU3+Uuu/pInVF+ua9asOWyMqVfecjWi0Ddr1ozVq1dXat1ly5aRnJzs3oBqKM3VN/lLrv6S\nJ1RfriKyx5XltOtGKaV8nBZ6pZTycVrolVLKx9WIPnqllO86ffo0mZmZREVFsWXLFrvDqRbuzjUs\nLIxGjRoRHBxcqfW10CulPCozM5PatWsTGxtLnTp17A6nWhw7dozatWu7ZVvGGLKyssjMzKR58+aV\n2oZ23SilPCo/P5/Y2FhExO5QvJKIEBsbS35+fqW3oYVeKeVxWuSrpqqfnxZ6pZTycV5d6D/Z/gkj\n147k1xO/2h2KUkrVWF5d6PPP5LPl2Bb2Hd1ndyhKqRosOzubiRMnVni9AQMGkJ2d7YGIqpdXF/rG\ndRoDkJmTaXMkSqmarLRCf+bMmTLX++yzz4iOjvZUWNXGq0+vbFSnEaCFXilvkjwl+YK2wQmDGXnl\nSPJO5zFg5oAL5g9vP5zh7YdzOO8wt8y55bx5y4YvK/c9x40bx86dO2nfvj3BwcGEhYURExPD1q1b\n2b59O4MGDWLfvn3k5+fzyCOPkJqaCpy7D1dubi79+/fn6quv5ttvv6Vhw4bMmzeP8PDwEt9vypQp\nTJs2jVOnTtGiRQumT59OREQEhw4d4sEHH2TXrl0ApKWlcdVVVzFt2jQmTJiAiNCuXTumT59ebk4V\n4dV79PUj6xNAgBZ6pVSZnn/+eS699FLWrVvHiy++yNq1a3n11VfZvn07AJMnT2bNmjWsXr2a1157\njaysrAu2sWPHDkaNGsWmTZuIjo7mww8/LPX9brzxRlatWsX69eu54oormDRpEgAPP/wwPXr0YP36\n9axdu5aEhAQ2bdrEs88+y9KlS1m/fj2vvvqq2/P36j36wIBAkmKSiIuIszsUpZSLytoDjwiOKHN+\nXEScS3vw5encufN5Fx+99tprzJ07F4B9+/axY8cOYmNjz1unefPmtG/fHoCkpCR2795d6va3bNnC\nnXfeSXZ2Nrm5ufTt2xeApUuXMm3aNAACAwOJiopi2rRp3HrrrcTFOepY3bp1q5xfcV5d6AH+0e4f\nJHdNtjsMpZQXcb5X/LJly/jiiy9Yvnw5ERERJCcnl3hxUmhoaNF4YGAgJ06cKHX7I0aMYN68eSQm\nJjJlyhSWLVvm1vgryqu7bpRSyhW1a9fm2LFjJc47evQoMTExREREsHXrVlasWFHl9zt27BgNGjTg\n9OnTzJw5s6i9V69epKWlAVBQUMDRo0fp2bMn77//flF30a+/uv90ca8v9HP2zaHVv1phjLE7FKVU\nDRUbG0v37t1p06YNjz/++Hnz+vXrx5kzZ7jiiisYN24cXbt2rfL7PfXUU3Tp0oXu3btz+eWXF7W/\n+uqrpKen07ZtW5KSkti8eTMJCQmMHz+eHj16kJiYyJgxY6r8/sV5fdeNwbA9azs5J3OICouyOxyl\nVA01a9asEttDQ0NZsGBBifMK++Hj4uLYuHFjUfvYsWPLfK/77ruP3//+9xe0x8fHM2/evAvahw0b\nxrBhw8rcZlV4/R59vVDH4xL35ehFU0opVRKXCr2I/F5ENonIRhH5j4iEiUhzEVkpIhki8p6IhFjL\nhlrTGdb8Zp5MoFmEY/NrDqzx5NsopdQFRo0aRfv27c8b3n33XbvDukC5XTci0hB4GGhtjDkhInOA\nIcAA4GVjzGwReRO4F0izXo8YY1qIyBDgBeA2TyXQrFYz4iLiWLp7KcPae+5fH6WUKu6NN94osb20\nA792cbXrJggIF5EgIAI4CPQEPrDmTwUGWeMDrWms+b3Eg/coDZAAHrryITo16OSpt1BKKa9W7h69\nMWa/iEwA9gIngEXAGiDbGFN4o4hMoKE13hDYZ617RkSOArHAYeftikgqkAqOAxSVPc80NzeXHpE9\n4AS2n6vqabm5uT6fYyHN1XdERUVx7NgxCgoKatyerqd4Itf8/PxK/5y40nUTg2MvvTmQDbwP9KvU\nuzkxxrwNvA3QqVMnk5ycXKntLFu2jOTkZI6fOs7+Y/u5LPayqoZWYxXm6g80V9+xZcuWovPY3fV4\nvZrOE7mGhYXRoUOHSq3rStfNdcCPxphfjDGngY+A7kC01ZUD0AjYb43vBxoDWPOjgAtvHOFmfWf0\n5Y6P7vD02yilvFBlb1MM8Morr5CXl+fmiKqXK4V+L9BVRCKsvvZewGYgHSi8jdwwoPDk0PnWNNb8\npaYarma6+YqbWXVgFVt+8Y+nzCvlq2bOhGbNICDA8ep0YWmlaaEvhzFmJY6DqmuBH6x13gaeAMaI\nSAaOPvhJ1iqTgFirfQwwzgNxX+D2trcTKIFMWz+tOt5OKeUBM2dCairs2QPGOF5TU6te7J1vU/z4\n44/z4osvcuWVV9KuXTv+/Oc/A3D8+HGuv/56EhMTadOmDe+99x6vvfYaBw4cICUlhZSUlFK3P2LE\nCDp16kRCQkLR9gBWrVrFVVddRWJiIp07dy46VjF27FjatGlDu3bteP3116uWnCuMMbYPSUlJprLS\n09OLxgfMHGAa/bOROVNwptLbq8mcc/V1mqvv2Lx5szHGmJycHGOMMT16XDi88YZj2caNjXGU+POH\n2FjH/F9+uXBdV/z4448mISHBGGPMwoULzf3332/Onj1rCgoKzPXXX2++/PJL88EHH5j77ruvaJ3s\n7GxjjDFNmzY1v/zyS5nbz8rKMsYYc+bMGdOjRw/z7bffmpMnT5rmzZub7777zhhjzNGjR83p06fN\nxIkTzc0332xOnz593rrlKfwcnQGrjQs11uuvjHV2V7u7yMzJZHnmcrtDUUpVQmYpj5Yo4fbwlbZo\n0SIWLVpEhw4d6NixI1u3bmXHjh20bduWxYsX88QTT/D1118TFeX6LVXmzJlDx44d6dChA5s2bWLr\n1q1s27aNBg0acOWVVwJQp04dgoKC+OKLL3jggQcICnIc4vTEbYmL8/p73Ti7qdVNrHtgHYn1E+0O\nRSlVirLOEGzSxNFdU1zTpo7XuLiy13eFMYYnn3ySBx544IJ5a9eu5bPPPuOpp56iV69ePP300+Vu\n78cff2TChAmsWrWKmJgYhg8fzsmTJ6sWpJv51B59eHC4FnmlvNhzz0FExPltERGO9qpwvk1x3759\nmTx5Mrm5uQDs37+fn3/+mQMHDhAREcEdd9zB448/ztq1ay9YtyQ5OTnUqlWLqKgoDh06VHSDtFat\nWnHw4EFWrVoFOE65PHPmDL179+att94qel6tJ25LXJxP7dEDnDVnSf1vKhfXvphnUp6xOxylVAUM\nHep4HT8e9u517OE/99y59spyvk1x//79+d3vfke3bt0AiIyMZMaMGWRkZPD4448TEBBAcHBw0X3j\nU1NT6devHxdffDHp6ekXbDsxMZEOHTpw+eWX07hxY7p37w5ASEgI7733HqNHj+bEiROEh4fzxRdf\ncN9997F9+3batWtHcHAw999/Pw899FDVEiyPKx35nh7cdTC20C1zbjFRf48yOfk5ld5uTeTrB+2c\naa6+o/jBWH/giVz1YGwxj1/1OEdPHuWdte/YHYpSStnO57puADo37My1Ta/l5RUv81DnhwgODLY7\nJKWUD+jSpcsFB1qnT59O27ZtbYrINT5Z6AGe6P4E18+6nmnrp3Fvx3vtDkcp5QNWrlxpdwiV4pNd\nNwD9W/Tn/3r+H30u7WN3KEr5PaPPdK6Sqn5+PrtHLyI8ec2TdoehlN8LCwsjKyuLkJAQu0PxSsYY\nsrKyCAsLq/Q2fLbQF9p2eBuPfP4Ik26aRMM6DctfQSnlVo0aNSIzM5Ps7OwqFStvkp+f79Zcw8LC\naNSoUaXX9/lCHxwYzJd7vuSxRY8x+5bZdoejlN8JDg6mefPmLFu2rNL3U/c2NS1Xn+2jL3RJzCX8\n8eo/8t6m91i8c7Hd4SilVLXz+UIP8Hj3x7ks9jKGzxvOz8d/tjscpZSqVn5R6MOCwphzyxx+PfEr\nz3ypt0VQSvkXn++jL5RYP5HFdy4mqUGS3aEopVS1KnePXkRaicg6pyFHRB4VkboislhEdlivMdby\nIiKviUiGiGwQkY6eT8M1Vze5mvDgcHJO5rAy0zsvfFBKqYpy5VGC24wx7Y0x7YEkIA+Yi+MRgUuM\nMS2BJZx7ZGB/oKU1pAJpngi8Ku6bfx/9Zvbjh0M/2B2KUkp5XEX76HsBO40xe4CBwFSrfSowyBof\nCEyzbq62AogWkQZuidZNXrjuBSKCI+g5rSdbD2+1OxyllPKoihb6IcB/rPF4Y8xBa/wnIN4abwjs\nc1on02qrMZrHNCd9WDoBEkD/mf05lHvI7pCUUspjxNV7KIhICHAASDDGHBKRbGNMtNP8I8aYGBH5\nBHjeGPON1b4EeMIYs7rY9lJxdO0QHx+fNHt25S5mys3NJTIyslLrbs3ZyqPrH6Vz3c48k1Dzz8ap\nSq7eRnP1Pf6SJ1RfrikpKWuMMZ3KXdCVm9ZbfwwGAoucprcBDazxBsA2a/wt4PaSlittcPeDRypi\n8c7FJvNoZpW2UV18/QEVzjRX3+MveRpTfbnigQeP3M65bhuA+cAwa3wYMM+p/S7r7JuuwFFzroun\nxrnukutoWKchBWcLeGzhY3o2jlLK57hU6EWkFtAb+Mip+Xmgt4jsAK6zpgE+A3YBGcC/gZFui9aD\ndh7ZyZzNc+g+uTv/+u5feltVpZTPcOmCKWPMcSC2WFsWjrNwii9rgFFuia4aXRZ7GRtHbOTOuXcy\nesFo1v20jjcGvEFoUKjdoSmlVJX4xS0QXBUVFsXHQz5m/DXjmfT9JH730e/sDkkpparMb26B4KoA\nCeDZns/SLr4djeo47v981pwlQPRvolLKO2mhL8XghMFF448tfIwDuQd4NuVZWsa2tDEqpZSqON1N\ndUFsRCyfbP+E1hNb89BnD3H81HG7Q1JKKZdpoXfBU9c+xc6Hd3J/x/tJW51G3xl9yTmZY3dYSinl\nEi30LqofWZ+J109k1m9nserAKgQBIDs/2+bIlFKqbFroK+i2Nrex+5Hd1A6tjTGGHlN60GNKDz7P\n+FzPvVdK1Uha6CuhQW3HzTgLTAH3driXXUd20X9mf5LeTuK7/d/ZHJ1SSp1PC30VBAUE8XCXh9n5\n8E4m3TSJw3mH6T65O+t/Wm93aEopVUQLvRuEBIZwT4d72DBiAw93fpjW9VoDMPn7ybz7/bvknsq1\nOUKllD/TQu9G0WHRvNT3JYIDgwGY9cMs7pl/Dw3/2ZC/f/13dh3Zpf34Sqlqp4XegxbfuZj/3fM/\nkpsl88elf+TS1y5l3Bfjyl9RKaXcSK+M9SAR4arGVzFvyDw2HNpA+o/ppDRPAWDV/lXM2zaPUVeO\nKjq4q5RSnqB79NWkXXw7Hun6CO3i2wGw5uAa/v7N37n4nxcT9EwQoz4dpefkK6U8QvfobfJgpwfp\nfUlvZm+czc4jO0lbncbyzOWsfWAt4Hjyl4jYHKVSyhdoobfRpXUvZfy14wEY3Xk0v+T9AsCJ0ydo\nPbE1zaKb0T6+PcPbDyexfqKdoSqlvJh23dQQHRp0oM+lfQD49cSv9L6kN3mn83hzzZt0eKsDPaf2\n5Ltf9WIspVTFubRHLyLRwDtAG8AA9+B46Pd7QDNgNzDYGHNEHP0NrwIDgDxguDFmrdsj92EN6zTk\n7RvfBuDIiSO8tPwlFu9aTJOIJgAs37ecY6eO0b1xd2qF1LIzVKWUF3B1j/5V4HNjzOVAIrAFGAcs\nMca0BJZY0wD9gZbWkAqkuTViPxMTHsOzPZ9l5X0rqR9WH4B/fPsP+s7oS+TfI7ntg9uYtn4a87bO\nK2dLSil/VW6hF5Eo4FpgEoAx5pQxJhsYCEy1FpsKDLLGBwLTjMMKIFpE9PxBN5p982zeu+U9xnYb\ny9wtcxn28TC+2ftN0Xw9e0cp5cyVrpvmwC/AuyKSCKwBHgHijTEHrWV+AuKt8YbAPqf1M622gyi3\nCA0KZXDCYAYnDObhLg9z9ORRmkc3B2DOpjk8+MmDJFyUQLuL2nF1k6tpX789V9S7wuaolVJ2kfIu\nyReRTsAKoLsxZqWIvArkAKONMdFOyx0xxsSIyCfA88aYb6z2JcATxpjVxbabiqNrh/j4+KTZs2dX\nKoHc3FwiIyMrta63cSXXvXl7mfTjJLJPZ7Pt2DZOnj1JkAQxq8ss6oXWq6ZIq06/V9/jL3lC9eWa\nkpKyxhjTqdwFjTFlDkB9YLfT9DXApzgOxjaw2hoA26zxt4DbnZYvWq60ISkpyVRWenp6pdf1NhXN\nNe9Untl4aKP5YNMHRW2JaYlm2NxhZumupabgbIGbI3Qf/V59j7/kaUz15QqsNuXUcGNM+X30xpif\ngH0i0spq6gVsBuYDw6y2YUDh0cD5wF3i0BU4as518ahqFB4cTsJFCdzc+uaitsT6iczdOpee03rS\n7JVmjF8ynh+P/GhjlEopT3P1gqnRwEwRCQF2AXfjOJA7R0TuBfYAg61lP8NxamUGjtMr73ZrxKpK\npg6ayonTJ5i3bR7T1k/j+f89T7v4djSPac6GQxtYsGMBd7S7g4Z1GtodqlLKTVwq9MaYdUBJ/UC9\nSljWAKOqGJfyoPDgcIa0GcKQNkP4KfcnosMch1p+OPQD45aM48klT3JjqxsZ220sLeq20JuuKeXl\n9BYIfq5+ZP2i8aHthtKlURemrpvKv1b9i/nb5lMntA57H91LVFiUjVEqpapCC706T4u6Lfhbz78x\nptsYlu1eRnRYNFFhURhj6D65O+3i29GvRT96Ne9F7dDadoerlHKBFnpVopjwGH5zxW+KpnNO5hAb\nEcusH2bx1pq3iAqN4qPbPqJn8542RqmUcoXe1Ey5JCosiv/e/l8O/+EwS+9aSqM6jeg1rRenC04D\nsOWXLWTlZdkcpVKqJFroVYWEBIaQ0jyFr+/+mr/3+jsGxwV3Iz8bSeOXGzN20VhOnjlpc5RKKWda\n6FWlxITHMO7qcYQEhgDwct+XGZwwmJeWv0Snf3firrl38d1+va2yUjWBFnrlFu3rt2fKoCl8cOsH\nhAWF8dmOzzhy4ggAX+7+koUZC22OUCn/pYVeudXNrW9m1f2ryByTWfQglUU7F9FvZj9+//nvtR9f\nKRtooVceERYUVvTM2z/1+BMjO43klZWv0OSVJoz+bDRrDqyxOUKl/IcWeuVxYUFhvHH9G2wcsZHB\nCYN5a81bLP1xqd1hKeU39Dx6VW0SLkrg3YHv8nr/1yk4WwDArB9mEV8rni6NuhAZ4h+3sFWqummh\nV9WusKCfLjjNU0uf4sfsH6kdUpspg6YwoOUAwoLCbI5QKd+iXTfKNsGBwax7cB3/vf2/tIprxc1z\nbqbW/9Vi1g+z7A5NKZ+ie/TKVnVC63DDZTfQq3kv5myaw8afN3LjZTeyZvkasvOziQqNKjqoq5Sq\nHC30qkYIDw5nWPthRdPHzxynyztd6NigIw93fphujbvZGJ1S3k27blSNFBYYxm0JtzFv6zyumnwV\nfWf0Ze3BtXaHpZRX0kKvaqRACeSZlGc4NPYQL/V5idUHVpP0dhLpP6bbHZpSXselQi8iu0XkBxFZ\nJyKrrba6IrJYRHZYrzFWu4jIayKSISIbRKSjJxNQvq12aG3GdBvDrod38c6N75DcLBmAp9Of5s3V\nbxY+gF4pVYaK7NGnGGPaG2MKHyk4DlhijGkJLLGmAfoDLa0hFUhzV7DKf0WFRXFvx3uLDsz+Z+N/\nGPHpCO6edze7s3fbG5xSNVxVum4GAlOt8anAIKf2acZhBRAtIvrQUeVW2x/azp+u/RNT10+l+avN\nGTBzABm/ZtgdllI1kquF3gCLRGSNiKRabfHGmIPW+E9AvDXeENjntG6m1aaU24gIz6Q8Q8boDP6a\n/FcMhlrBtewOS6kaSVzp4xSRhsaY/SJyEbAYGA3MN8ZEOy1zxBgTIyKfAM8bY76x2pcATxhjVhfb\nZiqOrh3i4+OTZs+eXakEcnNziYz0j0vnNdfy/XD0B2bvm819ze+jea3mHojM/fzle/WXPKH6ck1J\nSVnj1J1eOmNMhQbgL8BYYBvQwGprAGyzxt8Cbndavmi50oakpCRTWenp6ZVe19tori6s92O6iX4+\n2gQ9E2ReXv6yOXv2rHsD8wB/+V79JU9jqi9XYLVxoW6X23UjIrVEpHbhONAH2AjMBwqvcBkGzLPG\n5wN3WWffdAWOmnNdPEp5VHKzZDJGZ3B9y+v5/cLfc8v7t7Dz1512h6WUrVzpo48HvhGR9cB3wKfG\nmM+B54HeIrIDuM6aBvgM2AVkAP8GRro9aqXKEBsRy9zb5jKh9wTmb5vPh1s+BNBTMZXfKvcWCMaY\nXUBiCe1ZQK8S2g0wyi3RKVVJIsJjVz3GbW1uK7ob5tT1U5m7dS6zfjuLWiF64Fb5D70yVvm0RnUa\nERcRB4AgfLL9EwbMGkBmTqbNkSlVfbTQK78xrP0wZv52Jqv2r+KKN67gz+l/Ju90nt1hKeVxWuiV\nXxnSZggbR26kz6V9ePbrZ1m8c7HdISnlcXqbYuV3Lom5hA8Hf8jeo3tpEtUEgC92fUGv5r303vfK\nJ+kevfJbhUV+7cG19J7em5SpKWTlZdkclVLup4Ve+b3E+ETevuFtVmSu4Op3r2bv0b12h6SUW2mh\nV34vMCCQ+5PuZ9Gdizh47CDdJnVj7pa5doellNtooVfKcm3Ta/nq7q+4JOYSjp06Znc4SrmNHoxV\nykm7+HZ8fffXABScLWDkpyPp2KAjD3R6wObIlKo8LfRKlaLAFLD/2H7+vfbfGAz3dLiHkMAQu8NS\nqsK060apUoQEhvDB4A/odUkvRnw6gnov1uORBY9w5MQRu0NTqkJ0j16pMoQFhfHp7z5lwY4FvL/5\nfb7a+xUB4tg/MsboeffKK2ihV6ocIYEhDLx8IAMvH0jB2QICAwLJOZnDDbNu4OMhH1M3vK7dISpV\nJu26UaoCAgMCAdiTvYeV+1eSMjWFn4//bHNUSpVNC71SldA2vi2f3P4JO7J20H1ydz7d/qndISlV\nKi30SlVS70t7s+jORQDc8J8bmL5+us0RKVUyLfRKVcHVTa5m88jN/DX5r9xw2Q0AnDxz0uaolDqf\ny4VeRAJF5HsR+cSabi4iK0UkQ0TeE5EQqz3Ums6w5jfzTOhK1QzBgcE83eNpYsJjyPg1g5avtyR5\nSjI9pvQgOz/b7vCUqtAe/SPAFqfpF4CXjTEtgCPAvVb7vcARq/1lazml/EJUaBSt4lpxMPcgy/ct\n56b/3MSpglN2h6X8nEuFXkQaAdcD71jTAvQEPrAWmQoMssYHWtNY83uJnmys/ES9WvVYfOditj20\njamDpvL13q95ZcUrdoel/Jyre/SvAH8AzlrTsUC2MeaMNZ0JNLTGGwL7AKz5R63llfIrt7e9nTm3\nzGF059Hknspl5Kcj9X73yhZijCl7AZEbgAHGmJEikgyMBYYDK6zuGUSkMbDAGNNGRDYC/Ywxmda8\nnUAXY8zhYttNBVIB4uPjk2bPnl2pBHJzc4mMjKzUut5Gc/VeG49uZMz6MUQFR/Foy0fpHte9aJ6v\n5Voaf8kTqi/XlJSUNcaYTuUuaIwpcwD+jmOPfTfwE5AHzAQOA0HWMt2Ahdb4QqCbNR5kLSdlvUdS\nUpKprPT09Eqv6200V++2MnOlaf9mexP0TJD5aPNH5sTpE8YY38y1JP6SpzHVlyuw2pRTw40x5Xfd\nGGOeNMY0MsY0A4YAS40xQ4F04BZrsWHAPGt8vjWNNX+pFZBSfq1zw858OfxLEuol8Ns5v2XsorF2\nh6T8RFXudfMEMFtEngW+ByZZ7ZOA6SKSAfyK44+DUgqoE1qH9GHpLMhYwC2tHftJ249tJ/FEIjHh\nMTZHp3xVhS6YMsYsM8bcYI3vMsZ0Nsa0MMbcaow5abXnW9MtrPm7PBG4Ut4qJjyG37X9HSGBIZw8\nc5InfniC2H/Ecv/8+zlrzpa/AaUqSO9eqZSNQgJD+FvC39gctJm01WmEB4fzar9X9fbHyq200Ctl\nIxGhTVQbRvUYRVhQGC+veJlWsa0Y1XmU3aEpH6KFXqkaQESY0GcCOSdzOHDsAADZ+dnsyNrBlQ2v\ntDk65e200CtVQwRIAO/c9A7HTx0H4C/L/sK/vvsXQ9oM4f6O99OjWQ+bI1TeSu9eqVQNUyukFgB/\nSf4LqUmpLMhYQPLUZO6edzeZOZk2R6e8kRZ6pWqo6LBoJl4/kczfZ/KHq/7ArB9mMWbhGAAyczI5\nc/ZMOVtQykELvVI1XHhwOC/0foHtD21n2m+mAfDn9D/T4rUWvLz8ZXJO5tgcoarptNAr5SWaRjcl\nLCgMgPuT7qdpdFPGLBpD27S2bDi0weboVE2mhV4pL9S1UVe+HP4l39z9DQVnC7jm3WtY99M6u8NS\nNZQWeqW8WPcm3Vl+73K6NupK3fC6ALp3ry6ghV4pL9c4qjEL71hIk6gmvLfxPRLfTOSm/9zEu9+/\nqwdsFaCFXimfMvDygYzoNIIth7dwz/x7aP1Ga6avn253WMpmWuiV8iFhQWFMvH4i2x/azrwh8wgN\nCmXC8glF8w/nHS5jbeWr9MpYpXyQiHBTq5u44bIbih5f+HnG59z6/q0MSRjC5XGXc23Ta/X2Cn5C\nC71SPixAAqhXqx4A7eu357pLrmPetnm88/07AAxtO5S069OoHVrbzjCVh2mhV8pP1I+sz9zb5gKO\nLpzXV77O0t1LCQkMASD/TD5IYD4mAAAURklEQVSrD6wmJDCEzg072xmqcjMt9Er5obiIOP6a8lee\nPvs0gQGBrD24lr4z+nI47zDhQeEsuWsJ3Rp3sztM5SblHowVkTAR+U5E1ovIJhH5q9XeXERWikiG\niLwnIiFWe6g1nWHNb+bZFJRSlRUYEAhAbHgsbS5qw6grR9GwTkNSpqYwb+u8ctZW3sKVPfqTQE9j\nTK6IBAPfiMgCYAzwsjFmtoi8CdwLpFmvR4wxLURkCPACcJuH4ldKuUHT6KakD0sHYH/Ofv721d/o\n2qgrAC/+70VaxbXiplY32RmiqoJy9+iNQ641GWwNBugJfGC1TwUGWeMDrWms+b1En4umlNdoWKch\nb97wJvGR8ZwqOMX7m99n0OxBjP5sNFPWTSFlagr7ju7DGIMxxu5wlQtc6qMXkUBgDdACeAPYCWQb\nYwovu8sEGlrjDYF9AMaYMyJyFIgF9ARepbxMSGAI6cPSGb1gNG+vfZtTq05RL6IeR/KPsHDnQt7f\n/D7GGOpH1mdEpxF0urgTwYHBdoetipGK/EUWkWhgLvAnYIoxpoXV3hhYYIxpIyIbgX7GmExr3k6g\nizHmcLFtpQKpAPHx8UmzZ8+uVAK5ublERkZWal1vo7n6Jm/JNfdMLtmnsrko7CKCJZiPD3zM1D1T\niQqO4uf8n8k/m09CnQT+1eFfJa/vJXm6Q3XlmpKSssYY06ncBQv//XJ1AJ4GHsexhx5ktXUDFlrj\nC4Fu1niQtZyUtc2kpCRTWenp6ZVe19torr7JF3LNyssyczbOMek/phtjjDmaf9SMWzzOpK1KM2sO\nrDGnC06bOZ/PsTfIalRd3ymw2rhQt8vtuhGResBpY0y2iIQDvXEcYE0HbgFmA8OAwkP0863p5db8\npVZASikfVTe8Lrcm3Fo0/dWer3jx2xcpMAUEBwTTom4LQk6H8JvevyEoQM/qrm6u3OumAZAuIhuA\nVcBiY8wnwBPAGBHJwNEHP8lafhIQa7WPAca5P2ylVE12w2U38NPYn8gYnUGPZj3IzMlkeLPhBAUE\n8eQXTzL0o6EszFhod5h+o9w/rcaYDUCHEtp3ARdcPmeMyQduLd6ulPIvcRFxxEXEseiORZw5e4b/\nff0/AL7N/JZNP2/ioy0fMaLTCAIlkD8n/5nIkHN92oWdAHrCnnvo3SuVUh4lIuedifPl8C/Z+tBW\nLq59MS+veJmFOxcSFBBEwdkCYl6IoeNbHYmfEM9FEy7iicVP6B033UA7y5RS1S4uIo5V968i/0w+\n8bXiCQwIJP9MPne0vYPtv26ndb3WnDhzggnLJ9ChQQd+e8VveW/je3Rr3I0WdVvYHb7X0UKvlLJF\n4aMPC4UFhfH6gNfPa9uRtYNL617KsZPHuOvju6gbXpePb/uYa5pew+mC0xecs3/s5DFyT+XSoHYD\nj8fvTbTrRilVY7WMbUmABBAVFkX6sHTqhNbh2inX0uClBjR6uVHRoxI/3voxDy94mCavNOHyNy4n\nMycTcPT1f7r9Uzb/stnONGyne/RKKa+Q3CyZDQ9uYNL3k1h/aD0XR15M3uk8woLCGPXZKLLysujb\noi8Ldixg/NLx/PvGf9NmYht2/LqDuIg4ujXqxoQ+E7gs9jK2Z21n79G9XHnxlUSFRdmdmsdpoVdK\neY3aobV5tOujF7Rve2gbQQFBju6fla+TvjudQ7mHiIuIY2CrgUz6fhKfZ3zOA0kP0LhOY/rP7M+u\nI7uoE1qHu9vfze1tbqdzw86ICMYYsvOziQmPIfdULidOnyh6eIu30kKvlPJ6zqdmju4ymtSkVEKD\nQvn23m8BuLfjveSeyqXTxY67BcwfMp/9x/Yz+fvJpK1O49WVrzJ10FTuSryLO+feScavGVzT5BrS\nVqcRGhTK/jH7CQsKo8/0PhzJP8KgVoPo0KADO3/dycgrRxbd7nlH1g5iwmMA+OX4L0SHRRMcGEzB\n2YKiZeyghV4p5XNCg0LPm7487vLzphMuSiDhogT6XNqH7Pxs5m6ZS8HZAgAGtBzA0I+GsnL/Soa2\nHcqgywcRFhSGMYZ6tepx4swJnkp/CoBACaRb4250urgTN8+5mY+2fERkSCQd6nRgw4oN/CX5Lzza\n9VEmfz+ZRbsW8c6N7/DB5g94f/P7/OnaP9G9Sfdq+Ty00Cul/Fp0WDR3d7i7aPr2NreTmZPJFXFX\ncGOrG4vaRYSZv50JwAebPyA7P5vODTvTLr4dAInxiVzT5Bq+2vMV3+35jj6X9qFfi36A437/H235\niA83f4jBEB4Uzvas7VrolVLKDiLCH7r/ocxlbml9ywVtT/d4GoBHuz7KsmXLSE5OLprX59I+fHvP\ntyzIWEC9iHoMaz+MiOAIt8ZdFi30SilVDbo06kKXRl1seW89j14ppXycFnqllPJxWuiVUsrHaaFX\nSikfp4VeKaV8nBZ6pZTyceUWehFpLCLpIrJZRDaJyCNWe10RWSwiO6zXGKtdROQ1EckQkQ0i0tHT\nSSillCqdK3v0Z4DHjDGtga7AKBFpjeNZsEuMMS2BJZx7Nmx/oKU1pAJpbo9aKaWUy8ot9MaYg8aY\ntdb4MWAL0BAYCEy1FpsKDLLGBwLTjMMKIFpE9CkASillEyl8CK9LC4s0A74C2gB7jTHRVrsAR4wx\n0SLyCfC8MeYba94S4AljzOpi20rFscdPfHx80uzZsyuVQG5uLpGRkeUv6AM0V9/kL7n6S55Qfbmm\npKSsMcZ0Km85l2+BICKRwIfAo8aYHOensxtjjIi4/hfDsc7bwNsAnTp1Ms73haiI4veU8GWaq2/y\nl1z9JU+oebm6dNaNiATjKPIzjTEfWc2HCrtkrNefrfb9QGOn1RtZbUoppWzgylk3AkwCthhj/uk0\naz4wzBofBsxzar/LOvumK3DUGHPQjTErpZSqAFe6broDdwI/iMg6q+2PwPPAHBG5F9gDDLbmfQYM\nADKAPOBulFJK2abcQm8dVJVSZvcqYXkDjKpiXOWaORPGj4e9e3vQpAk89xwMHerpd1VKKe/jlfej\nnzkTUlMhLw9A2LPHMQ1a7JVSqjivvAXC+PGFRf6cvDxHu1JKVYeZM6FZMwgIcLzOnGl3RKXzykK/\nd2/J7Xv21PwPXClVMXYW1NLeu7BXYc8eMMbxescdEBdXM+uPVxb6Jk1Kn1fYjVMTP2yllOtmznQU\nzjvuOL+g3nkniFxYeIsX5OJtI0c6XkUgKOjCbZT0/sWLeWqqYzt33XVhrwJAVpYj3oEDryq3BlXr\nHzBjjO1DUlKSqYgZM4wRMcbx8Zc8NG1aoU16hfT0dLtDqDaaq3eZMcPxOyfieJ0x48JlKpLnjBnG\nRESU/TsOjvfr1evCZYODjQkJKX99cKxbGO+MGcbExrq2XkW2W/xzKoy9vOXLA6w2LtRYr9yjHzrU\n8dGUZc+e6olFKX83cqRjL7t4N0bt2iV3ecTFOfamRUrv6ijpOFxJjIElSy5c9vRpOHXKtfjz8s79\nl3DHHY69cnfIy4NHHjk/58L/TgpjL768p44zemWhB2jatPxlRo70fBxK+Qvnroa4uHPFKy2t5B2v\n3NzzC39KSo8LCmlhV0dh4S8cqntHrQK3/KqQrCzHHxFX/3iUdvyxqry20D/3HERElL1MWpoWe+V7\nSut7drWvt3B9577qgIBzRTYy0lHEnbdfvK88K6sye76lXY7j2yryR6Ss449V4ZXn0cO58+Ufeyyf\nQ4fCSl0uzbob/sSJ1RCUUh40c6ajK8C5wO7Zc+5nvHC68JoSOH/52FgYPBimTj3X1VFQ4Hh1LkbH\njzuGkravPCciwrED6wleu0cPjmI/e/aKcrtx0tLK7g9UqiYo3n9duGdduMftav9xXp5j2ZK6SdLS\nXOv7VtXv7bc9d8GnVxf6Qs895/hlKE9hf6B25yg7lHY63RdfXERk5IWF+fjxc9Oe6kNWVTNihGvH\nC8vTtKlnr+r3iUI/dCg8+KDry2vfvapupZ2ZIgLPPXdFUVeJqh6BgaXPi4yEGTMcQ2ERL74jKeIo\n8hMnuna8sCye7LIp5BOFHhwf+IgRri+flnbuAJReTatK4+pFLcW7XYoPpZ2Z4uA7BylDQhy/h7Gx\n7t1ucDDUqnVuulYtx3uIOIpxYWEu730jIhzLTZ164bKxsY55x445dh6HDoXdux3f2/TpjvcpfL/p\n088d9xs61NHtUjjfEWfJX3ZExLn/Agq35ckumyKunGzv6aGiF0w5K34RxogR5V9M5erFDTWNL1xY\n46rqzNX5Yp/YWMcgYkytWiX/rISGuueCGm8fin9GsbHn/w6VdXFQcPC5z7mkC6xcuQCrot9tVbZT\nUePHbyrKPTDQ8eqJ98fFC6ZsK+7OgzsLvTHuubqt+A9tTaCFvmK/uMULeElFydUrMH15cP4snH9v\natUq/Y9dRXeMZswwJj7+RLUXXLtU1++qXxf6QjNmlP6DWtEhIMB47K+yq/y90JdUlEvaM3TnZew1\ndSgt74r8vEdGuv6z7I49Y3//+fUELfRORoxw7y9Z4b+hnvyXrCQ1/RelosWgeEF23rOMjz9xwWfs\n68Xb1cGV/2SKf64jRtjTheGspv/8upPXFXpgMo4Hf290aqsLLAZ2WK8xVrsAr+F4jOAGoKMrQXi6\n0Bvj/mJf0lDSv7qu/JK5WiDd8cPjzj7L4l0jxW8iVfgHsWnTCz+DESMce6V2F82aOMTGXvifizcc\nQyqPFnr3c2ehvxboWKzQ/wMYZ42PA16wxgcAC6yC3xVY6UoQ1VHojalZ/9KHhpYei3OBdP7lLjzA\nU16RLinPwj84JRWQESNKj6WsflodKj44H/spqSuqsKDbdRDRk7TQu59bu26AZsUK/TaggTXeANhm\njb8F3F7ScmUN1VXonc2Y4Y17lGdrQAw6lDZUpovEnw5SaqF3P1cLvTiWLZuINAM+Mca0saazjTHR\n1rgAR4wx0SLyCfC8cTxQHBFZAjxhjFldwjZTgVSA+Pj4pNmzZ5cbR0lyc3OJjIys1LpffHERr7/e\ngpyc4BLm+s65zaqizuL4/s/9DISGFtCv30FWrIjj559Dueiik9x33y6uu+7nKr9bVX6GvYm/5AnV\nl2tKSsoaY0ynchd05a8BF+7RZxebf8R6/QS42ql9CdCpvO3bsUdflprUxaND9Q4hIdXfdeIve7r+\nkqcxNW+PvrJXxh4SkQYA1mvhbs1+oLHTco2sNq8ydCgcPuz41XflajtVcxVeQVl8WsTx6ny1ZWws\nTJ58/lWRZ886Xj1+5aJSHlTZQj8fGGaNDwPmObXfJQ5dgaPGmINVjNFWhUW/8L4XJRUIVf1CQ8+/\nBH7EiAvvNxIRAW+9de6PtjGOh2EcPuwo4IcPn3s4hjGOaS3oyheVW+hF5D/AcqCViGSKyL3A80Bv\nEdkBXGdNA3wG7MJxeuW/AZ+5dZjzHl5hgShe/IsXnuJ7kiEhtoVf7Vy5m2ih4nvZjnFz3j1MnO8N\nMmMG5OefK9i7dzvuO+J8v5Fqu4eIUt7Alf4dTw81rY/ek8q6SKj8y8/PljHv3OB8FW/hWSBQ/j2A\nnK+2dL7nS/Hz4AvnlbbN4qcQlpRvebzte60Kf8nVX/I0pub10XvtE6a8VWH/b2nzyrJs2ZckJye7\nLZaZMx0PI9671/EIs+eeq/gecHnbKCtfpVT10ELvx9xRhLWQK1Xz+cz96JVSSpVMC71SSvk4LfRK\nKeXjtNArpZSP00KvlFI+zqWbmnk8CJFfgD2VXD0OOOzGcGoyzdU3+Uuu/pInVF+uTY0x9cpbqEYU\n+qoQkdXGlbu3+QDN1Tf5S67+kifUvFy160YppXycFnqllPJxvlDo37Y7gGqkufomf8nVX/KEGpar\n1/fRK6WUKpsv7NErpZQqg1cXehHpJyLbRCRDRMbZHY+7ichuEflBRNaJyGqrra6ILBaRHdZrjN1x\nVpSITBaRn0Vko1NbiXlZD7F5zfqON4hIR/sir7hScv2LiOy3vtd1IjLAad6TVq7bRKSvPVFXjog0\nFpF0EdksIptE5BGr3ae+2zLyrLnfqyv3Mq6JAxAI7AQuAUKA9UBru+Nyc467gbhibf8Axlnj44AX\n7I6zEnldC3Tk/OcQl5gXMABYgONJ3V2BlXbH74Zc/wKMLWHZ1tbPcSjQ3Pr5DrQ7hwrk2gDoaI3X\nBrZbOfnUd1tGnjX2e/XmPfrOQIYxZpcx5hQwGxhoc0zVYSAw1RqfCgyyMZZKMcZ8BfxarLm0vAYC\n04zDCiC68HnF3qCUXEszEJhtjDlpjPkRx5PaOnssODczxhw0xqy1xo8BW4CG+Nh3W0aepbH9e/Xm\nQt8Q2Oc0nUnZH7Y3MsAiEVkjIqlWW7w59xzen4B4e0Jzu9Ly8tXv+SGru2KyU/ebz+QqIs2ADsBK\nfPi7LZYn1NDv1ZsLvT+42hjTEegPjBKRa51nGsf/hT532pSv5uUkDbgUaA8cBF6yNxz3EpFI4EPg\nUWNMjvM8X/puS8izxn6v3lzo9wONnaYbWW0+wxiz33r9GZiL49+9Q4X/3lqvP9sXoVuVlpfPfc/G\nmEPGmAJjzFng35z7N97rcxWRYBzFb6Yx5iOr2ee+25LyrMnfqzcX+lVASxFpLiIhwBBgvs0xuY2I\n1BKR2oXjQB9gI44ch1mLDQPm2ROh25WW13zgLusMja7AUaduAK9UrB/6Nzi+V3DkOkREQkWkOdAS\n+K6646ssERFgErDFGPNPp1k+9d2WlmeN/l7tPoJdlQHHUfvtOI5ij7c7HjfndgmOI/XrgU2F+QGx\nwBJgB/AFUNfuWCuR239w/Gt7Gkd/5b2l5YXjjIw3rO/4B6CT3fG7IdfpVi4bcBSBBk7Lj7dy3Qb0\ntzv+CuZ6NY5umQ3AOmsY4GvfbRl51tjvVa+MVUopH+fNXTdKKaVcoIVeKaV8nBZ6pZTycVrolVLK\nx2mhV0opH6eFXimlfJwWeqWU8nFa6JVSysf9P3l+x9kWaWgKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzi-Yr4gvye3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "55b8098a-ba51-46f7-fdab-f021504acb11"
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "import random\n",
        "length = len(testset.targets)\n",
        "percentage_corruption = 40\n",
        "n = length*percentage_corruption/100\n",
        "corrupt_classes = np.random.randint(0,10,int(n))\n",
        "# print(np.unique(corrupt_idx))\n",
        "corrupt_idx = random.sample(range(0,length), int(n))\n",
        "print(len(corrupt_idx) , len(np.unique(corrupt_idx)))\n",
        "print(corrupt_idx)\n",
        "print(len(corrupt_classes))\n",
        "\n",
        "st1 = testset.targets\n",
        "a = np.array(testset.targets)\n",
        "a[corrupt_idx] = corrupt_classes\n",
        "testset.targets = list(a)\n",
        "st2 = testset.targets\n",
        "\n",
        "print(st1 == st2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "4000 4000\n",
            "[6704, 8412, 5833, 9383, 6153, 2648, 2799, 6303, 355, 5251, 2321, 3985, 6536, 4239, 6851, 1419, 1928, 3014, 2343, 3220, 3219, 3330, 531, 736, 1707, 1074, 495, 4901, 9323, 5189, 5164, 9161, 618, 2094, 5356, 945, 1568, 5982, 5250, 5080, 7750, 1709, 2491, 1540, 2050, 5573, 2313, 6789, 2080, 6131, 4649, 8963, 3482, 8034, 9, 8217, 6413, 5133, 7037, 8135, 3105, 532, 3718, 6030, 9851, 7519, 8920, 9626, 6151, 5124, 7518, 1296, 2523, 6554, 4166, 4413, 2970, 759, 2370, 6152, 6749, 1293, 4711, 3451, 4704, 3186, 5270, 9743, 9158, 9943, 7569, 2444, 9947, 4113, 5397, 9357, 8707, 7221, 300, 1363, 659, 5842, 102, 9115, 6841, 5095, 8491, 2805, 8005, 3509, 2075, 8798, 6058, 6306, 5924, 5423, 2287, 4933, 3135, 9321, 7049, 1372, 2185, 6878, 7183, 3577, 7421, 7319, 5028, 3122, 6620, 8053, 9739, 4340, 3848, 2920, 1891, 5300, 5231, 4152, 5659, 1890, 2937, 7982, 703, 3275, 468, 9642, 1116, 504, 7770, 1747, 6393, 7462, 7439, 2376, 9853, 5414, 2584, 9910, 7503, 3024, 6403, 5955, 547, 9183, 3063, 2521, 3883, 8074, 8438, 4726, 7144, 4998, 284, 3179, 9832, 9247, 2990, 8771, 4613, 2672, 4538, 8784, 5704, 1658, 3880, 343, 479, 6009, 7092, 1134, 2159, 5394, 1443, 5171, 8548, 7632, 9103, 6068, 157, 462, 5161, 2096, 3857, 6402, 7984, 2909, 7578, 7737, 2477, 8180, 9602, 3814, 8453, 405, 9991, 6904, 8686, 2883, 409, 4715, 4153, 7567, 3337, 5207, 9088, 6499, 3641, 4982, 6909, 2573, 7629, 5513, 2474, 9335, 393, 8706, 7103, 4247, 6373, 5971, 7263, 2395, 9456, 5703, 9845, 2448, 8624, 6348, 8510, 7032, 2055, 7012, 496, 7052, 2163, 6889, 4578, 4265, 8357, 4621, 9595, 1023, 3085, 3198, 3396, 3113, 6292, 2743, 583, 3767, 6143, 7616, 587, 6617, 3719, 4452, 9194, 2769, 5755, 2420, 491, 2134, 4730, 4401, 2759, 8918, 332, 8120, 8813, 8726, 4199, 5750, 9141, 2633, 2776, 1303, 8311, 9686, 8749, 1850, 2828, 8285, 6207, 1608, 7109, 4637, 6986, 4291, 9744, 4615, 7943, 9377, 3930, 9250, 9426, 6644, 1388, 7896, 6367, 1297, 1945, 6868, 3691, 917, 2008, 9736, 6484, 2729, 9239, 833, 5585, 3768, 2978, 2157, 2724, 6828, 5522, 1495, 9472, 2250, 4111, 5575, 2064, 6204, 6862, 1260, 6926, 4167, 6352, 1416, 3329, 8831, 7333, 7233, 7546, 5772, 9804, 3839, 7334, 1227, 3283, 1216, 276, 5666, 7340, 37, 3235, 5185, 1763, 3711, 1145, 1055, 197, 2803, 6339, 4103, 125, 3406, 1758, 5416, 128, 3568, 6142, 5235, 18, 6136, 3548, 4243, 8096, 9482, 8071, 4331, 8118, 6398, 8068, 6113, 2780, 42, 8695, 2063, 3655, 8990, 9022, 7803, 1965, 207, 6101, 6790, 1511, 2754, 957, 1459, 8679, 6668, 2324, 7956, 2850, 7318, 639, 4076, 2993, 7083, 3361, 4525, 9493, 4211, 677, 5948, 8753, 3372, 9313, 5899, 2963, 6514, 7005, 53, 2509, 4377, 5151, 7754, 5921, 2746, 6335, 6927, 8943, 9639, 6091, 6431, 4037, 1641, 5342, 1655, 4042, 9560, 9405, 6127, 7456, 2903, 7880, 4251, 5237, 1777, 7549, 6036, 7040, 2881, 6725, 3863, 5241, 6319, 2938, 1798, 7819, 9976, 5635, 7600, 9149, 3291, 6601, 8508, 2245, 1723, 2793, 2264, 1165, 3668, 4365, 6498, 5036, 5891, 2260, 2915, 5118, 2335, 2311, 3662, 1980, 778, 9517, 9763, 9297, 7487, 3639, 3048, 2819, 664, 9333, 1668, 6158, 8617, 9133, 2265, 1437, 8223, 1200, 5433, 9004, 4157, 2194, 2483, 2997, 8989, 1856, 7579, 776, 1609, 1621, 8094, 5246, 8992, 7523, 9375, 6728, 3892, 8795, 2979, 4189, 2415, 7895, 2501, 9485, 8298, 4282, 5232, 3171, 6958, 8639, 720, 7386, 6618, 449, 1819, 4520, 2369, 6856, 956, 8432, 4147, 5344, 6944, 5672, 2270, 7780, 4735, 5145, 8862, 840, 2907, 5589, 8001, 8371, 2692, 3882, 4997, 9707, 2981, 4771, 1464, 3837, 2385, 9605, 5071, 9609, 2256, 335, 7570, 1307, 8623, 3191, 9608, 3131, 8218, 3524, 2678, 9535, 4257, 1663, 1061, 1292, 1381, 2299, 7470, 1214, 7920, 6735, 9880, 629, 3680, 2241, 3587, 6637, 2893, 633, 5299, 7218, 5108, 3487, 8825, 3790, 2725, 3617, 2550, 5648, 5236, 4198, 3042, 124, 792, 8715, 3696, 594, 3763, 8187, 7019, 2797, 6423, 7313, 1503, 5026, 5168, 3745, 4067, 1119, 9078, 5602, 5570, 2169, 7659, 1242, 6684, 302, 9185, 4720, 6170, 7310, 951, 5412, 3427, 4329, 8505, 8377, 6364, 2968, 4708, 4712, 5468, 822, 8365, 8470, 9700, 4533, 1044, 2519, 4483, 3453, 5200, 1736, 1689, 9563, 4506, 2062, 7785, 7558, 2667, 738, 6370, 8874, 4513, 1487, 9573, 2794, 6994, 3941, 7171, 6272, 239, 6246, 1294, 9000, 3676, 3622, 1600, 7635, 3521, 2572, 3965, 3756, 7990, 6313, 9216, 2341, 6563, 3545, 3216, 7125, 9939, 5673, 3285, 7363, 4462, 7249, 6713, 2875, 8546, 9369, 1436, 8175, 2214, 2451, 5718, 3690, 4110, 2904, 5698, 9899, 9479, 1149, 5239, 6923, 4951, 9721, 5000, 5545, 9785, 5229, 7867, 2702, 4363, 1269, 1940, 9866, 231, 4437, 202, 303, 3461, 7864, 8333, 2150, 1083, 1970, 9918, 7413, 6327, 6706, 7537, 2198, 6603, 8167, 6764, 4168, 7678, 7446, 8339, 5465, 3564, 3143, 3766, 364, 7524, 7211, 8743, 3513, 539, 5592, 2409, 9681, 4774, 4324, 429, 2407, 849, 7968, 6530, 8835, 8687, 7657, 6308, 5941, 3321, 3041, 1039, 6133, 3640, 5983, 8140, 8029, 3424, 9589, 9248, 5314, 3196, 5262, 2600, 9233, 5280, 888, 772, 8817, 4306, 1996, 2302, 2336, 1730, 4358, 8075, 3946, 7671, 8142, 1232, 1713, 6059, 1583, 4663, 1252, 7550, 2132, 4934, 3357, 5175, 4529, 5291, 7078, 7760, 910, 4842, 9067, 2480, 1312, 5144, 9079, 1285, 2384, 2872, 6097, 695, 3334, 1448, 3935, 9550, 6866, 3395, 7325, 5788, 5917, 1944, 6657, 6044, 8194, 8731, 9985, 1590, 8163, 4783, 6323, 7177, 4221, 3908, 4109, 8547, 3866, 3470, 7197, 8942, 2101, 9110, 939, 9164, 8027, 6475, 421, 9954, 6788, 6356, 1201, 3044, 2289, 3185, 4412, 4821, 440, 1835, 7903, 838, 6156, 3610, 937, 2955, 3590, 1003, 5617, 8332, 8968, 8741, 8054, 8782, 8243, 2115, 6977, 715, 484, 4258, 8791, 7566, 6167, 7438, 3851, 9066, 6972, 7593, 1123, 9281, 4874, 2333, 4112, 5066, 2290, 9641, 613, 47, 4557, 2160, 7791, 2625, 1468, 9570, 991, 3594, 4694, 8295, 1627, 481, 4156, 4594, 1410, 9399, 4974, 6579, 1657, 769, 3611, 2863, 3549, 2542, 5083, 4343, 2879, 5560, 2914, 9331, 5577, 9266, 3897, 8429, 3949, 7776, 148, 2983, 9190, 3376, 9577, 1336, 6048, 2643, 6183, 387, 5846, 226, 7342, 2155, 4573, 8896, 804, 6114, 680, 5569, 2113, 3011, 4813, 4479, 3933, 7829, 2246, 3222, 8228, 6262, 844, 4885, 347, 8124, 9749, 1153, 8044, 2010, 8863, 6524, 9255, 6976, 3827, 6225, 4430, 9514, 7482, 3987, 5708, 414, 464, 8654, 9440, 7467, 2471, 2301, 4287, 1256, 1373, 3352, 2664, 8794, 2285, 1025, 4163, 9581, 8844, 6519, 8872, 702, 7417, 6464, 9669, 8231, 4014, 3552, 3439, 7801, 3737, 384, 7400, 1876, 4679, 1441, 7669, 8498, 4790, 9349, 4359, 6567, 3734, 6247, 6387, 895, 6808, 4698, 6391, 1720, 8423, 9516, 8324, 8702, 3449, 227, 186, 6013, 711, 8184, 5777, 4350, 4036, 3033, 5681, 9873, 1721, 6311, 5934, 4418, 5914, 7009, 9097, 1344, 1265, 6680, 431, 1989, 7926, 412, 5420, 1852, 350, 6557, 5656, 3924, 9525, 3658, 7648, 8208, 9316, 8087, 7949, 4474, 5567, 6943, 661, 9262, 5758, 5343, 1184, 7014, 4910, 9497, 2942, 4444, 6236, 1698, 5862, 8506, 5534, 8335, 5409, 7238, 1360, 6238, 7545, 8246, 7965, 9960, 9798, 6253, 9356, 5296, 1861, 842, 3744, 4761, 4616, 1956, 4118, 7782, 8594, 3899, 5857, 9094, 7302, 6508, 9998, 9373, 5816, 213, 4048, 928, 6544, 9934, 432, 9520, 5169, 4543, 8757, 4526, 9019, 8042, 2278, 4507, 4304, 2463, 1611, 6049, 7506, 9557, 2766, 647, 2469, 8939, 2200, 2515, 3420, 1690, 723, 809, 3713, 4954, 85, 4794, 7922, 3872, 9072, 4495, 7808, 5009, 914, 3503, 3624, 6636, 2190, 8114, 7084, 3164, 6314, 7137, 5492, 9784, 1224, 9978, 3333, 5225, 2852, 7044, 8845, 9359, 4339, 8955, 784, 5678, 1387, 5965, 5894, 8424, 3576, 3648, 7825, 4280, 3506, 3986, 3129, 4128, 4364, 7647, 574, 5088, 9908, 8747, 3764, 9324, 4907, 6934, 8158, 5319, 8320, 4722, 2585, 6080, 747, 6745, 6220, 7492, 3401, 38, 8961, 2636, 8725, 6112, 7061, 294, 8410, 7328, 7095, 8156, 9685, 3817, 4273, 5958, 8734, 1942, 7757, 7405, 8258, 9076, 5987, 4384, 3881, 4187, 2224, 4426, 5431, 9271, 6182, 1814, 8319, 9264, 8847, 3117, 4938, 5942, 6271, 5498, 1463, 2721, 8064, 4327, 3140, 8902, 2644, 9858, 3919, 7169, 6166, 3557, 4808, 6818, 4682, 7613, 643, 8382, 7853, 2152, 9159, 5774, 9645, 6894, 4738, 3370, 1261, 8182, 2011, 7094, 3750, 690, 785, 9892, 7360, 5622, 5885, 4120, 3784, 192, 8375, 7741, 6740, 6083, 7493, 7959, 569, 4516, 1875, 1573, 3006, 366, 4746, 602, 7359, 2212, 6285, 8011, 7720, 3004, 107, 2524, 1377, 866, 146, 6896, 7708, 9650, 9315, 3749, 2723, 3479, 7642, 8055, 6053, 4576, 6076, 3759, 8973, 1915, 8857, 7618, 3878, 8097, 1974, 2036, 3408, 8799, 4006, 8407, 3379, 7369, 6870, 272, 3778, 1391, 8997, 6490, 1281, 5550, 9672, 9617, 5999, 1077, 4486, 5284, 3263, 805, 5058, 1618, 8346, 9755, 8274, 7693, 8229, 1660, 617, 3161, 7490, 3363, 771, 9713, 8088, 9812, 5912, 4763, 1105, 2843, 7250, 1033, 5869, 7532, 644, 4470, 7126, 6428, 8063, 1070, 9469, 2694, 273, 7449, 3526, 8944, 1505, 6556, 3532, 3316, 7442, 314, 2439, 2936, 9665, 6214, 2357, 5721, 9597, 1122, 6837, 6157, 5682, 6354, 4144, 3250, 2779, 9114, 7557, 3043, 6700, 725, 9118, 7068, 4647, 9048, 9298, 7288, 8626, 7207, 304, 4923, 8633, 1255, 6004, 4402, 288, 1129, 4705, 8030, 4212, 1304, 7845, 8820, 8959, 653, 1805, 6577, 5490, 4764, 5932, 8664, 9825, 9406, 7511, 7955, 1433, 8004, 4276, 6488, 796, 1412, 6783, 8290, 5753, 6814, 1171, 5131, 7866, 9499, 9567, 1280, 4836, 692, 5224, 7158, 3236, 4899, 6107, 901, 6714, 8500, 6819, 2895, 7630, 9460, 9885, 8413, 1844, 7396, 2436, 3079, 339, 4638, 4993, 8176, 7631, 2943, 3794, 174, 9538, 6542, 5599, 8998, 2637, 8528, 4344, 9990, 3523, 1100, 8422, 4758, 591, 6518, 5452, 4640, 7115, 4853, 8818, 8476, 1916, 8466, 456, 4311, 7619, 3515, 6163, 3177, 7603, 4940, 2713, 2987, 4862, 5539, 1871, 4947, 726, 6021, 5664, 5497, 8865, 8433, 5841, 8922, 5797, 6062, 9260, 317, 9996, 1744, 7441, 326, 8950, 3588, 993, 3026, 4592, 360, 4607, 4370, 6615, 5072, 5226, 9651, 5203, 6540, 1754, 5381, 5451, 3371, 7338, 2069, 6639, 3039, 8566, 5650, 5858, 1938, 9945, 909, 3180, 7257, 6959, 2517, 8449, 5419, 7343, 9300, 4944, 1053, 7751, 9906, 2330, 5634, 6795, 7931, 3200, 9364, 7497, 7323, 793, 8796, 6286, 6353, 1397, 3799, 3929, 7856, 5436, 9614, 2035, 3119, 9531, 9856, 6105, 9652, 6971, 9154, 2251, 697, 2800, 106, 3015, 4301, 8102, 4515, 275, 2377, 4071, 1180, 8072, 9539, 1701, 4994, 3806, 6408, 6598, 2716, 9418, 578, 411, 242, 5951, 6843, 6939, 768, 3905, 9332, 8670, 7510, 3294, 609, 6461, 4336, 7007, 7080, 3915, 3819, 9920, 6975, 8230, 6653, 5604, 2666, 8808, 8111, 8722, 9579, 2049, 6015, 1813, 7210, 3667, 8552, 2593, 8606, 6989, 8364, 7000, 4013, 6691, 8373, 4362, 1771, 7239, 9181, 4741, 6380, 4953, 8233, 6539, 5546, 6085, 165, 334, 3884, 6389, 8832, 5564, 4687, 3875, 5376, 3886, 7644, 2368, 112, 2267, 4026, 70, 6609, 4540, 7921, 3141, 3074, 220, 4411, 2848, 8530, 7051, 5475, 3709, 1821, 473, 2383, 2355, 4777, 6471, 188, 248, 2677, 620, 4056, 4898, 1435, 2548, 5931, 8416, 4919, 5391, 9870, 887, 8349, 2626, 8435, 1366, 5348, 2006, 8913, 931, 2681, 3469, 4789, 6666, 8146, 2319, 2929, 3504, 6384, 2610, 4228, 2244, 3412, 5587, 5581, 5505, 5208, 3228, 3385, 9232, 7187, 5297, 306, 9697, 265, 4657, 8012, 3452, 8484, 2405, 6067, 5318, 7437, 5641, 8987, 9859, 7500, 6663, 1299, 8397, 8452, 6349, 8855, 3505, 5173, 2317, 8492, 4942, 9914, 8277, 4204, 7529, 4659, 8195, 1863, 6001, 8582, 6434, 8081, 9125, 3126, 3512, 1268, 4093, 6506, 2710, 4961, 8878, 1654, 4785, 290, 4075, 7615, 8529, 1389, 3867, 7275, 858, 7235, 8352, 6858, 1258, 2589, 2583, 2156, 4612, 4046, 2339, 6533, 734, 867, 9588, 8910, 4887, 7486, 748, 7286, 8334, 3847, 4378, 6611, 5870, 1849, 8903, 8754, 2350, 936, 1992, 6559, 4088, 4840, 1384, 8712, 7351, 498, 5556, 958, 5438, 2655, 1378, 3511, 4670, 5808, 1976, 3729, 316, 5138, 9430, 6396, 5572, 1529, 8758, 9101, 4277, 1833, 8630, 940, 9427, 9257, 616, 2085, 1581, 1396, 8213, 7730, 7454, 6302, 7781, 3800, 1066, 9635, 2663, 5804, 4977, 5940, 4818, 7397, 3441, 14, 1946, 1155, 1843, 1199, 8226, 1401, 5733, 5619, 5812, 9108, 9865, 2528, 6427, 7735, 1761, 1076, 5395, 9611, 9814, 4092, 3384, 2236, 2239, 6942, 8883, 5408, 210, 1399, 7105, 9879, 2783, 1837, 6677, 1246, 9417, 7513, 6283, 7227, 6825, 5461, 8203, 9675, 3804, 4893, 4404, 9800, 3109, 1451, 1369, 5830, 6560, 9352, 7778, 6897, 7814, 2587, 3377, 8107, 5353, 9210, 4453, 4307, 295, 8040, 864, 9055, 670, 7747, 2878, 5186, 8533, 4018, 1275, 3957, 402, 4278, 4455, 2351, 9438, 2119, 606, 6685, 4391, 2927, 8456, 2002, 3208, 5765, 1454, 5710, 6550, 3721, 6996, 8948, 1205, 3928, 447, 3448, 7898, 6338, 5724, 8426, 3144, 3499, 1060, 9929, 1612, 9527, 1166, 6134, 3389, 1350, 7280, 1190, 3926, 5406, 3754, 9606, 2700, 2703, 2262, 5680, 1157, 7830, 5102, 6588, 3976, 537, 8522, 2252, 5828, 7685, 8125, 3679, 1264, 9486, 6301, 3751, 5667, 1696, 1684, 4665, 7204, 6915, 3399, 3955, 3585, 2641, 6496, 3268, 2038, 3402, 82, 5976, 3542, 492, 9074, 9500, 6768, 6296, 9524, 6908, 3084, 8026, 635, 9816, 7675, 2272, 9237, 8260, 9470, 2025, 4801, 1626, 4126, 5242, 3650, 9131, 985, 3620, 9306, 9670, 3758, 2602, 6366, 426, 2714, 7554, 198, 2430, 8885, 8752, 9932, 1933, 5653, 6676, 8073, 3307, 1405, 4716, 8302, 1899, 9441, 7979, 8915, 3917, 4209, 8745, 3251, 8524, 4814, 4429, 9604, 6196, 5937, 727, 3238, 5216, 4733, 903, 5930, 3852, 3544, 9690, 7582, 1259, 8301, 5363, 3887, 8406, 1648, 5328, 2765, 7899, 7255, 5639, 8036, 1340, 7402, 9759, 3433, 8250, 1961, 4028, 5017, 5466, 7590, 436, 5644, 2431, 3843, 5227, 8780, 3821, 9392, 1310, 3546, 1386, 9974, 6805, 8535, 6995, 1370, 5434, 9534, 8917, 9005, 7790, 392, 7251, 5657, 7620, 4442, 5514, 6800, 9940, 9799, 8833, 5332, 7797, 8637, 8400, 7027, 3925, 5949, 92, 6017, 6701, 6478, 3432, 4648, 7539, 4494, 1398, 4820, 9215, 5762, 2106, 9049, 9741, 2932, 9449, 7331, 1047, 777, 9980, 8788, 3693, 2381, 5196, 1212, 6614, 1651, 7726, 2189, 7477, 949, 2688, 5084, 2774, 6425, 6842, 7473, 9512, 7963, 5019, 4338, 1988, 5509, 1461, 7175, 6966, 1727, 5686, 5720, 3592, 5410, 7761, 5875, 6947, 2787, 3182, 7111, 581, 7762, 6674, 5872, 6727, 5588, 261, 7017, 3072, 2722, 6418, 7827, 6919, 9522, 2590, 6928, 5046, 1602, 5859, 8045, 3300, 3803, 6520, 6237, 2437, 564, 8652, 2276, 6123, 3165, 3145, 3244, 5043, 2950, 6078, 2889, 3776, 3583, 1772, 1557, 7748, 1957, 2717, 1469, 4610, 9552, 3716, 6493, 22, 4589, 1815, 7688, 1115, 2545, 7879, 4826, 2074, 6232, 4416, 4970, 975, 7339, 9196, 8038, 6116, 8350, 9750, 8797, 2261, 9709, 1499, 4521, 7596, 9087, 626, 5961, 3720, 6600, 9166, 2802, 9551, 2790, 6517, 7066, 7283, 6231, 7300, 716, 68, 5324, 4050, 5197, 9509, 1159, 7298, 7458, 6793, 8253, 1939, 4684, 9238, 2922, 219, 121, 628, 2283, 3860, 7476, 3593, 3130, 6360, 969, 6025, 3956, 9740, 1103, 5864, 637, 6239, 5457, 9361, 6669, 7373, 2127, 2187, 7287, 367, 7525, 9317, 6665, 9032, 5817, 4386, 1492, 5059, 5418, 2249, 5621, 3324, 1796, 6055, 9767, 7496, 5578, 8048, 7139, 6432, 813, 9415, 3490, 5193, 7058, 7, 3112, 4917, 3903, 2712, 3753, 4855, 5440, 9703, 5150, 9206, 7942, 4854, 1615, 5523, 8134, 6450, 7089, 7270, 9302, 281, 6154, 5920, 4586, 3121, 9576, 4090, 287, 1674, 1241, 7136, 3187, 4248, 3651, 5113, 3246, 9585, 1851, 78, 6459, 6385, 9731, 4454, 5926, 9854, 4572, 8343, 6395, 3388, 7580, 6359, 7077, 8739, 7568, 6063, 1097, 1139, 5748, 7934, 9596, 859, 6792, 7457, 9171, 1936, 1207, 6480, 1645, 5503, 147, 184, 1596, 6482, 3088, 8569, 1291, 8995, 2039, 3909, 7868, 7818, 3309, 8128, 1793, 8046, 6809, 1566, 8309, 5079, 3298, 6671, 4702, 3543, 3953, 2912, 476, 7008, 2434, 7661, 4279, 3742, 1477, 1562, 9034, 8115, 2969, 7304, 7912, 8221, 8056, 1341, 6129, 6233, 8396, 2034, 1719, 3375, 9886, 3290, 4229, 8129, 4848, 576, 289, 789, 7964, 4925, 8341, 1218, 8100, 4469, 9478, 9780, 8427, 651, 3414, 4081, 650, 4238, 7096, 5603, 4835, 1894, 5430, 3267, 1379, 1222, 8605, 4624, 1128, 2188, 2073, 478, 485, 4849, 1750, 4236, 9231, 2423, 658, 4956, 9732, 5339, 5316, 5856, 5726, 4061, 8170, 4177, 2709, 7533, 5893, 2885, 1628, 5537, 8307, 2020, 7913, 6441, 3045, 1099, 3529, 4875, 3488, 507, 3350, 527, 2084, 1029, 7573, 2060, 2182, 6259, 8292, 8162, 9992, 7256, 1174, 741, 8600, 611, 142, 4496, 1775, 7756, 6410, 856, 2126, 642, 167, 3167, 9857, 382, 675, 2492, 7683, 3967, 9338, 4115, 1780, 4482, 5070, 5078, 9240, 5069, 6211, 5167, 1172, 3977, 9801, 2329, 1220, 4819, 3095, 6444, 5373, 3455, 1555, 1624, 7528, 4567, 7101, 5837, 9823, 5068, 4839, 1671, 8008, 8786, 7820, 5202, 1392, 9923, 2613, 7433, 6721, 5696, 4185, 994, 4883, 61, 4606, 2130, 9385, 6411, 5018, 1111, 2948, 2706, 1527, 3947, 5471, 8644, 8876, 9714, 6008, 8520, 189, 9445, 6279, 4531, 1742, 9511, 9230, 6489, 7106, 9081, 9632, 5240, 8009, 8659, 9090, 3365, 6746, 9705, 6221, 7469, 1394, 1249, 7412, 2756, 2792, 9186, 2195, 1993, 8372, 1494, 1865, 7695, 1317, 6987, 5660, 5705, 9827, 3018, 311, 2033, 2345, 7555, 6077, 2177, 7155, 8337, 1830, 9716, 2512, 2269, 5198, 2400, 3258, 7908, 3920, 4059, 9354, 6667, 381, 333, 3714, 8565, 6867, 2592, 2413, 6946, 758, 8620, 545, 1943, 8331, 8800, 1051, 283, 6935, 1811, 399, 1295, 3437, 8493, 2576, 9487, 9307, 9746, 9702, 9466, 7435, 9212, 181, 2810, 5533, 883, 348, 8483, 9848, 1037, 2530, 9624, 1, 2772, 2016, 3980, 2698, 7822, 9933, 1421, 6037, 7131, 7378, 5973, 5518, 1925, 4674, 561, 5638, 1670, 9031, 3812, 6747, 3769, 9092, 5855, 1972, 8007, 2696, 5323, 892, 3116, 5791, 668, 6050, 6973, 774, 4188, 2105, 2945, 1189, 3845, 5873, 8137, 3254, 7855, 4159, 5512, 5938, 6992, 6988, 1906, 1239, 9269, 8358, 794, 134, 1679, 9997, 9507, 4158, 8775, 719, 5713, 54, 1738, 9344, 3889, 9116, 8200, 6487, 5722, 2248, 8361, 8315, 4195, 8849, 6275, 1418, 9760, 5850, 529, 3103, 1488, 8982, 934, 6948, 632, 1016, 8543, 6320, 7122, 7216, 1617, 6925, 6831, 5360, 2216, 5453, 1717, 6796, 4069, 2836, 737, 1095, 2730, 1113, 1442, 2891, 9864, 2747, 3614, 742, 254, 3192, 3893, 1545, 8640, 5051, 1148, 363, 6991, 4779, 7489, 2196, 4355, 3362, 3779, 8241, 1273, 3132, 2755, 1806, 4326, 9362, 971, 1630, 1700, 6997, 2941, 108, 5211, 3311, 118, 6115, 5649, 3584, 9353, 6697, 7499, 8870, 2141, 935, 1882, 6717, 2566, 7527, 1809, 5135, 3944, 2144, 8471, 2899, 5761, 4623, 8577, 63, 9862, 2083, 4433, 534, 7699, 3115, 8467, 9796, 6234, 3697, 7277, 482, 8196, 1272, 2798, 1431, 5266, 8132, 6092, 3031, 56, 9900, 4805, 4851, 7721, 2258, 5286, 9458, 8882, 898, 1452, 3354, 2442, 7267, 688, 610, 6382, 466, 4745, 5778, 3080, 4553, 9649, 7188, 7236, 4174, 1367, 1782, 8889, 997, 1509, 7682, 7798, 8890, 9582, 6965, 396, 3318, 3632, 2024, 3107, 2149, 5346, 8395, 9695, 3218, 5628, 7114, 8696, 5974, 3787, 1699, 8951, 2099, 4832, 814, 2916, 6159, 5140, 8656, 3556, 2428, 7336, 512, 4054, 9526, 7768, 9660, 1444, 5, 3966, 7584, 7091, 2136, 4503, 5913, 6596, 2739, 373, 4388, 7436, 3047, 9002, 6702, 5303, 5775, 9011, 1349, 2494, 1374, 3600, 672, 8503, 9144, 5946, 6634, 3932, 9495, 4605, 161, 1729, 1599, 1704, 6178, 8451, 9729, 9457, 6126, 5380, 6026, 6849, 6144, 1935, 584, 4309, 6855, 938, 7392, 1536, 542, 9042, 4644, 3853, 4395, 6515, 5997, 2809, 5860, 1825, 8660, 4290, 2975, 8186, 2066, 7777, 7292, 3232, 3292, 8906, 7586, 8322, 4475, 3260, 8104, 7375, 1251, 3684, 6511, 1290, 7135, 3682, 1790, 3443, 7960, 4729, 5148, 9289, 802, 3466, 825, 1685, 8164, 6374, 4439, 3907, 7810, 6731, 3059, 7516, 5756, 244, 3411, 8736, 4915, 8738, 1642, 3083, 8399, 7247, 4631, 4352, 1244, 9129, 6709, 4374, 1595, 829, 8912, 790, 5861, 9064, 4806, 5548, 2047, 2989, 3277, 6392, 7702, 1215, 2102, 9288, 6042, 4070, 1502, 8927, 6235, 1824, 9350, 203, 2461, 3678, 9895, 6902, 7905, 8677, 5803, 541, 5719, 6056, 2767, 9459, 3665, 3021, 9322, 5288, 4508, 954, 3533, 8494, 4085, 1892, 6568, 2569, 2536, 4373, 5470, 3016, 5824, 225, 6449, 158, 1406, 6121, 944, 5156, 3902, 1678, 9224, 4224, 5559, 3775, 4544, 4773, 3240, 4620, 6887, 1580, 8, 8700, 8488, 357, 7691, 27, 9421, 8932, 5783, 1512, 7132, 7744, 554, 4371, 6321, 9374, 2222, 2059, 6299, 6205, 3739, 3118, 7041, 8215, 1764, 9337, 8095, 1404, 897, 8006, 7460, 9942, 6759, 536, 9138, 3613, 4517, 6435, 271, 3879, 297, 5023, 8561, 7347, 6099, 2612, 1006, 508, 6458, 5764, 3823, 9904, 2123, 7802, 4008, 418, 603, 36, 139, 5097, 9898, 1912, 8877, 8190, 9484, 3598, 6881, 780, 9198, 7710, 832, 8161, 5549, 7212, 2866, 8929, 6040, 2147, 7674, 9404, 3297, 8698, 9820, 5474, 5467, 7357, 8278, 9100, 5882, 3442, 671, 9982, 7241, 9391, 4078, 5697, 9209, 4693, 2247, 9638, 6869, 7958, 5658, 8621, 4242, 980, 8838, 1309, 4912, 2454, 9735, 3485, 2693, 8363, 4447, 2580, 2849, 9453, 4677, 8138, 4476, 8425, 3988, 4408, 1732, 59, 2923, 8355, 1048, 434, 4240, 7203, 6982, 1858, 1162, 4686, 881, 7637, 3741, 5279, 9734, 4876, 5854, 9792, 6785, 3400, 3942, 1853, 6095, 351, 4865, 5661, 1090, 7265, 8159, 2603, 7099, 7966, 6516, 5815, 8225, 13, 6074, 755, 2924, 4619, 7248, 415, 3900, 3320, 4750, 4971, 1855, 7704, 8475, 5994, 981, 5222, 4332, 872, 3360, 3952, 6466, 1236, 8553, 4932, 8393, 5137, 5693, 2139, 286, 8076, 6227, 2387, 9797, 787, 5544, 724, 1739, 4466, 2071, 2782, 119, 9687, 8272, 9839, 4504, 712, 4699, 3501, 7121, 5879, 7772, 7419, 9861, 2476, 9738, 4445, 245, 9504, 2994, 5520, 1521, 5874, 8450, 7611, 2209, 8585, 9030, 4262, 8051, 1831, 5403, 6880, 5631, 6580, 9715, 1364, 2148, 1112, 1478, 6722, 7062, 9220, 2100, 9086, 9424, 1949, 8022, 7112, 101, 9465, 2217, 4595, 4487, 209, 2161, 6122, 3834, 4924, 3368, 7391, 5487, 7745, 2646, 4565, 6103, 4049, 5647, 6537, 6967, 9292, 948, 4500, 2657, 65, 1997, 6816, 5892, 6027, 6751, 5305, 7353, 7076, 2429, 9463, 4423, 1019, 4450, 196, 5598, 5960, 3344, 3162, 8151, 7142, 3056, 5321, 6771, 2758, 862, 5221, 8928, 8342, 2268, 8809, 7995, 7924, 179, 8415, 6460, 2227, 262, 8280, 3323, 2760, 1327, 9253, 9868, 8957, 6765, 9691, 1274, 2120, 7411, 6707, 8254, 6649, 3757, 3343, 1859, 3023, 9928, 1271, 1035, 7385, 5204, 3786, 1208, 4643, 7128, 5217, 4799, 1866, 3982, 3981, 1783, 1646, 7502, 7453, 853, 2985, 4215, 8328, 9944, 681, 7564, 8703, 2640, 4492, 459, 452, 1959, 9483, 285, 7234, 143, 7840, 9730, 1473, 1908, 2500, 9062, 5607, 3374, 9774, 8240, 7945, 5738, 7276, 3657, 7164, 3302, 4091, 8911, 2076, 9258, 1585, 6240, 8579, 6911, 2168, 8084, 5597, 9765, 7415, 471, 2846, 8351, 818, 3471, 3649, 6241, 7295, 1038, 320, 4566, 2089, 5852, 6712, 3356, 445, 1633, 3609, 8428, 3243, 8286, 5146, 847, 1101, 4958, 2628, 9831, 2864, 4597, 2781, 6111, 8710, 5482, 9146, 7585, 8693, 1620, 1084, 6931, 3844, 6149, 999, 51, 731, 7758, 1470, 354, 5566, 3325, 4031, 2622, 9842, 9607, 7431, 9515, 9123, 1559, 7494, 1434, 3831, 6724, 9915, 1756, 2162, 8697, 2347, 8153, 1982, 389, 9986, 8969, 3005, 49, 3995, 2689, 1118, 9378, 6199, 5771, 1300, 801, 987, 2114, 8866, 5311, 4904, 9061, 4786, 9578, 130, 1636, 1841, 5110, 9208, 4102, 8692, 3516, 369, 1755, 6491, 4767, 3092, 5810, 2815, 3528, 2944, 3940, 6222, 7865, 5398, 6760, 4464, 4246, 7498, 1592, 2489, 4949, 9283, 2286, 7422, 946, 1079, 4960, 7294, 7786, 9628, 6645, 7278, 855, 7526, 3073, 933, 8999, 6108, 9267, 2029, 4312, 2240, 7002, 3259, 1622, 2497, 1601, 1983, 6377, 7733, 9674, 2414, 9637, 8778, 4499, 9615, 3380, 8459, 2154, 4827, 640, 2660, 4541, 1921, 3962, 5439, 8336, 2949, 3125, 3413, 8744, 1080, 4844, 159, 5426, 3559, 5057, 8584, 6281, 6084, 7285, 4690, 5744, 1008, 7565, 3507, 9603, 4542, 3100, 7890, 655, 1922, 1354, 8755, 8539, 8578, 8174, 2857, 2840, 930, 5723, 4186, 5707, 2988, 905, 7466, 2133, 3934, 3502, 9104, 7026, 4114, 2221, 812, 2736, 990, 2208, 3423, 3310, 2804, 2795, 7149, 1059, 5929, 3727, 963, 1822, 3936, 4669, 3075, 9803, 1036, 775, 9452, 6094, 4987, 3500, 8834, 5369, 5798, 1616, 7488, 1238, 7595, 5126, 6061, 2789, 8440, 3599, 410, 9648, 4079, 2412, 1737, 8613, 8327, 7380, 6885, 5651, 5365, 8031, 6833, 7384, 8463, 3067, 2533, 9471, 4141, 2490, 4131, 3120, 9284, 8984, 9917, 8291, 3008, 3538, 8602, 5041, 5552, 5013, 827, 3531, 6529, 6363, 6980, 1800, 2734, 4449, 6689, 7892, 4063, 1024, 6229, 5998, 916, 9199, 7309, 6916, 137, 9329, 1776, 8884, 8369, 4886, 9935, 7181, 4653, 2418, 5675, 3152, 4398, 1235, 291, 5740, 5714, 3888, 4272, 2604, 3319, 3349, 8130, 4481, 9719, 966, 9768, 6147, 8597, 154, 3547, 515, 372, 9039, 8392, 8837, 3566]\n",
            "4000\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWLG2Hmx6gTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e162272d-773d-4952-b99f-afba6b205abe"
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "global best_acc\n",
        "net.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#             progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "#                 % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "print(\"test accuracy \", correct/total , test_loss)\n",
        "test_acc.append(test_loss)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.3783 301.15457129478455\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}