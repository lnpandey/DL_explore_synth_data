{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type_4_Second_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\r\n",
        "mosaic_label = data[0][\"mosaic_label\"]\r\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,50)  #,self.output)\n",
        "        self.linear2 = nn.Linear(50,50)\n",
        "        self.linear3 = nn.Linear(50,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,50], dtype=torch.float64)   # number of features of output\n",
        "        features = torch.zeros([batch,self.K,50],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x = F.relu(x) \n",
        "      x = self.linear2(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.linear3(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,50)\n",
        "        #self.linear2 = nn.Linear(50,50)\n",
        "        self.linear2 = nn.Linear(50,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      #x = F.relu(self.linear2(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOfxUJZ_eFKw",
        "outputId": "5c08b09c-5028-427b-fbd0-6a97a6b43660"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(2,1,9,2).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(50,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.001)#,momentum=0.9)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.001)#,momentum=0.9)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 2500\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: [0 ] loss: 1.219\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.192\n",
            "epoch: [3] loss: 1.188\n",
            "epoch: [4] loss: 1.185\n",
            "epoch: [5] loss: 1.181\n",
            "epoch: [6] loss: 1.175\n",
            "epoch: [7] loss: 1.166\n",
            "epoch: [8] loss: 1.155\n",
            "epoch: [9] loss: 1.140\n",
            "epoch: [10] loss: 1.117\n",
            "epoch: [11] loss: 1.086\n",
            "epoch: [12] loss: 1.044\n",
            "epoch: [13] loss: 0.998\n",
            "epoch: [14] loss: 0.940\n",
            "epoch: [15] loss: 0.873\n",
            "epoch: [16] loss: 0.808\n",
            "epoch: [17] loss: 0.727\n",
            "epoch: [18] loss: 0.651\n",
            "epoch: [19] loss: 0.583\n",
            "epoch: [20] loss: 0.511\n",
            "epoch: [21] loss: 0.436\n",
            "epoch: [22] loss: 0.374\n",
            "epoch: [23] loss: 0.310\n",
            "epoch: [24] loss: 0.259\n",
            "epoch: [25] loss: 0.216\n",
            "epoch: [26] loss: 0.181\n",
            "epoch: [27] loss: 0.145\n",
            "epoch: [28] loss: 0.130\n",
            "epoch: [29] loss: 0.104\n",
            "epoch: [30] loss: 0.088\n",
            "epoch: [31] loss: 0.072\n",
            "epoch: [32] loss: 0.064\n",
            "epoch: [33] loss: 0.052\n",
            "epoch: [34] loss: 0.048\n",
            "epoch: [35] loss: 0.041\n",
            "epoch: [36] loss: 0.035\n",
            "epoch: [37] loss: 0.030\n",
            "epoch: [38] loss: 0.027\n",
            "epoch: [39] loss: 0.024\n",
            "epoch: [40] loss: 0.024\n",
            "epoch: [41] loss: 0.019\n",
            "epoch: [42] loss: 0.017\n",
            "epoch: [43] loss: 0.016\n",
            "epoch: [44] loss: 0.014\n",
            "epoch: [45] loss: 0.014\n",
            "epoch: [46] loss: 0.012\n",
            "epoch: [47] loss: 0.010\n",
            "epoch: [48] loss: 0.010\n",
            "epoch: [49] loss: 0.010\n",
            "epoch: [50] loss: 0.010\n",
            "Finished Training run 0\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.188\n",
            "epoch: [3] loss: 1.183\n",
            "epoch: [4] loss: 1.178\n",
            "epoch: [5] loss: 1.168\n",
            "epoch: [6] loss: 1.162\n",
            "epoch: [7] loss: 1.144\n",
            "epoch: [8] loss: 1.119\n",
            "epoch: [9] loss: 1.077\n",
            "epoch: [10] loss: 1.015\n",
            "epoch: [11] loss: 0.922\n",
            "epoch: [12] loss: 0.841\n",
            "epoch: [13] loss: 0.747\n",
            "epoch: [14] loss: 0.664\n",
            "epoch: [15] loss: 0.581\n",
            "epoch: [16] loss: 0.504\n",
            "epoch: [17] loss: 0.434\n",
            "epoch: [18] loss: 0.363\n",
            "epoch: [19] loss: 0.299\n",
            "epoch: [20] loss: 0.257\n",
            "epoch: [21] loss: 0.200\n",
            "epoch: [22] loss: 0.154\n",
            "epoch: [23] loss: 0.122\n",
            "epoch: [24] loss: 0.101\n",
            "epoch: [25] loss: 0.090\n",
            "epoch: [26] loss: 0.065\n",
            "epoch: [27] loss: 0.054\n",
            "epoch: [28] loss: 0.045\n",
            "epoch: [29] loss: 0.039\n",
            "epoch: [30] loss: 0.033\n",
            "epoch: [31] loss: 0.027\n",
            "epoch: [32] loss: 0.024\n",
            "epoch: [33] loss: 0.040\n",
            "epoch: [34] loss: 0.024\n",
            "epoch: [35] loss: 0.017\n",
            "epoch: [36] loss: 0.014\n",
            "epoch: [37] loss: 0.012\n",
            "epoch: [38] loss: 0.011\n",
            "epoch: [39] loss: 0.011\n",
            "epoch: [40] loss: 0.009\n",
            "Finished Training run 1\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.221\n",
            "epoch: [1] loss: 1.197\n",
            "epoch: [2] loss: 1.193\n",
            "epoch: [3] loss: 1.190\n",
            "epoch: [4] loss: 1.188\n",
            "epoch: [5] loss: 1.182\n",
            "epoch: [6] loss: 1.175\n",
            "epoch: [7] loss: 1.169\n",
            "epoch: [8] loss: 1.158\n",
            "epoch: [9] loss: 1.144\n",
            "epoch: [10] loss: 1.124\n",
            "epoch: [11] loss: 1.090\n",
            "epoch: [12] loss: 1.045\n",
            "epoch: [13] loss: 0.982\n",
            "epoch: [14] loss: 0.918\n",
            "epoch: [15] loss: 0.835\n",
            "epoch: [16] loss: 0.750\n",
            "epoch: [17] loss: 0.680\n",
            "epoch: [18] loss: 0.625\n",
            "epoch: [19] loss: 0.573\n",
            "epoch: [20] loss: 0.528\n",
            "epoch: [21] loss: 0.475\n",
            "epoch: [22] loss: 0.426\n",
            "epoch: [23] loss: 0.384\n",
            "epoch: [24] loss: 0.320\n",
            "epoch: [25] loss: 0.265\n",
            "epoch: [26] loss: 0.220\n",
            "epoch: [27] loss: 0.177\n",
            "epoch: [28] loss: 0.141\n",
            "epoch: [29] loss: 0.118\n",
            "epoch: [30] loss: 0.095\n",
            "epoch: [31] loss: 0.102\n",
            "epoch: [32] loss: 0.066\n",
            "epoch: [33] loss: 0.057\n",
            "epoch: [34] loss: 0.047\n",
            "epoch: [35] loss: 0.043\n",
            "epoch: [36] loss: 0.035\n",
            "epoch: [37] loss: 0.030\n",
            "epoch: [38] loss: 0.029\n",
            "epoch: [39] loss: 0.023\n",
            "epoch: [40] loss: 0.021\n",
            "epoch: [41] loss: 0.021\n",
            "epoch: [42] loss: 0.020\n",
            "epoch: [43] loss: 0.020\n",
            "epoch: [44] loss: 0.015\n",
            "epoch: [45] loss: 0.013\n",
            "epoch: [46] loss: 0.012\n",
            "epoch: [47] loss: 0.011\n",
            "epoch: [48] loss: 0.010\n",
            "Finished Training run 2\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.193\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.186\n",
            "epoch: [4] loss: 1.180\n",
            "epoch: [5] loss: 1.176\n",
            "epoch: [6] loss: 1.169\n",
            "epoch: [7] loss: 1.160\n",
            "epoch: [8] loss: 1.153\n",
            "epoch: [9] loss: 1.133\n",
            "epoch: [10] loss: 1.115\n",
            "epoch: [11] loss: 1.087\n",
            "epoch: [12] loss: 1.043\n",
            "epoch: [13] loss: 1.014\n",
            "epoch: [14] loss: 0.945\n",
            "epoch: [15] loss: 0.881\n",
            "epoch: [16] loss: 0.811\n",
            "epoch: [17] loss: 0.729\n",
            "epoch: [18] loss: 0.662\n",
            "epoch: [19] loss: 0.606\n",
            "epoch: [20] loss: 0.533\n",
            "epoch: [21] loss: 0.462\n",
            "epoch: [22] loss: 0.419\n",
            "epoch: [23] loss: 0.322\n",
            "epoch: [24] loss: 0.260\n",
            "epoch: [25] loss: 0.215\n",
            "epoch: [26] loss: 0.167\n",
            "epoch: [27] loss: 0.149\n",
            "epoch: [28] loss: 0.117\n",
            "epoch: [29] loss: 0.094\n",
            "epoch: [30] loss: 0.089\n",
            "epoch: [31] loss: 0.081\n",
            "epoch: [32] loss: 0.061\n",
            "epoch: [33] loss: 0.056\n",
            "epoch: [34] loss: 0.048\n",
            "epoch: [35] loss: 0.044\n",
            "epoch: [36] loss: 0.039\n",
            "epoch: [37] loss: 0.036\n",
            "epoch: [38] loss: 0.033\n",
            "epoch: [39] loss: 0.029\n",
            "epoch: [40] loss: 0.027\n",
            "epoch: [41] loss: 0.024\n",
            "epoch: [42] loss: 0.026\n",
            "epoch: [43] loss: 0.021\n",
            "epoch: [44] loss: 0.021\n",
            "epoch: [45] loss: 0.019\n",
            "epoch: [46] loss: 0.018\n",
            "epoch: [47] loss: 0.019\n",
            "epoch: [48] loss: 0.015\n",
            "epoch: [49] loss: 0.014\n",
            "epoch: [50] loss: 0.015\n",
            "epoch: [51] loss: 0.012\n",
            "epoch: [52] loss: 0.011\n",
            "epoch: [53] loss: 0.011\n",
            "epoch: [54] loss: 0.010\n",
            "Finished Training run 3\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.185\n",
            "epoch: [3] loss: 1.178\n",
            "epoch: [4] loss: 1.166\n",
            "epoch: [5] loss: 1.152\n",
            "epoch: [6] loss: 1.129\n",
            "epoch: [7] loss: 1.093\n",
            "epoch: [8] loss: 1.043\n",
            "epoch: [9] loss: 0.972\n",
            "epoch: [10] loss: 0.891\n",
            "epoch: [11] loss: 0.798\n",
            "epoch: [12] loss: 0.724\n",
            "epoch: [13] loss: 0.648\n",
            "epoch: [14] loss: 0.609\n",
            "epoch: [15] loss: 0.536\n",
            "epoch: [16] loss: 0.460\n",
            "epoch: [17] loss: 0.386\n",
            "epoch: [18] loss: 0.321\n",
            "epoch: [19] loss: 0.269\n",
            "epoch: [20] loss: 0.222\n",
            "epoch: [21] loss: 0.191\n",
            "epoch: [22] loss: 0.165\n",
            "epoch: [23] loss: 0.138\n",
            "epoch: [24] loss: 0.136\n",
            "epoch: [25] loss: 0.100\n",
            "epoch: [26] loss: 0.104\n",
            "epoch: [27] loss: 0.086\n",
            "epoch: [28] loss: 0.074\n",
            "epoch: [29] loss: 0.064\n",
            "epoch: [30] loss: 0.055\n",
            "epoch: [31] loss: 0.051\n",
            "epoch: [32] loss: 0.044\n",
            "epoch: [33] loss: 0.041\n",
            "epoch: [34] loss: 0.044\n",
            "epoch: [35] loss: 0.049\n",
            "epoch: [36] loss: 0.037\n",
            "epoch: [37] loss: 0.030\n",
            "epoch: [38] loss: 0.029\n",
            "epoch: [39] loss: 0.026\n",
            "epoch: [40] loss: 0.024\n",
            "epoch: [41] loss: 0.025\n",
            "epoch: [42] loss: 0.020\n",
            "epoch: [43] loss: 0.019\n",
            "epoch: [44] loss: 0.020\n",
            "epoch: [45] loss: 0.017\n",
            "epoch: [46] loss: 0.019\n",
            "epoch: [47] loss: 0.015\n",
            "epoch: [48] loss: 0.016\n",
            "epoch: [49] loss: 0.017\n",
            "epoch: [50] loss: 0.013\n",
            "epoch: [51] loss: 0.014\n",
            "epoch: [52] loss: 0.010\n",
            "epoch: [53] loss: 0.011\n",
            "epoch: [54] loss: 0.010\n",
            "Finished Training run 4\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.196\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.190\n",
            "epoch: [3] loss: 1.186\n",
            "epoch: [4] loss: 1.181\n",
            "epoch: [5] loss: 1.173\n",
            "epoch: [6] loss: 1.160\n",
            "epoch: [7] loss: 1.136\n",
            "epoch: [8] loss: 1.095\n",
            "epoch: [9] loss: 1.041\n",
            "epoch: [10] loss: 0.967\n",
            "epoch: [11] loss: 0.888\n",
            "epoch: [12] loss: 0.800\n",
            "epoch: [13] loss: 0.711\n",
            "epoch: [14] loss: 0.625\n",
            "epoch: [15] loss: 0.537\n",
            "epoch: [16] loss: 0.439\n",
            "epoch: [17] loss: 0.358\n",
            "epoch: [18] loss: 0.280\n",
            "epoch: [19] loss: 0.223\n",
            "epoch: [20] loss: 0.186\n",
            "epoch: [21] loss: 0.146\n",
            "epoch: [22] loss: 0.123\n",
            "epoch: [23] loss: 0.112\n",
            "epoch: [24] loss: 0.089\n",
            "epoch: [25] loss: 0.075\n",
            "epoch: [26] loss: 0.064\n",
            "epoch: [27] loss: 0.057\n",
            "epoch: [28] loss: 0.064\n",
            "epoch: [29] loss: 0.049\n",
            "epoch: [30] loss: 0.040\n",
            "epoch: [31] loss: 0.040\n",
            "epoch: [32] loss: 0.034\n",
            "epoch: [33] loss: 0.032\n",
            "epoch: [34] loss: 0.028\n",
            "epoch: [35] loss: 0.023\n",
            "epoch: [36] loss: 0.021\n",
            "epoch: [37] loss: 0.019\n",
            "epoch: [38] loss: 0.018\n",
            "epoch: [39] loss: 0.023\n",
            "epoch: [40] loss: 0.019\n",
            "epoch: [41] loss: 0.018\n",
            "epoch: [42] loss: 0.013\n",
            "epoch: [43] loss: 0.014\n",
            "epoch: [44] loss: 0.012\n",
            "epoch: [45] loss: 0.010\n",
            "epoch: [46] loss: 0.010\n",
            "Finished Training run 5\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.205\n",
            "epoch: [1] loss: 1.193\n",
            "epoch: [2] loss: 1.190\n",
            "epoch: [3] loss: 1.188\n",
            "epoch: [4] loss: 1.181\n",
            "epoch: [5] loss: 1.174\n",
            "epoch: [6] loss: 1.161\n",
            "epoch: [7] loss: 1.144\n",
            "epoch: [8] loss: 1.118\n",
            "epoch: [9] loss: 1.074\n",
            "epoch: [10] loss: 1.012\n",
            "epoch: [11] loss: 0.938\n",
            "epoch: [12] loss: 0.866\n",
            "epoch: [13] loss: 0.794\n",
            "epoch: [14] loss: 0.742\n",
            "epoch: [15] loss: 0.666\n",
            "epoch: [16] loss: 0.618\n",
            "epoch: [17] loss: 0.575\n",
            "epoch: [18] loss: 0.534\n",
            "epoch: [19] loss: 0.495\n",
            "epoch: [20] loss: 0.440\n",
            "epoch: [21] loss: 0.403\n",
            "epoch: [22] loss: 0.365\n",
            "epoch: [23] loss: 0.305\n",
            "epoch: [24] loss: 0.264\n",
            "epoch: [25] loss: 0.233\n",
            "epoch: [26] loss: 0.197\n",
            "epoch: [27] loss: 0.174\n",
            "epoch: [28] loss: 0.155\n",
            "epoch: [29] loss: 0.133\n",
            "epoch: [30] loss: 0.118\n",
            "epoch: [31] loss: 0.106\n",
            "epoch: [32] loss: 0.101\n",
            "epoch: [33] loss: 0.093\n",
            "epoch: [34] loss: 0.075\n",
            "epoch: [35] loss: 0.068\n",
            "epoch: [36] loss: 0.063\n",
            "epoch: [37] loss: 0.065\n",
            "epoch: [38] loss: 0.059\n",
            "epoch: [39] loss: 0.046\n",
            "epoch: [40] loss: 0.040\n",
            "epoch: [41] loss: 0.040\n",
            "epoch: [42] loss: 0.036\n",
            "epoch: [43] loss: 0.034\n",
            "epoch: [44] loss: 0.035\n",
            "epoch: [45] loss: 0.028\n",
            "epoch: [46] loss: 0.024\n",
            "epoch: [47] loss: 0.026\n",
            "epoch: [48] loss: 0.023\n",
            "epoch: [49] loss: 0.021\n",
            "epoch: [50] loss: 0.018\n",
            "epoch: [51] loss: 0.019\n",
            "epoch: [52] loss: 0.019\n",
            "epoch: [53] loss: 0.015\n",
            "epoch: [54] loss: 0.015\n",
            "epoch: [55] loss: 0.015\n",
            "epoch: [56] loss: 0.015\n",
            "epoch: [57] loss: 0.012\n",
            "epoch: [58] loss: 0.010\n",
            "Finished Training run 6\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.211\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.192\n",
            "epoch: [3] loss: 1.189\n",
            "epoch: [4] loss: 1.186\n",
            "epoch: [5] loss: 1.181\n",
            "epoch: [6] loss: 1.174\n",
            "epoch: [7] loss: 1.166\n",
            "epoch: [8] loss: 1.167\n",
            "epoch: [9] loss: 1.139\n",
            "epoch: [10] loss: 1.114\n",
            "epoch: [11] loss: 1.086\n",
            "epoch: [12] loss: 1.028\n",
            "epoch: [13] loss: 0.961\n",
            "epoch: [14] loss: 0.885\n",
            "epoch: [15] loss: 0.805\n",
            "epoch: [16] loss: 0.725\n",
            "epoch: [17] loss: 0.653\n",
            "epoch: [18] loss: 0.577\n",
            "epoch: [19] loss: 0.501\n",
            "epoch: [20] loss: 0.411\n",
            "epoch: [21] loss: 0.338\n",
            "epoch: [22] loss: 0.262\n",
            "epoch: [23] loss: 0.207\n",
            "epoch: [24] loss: 0.172\n",
            "epoch: [25] loss: 0.133\n",
            "epoch: [26] loss: 0.102\n",
            "epoch: [27] loss: 0.087\n",
            "epoch: [28] loss: 0.085\n",
            "epoch: [29] loss: 0.060\n",
            "epoch: [30] loss: 0.053\n",
            "epoch: [31] loss: 0.044\n",
            "epoch: [32] loss: 0.039\n",
            "epoch: [33] loss: 0.033\n",
            "epoch: [34] loss: 0.034\n",
            "epoch: [35] loss: 0.026\n",
            "epoch: [36] loss: 0.022\n",
            "epoch: [37] loss: 0.020\n",
            "epoch: [38] loss: 0.020\n",
            "epoch: [39] loss: 0.017\n",
            "epoch: [40] loss: 0.015\n",
            "epoch: [41] loss: 0.014\n",
            "epoch: [42] loss: 0.014\n",
            "epoch: [43] loss: 0.014\n",
            "epoch: [44] loss: 0.013\n",
            "epoch: [45] loss: 0.012\n",
            "epoch: [46] loss: 0.010\n",
            "epoch: [47] loss: 0.009\n",
            "Finished Training run 7\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.191\n",
            "epoch: [2] loss: 1.186\n",
            "epoch: [3] loss: 1.181\n",
            "epoch: [4] loss: 1.172\n",
            "epoch: [5] loss: 1.160\n",
            "epoch: [6] loss: 1.149\n",
            "epoch: [7] loss: 1.128\n",
            "epoch: [8] loss: 1.103\n",
            "epoch: [9] loss: 1.067\n",
            "epoch: [10] loss: 1.036\n",
            "epoch: [11] loss: 0.998\n",
            "epoch: [12] loss: 0.941\n",
            "epoch: [13] loss: 0.882\n",
            "epoch: [14] loss: 0.815\n",
            "epoch: [15] loss: 0.747\n",
            "epoch: [16] loss: 0.682\n",
            "epoch: [17] loss: 0.615\n",
            "epoch: [18] loss: 0.565\n",
            "epoch: [19] loss: 0.493\n",
            "epoch: [20] loss: 0.410\n",
            "epoch: [21] loss: 0.340\n",
            "epoch: [22] loss: 0.264\n",
            "epoch: [23] loss: 0.210\n",
            "epoch: [24] loss: 0.169\n",
            "epoch: [25] loss: 0.137\n",
            "epoch: [26] loss: 0.107\n",
            "epoch: [27] loss: 0.089\n",
            "epoch: [28] loss: 0.077\n",
            "epoch: [29] loss: 0.063\n",
            "epoch: [30] loss: 0.054\n",
            "epoch: [31] loss: 0.048\n",
            "epoch: [32] loss: 0.040\n",
            "epoch: [33] loss: 0.036\n",
            "epoch: [34] loss: 0.032\n",
            "epoch: [35] loss: 0.028\n",
            "epoch: [36] loss: 0.024\n",
            "epoch: [37] loss: 0.022\n",
            "epoch: [38] loss: 0.023\n",
            "epoch: [39] loss: 0.020\n",
            "epoch: [40] loss: 0.015\n",
            "epoch: [41] loss: 0.014\n",
            "epoch: [42] loss: 0.013\n",
            "epoch: [43] loss: 0.011\n",
            "epoch: [44] loss: 0.010\n",
            "epoch: [45] loss: 0.010\n",
            "Finished Training run 8\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.215\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.187\n",
            "epoch: [3] loss: 1.183\n",
            "epoch: [4] loss: 1.176\n",
            "epoch: [5] loss: 1.166\n",
            "epoch: [6] loss: 1.154\n",
            "epoch: [7] loss: 1.136\n",
            "epoch: [8] loss: 1.104\n",
            "epoch: [9] loss: 1.062\n",
            "epoch: [10] loss: 0.993\n",
            "epoch: [11] loss: 0.933\n",
            "epoch: [12] loss: 0.865\n",
            "epoch: [13] loss: 0.776\n",
            "epoch: [14] loss: 0.668\n",
            "epoch: [15] loss: 0.557\n",
            "epoch: [16] loss: 0.443\n",
            "epoch: [17] loss: 0.331\n",
            "epoch: [18] loss: 0.249\n",
            "epoch: [19] loss: 0.205\n",
            "epoch: [20] loss: 0.158\n",
            "epoch: [21] loss: 0.135\n",
            "epoch: [22] loss: 0.104\n",
            "epoch: [23] loss: 0.088\n",
            "epoch: [24] loss: 0.095\n",
            "epoch: [25] loss: 0.078\n",
            "epoch: [26] loss: 0.062\n",
            "epoch: [27] loss: 0.056\n",
            "epoch: [28] loss: 0.051\n",
            "epoch: [29] loss: 0.042\n",
            "epoch: [30] loss: 0.035\n",
            "epoch: [31] loss: 0.032\n",
            "epoch: [32] loss: 0.031\n",
            "epoch: [33] loss: 0.027\n",
            "epoch: [34] loss: 0.026\n",
            "epoch: [35] loss: 0.024\n",
            "epoch: [36] loss: 0.019\n",
            "epoch: [37] loss: 0.025\n",
            "epoch: [38] loss: 0.017\n",
            "epoch: [39] loss: 0.015\n",
            "epoch: [40] loss: 0.014\n",
            "epoch: [41] loss: 0.014\n",
            "epoch: [42] loss: 0.012\n",
            "epoch: [43] loss: 0.013\n",
            "epoch: [44] loss: 0.018\n",
            "epoch: [45] loss: 0.010\n",
            "epoch: [46] loss: 0.008\n",
            "Finished Training run 9\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.226\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.186\n",
            "epoch: [3] loss: 1.179\n",
            "epoch: [4] loss: 1.174\n",
            "epoch: [5] loss: 1.165\n",
            "epoch: [6] loss: 1.154\n",
            "epoch: [7] loss: 1.133\n",
            "epoch: [8] loss: 1.102\n",
            "epoch: [9] loss: 1.061\n",
            "epoch: [10] loss: 1.004\n",
            "epoch: [11] loss: 0.950\n",
            "epoch: [12] loss: 0.882\n",
            "epoch: [13] loss: 0.809\n",
            "epoch: [14] loss: 0.738\n",
            "epoch: [15] loss: 0.671\n",
            "epoch: [16] loss: 0.616\n",
            "epoch: [17] loss: 0.576\n",
            "epoch: [18] loss: 0.535\n",
            "epoch: [19] loss: 0.508\n",
            "epoch: [20] loss: 0.467\n",
            "epoch: [21] loss: 0.427\n",
            "epoch: [22] loss: 0.386\n",
            "epoch: [23] loss: 0.334\n",
            "epoch: [24] loss: 0.287\n",
            "epoch: [25] loss: 0.245\n",
            "epoch: [26] loss: 0.191\n",
            "epoch: [27] loss: 0.148\n",
            "epoch: [28] loss: 0.118\n",
            "epoch: [29] loss: 0.096\n",
            "epoch: [30] loss: 0.099\n",
            "epoch: [31] loss: 0.071\n",
            "epoch: [32] loss: 0.060\n",
            "epoch: [33] loss: 0.052\n",
            "epoch: [34] loss: 0.048\n",
            "epoch: [35] loss: 0.051\n",
            "epoch: [36] loss: 0.039\n",
            "epoch: [37] loss: 0.031\n",
            "epoch: [38] loss: 0.029\n",
            "epoch: [39] loss: 0.027\n",
            "epoch: [40] loss: 0.021\n",
            "epoch: [41] loss: 0.020\n",
            "epoch: [42] loss: 0.018\n",
            "epoch: [43] loss: 0.016\n",
            "epoch: [44] loss: 0.029\n",
            "epoch: [45] loss: 0.014\n",
            "epoch: [46] loss: 0.013\n",
            "epoch: [47] loss: 0.010\n",
            "epoch: [48] loss: 0.010\n",
            "Finished Training run 10\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.213\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.186\n",
            "epoch: [3] loss: 1.180\n",
            "epoch: [4] loss: 1.173\n",
            "epoch: [5] loss: 1.166\n",
            "epoch: [6] loss: 1.150\n",
            "epoch: [7] loss: 1.139\n",
            "epoch: [8] loss: 1.113\n",
            "epoch: [9] loss: 1.084\n",
            "epoch: [10] loss: 1.041\n",
            "epoch: [11] loss: 0.995\n",
            "epoch: [12] loss: 0.943\n",
            "epoch: [13] loss: 0.874\n",
            "epoch: [14] loss: 0.809\n",
            "epoch: [15] loss: 0.743\n",
            "epoch: [16] loss: 0.661\n",
            "epoch: [17] loss: 0.586\n",
            "epoch: [18] loss: 0.494\n",
            "epoch: [19] loss: 0.393\n",
            "epoch: [20] loss: 0.326\n",
            "epoch: [21] loss: 0.250\n",
            "epoch: [22] loss: 0.197\n",
            "epoch: [23] loss: 0.160\n",
            "epoch: [24] loss: 0.142\n",
            "epoch: [25] loss: 0.108\n",
            "epoch: [26] loss: 0.090\n",
            "epoch: [27] loss: 0.078\n",
            "epoch: [28] loss: 0.065\n",
            "epoch: [29] loss: 0.057\n",
            "epoch: [30] loss: 0.048\n",
            "epoch: [31] loss: 0.043\n",
            "epoch: [32] loss: 0.038\n",
            "epoch: [33] loss: 0.032\n",
            "epoch: [34] loss: 0.028\n",
            "epoch: [35] loss: 0.028\n",
            "epoch: [36] loss: 0.022\n",
            "epoch: [37] loss: 0.020\n",
            "epoch: [38] loss: 0.018\n",
            "epoch: [39] loss: 0.023\n",
            "epoch: [40] loss: 0.018\n",
            "epoch: [41] loss: 0.013\n",
            "epoch: [42] loss: 0.012\n",
            "epoch: [43] loss: 0.010\n",
            "epoch: [44] loss: 0.012\n",
            "epoch: [45] loss: 0.009\n",
            "Finished Training run 11\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.227\n",
            "epoch: [1] loss: 1.197\n",
            "epoch: [2] loss: 1.190\n",
            "epoch: [3] loss: 1.186\n",
            "epoch: [4] loss: 1.178\n",
            "epoch: [5] loss: 1.168\n",
            "epoch: [6] loss: 1.154\n",
            "epoch: [7] loss: 1.140\n",
            "epoch: [8] loss: 1.120\n",
            "epoch: [9] loss: 1.097\n",
            "epoch: [10] loss: 1.039\n",
            "epoch: [11] loss: 0.972\n",
            "epoch: [12] loss: 0.883\n",
            "epoch: [13] loss: 0.784\n",
            "epoch: [14] loss: 0.661\n",
            "epoch: [15] loss: 0.533\n",
            "epoch: [16] loss: 0.408\n",
            "epoch: [17] loss: 0.328\n",
            "epoch: [18] loss: 0.259\n",
            "epoch: [19] loss: 0.205\n",
            "epoch: [20] loss: 0.169\n",
            "epoch: [21] loss: 0.135\n",
            "epoch: [22] loss: 0.113\n",
            "epoch: [23] loss: 0.096\n",
            "epoch: [24] loss: 0.082\n",
            "epoch: [25] loss: 0.075\n",
            "epoch: [26] loss: 0.063\n",
            "epoch: [27] loss: 0.051\n",
            "epoch: [28] loss: 0.048\n",
            "epoch: [29] loss: 0.052\n",
            "epoch: [30] loss: 0.039\n",
            "epoch: [31] loss: 0.031\n",
            "epoch: [32] loss: 0.029\n",
            "epoch: [33] loss: 0.028\n",
            "epoch: [34] loss: 0.023\n",
            "epoch: [35] loss: 0.029\n",
            "epoch: [36] loss: 0.018\n",
            "epoch: [37] loss: 0.016\n",
            "epoch: [38] loss: 0.015\n",
            "epoch: [39] loss: 0.015\n",
            "epoch: [40] loss: 0.013\n",
            "epoch: [41] loss: 0.012\n",
            "epoch: [42] loss: 0.011\n",
            "epoch: [43] loss: 0.012\n",
            "epoch: [44] loss: 0.009\n",
            "Finished Training run 12\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.187\n",
            "epoch: [2] loss: 1.177\n",
            "epoch: [3] loss: 1.163\n",
            "epoch: [4] loss: 1.143\n",
            "epoch: [5] loss: 1.118\n",
            "epoch: [6] loss: 1.075\n",
            "epoch: [7] loss: 1.028\n",
            "epoch: [8] loss: 0.980\n",
            "epoch: [9] loss: 0.919\n",
            "epoch: [10] loss: 0.850\n",
            "epoch: [11] loss: 0.774\n",
            "epoch: [12] loss: 0.697\n",
            "epoch: [13] loss: 0.609\n",
            "epoch: [14] loss: 0.520\n",
            "epoch: [15] loss: 0.443\n",
            "epoch: [16] loss: 0.384\n",
            "epoch: [17] loss: 0.309\n",
            "epoch: [18] loss: 0.261\n",
            "epoch: [19] loss: 0.224\n",
            "epoch: [20] loss: 0.185\n",
            "epoch: [21] loss: 0.154\n",
            "epoch: [22] loss: 0.134\n",
            "epoch: [23] loss: 0.130\n",
            "epoch: [24] loss: 0.117\n",
            "epoch: [25] loss: 0.111\n",
            "epoch: [26] loss: 0.080\n",
            "epoch: [27] loss: 0.070\n",
            "epoch: [28] loss: 0.065\n",
            "epoch: [29] loss: 0.060\n",
            "epoch: [30] loss: 0.050\n",
            "epoch: [31] loss: 0.048\n",
            "epoch: [32] loss: 0.041\n",
            "epoch: [33] loss: 0.037\n",
            "epoch: [34] loss: 0.036\n",
            "epoch: [35] loss: 0.036\n",
            "epoch: [36] loss: 0.030\n",
            "epoch: [37] loss: 0.032\n",
            "epoch: [38] loss: 0.025\n",
            "epoch: [39] loss: 0.022\n",
            "epoch: [40] loss: 0.023\n",
            "epoch: [41] loss: 0.022\n",
            "epoch: [42] loss: 0.017\n",
            "epoch: [43] loss: 0.017\n",
            "epoch: [44] loss: 0.015\n",
            "epoch: [45] loss: 0.013\n",
            "epoch: [46] loss: 0.014\n",
            "epoch: [47] loss: 0.011\n",
            "epoch: [48] loss: 0.012\n",
            "epoch: [49] loss: 0.010\n",
            "Finished Training run 13\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.215\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.190\n",
            "epoch: [3] loss: 1.185\n",
            "epoch: [4] loss: 1.180\n",
            "epoch: [5] loss: 1.173\n",
            "epoch: [6] loss: 1.161\n",
            "epoch: [7] loss: 1.154\n",
            "epoch: [8] loss: 1.129\n",
            "epoch: [9] loss: 1.101\n",
            "epoch: [10] loss: 1.067\n",
            "epoch: [11] loss: 1.012\n",
            "epoch: [12] loss: 0.955\n",
            "epoch: [13] loss: 0.883\n",
            "epoch: [14] loss: 0.798\n",
            "epoch: [15] loss: 0.713\n",
            "epoch: [16] loss: 0.639\n",
            "epoch: [17] loss: 0.560\n",
            "epoch: [18] loss: 0.490\n",
            "epoch: [19] loss: 0.410\n",
            "epoch: [20] loss: 0.333\n",
            "epoch: [21] loss: 0.261\n",
            "epoch: [22] loss: 0.208\n",
            "epoch: [23] loss: 0.150\n",
            "epoch: [24] loss: 0.115\n",
            "epoch: [25] loss: 0.091\n",
            "epoch: [26] loss: 0.073\n",
            "epoch: [27] loss: 0.061\n",
            "epoch: [28] loss: 0.052\n",
            "epoch: [29] loss: 0.046\n",
            "epoch: [30] loss: 0.039\n",
            "epoch: [31] loss: 0.032\n",
            "epoch: [32] loss: 0.029\n",
            "epoch: [33] loss: 0.025\n",
            "epoch: [34] loss: 0.024\n",
            "epoch: [35] loss: 0.025\n",
            "epoch: [36] loss: 0.021\n",
            "epoch: [37] loss: 0.016\n",
            "epoch: [38] loss: 0.015\n",
            "epoch: [39] loss: 0.013\n",
            "epoch: [40] loss: 0.011\n",
            "epoch: [41] loss: 0.010\n",
            "epoch: [42] loss: 0.010\n",
            "Finished Training run 14\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.199\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.182\n",
            "epoch: [4] loss: 1.176\n",
            "epoch: [5] loss: 1.161\n",
            "epoch: [6] loss: 1.147\n",
            "epoch: [7] loss: 1.120\n",
            "epoch: [8] loss: 1.090\n",
            "epoch: [9] loss: 1.044\n",
            "epoch: [10] loss: 0.987\n",
            "epoch: [11] loss: 0.915\n",
            "epoch: [12] loss: 0.839\n",
            "epoch: [13] loss: 0.752\n",
            "epoch: [14] loss: 0.675\n",
            "epoch: [15] loss: 0.610\n",
            "epoch: [16] loss: 0.559\n",
            "epoch: [17] loss: 0.500\n",
            "epoch: [18] loss: 0.442\n",
            "epoch: [19] loss: 0.389\n",
            "epoch: [20] loss: 0.335\n",
            "epoch: [21] loss: 0.277\n",
            "epoch: [22] loss: 0.218\n",
            "epoch: [23] loss: 0.178\n",
            "epoch: [24] loss: 0.148\n",
            "epoch: [25] loss: 0.116\n",
            "epoch: [26] loss: 0.115\n",
            "epoch: [27] loss: 0.075\n",
            "epoch: [28] loss: 0.063\n",
            "epoch: [29] loss: 0.051\n",
            "epoch: [30] loss: 0.043\n",
            "epoch: [31] loss: 0.033\n",
            "epoch: [32] loss: 0.028\n",
            "epoch: [33] loss: 0.026\n",
            "epoch: [34] loss: 0.023\n",
            "epoch: [35] loss: 0.018\n",
            "epoch: [36] loss: 0.015\n",
            "epoch: [37] loss: 0.013\n",
            "epoch: [38] loss: 0.012\n",
            "epoch: [39] loss: 0.011\n",
            "epoch: [40] loss: 0.010\n",
            "Finished Training run 15\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.201\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.191\n",
            "epoch: [3] loss: 1.185\n",
            "epoch: [4] loss: 1.179\n",
            "epoch: [5] loss: 1.170\n",
            "epoch: [6] loss: 1.161\n",
            "epoch: [7] loss: 1.141\n",
            "epoch: [8] loss: 1.116\n",
            "epoch: [9] loss: 1.077\n",
            "epoch: [10] loss: 1.020\n",
            "epoch: [11] loss: 0.959\n",
            "epoch: [12] loss: 0.892\n",
            "epoch: [13] loss: 0.810\n",
            "epoch: [14] loss: 0.718\n",
            "epoch: [15] loss: 0.629\n",
            "epoch: [16] loss: 0.575\n",
            "epoch: [17] loss: 0.447\n",
            "epoch: [18] loss: 0.372\n",
            "epoch: [19] loss: 0.304\n",
            "epoch: [20] loss: 0.250\n",
            "epoch: [21] loss: 0.195\n",
            "epoch: [22] loss: 0.153\n",
            "epoch: [23] loss: 0.123\n",
            "epoch: [24] loss: 0.102\n",
            "epoch: [25] loss: 0.082\n",
            "epoch: [26] loss: 0.077\n",
            "epoch: [27] loss: 0.057\n",
            "epoch: [28] loss: 0.048\n",
            "epoch: [29] loss: 0.041\n",
            "epoch: [30] loss: 0.034\n",
            "epoch: [31] loss: 0.035\n",
            "epoch: [32] loss: 0.027\n",
            "epoch: [33] loss: 0.029\n",
            "epoch: [34] loss: 0.021\n",
            "epoch: [35] loss: 0.018\n",
            "epoch: [36] loss: 0.017\n",
            "epoch: [37] loss: 0.015\n",
            "epoch: [38] loss: 0.013\n",
            "epoch: [39] loss: 0.012\n",
            "epoch: [40] loss: 0.011\n",
            "epoch: [41] loss: 0.010\n",
            "epoch: [42] loss: 0.009\n",
            "Finished Training run 16\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.201\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.192\n",
            "epoch: [3] loss: 1.190\n",
            "epoch: [4] loss: 1.182\n",
            "epoch: [5] loss: 1.176\n",
            "epoch: [6] loss: 1.163\n",
            "epoch: [7] loss: 1.145\n",
            "epoch: [8] loss: 1.115\n",
            "epoch: [9] loss: 1.068\n",
            "epoch: [10] loss: 0.998\n",
            "epoch: [11] loss: 0.911\n",
            "epoch: [12] loss: 0.811\n",
            "epoch: [13] loss: 0.728\n",
            "epoch: [14] loss: 0.638\n",
            "epoch: [15] loss: 0.566\n",
            "epoch: [16] loss: 0.495\n",
            "epoch: [17] loss: 0.415\n",
            "epoch: [18] loss: 0.337\n",
            "epoch: [19] loss: 0.261\n",
            "epoch: [20] loss: 0.213\n",
            "epoch: [21] loss: 0.167\n",
            "epoch: [22] loss: 0.135\n",
            "epoch: [23] loss: 0.123\n",
            "epoch: [24] loss: 0.094\n",
            "epoch: [25] loss: 0.081\n",
            "epoch: [26] loss: 0.067\n",
            "epoch: [27] loss: 0.057\n",
            "epoch: [28] loss: 0.052\n",
            "epoch: [29] loss: 0.045\n",
            "epoch: [30] loss: 0.037\n",
            "epoch: [31] loss: 0.034\n",
            "epoch: [32] loss: 0.029\n",
            "epoch: [33] loss: 0.026\n",
            "epoch: [34] loss: 0.023\n",
            "epoch: [35] loss: 0.021\n",
            "epoch: [36] loss: 0.019\n",
            "epoch: [37] loss: 0.018\n",
            "epoch: [38] loss: 0.015\n",
            "epoch: [39] loss: 0.014\n",
            "epoch: [40] loss: 0.013\n",
            "epoch: [41] loss: 0.011\n",
            "epoch: [42] loss: 0.011\n",
            "epoch: [43] loss: 0.011\n",
            "epoch: [44] loss: 0.010\n",
            "Finished Training run 17\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.197\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.185\n",
            "epoch: [4] loss: 1.177\n",
            "epoch: [5] loss: 1.165\n",
            "epoch: [6] loss: 1.151\n",
            "epoch: [7] loss: 1.128\n",
            "epoch: [8] loss: 1.088\n",
            "epoch: [9] loss: 1.035\n",
            "epoch: [10] loss: 0.980\n",
            "epoch: [11] loss: 0.882\n",
            "epoch: [12] loss: 0.780\n",
            "epoch: [13] loss: 0.670\n",
            "epoch: [14] loss: 0.554\n",
            "epoch: [15] loss: 0.458\n",
            "epoch: [16] loss: 0.362\n",
            "epoch: [17] loss: 0.288\n",
            "epoch: [18] loss: 0.245\n",
            "epoch: [19] loss: 0.186\n",
            "epoch: [20] loss: 0.156\n",
            "epoch: [21] loss: 0.131\n",
            "epoch: [22] loss: 0.105\n",
            "epoch: [23] loss: 0.088\n",
            "epoch: [24] loss: 0.078\n",
            "epoch: [25] loss: 0.066\n",
            "epoch: [26] loss: 0.056\n",
            "epoch: [27] loss: 0.049\n",
            "epoch: [28] loss: 0.044\n",
            "epoch: [29] loss: 0.039\n",
            "epoch: [30] loss: 0.032\n",
            "epoch: [31] loss: 0.029\n",
            "epoch: [32] loss: 0.027\n",
            "epoch: [33] loss: 0.022\n",
            "epoch: [34] loss: 0.020\n",
            "epoch: [35] loss: 0.017\n",
            "epoch: [36] loss: 0.015\n",
            "epoch: [37] loss: 0.014\n",
            "epoch: [38] loss: 0.013\n",
            "epoch: [39] loss: 0.016\n",
            "epoch: [40] loss: 0.011\n",
            "epoch: [41] loss: 0.010\n",
            "Finished Training run 18\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.204\n",
            "epoch: [1] loss: 1.196\n",
            "epoch: [2] loss: 1.194\n",
            "epoch: [3] loss: 1.192\n",
            "epoch: [4] loss: 1.189\n",
            "epoch: [5] loss: 1.186\n",
            "epoch: [6] loss: 1.182\n",
            "epoch: [7] loss: 1.178\n",
            "epoch: [8] loss: 1.171\n",
            "epoch: [9] loss: 1.164\n",
            "epoch: [10] loss: 1.154\n",
            "epoch: [11] loss: 1.142\n",
            "epoch: [12] loss: 1.116\n",
            "epoch: [13] loss: 1.092\n",
            "epoch: [14] loss: 1.060\n",
            "epoch: [15] loss: 1.026\n",
            "epoch: [16] loss: 0.984\n",
            "epoch: [17] loss: 0.953\n",
            "epoch: [18] loss: 0.884\n",
            "epoch: [19] loss: 0.825\n",
            "epoch: [20] loss: 0.774\n",
            "epoch: [21] loss: 0.723\n",
            "epoch: [22] loss: 0.640\n",
            "epoch: [23] loss: 0.587\n",
            "epoch: [24] loss: 0.519\n",
            "epoch: [25] loss: 0.454\n",
            "epoch: [26] loss: 0.384\n",
            "epoch: [27] loss: 0.319\n",
            "epoch: [28] loss: 0.259\n",
            "epoch: [29] loss: 0.216\n",
            "epoch: [30] loss: 0.161\n",
            "epoch: [31] loss: 0.127\n",
            "epoch: [32] loss: 0.096\n",
            "epoch: [33] loss: 0.077\n",
            "epoch: [34] loss: 0.064\n",
            "epoch: [35] loss: 0.056\n",
            "epoch: [36] loss: 0.048\n",
            "epoch: [37] loss: 0.042\n",
            "epoch: [38] loss: 0.036\n",
            "epoch: [39] loss: 0.034\n",
            "epoch: [40] loss: 0.029\n",
            "epoch: [41] loss: 0.027\n",
            "epoch: [42] loss: 0.024\n",
            "epoch: [43] loss: 0.024\n",
            "epoch: [44] loss: 0.021\n",
            "epoch: [45] loss: 0.019\n",
            "epoch: [46] loss: 0.017\n",
            "epoch: [47] loss: 0.017\n",
            "epoch: [48] loss: 0.015\n",
            "epoch: [49] loss: 0.018\n",
            "epoch: [50] loss: 0.013\n",
            "epoch: [51] loss: 0.014\n",
            "epoch: [52] loss: 0.014\n",
            "epoch: [53] loss: 0.012\n",
            "epoch: [54] loss: 0.010\n",
            "Finished Training run 19\n",
            "Accuracy of the network on the 3000 train images: 99 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yEabNK9Q1bTE",
        "outputId": "238db1e0-4db8-418d-84ab-f5d1c7b3600b"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f18acaee1d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "cdde04ee-47aa-45e4-96ad-000c610d7854"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.94016667e+01, 2.04900000e+01, 8.83333333e-02, 2.00000000e-02])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"type4_second.csv\",index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "PzR8ISPlOSbP",
        "outputId": "096497b2-7e95-45b7-b1c8-63b3fc5c047c"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.166667</td>\n",
              "      <td>28.733333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>93.233333</td>\n",
              "      <td>6.700000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>78.466667</td>\n",
              "      <td>21.366667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>71.033333</td>\n",
              "      <td>28.833333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70.166667</td>\n",
              "      <td>29.666667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>71.000000</td>\n",
              "      <td>28.966667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>75.266667</td>\n",
              "      <td>24.633333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>70.500000</td>\n",
              "      <td>29.366667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>95.733333</td>\n",
              "      <td>4.200000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>74.333333</td>\n",
              "      <td>25.633333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>91.966667</td>\n",
              "      <td>7.900000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>97.033333</td>\n",
              "      <td>2.833333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>74.333333</td>\n",
              "      <td>25.600000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>72.933333</td>\n",
              "      <td>26.966667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>78.966667</td>\n",
              "      <td>20.866667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>74.233333</td>\n",
              "      <td>25.633333</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>89.966667</td>\n",
              "      <td>9.966667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>71.200000</td>\n",
              "      <td>28.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>75.600000</td>\n",
              "      <td>24.266667</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>90.900000</td>\n",
              "      <td>8.866667</td>\n",
              "      <td>0.233333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         FTPT       FFPT      FTPF      FFPF\n",
              "0   71.166667  28.733333  0.100000  0.000000\n",
              "1   93.233333   6.700000  0.033333  0.033333\n",
              "2   78.466667  21.366667  0.133333  0.033333\n",
              "3   71.033333  28.833333  0.066667  0.066667\n",
              "4   70.166667  29.666667  0.133333  0.033333\n",
              "5   71.000000  28.966667  0.000000  0.033333\n",
              "6   75.266667  24.633333  0.066667  0.033333\n",
              "7   70.500000  29.366667  0.133333  0.000000\n",
              "8   95.733333   4.200000  0.033333  0.033333\n",
              "9   74.333333  25.633333  0.033333  0.000000\n",
              "10  91.966667   7.900000  0.133333  0.000000\n",
              "11  97.033333   2.833333  0.066667  0.066667\n",
              "12  74.333333  25.600000  0.066667  0.000000\n",
              "13  72.933333  26.966667  0.100000  0.000000\n",
              "14  78.966667  20.866667  0.133333  0.033333\n",
              "15  74.233333  25.633333  0.133333  0.000000\n",
              "16  89.966667   9.966667  0.066667  0.000000\n",
              "17  71.200000  28.800000  0.000000  0.000000\n",
              "18  75.600000  24.266667  0.100000  0.033333\n",
              "19  90.900000   8.866667  0.233333  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}