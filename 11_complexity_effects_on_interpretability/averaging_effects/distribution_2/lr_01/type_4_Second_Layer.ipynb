{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "type_4_Second_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"type4_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\r\n",
        "mosaic_label = data[0][\"mosaic_label\"]\r\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,50)  #,self.output)\n",
        "        self.linear2 = nn.Linear(50,50)\n",
        "        self.linear3 = nn.Linear(50,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,50], dtype=torch.float64)   # number of features of output\n",
        "        features = torch.zeros([batch,self.K,50],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x = F.relu(x) \n",
        "      x = self.linear2(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.linear3(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,50)\n",
        "        #self.linear2 = nn.Linear(50,50)\n",
        "        self.linear2 = nn.Linear(50,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      #x = F.relu(self.linear2(x))\n",
        "      x = self.linear2(x)\n",
        "      return x    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOfxUJZ_eFKw",
        "outputId": "268cc009-60a7-4e4b-e716-767056ace667"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(2,1,9,2).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(50,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.01)#,momentum=0.9)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.01)#,momentum=0.9)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 2500\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: [0 ] loss: 1.219\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.156\n",
            "epoch: [3] loss: 1.039\n",
            "epoch: [4] loss: 0.833\n",
            "epoch: [5] loss: 0.695\n",
            "epoch: [6] loss: 0.526\n",
            "epoch: [7] loss: 0.310\n",
            "epoch: [8] loss: 0.188\n",
            "epoch: [9] loss: 0.051\n",
            "epoch: [10] loss: 0.023\n",
            "epoch: [11] loss: 0.021\n",
            "epoch: [12] loss: 0.025\n",
            "epoch: [13] loss: 0.008\n",
            "Finished Training run 0\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.197\n",
            "epoch: [2] loss: 1.182\n",
            "epoch: [3] loss: 1.077\n",
            "epoch: [4] loss: 0.920\n",
            "epoch: [5] loss: 0.716\n",
            "epoch: [6] loss: 0.563\n",
            "epoch: [7] loss: 0.474\n",
            "epoch: [8] loss: 0.271\n",
            "epoch: [9] loss: 0.078\n",
            "epoch: [10] loss: 0.019\n",
            "epoch: [11] loss: 0.006\n",
            "Finished Training run 1\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.221\n",
            "epoch: [1] loss: 1.195\n",
            "epoch: [2] loss: 1.189\n",
            "epoch: [3] loss: 1.148\n",
            "epoch: [4] loss: 1.068\n",
            "epoch: [5] loss: 0.832\n",
            "epoch: [6] loss: 0.616\n",
            "epoch: [7] loss: 0.535\n",
            "epoch: [8] loss: 0.416\n",
            "epoch: [9] loss: 0.272\n",
            "epoch: [10] loss: 0.134\n",
            "epoch: [11] loss: 0.130\n",
            "epoch: [12] loss: 0.064\n",
            "epoch: [13] loss: 0.015\n",
            "epoch: [14] loss: 0.010\n",
            "Finished Training run 2\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.196\n",
            "epoch: [2] loss: 1.182\n",
            "epoch: [3] loss: 1.118\n",
            "epoch: [4] loss: 1.060\n",
            "epoch: [5] loss: 0.862\n",
            "epoch: [6] loss: 0.772\n",
            "epoch: [7] loss: 0.552\n",
            "epoch: [8] loss: 0.602\n",
            "epoch: [9] loss: 0.131\n",
            "epoch: [10] loss: 0.056\n",
            "epoch: [11] loss: 0.034\n",
            "epoch: [12] loss: 0.013\n",
            "epoch: [13] loss: 0.007\n",
            "Finished Training run 3\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.198\n",
            "epoch: [1] loss: 1.199\n",
            "epoch: [2] loss: 1.198\n",
            "epoch: [3] loss: 1.194\n",
            "epoch: [4] loss: 1.191\n",
            "epoch: [5] loss: 1.176\n",
            "epoch: [6] loss: 1.140\n",
            "epoch: [7] loss: 1.048\n",
            "epoch: [8] loss: 0.828\n",
            "epoch: [9] loss: 0.664\n",
            "epoch: [10] loss: 0.548\n",
            "epoch: [11] loss: 0.521\n",
            "epoch: [12] loss: 0.468\n",
            "epoch: [13] loss: 0.500\n",
            "epoch: [14] loss: 0.301\n",
            "epoch: [15] loss: 0.244\n",
            "epoch: [16] loss: 0.172\n",
            "epoch: [17] loss: 0.141\n",
            "epoch: [18] loss: 0.049\n",
            "epoch: [19] loss: 0.040\n",
            "epoch: [20] loss: 0.013\n",
            "epoch: [21] loss: 0.015\n",
            "epoch: [22] loss: 0.013\n",
            "epoch: [23] loss: 0.007\n",
            "Finished Training run 4\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.196\n",
            "epoch: [1] loss: 1.205\n",
            "epoch: [2] loss: 1.164\n",
            "epoch: [3] loss: 0.995\n",
            "epoch: [4] loss: 0.817\n",
            "epoch: [5] loss: 0.669\n",
            "epoch: [6] loss: 0.606\n",
            "epoch: [7] loss: 0.382\n",
            "epoch: [8] loss: 0.191\n",
            "epoch: [9] loss: 0.099\n",
            "epoch: [10] loss: 0.082\n",
            "epoch: [11] loss: 0.036\n",
            "epoch: [12] loss: 0.023\n",
            "epoch: [13] loss: 0.014\n",
            "epoch: [14] loss: 0.013\n",
            "epoch: [15] loss: 0.012\n",
            "epoch: [16] loss: 0.007\n",
            "Finished Training run 5\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.205\n",
            "epoch: [1] loss: 1.189\n",
            "epoch: [2] loss: 1.145\n",
            "epoch: [3] loss: 0.963\n",
            "epoch: [4] loss: 0.691\n",
            "epoch: [5] loss: 0.559\n",
            "epoch: [6] loss: 0.511\n",
            "epoch: [7] loss: 0.458\n",
            "epoch: [8] loss: 0.389\n",
            "epoch: [9] loss: 0.255\n",
            "epoch: [10] loss: 0.187\n",
            "epoch: [11] loss: 0.116\n",
            "epoch: [12] loss: 0.081\n",
            "epoch: [13] loss: 0.024\n",
            "epoch: [14] loss: 0.096\n",
            "epoch: [15] loss: 0.027\n",
            "epoch: [16] loss: 0.037\n",
            "epoch: [17] loss: 0.002\n",
            "Finished Training run 6\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.211\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.181\n",
            "epoch: [3] loss: 1.148\n",
            "epoch: [4] loss: 1.004\n",
            "epoch: [5] loss: 0.821\n",
            "epoch: [6] loss: 0.623\n",
            "epoch: [7] loss: 0.535\n",
            "epoch: [8] loss: 0.499\n",
            "epoch: [9] loss: 0.445\n",
            "epoch: [10] loss: 0.314\n",
            "epoch: [11] loss: 0.145\n",
            "epoch: [12] loss: 0.062\n",
            "epoch: [13] loss: 0.025\n",
            "epoch: [14] loss: 0.015\n",
            "epoch: [15] loss: 0.014\n",
            "epoch: [16] loss: 0.005\n",
            "Finished Training run 7\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.161\n",
            "epoch: [3] loss: 1.031\n",
            "epoch: [4] loss: 0.872\n",
            "epoch: [5] loss: 0.680\n",
            "epoch: [6] loss: 0.574\n",
            "epoch: [7] loss: 0.558\n",
            "epoch: [8] loss: 0.467\n",
            "epoch: [9] loss: 0.475\n",
            "epoch: [10] loss: 0.357\n",
            "epoch: [11] loss: 0.289\n",
            "epoch: [12] loss: 0.199\n",
            "epoch: [13] loss: 0.079\n",
            "epoch: [14] loss: 0.058\n",
            "epoch: [15] loss: 0.022\n",
            "epoch: [16] loss: 0.012\n",
            "epoch: [17] loss: 0.005\n",
            "Finished Training run 8\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.215\n",
            "epoch: [1] loss: 1.194\n",
            "epoch: [2] loss: 1.171\n",
            "epoch: [3] loss: 1.040\n",
            "epoch: [4] loss: 0.862\n",
            "epoch: [5] loss: 0.682\n",
            "epoch: [6] loss: 0.529\n",
            "epoch: [7] loss: 0.287\n",
            "epoch: [8] loss: 0.153\n",
            "epoch: [9] loss: 0.062\n",
            "epoch: [10] loss: 0.025\n",
            "epoch: [11] loss: 0.028\n",
            "epoch: [12] loss: 0.169\n",
            "epoch: [13] loss: 0.015\n",
            "epoch: [14] loss: 0.036\n",
            "epoch: [15] loss: 0.062\n",
            "epoch: [16] loss: 0.013\n",
            "epoch: [17] loss: 0.009\n",
            "Finished Training run 9\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.226\n",
            "epoch: [1] loss: 1.185\n",
            "epoch: [2] loss: 1.114\n",
            "epoch: [3] loss: 0.986\n",
            "epoch: [4] loss: 0.840\n",
            "epoch: [5] loss: 0.663\n",
            "epoch: [6] loss: 0.556\n",
            "epoch: [7] loss: 0.400\n",
            "epoch: [8] loss: 0.206\n",
            "epoch: [9] loss: 0.341\n",
            "epoch: [10] loss: 0.086\n",
            "epoch: [11] loss: 0.067\n",
            "epoch: [12] loss: 0.017\n",
            "epoch: [13] loss: 0.007\n",
            "Finished Training run 10\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.213\n",
            "epoch: [1] loss: 1.182\n",
            "epoch: [2] loss: 1.119\n",
            "epoch: [3] loss: 1.023\n",
            "epoch: [4] loss: 0.852\n",
            "epoch: [5] loss: 0.711\n",
            "epoch: [6] loss: 0.556\n",
            "epoch: [7] loss: 0.557\n",
            "epoch: [8] loss: 0.428\n",
            "epoch: [9] loss: 0.209\n",
            "epoch: [10] loss: 0.141\n",
            "epoch: [11] loss: 0.065\n",
            "epoch: [12] loss: 0.145\n",
            "epoch: [13] loss: 0.016\n",
            "epoch: [14] loss: 0.012\n",
            "epoch: [15] loss: 0.006\n",
            "Finished Training run 11\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.227\n",
            "epoch: [1] loss: 1.193\n",
            "epoch: [2] loss: 1.181\n",
            "epoch: [3] loss: 1.141\n",
            "epoch: [4] loss: 1.001\n",
            "epoch: [5] loss: 0.726\n",
            "epoch: [6] loss: 0.560\n",
            "epoch: [7] loss: 0.379\n",
            "epoch: [8] loss: 0.234\n",
            "epoch: [9] loss: 0.136\n",
            "epoch: [10] loss: 0.073\n",
            "epoch: [11] loss: 0.041\n",
            "epoch: [12] loss: 0.027\n",
            "epoch: [13] loss: 0.013\n",
            "epoch: [14] loss: 0.009\n",
            "Finished Training run 12\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.200\n",
            "epoch: [1] loss: 1.170\n",
            "epoch: [2] loss: 0.986\n",
            "epoch: [3] loss: 0.717\n",
            "epoch: [4] loss: 0.577\n",
            "epoch: [5] loss: 0.550\n",
            "epoch: [6] loss: 0.122\n",
            "epoch: [7] loss: 0.039\n",
            "epoch: [8] loss: 0.043\n",
            "epoch: [9] loss: 0.132\n",
            "epoch: [10] loss: 0.067\n",
            "epoch: [11] loss: 0.007\n",
            "Finished Training run 13\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.215\n",
            "epoch: [1] loss: 1.184\n",
            "epoch: [2] loss: 1.176\n",
            "epoch: [3] loss: 1.062\n",
            "epoch: [4] loss: 0.856\n",
            "epoch: [5] loss: 0.628\n",
            "epoch: [6] loss: 0.564\n",
            "epoch: [7] loss: 0.550\n",
            "epoch: [8] loss: 0.532\n",
            "epoch: [9] loss: 0.415\n",
            "epoch: [10] loss: 0.295\n",
            "epoch: [11] loss: 0.233\n",
            "epoch: [12] loss: 0.134\n",
            "epoch: [13] loss: 0.053\n",
            "epoch: [14] loss: 0.014\n",
            "epoch: [15] loss: 0.006\n",
            "Finished Training run 14\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.199\n",
            "epoch: [1] loss: 1.204\n",
            "epoch: [2] loss: 1.174\n",
            "epoch: [3] loss: 1.108\n",
            "epoch: [4] loss: 0.921\n",
            "epoch: [5] loss: 0.753\n",
            "epoch: [6] loss: 0.572\n",
            "epoch: [7] loss: 0.515\n",
            "epoch: [8] loss: 0.612\n",
            "epoch: [9] loss: 0.312\n",
            "epoch: [10] loss: 0.170\n",
            "epoch: [11] loss: 0.095\n",
            "epoch: [12] loss: 0.150\n",
            "epoch: [13] loss: 0.021\n",
            "epoch: [14] loss: 0.009\n",
            "Finished Training run 15\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.201\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.177\n",
            "epoch: [3] loss: 1.123\n",
            "epoch: [4] loss: 0.983\n",
            "epoch: [5] loss: 0.813\n",
            "epoch: [6] loss: 0.777\n",
            "epoch: [7] loss: 0.578\n",
            "epoch: [8] loss: 0.449\n",
            "epoch: [9] loss: 0.351\n",
            "epoch: [10] loss: 0.218\n",
            "epoch: [11] loss: 0.139\n",
            "epoch: [12] loss: 0.056\n",
            "epoch: [13] loss: 0.059\n",
            "epoch: [14] loss: 0.021\n",
            "epoch: [15] loss: 0.006\n",
            "Finished Training run 16\n",
            "Accuracy of the network on the 3000 train images: 100 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.201\n",
            "epoch: [1] loss: 1.192\n",
            "epoch: [2] loss: 1.142\n",
            "epoch: [3] loss: 0.985\n",
            "epoch: [4] loss: 0.796\n",
            "epoch: [5] loss: 0.520\n",
            "epoch: [6] loss: 0.349\n",
            "epoch: [7] loss: 0.183\n",
            "epoch: [8] loss: 0.079\n",
            "epoch: [9] loss: 0.032\n",
            "epoch: [10] loss: 0.019\n",
            "epoch: [11] loss: 0.013\n",
            "epoch: [12] loss: 0.078\n",
            "epoch: [13] loss: 0.106\n",
            "epoch: [14] loss: 0.006\n",
            "Finished Training run 17\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.197\n",
            "epoch: [1] loss: 1.185\n",
            "epoch: [2] loss: 1.102\n",
            "epoch: [3] loss: 0.869\n",
            "epoch: [4] loss: 0.665\n",
            "epoch: [5] loss: 0.485\n",
            "epoch: [6] loss: 0.208\n",
            "epoch: [7] loss: 0.118\n",
            "epoch: [8] loss: 0.033\n",
            "epoch: [9] loss: 0.008\n",
            "Finished Training run 18\n",
            "Accuracy of the network on the 3000 train images: 99 %\n",
            "--------------------------------------------------------------------------------\n",
            "epoch: [0 ] loss: 1.204\n",
            "epoch: [1] loss: 1.197\n",
            "epoch: [2] loss: 1.195\n",
            "epoch: [3] loss: 1.189\n",
            "epoch: [4] loss: 1.166\n",
            "epoch: [5] loss: 1.109\n",
            "epoch: [6] loss: 1.021\n",
            "epoch: [7] loss: 0.966\n",
            "epoch: [8] loss: 0.760\n",
            "epoch: [9] loss: 0.659\n",
            "epoch: [10] loss: 0.595\n",
            "epoch: [11] loss: 0.558\n",
            "epoch: [12] loss: 0.545\n",
            "epoch: [13] loss: 0.511\n",
            "epoch: [14] loss: 0.475\n",
            "epoch: [15] loss: 0.332\n",
            "epoch: [16] loss: 0.140\n",
            "epoch: [17] loss: 0.103\n",
            "epoch: [18] loss: 0.014\n",
            "epoch: [19] loss: 0.004\n",
            "Finished Training run 19\n",
            "Accuracy of the network on the 3000 train images: 99 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yEabNK9Q1bTE",
        "outputId": "c331cf23-196e-47a1-f515-4b2feb226544"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f271b723150>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "490d37d0-76eb-41bc-9edc-a707822014af"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8.04633333e+01, 1.94500000e+01, 6.66666667e-02, 2.00000000e-02])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"type4_second.csv\",index=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "PzR8ISPlOSbP",
        "outputId": "1a4dded5-ce43-4757-8048-df3d2bd68d52"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71.566667</td>\n",
              "      <td>28.400000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98.000000</td>\n",
              "      <td>1.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>76.566667</td>\n",
              "      <td>23.400000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>72.200000</td>\n",
              "      <td>27.733333</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>98.400000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>70.900000</td>\n",
              "      <td>28.900000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>81.666667</td>\n",
              "      <td>18.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>77.966667</td>\n",
              "      <td>21.933333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>73.466667</td>\n",
              "      <td>26.466667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>72.533333</td>\n",
              "      <td>27.200000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.233333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>79.266667</td>\n",
              "      <td>20.633333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>80.766667</td>\n",
              "      <td>19.166667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>72.000000</td>\n",
              "      <td>27.933333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>70.966667</td>\n",
              "      <td>28.933333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>71.166667</td>\n",
              "      <td>28.766667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>99.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>99.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>71.066667</td>\n",
              "      <td>28.833333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>70.966667</td>\n",
              "      <td>28.933333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>99.933333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         FTPT       FFPT      FTPF      FFPF\n",
              "0   71.566667  28.400000  0.033333  0.000000\n",
              "1   98.000000   1.966667  0.033333  0.000000\n",
              "2   76.566667  23.400000  0.033333  0.000000\n",
              "3   72.200000  27.733333  0.066667  0.000000\n",
              "4   98.400000   1.400000  0.166667  0.033333\n",
              "5   70.900000  28.900000  0.200000  0.000000\n",
              "6   81.666667  18.333333  0.000000  0.000000\n",
              "7   77.966667  21.933333  0.100000  0.000000\n",
              "8   73.466667  26.466667  0.033333  0.033333\n",
              "9   72.533333  27.200000  0.033333  0.233333\n",
              "10  79.266667  20.633333  0.100000  0.000000\n",
              "11  80.766667  19.166667  0.066667  0.000000\n",
              "12  72.000000  27.933333  0.033333  0.033333\n",
              "13  70.966667  28.933333  0.100000  0.000000\n",
              "14  71.166667  28.766667  0.066667  0.000000\n",
              "15  99.900000   0.000000  0.100000  0.000000\n",
              "16  99.966667   0.033333  0.000000  0.000000\n",
              "17  71.066667  28.833333  0.033333  0.066667\n",
              "18  70.966667  28.933333  0.100000  0.000000\n",
              "19  99.933333   0.033333  0.033333  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}