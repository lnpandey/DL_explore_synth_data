{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "First_Layer_averaging_Complexity Synthetic elliptical blobs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAYu3ISwwGks"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjEp-LtqiWAf"
      },
      "source": [
        "# mu1 = np.array([3,3,3,3,0])\n",
        "# sigma1 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu2 = np.array([4,4,4,4,0])\n",
        "# sigma2 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu3 = np.array([10,5,5,10,0])\n",
        "# sigma3 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu4 = np.array([-10,-10,-10,-10,0])\n",
        "# sigma4 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu5 = np.array([-21,4,4,-21,0])\n",
        "# sigma5 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu6 = np.array([-10,18,18,-10,0])\n",
        "# sigma6 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu7 = np.array([4,20,4,20,0])\n",
        "# sigma7 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu8 = np.array([4,-20,-20,4,0])\n",
        "# sigma8 = np.array([[16,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu9 = np.array([20,20,20,20,0])\n",
        "# sigma9 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "# mu10 = np.array([20,-10,-10,20,0])\n",
        "# sigma10 = np.array([[1,1,1,1,1],[1,16,1,1,1],[1,1,1,1,1],[1,1,1,1,1],[1,1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "# sample1 = np.random.multivariate_normal(mean=mu1,cov= sigma1,size=500)\n",
        "# sample2 = np.random.multivariate_normal(mean=mu2,cov= sigma2,size=500)\n",
        "# sample3 = np.random.multivariate_normal(mean=mu3,cov= sigma3,size=500)\n",
        "# sample4 = np.random.multivariate_normal(mean=mu4,cov= sigma4,size=500)\n",
        "# sample5 = np.random.multivariate_normal(mean=mu5,cov= sigma5,size=500)\n",
        "# sample6 = np.random.multivariate_normal(mean=mu6,cov= sigma6,size=500)\n",
        "# sample7 = np.random.multivariate_normal(mean=mu7,cov= sigma7,size=500)\n",
        "# sample8 = np.random.multivariate_normal(mean=mu8,cov= sigma8,size=500)\n",
        "# sample9 = np.random.multivariate_normal(mean=mu9,cov= sigma9,size=500)\n",
        "# sample10 = np.random.multivariate_normal(mean=mu10,cov= sigma10,size=500)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YDnxeP-2_1V"
      },
      "source": [
        "# X = np.concatenate((sample1,sample2,sample3,sample4,sample5,sample6,sample7,sample8,sample9,sample10),axis=0)\n",
        "# Y = np.concatenate((np.zeros((500,1)),np.ones((500,1)),2*np.ones((500,1)),3*np.ones((500,1)),4*np.ones((500,1)),\n",
        "#                     5*np.ones((500,1)),6*np.ones((500,1)),7*np.ones((500,1)),8*np.ones((500,1)),9*np.ones((500,1))),axis=0).astype(int)\n",
        "# print(X.shape,Y.shape)\n",
        "# plt.scatter(sample1[:,0],sample1[:,1],label=\"class_0\")\n",
        "# plt.scatter(sample2[:,0],sample2[:,1],label=\"class_1\")\n",
        "# plt.scatter(sample3[:,0],sample3[:,1],label=\"class_2\")\n",
        "# plt.scatter(sample4[:,0],sample4[:,1],label=\"class_3\")\n",
        "# plt.scatter(sample5[:,0],sample5[:,1],label=\"class_4\")\n",
        "# plt.scatter(sample6[:,0],sample6[:,1],label=\"class_5\")\n",
        "# plt.scatter(sample7[:,0],sample7[:,1],label=\"class_6\")\n",
        "# plt.scatter(sample8[:,0],sample8[:,1],label=\"class_7\")\n",
        "# plt.scatter(sample9[:,0],sample9[:,1],label=\"class_8\")\n",
        "# plt.scatter(sample10[:,0],sample10[:,1],label=\"class_9\")\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6YzqPUf3CHa"
      },
      "source": [
        "# class SyntheticDataset(Dataset):\n",
        "#   \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "#   def __init__(self, x, y):\n",
        "#     \"\"\"\n",
        "#       Args:\n",
        "#         csv_file (string): Path to the csv file with annotations.\n",
        "#         root_dir (string): Directory with all the images.\n",
        "#         transform (callable, optional): Optional transform to be applied\n",
        "#             on a sample.\n",
        "#     \"\"\"\n",
        "#     self.x = x\n",
        "#     self.y = y\n",
        "#     #self.fore_idx = fore_idx\n",
        "    \n",
        "#   def __len__(self):\n",
        "#     return len(self.y)\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "#     return self.x[idx] , self.y[idx] #, self.fore_idx[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mi3nL5-4D7_"
      },
      "source": [
        "# trainset = SyntheticDataset(X,Y)\n",
        "# #\n",
        "\n",
        "# # testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKzc7IgwqoU2"
      },
      "source": [
        "# classes = ('zero','one','two','three','four','five','six','seven','eight','nine')\n",
        "\n",
        "# foreground_classes = {'zero','one','two'}\n",
        "# fg_used = '012'\n",
        "# fg1, fg2, fg3 = 0,1,2\n",
        "\n",
        "\n",
        "# all_classes = {'zero','one','two','three','four','five','six','seven','eight','nine'}\n",
        "# background_classes = all_classes - foreground_classes\n",
        "# background_classes"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT6iKHutquR8"
      },
      "source": [
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWKzXkPSq5KU"
      },
      "source": [
        "# dataiter = iter(trainloader)\n",
        "# background_data=[]\n",
        "# background_label=[]\n",
        "# foreground_data=[]\n",
        "# foreground_label=[]\n",
        "# batch_size=100\n",
        "\n",
        "# for i in range(50):\n",
        "#   images, labels = dataiter.next()\n",
        "#   for j in range(batch_size):\n",
        "#     if(classes[labels[j]] in background_classes):\n",
        "#       img = images[j].tolist()\n",
        "#       background_data.append(img)\n",
        "#       background_label.append(labels[j])\n",
        "#     else:\n",
        "#       img = images[j].tolist()\n",
        "#       foreground_data.append(img)\n",
        "#       foreground_label.append(labels[j])\n",
        "            \n",
        "# foreground_data = torch.tensor(foreground_data)\n",
        "# foreground_label = torch.tensor(foreground_label)\n",
        "# background_data = torch.tensor(background_data)\n",
        "# background_label = torch.tensor(background_label)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChdziOP3rF1G"
      },
      "source": [
        "# def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "#   \"\"\"\n",
        "#   bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "#   fg_idx : index of image to be used as foreground image from foreground data\n",
        "#   fg : at what position/index foreground image has to be stored out of 0-8\n",
        "#   \"\"\"\n",
        "#   image_list=[]\n",
        "#   j=0\n",
        "#   for i in range(9):\n",
        "#     if i != fg:\n",
        "#       image_list.append(background_data[bg_idx[j]])\n",
        "#       j+=1\n",
        "#     else: \n",
        "#       image_list.append(foreground_data[fg_idx])\n",
        "#       label = foreground_label[fg_idx] - fg1  # minus fg1 because our fore ground classes are fg1,fg2,fg3 but we have to store it as 0,1,2\n",
        "#   #image_list = np.concatenate(image_list ,axis=0)\n",
        "#   image_list = torch.stack(image_list) \n",
        "#   return image_list,label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASrmPqErIDM"
      },
      "source": [
        "# desired_num = 3000\n",
        "# mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "# fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "# mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "# list_set_labels = [] \n",
        "# for i in range(desired_num):\n",
        "#   set_idx = set()\n",
        "#   np.random.seed(i)\n",
        "#   bg_idx = np.random.randint(0,3500,8)\n",
        "#   set_idx = set(background_label[bg_idx].tolist())\n",
        "#   fg_idx = np.random.randint(0,1500)\n",
        "#   set_idx.add(foreground_label[fg_idx].item())\n",
        "#   fg = np.random.randint(0,9)\n",
        "#   fore_idx.append(fg)\n",
        "#   image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "#   mosaic_list_of_images.append(image_list)\n",
        "#   mosaic_label.append(label)\n",
        "#   list_set_labels.append(set_idx)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGsdvMSzIUK"
      },
      "source": [
        "class MosaicDataset1(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list, mosaic_label,fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] , self.fore_idx[idx]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm4jv30eD_hl"
      },
      "source": [
        "# data =  [{\"mosaic_list\":mosaic_list_of_images, \"mosaic_label\": mosaic_label, \"fore_idx\":fore_idx}]\r\n",
        "# np.save(\"mosaic_data.npy\",data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN7ItmyIEdnB"
      },
      "source": [
        "data = np.load(\"mosaic_data.npy\",allow_pickle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iifTn7hNEmCU"
      },
      "source": [
        "mosaic_list_of_images = data[0][\"mosaic_list\"]\r\n",
        "mosaic_label = data[0][\"mosaic_label\"]\r\n",
        "fore_idx = data[0][\"fore_idx\"]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP5NPRPmb904"
      },
      "source": [
        "batch = 250\n",
        "msd = MosaicDataset1(mosaic_list_of_images, mosaic_label, fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzN3Bbs8c0fA"
      },
      "source": [
        "class Focus_deep(nn.Module):\n",
        "    '''\n",
        "       deep focus network averaged at zeroth layer\n",
        "       input : elemental data\n",
        "    '''\n",
        "    def __init__(self,inputs,output,K,d):\n",
        "        super(Focus_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.K = K\n",
        "        self.d  = d\n",
        "        self.linear1 = nn.Linear(self.inputs,6)  #,self.output)\n",
        "        self.linear2 = nn.Linear(6,12)\n",
        "        self.linear3 = nn.Linear(12,self.output) \n",
        "    def forward(self,z):\n",
        "        batch = z.shape[0]\n",
        "        x = torch.zeros([batch,self.K],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,6], dtype=torch.float64)\n",
        "        features = torch.zeros([batch,self.K,6],dtype=torch.float64)\n",
        "        x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        features = features.to(\"cuda\")\n",
        "        for i in range(self.K):\n",
        "            alp,ftrs = self.helper(z[:,i] )  # self.d*i:self.d*i+self.d\n",
        "            x[:,i] = alp[:,0]\n",
        "            features[:,i]  = ftrs \n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        for i in range(self.K):\n",
        "            x1 = x[:,i]          \n",
        "            y = y+torch.mul(x1[:,None],features[:,i])  # self.d*i:self.d*i+self.d\n",
        "        return y , x \n",
        "    def helper(self,x):\n",
        "      x = self.linear1(x)\n",
        "      x1 = F.tanh(x)\n",
        "      x = F.relu(x) \n",
        "      x = F.relu(self.linear2(x))\n",
        "      x = self.linear3(x)\n",
        "      #print(x1.shape)\n",
        "      return x,x1\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0W0oKcClFZY"
      },
      "source": [
        "class Classification_deep(nn.Module):\n",
        "    '''\n",
        "       input : elemental data\n",
        "       deep classification module data averaged at zeroth layer\n",
        "    '''\n",
        "    def __init__(self,inputs,output):\n",
        "        super(Classification_deep,self).__init__()\n",
        "        self.inputs = inputs\n",
        "        self.output = output\n",
        "        self.linear1 = nn.Linear(self.inputs,6)\n",
        "        self.linear2 = nn.Linear(6,12)\n",
        "        self.linear3 = nn.Linear(12,self.output)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = F.relu(self.linear1(x))\n",
        "      x = F.relu(self.linear2(x))\n",
        "      x = self.linear3(x)\n",
        "      return x    "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehAfQnNwgFYX"
      },
      "source": [
        "def calculate_attn_loss(dataloader,what,where,criter):\n",
        "  what.eval()\n",
        "  where.eval()\n",
        "  r_loss = 0\n",
        "  alphas = []\n",
        "  lbls = []\n",
        "  pred = []\n",
        "  fidices = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      inputs, labels,fidx = data\n",
        "      lbls.append(labels)\n",
        "      fidices.append(fidx)\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "      avg,alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.append(predicted.cpu().numpy())\n",
        "      alphas.append(alpha.cpu().numpy())\n",
        "      loss = criter(outputs, labels)\n",
        "      r_loss += loss.item()\n",
        "  alphas = np.concatenate(alphas,axis=0)\n",
        "  pred = np.concatenate(pred,axis=0)\n",
        "  lbls = np.concatenate(lbls,axis=0)\n",
        "  fidices = np.concatenate(fidices,axis=0)\n",
        "  #print(alphas.shape,pred.shape,lbls.shape,fidices.shape) \n",
        "  analysis = analyse_data(alphas,lbls,pred,fidices)\n",
        "  return r_loss/i,analysis"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9HQJMzxBhp"
      },
      "source": [
        "def analyse_data(alphas,lbls,predicted,f_idx):\n",
        "    '''\n",
        "       analysis data is created here\n",
        "    '''\n",
        "    batch = len(predicted)\n",
        "    amth,alth,ftpt,ffpt,ftpf,ffpf = 0,0,0,0,0,0\n",
        "    for j in range (batch):\n",
        "      focus = np.argmax(alphas[j])\n",
        "      if(alphas[j][focus] >= 0.5):\n",
        "        amth +=1\n",
        "      else:\n",
        "        alth +=1\n",
        "      if(focus == f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ftpt += 1\n",
        "      elif(focus != f_idx[j] and predicted[j] == lbls[j]):\n",
        "        ffpt +=1\n",
        "      elif(focus == f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ftpf +=1\n",
        "      elif(focus != f_idx[j] and predicted[j] != lbls[j]):\n",
        "        ffpf +=1\n",
        "    #print(sum(predicted==lbls),ftpt+ffpt)\n",
        "    return [ftpt,ffpt,ftpf,ffpf,amth,alth]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOfxUJZ_eFKw"
      },
      "source": [
        "number_runs = 20\n",
        "FTPT_analysis = pd.DataFrame(columns = [\"FTPT\",\"FFPT\", \"FTPF\",\"FFPF\"])\n",
        "for n in range(number_runs):\n",
        "  print(\"--\"*40)\n",
        "  \n",
        "  # instantiate focus and classification Model\n",
        "  torch.manual_seed(n)\n",
        "  where = Focus_deep(5,1,9,5).double()\n",
        "  torch.manual_seed(n)\n",
        "  what = Classification_deep(6,3).double()\n",
        "  where = where.to(\"cuda\")\n",
        "  what = what.to(\"cuda\")\n",
        "\n",
        "\n",
        "\n",
        "  # instantiate optimizer\n",
        "  optimizer_where = optim.Adam(where.parameters(),lr =0.01)\n",
        "  optimizer_what = optim.Adam(what.parameters(), lr=0.01)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  acti = []\n",
        "  analysis_data = []\n",
        "  loss_curi = []\n",
        "  epochs = 2500\n",
        "\n",
        "\n",
        "  # calculate zeroth epoch loss and FTPT values\n",
        "  running_loss,anlys_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "  loss_curi.append(running_loss)\n",
        "  analysis_data.append(anlys_data)\n",
        "\n",
        "  print('epoch: [%d ] loss: %.3f' %(0,running_loss)) \n",
        "\n",
        "  # training starts \n",
        "  for epoch in range(epochs): # loop over the dataset multiple times\n",
        "    ep_lossi = []\n",
        "    running_loss = 0.0\n",
        "    what.train()\n",
        "    where.train()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      # get the inputs\n",
        "      inputs, labels,_ = data\n",
        "      inputs = inputs.double()\n",
        "      inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer_where.zero_grad()\n",
        "      optimizer_what.zero_grad()\n",
        "      \n",
        "      # forward + backward + optimize\n",
        "      avg, alpha = where(inputs)\n",
        "      outputs = what(avg)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer_where.step()\n",
        "      optimizer_what.step()\n",
        "\n",
        "    running_loss,anls_data = calculate_attn_loss(train_loader,what,where,criterion)\n",
        "    analysis_data.append(anls_data)\n",
        "    print('epoch: [%d] loss: %.3f' %(epoch + 1,running_loss)) \n",
        "    loss_curi.append(running_loss)   #loss per epoch\n",
        "    if running_loss<=0.01:\n",
        "      break\n",
        "  print('Finished Training run ' +str(n))\n",
        "  analysis_data = np.array(analysis_data)\n",
        "  FTPT_analysis.loc[n] = analysis_data[-1,:4]/30\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "      images, labels,_ = data\n",
        "      images = images.double()\n",
        "      images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "      avg, alpha = where(images)\n",
        "      outputs  = what(avg)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 3000 train images: %d %%' % (  100 * correct / total))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L31RVViMkYM-"
      },
      "source": [
        "# plt.figure(figsize=(6,6))\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,0],label=\"ftpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,1],label=\"ffpt\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,2],label=\"ftpf\")\n",
        "# plt.plot(np.arange(0,epoch+2,1),analysis_data[:,3],label=\"ffpf\")\n",
        "\n",
        "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEabNK9Q1bTE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e0558000-d19f-4767-d557-0deb4bee81c1"
      },
      "source": [
        "plt.plot(loss_curi)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd4a8723d90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBbboK0mtLTL",
        "outputId": "4c4d358c-6e20-4be0-f442-82508e4187ba"
      },
      "source": [
        "np.mean(np.array(FTPT_analysis),axis=0)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.33916667e+01, 6.58000000e+00, 1.66666667e-02, 1.16666667e-02])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYS7jRsCz30j"
      },
      "source": [
        "FTPT_analysis.to_csv(\"synthetic_first.csv\",index=False)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwzQFzul37sQ"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "PzR8ISPlOSbP",
        "outputId": "0955f6db-48d6-4a4c-bee4-1dc61a99101a"
      },
      "source": [
        "FTPT_analysis"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FTPT</th>\n",
              "      <th>FFPT</th>\n",
              "      <th>FTPF</th>\n",
              "      <th>FFPF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>67.833333</td>\n",
              "      <td>32.100000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>93.600000</td>\n",
              "      <td>6.366667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>88.366667</td>\n",
              "      <td>11.566667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>69.800000</td>\n",
              "      <td>30.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>99.733333</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>99.200000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>66.133333</td>\n",
              "      <td>33.666667</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>99.966667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>98.166667</td>\n",
              "      <td>1.833333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>99.833333</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>85.200000</td>\n",
              "      <td>14.766667</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          FTPT       FFPT      FTPF      FFPF\n",
              "0    67.833333  32.100000  0.066667  0.000000\n",
              "1    93.600000   6.366667  0.033333  0.000000\n",
              "2   100.000000   0.000000  0.000000  0.000000\n",
              "3   100.000000   0.000000  0.000000  0.000000\n",
              "4    88.366667  11.566667  0.033333  0.033333\n",
              "5   100.000000   0.000000  0.000000  0.000000\n",
              "6    69.800000  30.200000  0.000000  0.000000\n",
              "7    99.733333   0.100000  0.100000  0.066667\n",
              "8    99.200000   0.800000  0.000000  0.000000\n",
              "9    66.133333  33.666667  0.066667  0.133333\n",
              "10  100.000000   0.000000  0.000000  0.000000\n",
              "11  100.000000   0.000000  0.000000  0.000000\n",
              "12   99.966667   0.033333  0.000000  0.000000\n",
              "13  100.000000   0.000000  0.000000  0.000000\n",
              "14   98.166667   1.833333  0.000000  0.000000\n",
              "15  100.000000   0.000000  0.000000  0.000000\n",
              "16   99.833333   0.166667  0.000000  0.000000\n",
              "17   85.200000  14.766667  0.033333  0.000000\n",
              "18  100.000000   0.000000  0.000000  0.000000\n",
              "19  100.000000   0.000000  0.000000  0.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUoGWONAXEk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}