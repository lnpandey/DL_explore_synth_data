{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "JSjG64ra4aFu",
    "outputId": "67a99445-6d69-4a4d-bbc8-0f6a54df7407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.3)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8-7SARDZErK"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "vwJv7Y8Rewez",
    "outputId": "3ebf2bc5-7aff-4d79-9e52-06bea6e8ed5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/170498071 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170500096it [00:02, 72611639.22it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "foreground_classes = {'plane', 'car', 'bird'}\n",
    "\n",
    "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
    "\n",
    "# print(type(foreground_classes))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "background_data=[]\n",
    "background_label=[]\n",
    "foreground_data=[]\n",
    "foreground_label=[]\n",
    "batch_size=10\n",
    "\n",
    "for i in range(5000):   #5000*batch_size = 50000 data points\n",
    "  images, labels = dataiter.next()\n",
    "  for j in range(batch_size):\n",
    "    if(classes[labels[j]] in background_classes):\n",
    "      img = images[j].tolist()\n",
    "      background_data.append(img)\n",
    "      background_label.append(labels[j])\n",
    "    else:\n",
    "      img = images[j].tolist()\n",
    "      foreground_data.append(img)\n",
    "      foreground_label.append(labels[j])\n",
    "            \n",
    "foreground_data = torch.tensor(foreground_data)\n",
    "foreground_label = torch.tensor(foreground_label)\n",
    "background_data = torch.tensor(background_data)\n",
    "background_label = torch.tensor(background_label)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nDYhjJse6Qq"
   },
   "outputs": [],
   "source": [
    "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
    "  \"\"\"\n",
    "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
    "  fg_idx : index of image to be used as foreground image from foreground data\n",
    "  fg : at what position/index foreground image has to be stored out of 0-8\n",
    "  \"\"\"\n",
    "  image_list=[]\n",
    "  j=0\n",
    "  for i in range(9):\n",
    "    if i != fg:\n",
    "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
    "      j+=1\n",
    "    else: \n",
    "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
    "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
    "  #image_list = np.concatenate(image_list ,axis=0)\n",
    "  image_list = torch.stack(image_list) \n",
    "  return image_list,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aivGVg14e9iZ"
   },
   "outputs": [],
   "source": [
    "desired_num = 20000\n",
    "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
    "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
    "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
    "for i in range(desired_num):\n",
    "  bg_idx = np.random.randint(0,35000,8)\n",
    "  fg_idx = np.random.randint(0,15000)\n",
    "  fg = np.random.randint(0,9)\n",
    "  fore_idx.append(fg)\n",
    "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
    "  mosaic_list_of_images.append(image_list)\n",
    "  mosaic_label.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6Jy35SSfBS9"
   },
   "outputs": [],
   "source": [
    "# dict = {\"mosaic_list_of_images\": mosaic_list_of_images, \"mosaic_label\": mosaic_label , \"fore_idx\":fore_idx}\n",
    "# f = open(\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl\",\"wb\")\n",
    "# pickle.dump(dict,f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nIuiboIUfViV"
   },
   "source": [
    "# Load data from saved file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MbYf6zuBfViX"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/file.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AwZjDB5lfVib"
   },
   "outputs": [],
   "source": [
    "mosaic_list_of_images = data[\"mosaic_list_of_images\"]\n",
    "mosaic_label = data[\"mosaic_label\"]\n",
    "fore_idx = data[\"fore_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cog5VUzGgE5L",
    "outputId": "d88665ab-4661-4e0d-8a46-92796b3c8d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000 10000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
    "print(len(fore_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xX91RwMy-IP4"
   },
   "outputs": [],
   "source": [
    "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
    "  \"\"\"\n",
    "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
    "  labels : mosaic_dataset labels\n",
    "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
    "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
    "  \"\"\"\n",
    "  avg_image_dataset = []\n",
    "  for i in range(len(mosaic_dataset)):\n",
    "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
    "    for j in range(9):\n",
    "      if j == foreground_index[i]:\n",
    "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
    "      else :\n",
    "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
    "    \n",
    "    avg_image_dataset.append(img)\n",
    "    \n",
    "  return avg_image_dataset , labels , foreground_index\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGz8Y88vIZPT"
   },
   "outputs": [],
   "source": [
    "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
    "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 2)\n",
    "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 3)\n",
    "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
    "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 5)\n",
    "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 6)\n",
    "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 7)\n",
    "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 8)\n",
    "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
    "\n",
    "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSO9SFE25Lrk"
   },
   "outputs": [],
   "source": [
    "class MosaicDataset(Dataset):\n",
    "  \"\"\"MosaicDataset dataset.\"\"\"\n",
    "\n",
    "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        csv_file (string): Path to the csv file with annotations.\n",
    "        root_dir (string): Directory with all the images.\n",
    "        transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "    \"\"\"\n",
    "    self.mosaic = mosaic_list_of_images\n",
    "    self.label = mosaic_label\n",
    "    #self.fore_idx = fore_idx\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obE1xeyRks1Q"
   },
   "outputs": [],
   "source": [
    "batch = 256\n",
    "epochs = 70\n",
    "\n",
    "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
    "# training_label = labels_5\n",
    "\n",
    "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
    "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
    "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
    "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
    "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
    "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
    "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
    "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
    "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
    "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
    "\n",
    "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
    "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
    "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
    "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
    "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
    "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
    "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
    "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
    "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
    "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
    "\n",
    "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
    "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SadRzWBBZEsP"
   },
   "outputs": [],
   "source": [
    "class Conv_module(nn.Module):\n",
    "    def __init__(self,inp_ch,f,s,k,pad):\n",
    "        super(Conv_module,self).__init__()\n",
    "        self.inp_ch = inp_ch\n",
    "        self.f = f\n",
    "        self.s = s \n",
    "        self.k = k \n",
    "        self.pad = pad\n",
    "        \n",
    "        \n",
    "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
    "        self.bn = nn.BatchNorm2d(self.f)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgGYMG_ZZEsT"
   },
   "outputs": [],
   "source": [
    "class inception_module(nn.Module):\n",
    "    def __init__(self,inp_ch,f0,f1):\n",
    "        super(inception_module, self).__init__()\n",
    "        self.inp_ch = inp_ch\n",
    "        self.f0 = f0\n",
    "        self.f1 = f1\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
    "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
    "        #self.conv1 = nn.Conv2d(3,self.f0,1)\n",
    "        #self.conv3 = nn.Conv2d(3,self.f1,3,padding=1)\n",
    "    def forward(self,x):\n",
    "        x1 = self.conv1.forward(x)\n",
    "        x3 = self.conv3.forward(x)\n",
    "        #print(x1.shape,x3.shape)\n",
    "        \n",
    "        x = torch.cat((x1,x3),dim=1)\n",
    "        \n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thkdqW91Hpju"
   },
   "outputs": [],
   "source": [
    "class downsample_module(nn.Module):\n",
    "    def __init__(self,inp_ch,f):\n",
    "        super(downsample_module,self).__init__()\n",
    "        self.inp_ch = inp_ch\n",
    "        self.f = f\n",
    "        self.conv = Conv_module(self.inp_ch,self.f,2,3,pad=0)\n",
    "        self.pool = nn.MaxPool2d(3,stride=2,padding=0)\n",
    "    def forward(self,x):\n",
    "        x1 = self.conv(x)\n",
    "        #print(x1.shape)\n",
    "        x2 = self.pool(x)\n",
    "        #print(x2.shape)\n",
    "        x = torch.cat((x1,x2),dim=1)\n",
    "        \n",
    "        return x,x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1yVWgR4vFhe"
   },
   "outputs": [],
   "source": [
    "class inception_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(inception_net,self).__init__()\n",
    "        self.conv1 = Conv_module(3,96,1,3,0)\n",
    "        \n",
    "        self.incept1 = inception_module(96,32,32)\n",
    "        self.incept2 = inception_module(64,32,48)\n",
    "        \n",
    "        self.downsample1 = downsample_module(80,80)\n",
    "        \n",
    "        self.incept3 = inception_module(160,112,48)\n",
    "        self.incept4 = inception_module(160,96,64)\n",
    "        self.incept5 = inception_module(160,80,80)\n",
    "        self.incept6 = inception_module(160,48,96)\n",
    "        \n",
    "        self.downsample2 = downsample_module(144,96)\n",
    "        \n",
    "        self.incept7 = inception_module(240,176,60)\n",
    "        self.incept8 = inception_module(236,176,60)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(236,10)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1.forward(x)\n",
    "        #act1 = x\n",
    "        \n",
    "        x = self.incept1.forward(x)\n",
    "        #act2 = x\n",
    "        \n",
    "        x = self.incept2.forward(x)\n",
    "        #act3 = x\n",
    "        \n",
    "        x,act4 = self.downsample1.forward(x)\n",
    "        \n",
    "        x = self.incept3.forward(x)\n",
    "        #act5 = x\n",
    "        \n",
    "        x = self.incept4.forward(x)\n",
    "        #act6 = x\n",
    "        \n",
    "        x = self.incept5.forward(x)\n",
    "        #act7 = x\n",
    "        \n",
    "        x = self.incept6.forward(x)\n",
    "        #act8 = x\n",
    "        \n",
    "        x,act9 = self.downsample2.forward(x)\n",
    "        \n",
    "        x = self.incept7.forward(x)\n",
    "        #act10 = x\n",
    "        x = self.incept8.forward(x)\n",
    "        #act11 = x\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1,1*1*236)\n",
    "        x = self.linear(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOWrnzv1fVjD"
   },
   "outputs": [],
   "source": [
    "def test_all(number, testloader,inc):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    out = []\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
    "            out.append(labels.cpu().numpy())\n",
    "            outputs= inc(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            pred.append(predicted.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFfAJZkcZEsY"
   },
   "outputs": [],
   "source": [
    "def train_all(trainloader, ds_number, testloader_list):\n",
    "    \n",
    "    print(\"--\"*40)\n",
    "    print(\"training on data set  \", ds_number)\n",
    "    \n",
    "    inc = inception_net().double()\n",
    "    inc = inc.to(\"cuda\")\n",
    "    \n",
    "    criterion_inception = nn.CrossEntropyLoss()\n",
    "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    acti = []\n",
    "    loss_curi = []\n",
    "    epochs = 70\n",
    "    \n",
    "    for epoch in range(epochs): # loop over the dataset multiple times\n",
    "        ep_lossi = []\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer_inception.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = inc(inputs)\n",
    "            loss = criterion_inception(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_inception.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 10))\n",
    "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
    "    #     if (epoch%5 == 0):\n",
    "    #         _,actis= inc(inputs)\n",
    "    #         acti.append(actis)\n",
    "\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic/weights/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = inc(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
    "    \n",
    "    for i, j in enumerate(testloader_list):\n",
    "        test_all(i+1, j,inc)\n",
    "    \n",
    "    print(\"--\"*40)\n",
    "    \n",
    "    return loss_curi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mI-vqhB-fVjJ",
    "outputId": "2cfea5b5-a4de-43e6-c5bc-9d9ffd5d3472"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "training on data set   1\n",
      "[1,    10] loss: 1.689\n",
      "[1,    20] loss: 1.098\n",
      "[1,    30] loss: 1.076\n",
      "[1,    40] loss: 1.070\n",
      "[2,    10] loss: 1.062\n",
      "[2,    20] loss: 1.061\n",
      "[2,    30] loss: 1.071\n",
      "[2,    40] loss: 1.070\n",
      "[3,    10] loss: 1.067\n",
      "[3,    20] loss: 1.054\n",
      "[3,    30] loss: 1.050\n",
      "[3,    40] loss: 1.033\n",
      "[4,    10] loss: 1.025\n",
      "[4,    20] loss: 1.019\n",
      "[4,    30] loss: 1.028\n",
      "[4,    40] loss: 1.049\n",
      "[5,    10] loss: 1.022\n",
      "[5,    20] loss: 1.023\n",
      "[5,    30] loss: 1.016\n",
      "[5,    40] loss: 0.996\n",
      "[6,    10] loss: 0.982\n",
      "[6,    20] loss: 0.978\n",
      "[6,    30] loss: 0.970\n",
      "[6,    40] loss: 0.971\n",
      "[7,    10] loss: 0.934\n",
      "[7,    20] loss: 0.936\n",
      "[7,    30] loss: 0.929\n",
      "[7,    40] loss: 0.939\n",
      "[8,    10] loss: 0.915\n",
      "[8,    20] loss: 0.927\n",
      "[8,    30] loss: 0.914\n",
      "[8,    40] loss: 0.944\n",
      "[9,    10] loss: 0.877\n",
      "[9,    20] loss: 0.890\n",
      "[9,    30] loss: 0.903\n",
      "[9,    40] loss: 0.907\n",
      "[10,    10] loss: 0.829\n",
      "[10,    20] loss: 0.812\n",
      "[10,    30] loss: 0.794\n",
      "[10,    40] loss: 0.749\n",
      "[11,    10] loss: 0.717\n",
      "[11,    20] loss: 0.725\n",
      "[11,    30] loss: 0.725\n",
      "[11,    40] loss: 0.781\n",
      "[12,    10] loss: 0.708\n",
      "[12,    20] loss: 0.719\n",
      "[12,    30] loss: 0.712\n",
      "[12,    40] loss: 0.698\n",
      "[13,    10] loss: 0.626\n",
      "[13,    20] loss: 0.615\n",
      "[13,    30] loss: 0.576\n",
      "[13,    40] loss: 0.583\n",
      "[14,    10] loss: 0.566\n",
      "[14,    20] loss: 0.642\n",
      "[14,    30] loss: 0.590\n",
      "[14,    40] loss: 0.617\n",
      "[15,    10] loss: 0.509\n",
      "[15,    20] loss: 0.519\n",
      "[15,    30] loss: 0.517\n",
      "[15,    40] loss: 0.457\n",
      "[16,    10] loss: 0.297\n",
      "[16,    20] loss: 0.297\n",
      "[16,    30] loss: 0.272\n",
      "[16,    40] loss: 0.308\n",
      "[17,    10] loss: 0.315\n",
      "[17,    20] loss: 0.338\n",
      "[17,    30] loss: 0.331\n",
      "[17,    40] loss: 0.288\n",
      "[18,    10] loss: 0.212\n",
      "[18,    20] loss: 0.214\n",
      "[18,    30] loss: 0.194\n",
      "[18,    40] loss: 0.214\n",
      "[19,    10] loss: 0.339\n",
      "[19,    20] loss: 0.404\n",
      "[19,    30] loss: 0.398\n",
      "[19,    40] loss: 0.396\n",
      "[20,    10] loss: 0.432\n",
      "[20,    20] loss: 0.422\n",
      "[20,    30] loss: 0.363\n",
      "[20,    40] loss: 0.287\n",
      "[21,    10] loss: 0.178\n",
      "[21,    20] loss: 0.204\n",
      "[21,    30] loss: 0.164\n",
      "[21,    40] loss: 0.138\n",
      "[22,    10] loss: 0.080\n",
      "[22,    20] loss: 0.084\n",
      "[22,    30] loss: 0.063\n",
      "[22,    40] loss: 0.059\n",
      "[23,    10] loss: 0.029\n",
      "[23,    20] loss: 0.029\n",
      "[23,    30] loss: 0.022\n",
      "[23,    40] loss: 0.019\n",
      "[24,    10] loss: 0.007\n",
      "[24,    20] loss: 0.006\n",
      "[24,    30] loss: 0.006\n",
      "[24,    40] loss: 0.004\n",
      "[25,    10] loss: 0.003\n",
      "[25,    20] loss: 0.002\n",
      "[25,    30] loss: 0.002\n",
      "[25,    40] loss: 0.003\n",
      "[26,    10] loss: 0.002\n",
      "[26,    20] loss: 0.002\n",
      "[26,    30] loss: 0.002\n",
      "[26,    40] loss: 0.002\n",
      "[27,    10] loss: 0.002\n",
      "[27,    20] loss: 0.001\n",
      "[27,    30] loss: 0.001\n",
      "[27,    40] loss: 0.003\n",
      "[28,    10] loss: 0.002\n",
      "[28,    20] loss: 0.002\n",
      "[28,    30] loss: 0.002\n",
      "[28,    40] loss: 0.006\n",
      "[29,    10] loss: 0.011\n",
      "[29,    20] loss: 0.022\n",
      "[29,    30] loss: 0.021\n",
      "[29,    40] loss: 0.025\n",
      "[30,    10] loss: 0.132\n",
      "[30,    20] loss: 0.168\n",
      "[30,    30] loss: 0.162\n",
      "[30,    40] loss: 0.146\n",
      "[31,    10] loss: 0.251\n",
      "[31,    20] loss: 0.324\n",
      "[31,    30] loss: 0.286\n",
      "[31,    40] loss: 0.255\n",
      "[32,    10] loss: 0.353\n",
      "[32,    20] loss: 0.356\n",
      "[32,    30] loss: 0.290\n",
      "[32,    40] loss: 0.236\n",
      "[33,    10] loss: 0.107\n",
      "[33,    20] loss: 0.102\n",
      "[33,    30] loss: 0.084\n",
      "[33,    40] loss: 0.088\n",
      "[34,    10] loss: 0.113\n",
      "[34,    20] loss: 0.101\n",
      "[34,    30] loss: 0.100\n",
      "[34,    40] loss: 0.096\n",
      "[35,    10] loss: 0.093\n",
      "[35,    20] loss: 0.085\n",
      "[35,    30] loss: 0.060\n",
      "[35,    40] loss: 0.048\n",
      "[36,    10] loss: 0.038\n",
      "[36,    20] loss: 0.029\n",
      "[36,    30] loss: 0.026\n",
      "[36,    40] loss: 0.021\n",
      "[37,    10] loss: 0.009\n",
      "[37,    20] loss: 0.006\n",
      "[37,    30] loss: 0.006\n",
      "[37,    40] loss: 0.006\n",
      "[38,    10] loss: 0.003\n",
      "[38,    20] loss: 0.003\n",
      "[38,    30] loss: 0.003\n",
      "[38,    40] loss: 0.002\n",
      "[39,    10] loss: 0.002\n",
      "[39,    20] loss: 0.002\n",
      "[39,    30] loss: 0.001\n",
      "[39,    40] loss: 0.003\n",
      "[40,    10] loss: 0.002\n",
      "[40,    20] loss: 0.004\n",
      "[40,    30] loss: 0.003\n",
      "[40,    40] loss: 0.003\n",
      "[41,    10] loss: 0.001\n",
      "[41,    20] loss: 0.001\n",
      "[41,    30] loss: 0.001\n",
      "[41,    40] loss: 0.007\n",
      "[42,    10] loss: 0.011\n",
      "[42,    20] loss: 0.008\n",
      "[42,    30] loss: 0.010\n",
      "[42,    40] loss: 0.011\n",
      "[43,    10] loss: 0.013\n",
      "[43,    20] loss: 0.018\n",
      "[43,    30] loss: 0.014\n",
      "[43,    40] loss: 0.009\n",
      "[44,    10] loss: 0.006\n",
      "[44,    20] loss: 0.004\n",
      "[44,    30] loss: 0.003\n",
      "[44,    40] loss: 0.003\n",
      "[45,    10] loss: 0.001\n",
      "[45,    20] loss: 0.001\n",
      "[45,    30] loss: 0.001\n",
      "[45,    40] loss: 0.001\n",
      "[46,    10] loss: 0.001\n",
      "[46,    20] loss: 0.001\n",
      "[46,    30] loss: 0.001\n",
      "[46,    40] loss: 0.001\n",
      "[47,    10] loss: 0.001\n",
      "[47,    20] loss: 0.001\n",
      "[47,    30] loss: 0.001\n",
      "[47,    40] loss: 0.004\n",
      "[48,    10] loss: 0.011\n",
      "[48,    20] loss: 0.023\n",
      "[48,    30] loss: 0.019\n",
      "[48,    40] loss: 0.013\n",
      "[49,    10] loss: 0.005\n",
      "[49,    20] loss: 0.005\n",
      "[49,    30] loss: 0.004\n",
      "[49,    40] loss: 0.003\n",
      "[50,    10] loss: 0.002\n",
      "[50,    20] loss: 0.002\n",
      "[50,    30] loss: 0.002\n",
      "[50,    40] loss: 0.003\n",
      "[51,    10] loss: 0.001\n",
      "[51,    20] loss: 0.002\n",
      "[51,    30] loss: 0.002\n",
      "[51,    40] loss: 0.001\n",
      "[52,    10] loss: 0.001\n",
      "[52,    20] loss: 0.001\n",
      "[52,    30] loss: 0.001\n",
      "[52,    40] loss: 0.002\n",
      "[53,    10] loss: 0.001\n",
      "[53,    20] loss: 0.001\n",
      "[53,    30] loss: 0.001\n",
      "[53,    40] loss: 0.001\n",
      "[54,    10] loss: 0.001\n",
      "[54,    20] loss: 0.001\n",
      "[54,    30] loss: 0.001\n",
      "[54,    40] loss: 0.006\n",
      "[55,    10] loss: 0.010\n",
      "[55,    20] loss: 0.017\n",
      "[55,    30] loss: 0.014\n",
      "[55,    40] loss: 0.014\n",
      "[56,    10] loss: 0.032\n",
      "[56,    20] loss: 0.052\n",
      "[56,    30] loss: 0.034\n",
      "[56,    40] loss: 0.068\n",
      "[57,    10] loss: 0.249\n",
      "[57,    20] loss: 0.366\n",
      "[57,    30] loss: 0.307\n",
      "[57,    40] loss: 0.391\n",
      "[58,    10] loss: 0.588\n",
      "[58,    20] loss: 0.571\n",
      "[58,    30] loss: 0.473\n",
      "[58,    40] loss: 0.347\n",
      "[59,    10] loss: 0.164\n",
      "[59,    20] loss: 0.133\n",
      "[59,    30] loss: 0.107\n",
      "[59,    40] loss: 0.082\n",
      "[60,    10] loss: 0.031\n",
      "[60,    20] loss: 0.038\n",
      "[60,    30] loss: 0.028\n",
      "[60,    40] loss: 0.027\n",
      "[61,    10] loss: 0.010\n",
      "[61,    20] loss: 0.012\n",
      "[61,    30] loss: 0.009\n",
      "[61,    40] loss: 0.012\n",
      "[62,    10] loss: 0.007\n",
      "[62,    20] loss: 0.006\n",
      "[62,    30] loss: 0.005\n",
      "[62,    40] loss: 0.004\n",
      "[63,    10] loss: 0.003\n",
      "[63,    20] loss: 0.002\n",
      "[63,    30] loss: 0.003\n",
      "[63,    40] loss: 0.003\n",
      "[64,    10] loss: 0.002\n",
      "[64,    20] loss: 0.003\n",
      "[64,    30] loss: 0.002\n",
      "[64,    40] loss: 0.003\n",
      "[65,    10] loss: 0.002\n",
      "[65,    20] loss: 0.002\n",
      "[65,    30] loss: 0.002\n",
      "[65,    40] loss: 0.002\n",
      "[66,    10] loss: 0.001\n",
      "[66,    20] loss: 0.001\n",
      "[66,    30] loss: 0.001\n",
      "[66,    40] loss: 0.002\n",
      "[67,    10] loss: 0.001\n",
      "[67,    20] loss: 0.001\n",
      "[67,    30] loss: 0.001\n",
      "[67,    40] loss: 0.002\n",
      "[68,    10] loss: 0.001\n",
      "[68,    20] loss: 0.001\n",
      "[68,    30] loss: 0.001\n",
      "[68,    40] loss: 0.002\n",
      "[69,    10] loss: 0.001\n",
      "[69,    20] loss: 0.001\n",
      "[69,    30] loss: 0.001\n",
      "[69,    40] loss: 0.001\n",
      "[70,    10] loss: 0.001\n",
      "[70,    20] loss: 0.001\n",
      "[70,    30] loss: 0.001\n",
      "[70,    40] loss: 0.001\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 92 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 75 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 69 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 67 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 66 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 66 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 66 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 65 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 64 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   2\n",
      "[1,    10] loss: 1.495\n",
      "[1,    20] loss: 0.951\n",
      "[1,    30] loss: 0.864\n",
      "[1,    40] loss: 0.864\n",
      "[2,    10] loss: 0.881\n",
      "[2,    20] loss: 0.805\n",
      "[2,    30] loss: 0.817\n",
      "[2,    40] loss: 0.775\n",
      "[3,    10] loss: 0.759\n",
      "[3,    20] loss: 0.752\n",
      "[3,    30] loss: 0.733\n",
      "[3,    40] loss: 0.729\n",
      "[4,    10] loss: 0.718\n",
      "[4,    20] loss: 0.695\n",
      "[4,    30] loss: 0.712\n",
      "[4,    40] loss: 0.653\n",
      "[5,    10] loss: 0.628\n",
      "[5,    20] loss: 0.611\n",
      "[5,    30] loss: 0.636\n",
      "[5,    40] loss: 0.611\n",
      "[6,    10] loss: 0.578\n",
      "[6,    20] loss: 0.541\n",
      "[6,    30] loss: 0.556\n",
      "[6,    40] loss: 0.594\n",
      "[7,    10] loss: 0.592\n",
      "[7,    20] loss: 0.599\n",
      "[7,    30] loss: 0.585\n",
      "[7,    40] loss: 0.545\n",
      "[8,    10] loss: 0.542\n",
      "[8,    20] loss: 0.482\n",
      "[8,    30] loss: 0.478\n",
      "[8,    40] loss: 0.488\n",
      "[9,    10] loss: 0.474\n",
      "[9,    20] loss: 0.422\n",
      "[9,    30] loss: 0.412\n",
      "[9,    40] loss: 0.401\n",
      "[10,    10] loss: 0.476\n",
      "[10,    20] loss: 0.420\n",
      "[10,    30] loss: 0.391\n",
      "[10,    40] loss: 0.364\n",
      "[11,    10] loss: 0.352\n",
      "[11,    20] loss: 0.319\n",
      "[11,    30] loss: 0.292\n",
      "[11,    40] loss: 0.284\n",
      "[12,    10] loss: 0.246\n",
      "[12,    20] loss: 0.277\n",
      "[12,    30] loss: 0.231\n",
      "[12,    40] loss: 0.218\n",
      "[13,    10] loss: 0.122\n",
      "[13,    20] loss: 0.116\n",
      "[13,    30] loss: 0.087\n",
      "[13,    40] loss: 0.099\n",
      "[14,    10] loss: 0.089\n",
      "[14,    20] loss: 0.074\n",
      "[14,    30] loss: 0.069\n",
      "[14,    40] loss: 0.056\n",
      "[15,    10] loss: 0.053\n",
      "[15,    20] loss: 0.047\n",
      "[15,    30] loss: 0.046\n",
      "[15,    40] loss: 0.073\n",
      "[16,    10] loss: 0.265\n",
      "[16,    20] loss: 0.308\n",
      "[16,    30] loss: 0.339\n",
      "[16,    40] loss: 0.289\n",
      "[17,    10] loss: 0.206\n",
      "[17,    20] loss: 0.189\n",
      "[17,    30] loss: 0.145\n",
      "[17,    40] loss: 0.121\n",
      "[18,    10] loss: 0.060\n",
      "[18,    20] loss: 0.051\n",
      "[18,    30] loss: 0.040\n",
      "[18,    40] loss: 0.035\n",
      "[19,    10] loss: 0.025\n",
      "[19,    20] loss: 0.024\n",
      "[19,    30] loss: 0.019\n",
      "[19,    40] loss: 0.021\n",
      "[20,    10] loss: 0.017\n",
      "[20,    20] loss: 0.017\n",
      "[20,    30] loss: 0.011\n",
      "[20,    40] loss: 0.012\n",
      "[21,    10] loss: 0.011\n",
      "[21,    20] loss: 0.009\n",
      "[21,    30] loss: 0.007\n",
      "[21,    40] loss: 0.007\n",
      "[22,    10] loss: 0.003\n",
      "[22,    20] loss: 0.003\n",
      "[22,    30] loss: 0.003\n",
      "[22,    40] loss: 0.002\n",
      "[23,    10] loss: 0.002\n",
      "[23,    20] loss: 0.002\n",
      "[23,    30] loss: 0.001\n",
      "[23,    40] loss: 0.001\n",
      "[24,    10] loss: 0.001\n",
      "[24,    20] loss: 0.001\n",
      "[24,    30] loss: 0.001\n",
      "[24,    40] loss: 0.007\n",
      "[25,    10] loss: 0.027\n",
      "[25,    20] loss: 0.018\n",
      "[25,    30] loss: 0.017\n",
      "[25,    40] loss: 0.017\n",
      "[26,    10] loss: 0.009\n",
      "[26,    20] loss: 0.008\n",
      "[26,    30] loss: 0.005\n",
      "[26,    40] loss: 0.005\n",
      "[27,    10] loss: 0.002\n",
      "[27,    20] loss: 0.002\n",
      "[27,    30] loss: 0.002\n",
      "[27,    40] loss: 0.005\n",
      "[28,    10] loss: 0.024\n",
      "[28,    20] loss: 0.043\n",
      "[28,    30] loss: 0.043\n",
      "[28,    40] loss: 0.026\n",
      "[29,    10] loss: 0.017\n",
      "[29,    20] loss: 0.019\n",
      "[29,    30] loss: 0.014\n",
      "[29,    40] loss: 0.031\n",
      "[30,    10] loss: 0.217\n",
      "[30,    20] loss: 0.258\n",
      "[30,    30] loss: 0.288\n",
      "[30,    40] loss: 0.239\n",
      "[31,    10] loss: 0.225\n",
      "[31,    20] loss: 0.185\n",
      "[31,    30] loss: 0.165\n",
      "[31,    40] loss: 0.170\n",
      "[32,    10] loss: 0.177\n",
      "[32,    20] loss: 0.183\n",
      "[32,    30] loss: 0.111\n",
      "[32,    40] loss: 0.087\n",
      "[33,    10] loss: 0.041\n",
      "[33,    20] loss: 0.028\n",
      "[33,    30] loss: 0.018\n",
      "[33,    40] loss: 0.022\n",
      "[34,    10] loss: 0.014\n",
      "[34,    20] loss: 0.010\n",
      "[34,    30] loss: 0.008\n",
      "[34,    40] loss: 0.007\n",
      "[35,    10] loss: 0.004\n",
      "[35,    20] loss: 0.003\n",
      "[35,    30] loss: 0.003\n",
      "[35,    40] loss: 0.002\n",
      "[36,    10] loss: 0.002\n",
      "[36,    20] loss: 0.002\n",
      "[36,    30] loss: 0.001\n",
      "[36,    40] loss: 0.002\n",
      "[37,    10] loss: 0.001\n",
      "[37,    20] loss: 0.002\n",
      "[37,    30] loss: 0.001\n",
      "[37,    40] loss: 0.001\n",
      "[38,    10] loss: 0.001\n",
      "[38,    20] loss: 0.001\n",
      "[38,    30] loss: 0.001\n",
      "[38,    40] loss: 0.001\n",
      "[39,    10] loss: 0.001\n",
      "[39,    20] loss: 0.001\n",
      "[39,    30] loss: 0.001\n",
      "[39,    40] loss: 0.001\n",
      "[40,    10] loss: 0.001\n",
      "[40,    20] loss: 0.001\n",
      "[40,    30] loss: 0.001\n",
      "[40,    40] loss: 0.005\n",
      "[41,    10] loss: 0.018\n",
      "[41,    20] loss: 0.009\n",
      "[41,    30] loss: 0.011\n",
      "[41,    40] loss: 0.009\n",
      "[42,    10] loss: 0.007\n",
      "[42,    20] loss: 0.005\n",
      "[42,    30] loss: 0.004\n",
      "[42,    40] loss: 0.028\n",
      "[43,    10] loss: 0.136\n",
      "[43,    20] loss: 0.157\n",
      "[43,    30] loss: 0.135\n",
      "[43,    40] loss: 0.092\n",
      "[44,    10] loss: 0.063\n",
      "[44,    20] loss: 0.046\n",
      "[44,    30] loss: 0.032\n",
      "[44,    40] loss: 0.031\n",
      "[45,    10] loss: 0.012\n",
      "[45,    20] loss: 0.010\n",
      "[45,    30] loss: 0.008\n",
      "[45,    40] loss: 0.006\n",
      "[46,    10] loss: 0.002\n",
      "[46,    20] loss: 0.002\n",
      "[46,    30] loss: 0.002\n",
      "[46,    40] loss: 0.003\n",
      "[47,    10] loss: 0.002\n",
      "[47,    20] loss: 0.002\n",
      "[47,    30] loss: 0.002\n",
      "[47,    40] loss: 0.008\n",
      "[48,    10] loss: 0.018\n",
      "[48,    20] loss: 0.023\n",
      "[48,    30] loss: 0.013\n",
      "[48,    40] loss: 0.012\n",
      "[49,    10] loss: 0.017\n",
      "[49,    20] loss: 0.010\n",
      "[49,    30] loss: 0.010\n",
      "[49,    40] loss: 0.008\n",
      "[50,    10] loss: 0.002\n",
      "[50,    20] loss: 0.004\n",
      "[50,    30] loss: 0.002\n",
      "[50,    40] loss: 0.002\n",
      "[51,    10] loss: 0.001\n",
      "[51,    20] loss: 0.001\n",
      "[51,    30] loss: 0.001\n",
      "[51,    40] loss: 0.002\n",
      "[52,    10] loss: 0.001\n",
      "[52,    20] loss: 0.003\n",
      "[52,    30] loss: 0.001\n",
      "[52,    40] loss: 0.004\n",
      "[53,    10] loss: 0.002\n",
      "[53,    20] loss: 0.002\n",
      "[53,    30] loss: 0.001\n",
      "[53,    40] loss: 0.002\n",
      "[54,    10] loss: 0.001\n",
      "[54,    20] loss: 0.001\n",
      "[54,    30] loss: 0.001\n",
      "[54,    40] loss: 0.002\n",
      "[55,    10] loss: 0.001\n",
      "[55,    20] loss: 0.001\n",
      "[55,    30] loss: 0.001\n",
      "[55,    40] loss: 0.002\n",
      "[56,    10] loss: 0.001\n",
      "[56,    20] loss: 0.001\n",
      "[56,    30] loss: 0.001\n",
      "[56,    40] loss: 0.001\n",
      "[57,    10] loss: 0.001\n",
      "[57,    20] loss: 0.001\n",
      "[57,    30] loss: 0.001\n",
      "[57,    40] loss: 0.001\n",
      "[58,    10] loss: 0.001\n",
      "[58,    20] loss: 0.001\n",
      "[58,    30] loss: 0.001\n",
      "[58,    40] loss: 0.001\n",
      "[59,    10] loss: 0.000\n",
      "[59,    20] loss: 0.000\n",
      "[59,    30] loss: 0.000\n",
      "[59,    40] loss: 0.001\n",
      "[60,    10] loss: 0.000\n",
      "[60,    20] loss: 0.001\n",
      "[60,    30] loss: 0.000\n",
      "[60,    40] loss: 0.001\n",
      "[61,    10] loss: 0.000\n",
      "[61,    20] loss: 0.000\n",
      "[61,    30] loss: 0.000\n",
      "[61,    40] loss: 0.001\n",
      "[62,    10] loss: 0.000\n",
      "[62,    20] loss: 0.001\n",
      "[62,    30] loss: 0.000\n",
      "[62,    40] loss: 0.000\n",
      "[63,    10] loss: 0.000\n",
      "[63,    20] loss: 0.000\n",
      "[63,    30] loss: 0.000\n",
      "[63,    40] loss: 0.000\n",
      "[64,    10] loss: 0.000\n",
      "[64,    20] loss: 0.000\n",
      "[64,    30] loss: 0.000\n",
      "[64,    40] loss: 0.001\n",
      "[65,    10] loss: 0.000\n",
      "[65,    20] loss: 0.000\n",
      "[65,    30] loss: 0.000\n",
      "[65,    40] loss: 0.001\n",
      "[66,    10] loss: 0.000\n",
      "[66,    20] loss: 0.000\n",
      "[66,    30] loss: 0.000\n",
      "[66,    40] loss: 0.000\n",
      "[67,    10] loss: 0.000\n",
      "[67,    20] loss: 0.000\n",
      "[67,    30] loss: 0.000\n",
      "[67,    40] loss: 0.000\n",
      "[68,    10] loss: 0.000\n",
      "[68,    20] loss: 0.000\n",
      "[68,    30] loss: 0.000\n",
      "[68,    40] loss: 0.001\n",
      "[69,    10] loss: 0.000\n",
      "[69,    20] loss: 0.000\n",
      "[69,    30] loss: 0.000\n",
      "[69,    40] loss: 0.000\n",
      "[70,    10] loss: 0.000\n",
      "[70,    20] loss: 0.000\n",
      "[70,    30] loss: 0.000\n",
      "[70,    40] loss: 0.001\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 74 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 93 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 92 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 92 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 91 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 91 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 89 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   3\n",
      "[1,    10] loss: 1.535\n",
      "[1,    20] loss: 0.792\n",
      "[1,    30] loss: 0.700\n",
      "[1,    40] loss: 0.651\n",
      "[2,    10] loss: 0.630\n",
      "[2,    20] loss: 0.598\n",
      "[2,    30] loss: 0.558\n",
      "[2,    40] loss: 0.529\n",
      "[3,    10] loss: 0.542\n",
      "[3,    20] loss: 0.464\n",
      "[3,    30] loss: 0.478\n",
      "[3,    40] loss: 0.470\n",
      "[4,    10] loss: 0.433\n",
      "[4,    20] loss: 0.411\n",
      "[4,    30] loss: 0.397\n",
      "[4,    40] loss: 0.402\n",
      "[5,    10] loss: 0.389\n",
      "[5,    20] loss: 0.322\n",
      "[5,    30] loss: 0.319\n",
      "[5,    40] loss: 0.345\n",
      "[6,    10] loss: 0.325\n",
      "[6,    20] loss: 0.315\n",
      "[6,    30] loss: 0.323\n",
      "[6,    40] loss: 0.279\n",
      "[7,    10] loss: 0.263\n",
      "[7,    20] loss: 0.265\n",
      "[7,    30] loss: 0.232\n",
      "[7,    40] loss: 0.255\n",
      "[8,    10] loss: 0.269\n",
      "[8,    20] loss: 0.257\n",
      "[8,    30] loss: 0.237\n",
      "[8,    40] loss: 0.237\n",
      "[9,    10] loss: 0.240\n",
      "[9,    20] loss: 0.250\n",
      "[9,    30] loss: 0.204\n",
      "[9,    40] loss: 0.172\n",
      "[10,    10] loss: 0.113\n",
      "[10,    20] loss: 0.115\n",
      "[10,    30] loss: 0.098\n",
      "[10,    40] loss: 0.105\n",
      "[11,    10] loss: 0.149\n",
      "[11,    20] loss: 0.144\n",
      "[11,    30] loss: 0.133\n",
      "[11,    40] loss: 0.123\n",
      "[12,    10] loss: 0.104\n",
      "[12,    20] loss: 0.095\n",
      "[12,    30] loss: 0.079\n",
      "[12,    40] loss: 0.076\n",
      "[13,    10] loss: 0.065\n",
      "[13,    20] loss: 0.059\n",
      "[13,    30] loss: 0.047\n",
      "[13,    40] loss: 0.066\n",
      "[14,    10] loss: 0.104\n",
      "[14,    20] loss: 0.105\n",
      "[14,    30] loss: 0.095\n",
      "[14,    40] loss: 0.073\n",
      "[15,    10] loss: 0.073\n",
      "[15,    20] loss: 0.053\n",
      "[15,    30] loss: 0.044\n",
      "[15,    40] loss: 0.033\n",
      "[16,    10] loss: 0.016\n",
      "[16,    20] loss: 0.015\n",
      "[16,    30] loss: 0.014\n",
      "[16,    40] loss: 0.011\n",
      "[17,    10] loss: 0.006\n",
      "[17,    20] loss: 0.005\n",
      "[17,    30] loss: 0.003\n",
      "[17,    40] loss: 0.004\n",
      "[18,    10] loss: 0.003\n",
      "[18,    20] loss: 0.003\n",
      "[18,    30] loss: 0.002\n",
      "[18,    40] loss: 0.009\n",
      "[19,    10] loss: 0.018\n",
      "[19,    20] loss: 0.021\n",
      "[19,    30] loss: 0.020\n",
      "[19,    40] loss: 0.075\n",
      "[20,    10] loss: 0.426\n",
      "[20,    20] loss: 0.389\n",
      "[20,    30] loss: 0.293\n",
      "[20,    40] loss: 0.274\n",
      "[21,    10] loss: 0.210\n",
      "[21,    20] loss: 0.132\n",
      "[21,    30] loss: 0.113\n",
      "[21,    40] loss: 0.088\n",
      "[22,    10] loss: 0.055\n",
      "[22,    20] loss: 0.038\n",
      "[22,    30] loss: 0.035\n",
      "[22,    40] loss: 0.032\n",
      "[23,    10] loss: 0.017\n",
      "[23,    20] loss: 0.018\n",
      "[23,    30] loss: 0.013\n",
      "[23,    40] loss: 0.022\n",
      "[24,    10] loss: 0.047\n",
      "[24,    20] loss: 0.032\n",
      "[24,    30] loss: 0.029\n",
      "[24,    40] loss: 0.038\n",
      "[25,    10] loss: 0.142\n",
      "[25,    20] loss: 0.125\n",
      "[25,    30] loss: 0.096\n",
      "[25,    40] loss: 0.085\n",
      "[26,    10] loss: 0.072\n",
      "[26,    20] loss: 0.055\n",
      "[26,    30] loss: 0.043\n",
      "[26,    40] loss: 0.035\n",
      "[27,    10] loss: 0.014\n",
      "[27,    20] loss: 0.015\n",
      "[27,    30] loss: 0.011\n",
      "[27,    40] loss: 0.017\n",
      "[28,    10] loss: 0.034\n",
      "[28,    20] loss: 0.023\n",
      "[28,    30] loss: 0.027\n",
      "[28,    40] loss: 0.017\n",
      "[29,    10] loss: 0.011\n",
      "[29,    20] loss: 0.006\n",
      "[29,    30] loss: 0.005\n",
      "[29,    40] loss: 0.004\n",
      "[30,    10] loss: 0.003\n",
      "[30,    20] loss: 0.002\n",
      "[30,    30] loss: 0.003\n",
      "[30,    40] loss: 0.021\n",
      "[31,    10] loss: 0.058\n",
      "[31,    20] loss: 0.047\n",
      "[31,    30] loss: 0.038\n",
      "[31,    40] loss: 0.035\n",
      "[32,    10] loss: 0.039\n",
      "[32,    20] loss: 0.021\n",
      "[32,    30] loss: 0.019\n",
      "[32,    40] loss: 0.011\n",
      "[33,    10] loss: 0.006\n",
      "[33,    20] loss: 0.004\n",
      "[33,    30] loss: 0.004\n",
      "[33,    40] loss: 0.004\n",
      "[34,    10] loss: 0.002\n",
      "[34,    20] loss: 0.002\n",
      "[34,    30] loss: 0.002\n",
      "[34,    40] loss: 0.002\n",
      "[35,    10] loss: 0.001\n",
      "[35,    20] loss: 0.001\n",
      "[35,    30] loss: 0.001\n",
      "[35,    40] loss: 0.029\n",
      "[36,    10] loss: 0.106\n",
      "[36,    20] loss: 0.107\n",
      "[36,    30] loss: 0.063\n",
      "[36,    40] loss: 0.054\n",
      "[37,    10] loss: 0.028\n",
      "[37,    20] loss: 0.019\n",
      "[37,    30] loss: 0.014\n",
      "[37,    40] loss: 0.051\n",
      "[38,    10] loss: 0.181\n",
      "[38,    20] loss: 0.137\n",
      "[38,    30] loss: 0.124\n",
      "[38,    40] loss: 0.117\n",
      "[39,    10] loss: 0.149\n",
      "[39,    20] loss: 0.121\n",
      "[39,    30] loss: 0.094\n",
      "[39,    40] loss: 0.059\n",
      "[40,    10] loss: 0.045\n",
      "[40,    20] loss: 0.034\n",
      "[40,    30] loss: 0.023\n",
      "[40,    40] loss: 0.019\n",
      "[41,    10] loss: 0.007\n",
      "[41,    20] loss: 0.009\n",
      "[41,    30] loss: 0.006\n",
      "[41,    40] loss: 0.006\n",
      "[42,    10] loss: 0.003\n",
      "[42,    20] loss: 0.003\n",
      "[42,    30] loss: 0.002\n",
      "[42,    40] loss: 0.003\n",
      "[43,    10] loss: 0.002\n",
      "[43,    20] loss: 0.002\n",
      "[43,    30] loss: 0.002\n",
      "[43,    40] loss: 0.002\n",
      "[44,    10] loss: 0.001\n",
      "[44,    20] loss: 0.001\n",
      "[44,    30] loss: 0.001\n",
      "[44,    40] loss: 0.015\n",
      "[45,    10] loss: 0.052\n",
      "[45,    20] loss: 0.028\n",
      "[45,    30] loss: 0.018\n",
      "[45,    40] loss: 0.056\n",
      "[46,    10] loss: 0.141\n",
      "[46,    20] loss: 0.115\n",
      "[46,    30] loss: 0.056\n",
      "[46,    40] loss: 0.092\n",
      "[47,    10] loss: 0.128\n",
      "[47,    20] loss: 0.148\n",
      "[47,    30] loss: 0.090\n",
      "[47,    40] loss: 0.078\n",
      "[48,    10] loss: 0.061\n",
      "[48,    20] loss: 0.050\n",
      "[48,    30] loss: 0.027\n",
      "[48,    40] loss: 0.033\n",
      "[49,    10] loss: 0.038\n",
      "[49,    20] loss: 0.033\n",
      "[49,    30] loss: 0.021\n",
      "[49,    40] loss: 0.024\n",
      "[50,    10] loss: 0.022\n",
      "[50,    20] loss: 0.014\n",
      "[50,    30] loss: 0.008\n",
      "[50,    40] loss: 0.009\n",
      "[51,    10] loss: 0.007\n",
      "[51,    20] loss: 0.006\n",
      "[51,    30] loss: 0.004\n",
      "[51,    40] loss: 0.007\n",
      "[52,    10] loss: 0.013\n",
      "[52,    20] loss: 0.008\n",
      "[52,    30] loss: 0.006\n",
      "[52,    40] loss: 0.004\n",
      "[53,    10] loss: 0.002\n",
      "[53,    20] loss: 0.002\n",
      "[53,    30] loss: 0.002\n",
      "[53,    40] loss: 0.009\n",
      "[54,    10] loss: 0.021\n",
      "[54,    20] loss: 0.012\n",
      "[54,    30] loss: 0.013\n",
      "[54,    40] loss: 0.022\n",
      "[55,    10] loss: 0.041\n",
      "[55,    20] loss: 0.030\n",
      "[55,    30] loss: 0.022\n",
      "[55,    40] loss: 0.024\n",
      "[56,    10] loss: 0.029\n",
      "[56,    20] loss: 0.034\n",
      "[56,    30] loss: 0.019\n",
      "[56,    40] loss: 0.011\n",
      "[57,    10] loss: 0.006\n",
      "[57,    20] loss: 0.004\n",
      "[57,    30] loss: 0.003\n",
      "[57,    40] loss: 0.002\n",
      "[58,    10] loss: 0.002\n",
      "[58,    20] loss: 0.001\n",
      "[58,    30] loss: 0.001\n",
      "[58,    40] loss: 0.013\n",
      "[59,    10] loss: 0.030\n",
      "[59,    20] loss: 0.018\n",
      "[59,    30] loss: 0.014\n",
      "[59,    40] loss: 0.010\n",
      "[60,    10] loss: 0.005\n",
      "[60,    20] loss: 0.004\n",
      "[60,    30] loss: 0.003\n",
      "[60,    40] loss: 0.007\n",
      "[61,    10] loss: 0.015\n",
      "[61,    20] loss: 0.009\n",
      "[61,    30] loss: 0.010\n",
      "[61,    40] loss: 0.005\n",
      "[62,    10] loss: 0.002\n",
      "[62,    20] loss: 0.003\n",
      "[62,    30] loss: 0.002\n",
      "[62,    40] loss: 0.001\n",
      "[63,    10] loss: 0.001\n",
      "[63,    20] loss: 0.001\n",
      "[63,    30] loss: 0.001\n",
      "[63,    40] loss: 0.001\n",
      "[64,    10] loss: 0.001\n",
      "[64,    20] loss: 0.001\n",
      "[64,    30] loss: 0.001\n",
      "[64,    40] loss: 0.001\n",
      "[65,    10] loss: 0.001\n",
      "[65,    20] loss: 0.001\n",
      "[65,    30] loss: 0.001\n",
      "[65,    40] loss: 0.001\n",
      "[66,    10] loss: 0.001\n",
      "[66,    20] loss: 0.000\n",
      "[66,    30] loss: 0.001\n",
      "[66,    40] loss: 0.001\n",
      "[67,    10] loss: 0.000\n",
      "[67,    20] loss: 0.001\n",
      "[67,    30] loss: 0.001\n",
      "[67,    40] loss: 0.000\n",
      "[68,    10] loss: 0.000\n",
      "[68,    20] loss: 0.000\n",
      "[68,    30] loss: 0.000\n",
      "[68,    40] loss: 0.001\n",
      "[69,    10] loss: 0.000\n",
      "[69,    20] loss: 0.000\n",
      "[69,    30] loss: 0.000\n",
      "[69,    40] loss: 0.001\n",
      "[70,    10] loss: 0.000\n",
      "[70,    20] loss: 0.000\n",
      "[70,    30] loss: 0.000\n",
      "[70,    40] loss: 0.000\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 52 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 93 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 98 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 98 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 97 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 97 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 93 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   4\n",
      "[1,    10] loss: 1.484\n",
      "[1,    20] loss: 0.691\n",
      "[1,    30] loss: 0.597\n",
      "[1,    40] loss: 0.589\n",
      "[2,    10] loss: 0.543\n",
      "[2,    20] loss: 0.507\n",
      "[2,    30] loss: 0.474\n",
      "[2,    40] loss: 0.454\n",
      "[3,    10] loss: 0.481\n",
      "[3,    20] loss: 0.440\n",
      "[3,    30] loss: 0.427\n",
      "[3,    40] loss: 0.405\n",
      "[4,    10] loss: 0.363\n",
      "[4,    20] loss: 0.355\n",
      "[4,    30] loss: 0.322\n",
      "[4,    40] loss: 0.334\n",
      "[5,    10] loss: 0.324\n",
      "[5,    20] loss: 0.295\n",
      "[5,    30] loss: 0.281\n",
      "[5,    40] loss: 0.281\n",
      "[6,    10] loss: 0.341\n",
      "[6,    20] loss: 0.300\n",
      "[6,    30] loss: 0.267\n",
      "[6,    40] loss: 0.252\n",
      "[7,    10] loss: 0.216\n",
      "[7,    20] loss: 0.195\n",
      "[7,    30] loss: 0.180\n",
      "[7,    40] loss: 0.175\n",
      "[8,    10] loss: 0.122\n",
      "[8,    20] loss: 0.127\n",
      "[8,    30] loss: 0.108\n",
      "[8,    40] loss: 0.117\n",
      "[9,    10] loss: 0.147\n",
      "[9,    20] loss: 0.127\n",
      "[9,    30] loss: 0.118\n",
      "[9,    40] loss: 0.127\n",
      "[10,    10] loss: 0.255\n",
      "[10,    20] loss: 0.200\n",
      "[10,    30] loss: 0.188\n",
      "[10,    40] loss: 0.172\n",
      "[11,    10] loss: 0.151\n",
      "[11,    20] loss: 0.128\n",
      "[11,    30] loss: 0.102\n",
      "[11,    40] loss: 0.077\n",
      "[12,    10] loss: 0.100\n",
      "[12,    20] loss: 0.087\n",
      "[12,    30] loss: 0.074\n",
      "[12,    40] loss: 0.053\n",
      "[13,    10] loss: 0.063\n",
      "[13,    20] loss: 0.049\n",
      "[13,    30] loss: 0.033\n",
      "[13,    40] loss: 0.036\n",
      "[14,    10] loss: 0.029\n",
      "[14,    20] loss: 0.029\n",
      "[14,    30] loss: 0.017\n",
      "[14,    40] loss: 0.021\n",
      "[15,    10] loss: 0.034\n",
      "[15,    20] loss: 0.025\n",
      "[15,    30] loss: 0.020\n",
      "[15,    40] loss: 0.019\n",
      "[16,    10] loss: 0.029\n",
      "[16,    20] loss: 0.023\n",
      "[16,    30] loss: 0.017\n",
      "[16,    40] loss: 0.011\n",
      "[17,    10] loss: 0.006\n",
      "[17,    20] loss: 0.005\n",
      "[17,    30] loss: 0.004\n",
      "[17,    40] loss: 0.004\n",
      "[18,    10] loss: 0.002\n",
      "[18,    20] loss: 0.002\n",
      "[18,    30] loss: 0.002\n",
      "[18,    40] loss: 0.011\n",
      "[19,    10] loss: 0.056\n",
      "[19,    20] loss: 0.065\n",
      "[19,    30] loss: 0.060\n",
      "[19,    40] loss: 0.059\n",
      "[20,    10] loss: 0.111\n",
      "[20,    20] loss: 0.089\n",
      "[20,    30] loss: 0.050\n",
      "[20,    40] loss: 0.115\n",
      "[21,    10] loss: 0.069\n",
      "[21,    20] loss: 0.073\n",
      "[21,    30] loss: 0.055\n",
      "[21,    40] loss: 0.045\n",
      "[22,    10] loss: 0.019\n",
      "[22,    20] loss: 0.020\n",
      "[22,    30] loss: 0.015\n",
      "[22,    40] loss: 0.009\n",
      "[23,    10] loss: 0.006\n",
      "[23,    20] loss: 0.004\n",
      "[23,    30] loss: 0.003\n",
      "[23,    40] loss: 0.004\n",
      "[24,    10] loss: 0.003\n",
      "[24,    20] loss: 0.003\n",
      "[24,    30] loss: 0.002\n",
      "[24,    40] loss: 0.003\n",
      "[25,    10] loss: 0.002\n",
      "[25,    20] loss: 0.003\n",
      "[25,    30] loss: 0.001\n",
      "[25,    40] loss: 0.003\n",
      "[26,    10] loss: 0.004\n",
      "[26,    20] loss: 0.003\n",
      "[26,    30] loss: 0.002\n",
      "[26,    40] loss: 0.002\n",
      "[27,    10] loss: 0.001\n",
      "[27,    20] loss: 0.001\n",
      "[27,    30] loss: 0.001\n",
      "[27,    40] loss: 0.015\n",
      "[28,    10] loss: 0.079\n",
      "[28,    20] loss: 0.066\n",
      "[28,    30] loss: 0.048\n",
      "[28,    40] loss: 0.035\n",
      "[29,    10] loss: 0.021\n",
      "[29,    20] loss: 0.018\n",
      "[29,    30] loss: 0.014\n",
      "[29,    40] loss: 0.010\n",
      "[30,    10] loss: 0.007\n",
      "[30,    20] loss: 0.006\n",
      "[30,    30] loss: 0.004\n",
      "[30,    40] loss: 0.004\n",
      "[31,    10] loss: 0.002\n",
      "[31,    20] loss: 0.001\n",
      "[31,    30] loss: 0.002\n",
      "[31,    40] loss: 0.002\n",
      "[32,    10] loss: 0.001\n",
      "[32,    20] loss: 0.001\n",
      "[32,    30] loss: 0.001\n",
      "[32,    40] loss: 0.001\n",
      "[33,    10] loss: 0.001\n",
      "[33,    20] loss: 0.001\n",
      "[33,    30] loss: 0.001\n",
      "[33,    40] loss: 0.002\n",
      "[34,    10] loss: 0.001\n",
      "[34,    20] loss: 0.001\n",
      "[34,    30] loss: 0.001\n",
      "[34,    40] loss: 0.002\n",
      "[35,    10] loss: 0.001\n",
      "[35,    20] loss: 0.001\n",
      "[35,    30] loss: 0.001\n",
      "[35,    40] loss: 0.003\n",
      "[36,    10] loss: 0.012\n",
      "[36,    20] loss: 0.009\n",
      "[36,    30] loss: 0.006\n",
      "[36,    40] loss: 0.006\n",
      "[37,    10] loss: 0.004\n",
      "[37,    20] loss: 0.002\n",
      "[37,    30] loss: 0.002\n",
      "[37,    40] loss: 0.001\n",
      "[38,    10] loss: 0.001\n",
      "[38,    20] loss: 0.001\n",
      "[38,    30] loss: 0.001\n",
      "[38,    40] loss: 0.001\n",
      "[39,    10] loss: 0.001\n",
      "[39,    20] loss: 0.001\n",
      "[39,    30] loss: 0.000\n",
      "[39,    40] loss: 0.001\n",
      "[40,    10] loss: 0.001\n",
      "[40,    20] loss: 0.001\n",
      "[40,    30] loss: 0.000\n",
      "[40,    40] loss: 0.000\n",
      "[41,    10] loss: 0.000\n",
      "[41,    20] loss: 0.000\n",
      "[41,    30] loss: 0.000\n",
      "[41,    40] loss: 0.003\n",
      "[42,    10] loss: 0.017\n",
      "[42,    20] loss: 0.010\n",
      "[42,    30] loss: 0.006\n",
      "[42,    40] loss: 0.018\n",
      "[43,    10] loss: 0.069\n",
      "[43,    20] loss: 0.068\n",
      "[43,    30] loss: 0.042\n",
      "[43,    40] loss: 0.044\n",
      "[44,    10] loss: 0.058\n",
      "[44,    20] loss: 0.051\n",
      "[44,    30] loss: 0.046\n",
      "[44,    40] loss: 0.039\n",
      "[45,    10] loss: 0.052\n",
      "[45,    20] loss: 0.057\n",
      "[45,    30] loss: 0.029\n",
      "[45,    40] loss: 0.108\n",
      "[46,    10] loss: 0.256\n",
      "[46,    20] loss: 0.220\n",
      "[46,    30] loss: 0.171\n",
      "[46,    40] loss: 0.118\n",
      "[47,    10] loss: 0.062\n",
      "[47,    20] loss: 0.057\n",
      "[47,    30] loss: 0.038\n",
      "[47,    40] loss: 0.029\n",
      "[48,    10] loss: 0.017\n",
      "[48,    20] loss: 0.010\n",
      "[48,    30] loss: 0.011\n",
      "[48,    40] loss: 0.010\n",
      "[49,    10] loss: 0.006\n",
      "[49,    20] loss: 0.005\n",
      "[49,    30] loss: 0.004\n",
      "[49,    40] loss: 0.049\n",
      "[50,    10] loss: 0.161\n",
      "[50,    20] loss: 0.096\n",
      "[50,    30] loss: 0.070\n",
      "[50,    40] loss: 0.071\n",
      "[51,    10] loss: 0.085\n",
      "[51,    20] loss: 0.061\n",
      "[51,    30] loss: 0.041\n",
      "[51,    40] loss: 0.024\n",
      "[52,    10] loss: 0.012\n",
      "[52,    20] loss: 0.009\n",
      "[52,    30] loss: 0.006\n",
      "[52,    40] loss: 0.005\n",
      "[53,    10] loss: 0.003\n",
      "[53,    20] loss: 0.003\n",
      "[53,    30] loss: 0.004\n",
      "[53,    40] loss: 0.003\n",
      "[54,    10] loss: 0.002\n",
      "[54,    20] loss: 0.002\n",
      "[54,    30] loss: 0.002\n",
      "[54,    40] loss: 0.002\n",
      "[55,    10] loss: 0.002\n",
      "[55,    20] loss: 0.001\n",
      "[55,    30] loss: 0.001\n",
      "[55,    40] loss: 0.002\n",
      "[56,    10] loss: 0.002\n",
      "[56,    20] loss: 0.002\n",
      "[56,    30] loss: 0.001\n",
      "[56,    40] loss: 0.075\n",
      "[57,    10] loss: 0.169\n",
      "[57,    20] loss: 0.120\n",
      "[57,    30] loss: 0.091\n",
      "[57,    40] loss: 0.076\n",
      "[58,    10] loss: 0.032\n",
      "[58,    20] loss: 0.019\n",
      "[58,    30] loss: 0.021\n",
      "[58,    40] loss: 0.015\n",
      "[59,    10] loss: 0.009\n",
      "[59,    20] loss: 0.008\n",
      "[59,    30] loss: 0.008\n",
      "[59,    40] loss: 0.006\n",
      "[60,    10] loss: 0.004\n",
      "[60,    20] loss: 0.004\n",
      "[60,    30] loss: 0.003\n",
      "[60,    40] loss: 0.002\n",
      "[61,    10] loss: 0.002\n",
      "[61,    20] loss: 0.002\n",
      "[61,    30] loss: 0.002\n",
      "[61,    40] loss: 0.003\n",
      "[62,    10] loss: 0.002\n",
      "[62,    20] loss: 0.002\n",
      "[62,    30] loss: 0.001\n",
      "[62,    40] loss: 0.006\n",
      "[63,    10] loss: 0.009\n",
      "[63,    20] loss: 0.006\n",
      "[63,    30] loss: 0.004\n",
      "[63,    40] loss: 0.007\n",
      "[64,    10] loss: 0.016\n",
      "[64,    20] loss: 0.010\n",
      "[64,    30] loss: 0.009\n",
      "[64,    40] loss: 0.019\n",
      "[65,    10] loss: 0.078\n",
      "[65,    20] loss: 0.061\n",
      "[65,    30] loss: 0.033\n",
      "[65,    40] loss: 0.027\n",
      "[66,    10] loss: 0.050\n",
      "[66,    20] loss: 0.027\n",
      "[66,    30] loss: 0.018\n",
      "[66,    40] loss: 0.014\n",
      "[67,    10] loss: 0.006\n",
      "[67,    20] loss: 0.005\n",
      "[67,    30] loss: 0.003\n",
      "[67,    40] loss: 0.002\n",
      "[68,    10] loss: 0.002\n",
      "[68,    20] loss: 0.001\n",
      "[68,    30] loss: 0.001\n",
      "[68,    40] loss: 0.002\n",
      "[69,    10] loss: 0.001\n",
      "[69,    20] loss: 0.001\n",
      "[69,    30] loss: 0.002\n",
      "[69,    40] loss: 0.001\n",
      "[70,    10] loss: 0.001\n",
      "[70,    20] loss: 0.001\n",
      "[70,    30] loss: 0.001\n",
      "[70,    40] loss: 0.002\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 47 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 80 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   5\n",
      "[1,    10] loss: 1.410\n",
      "[1,    20] loss: 0.699\n",
      "[1,    30] loss: 0.569\n",
      "[1,    40] loss: 0.501\n",
      "[2,    10] loss: 0.489\n",
      "[2,    20] loss: 0.446\n",
      "[2,    30] loss: 0.414\n",
      "[2,    40] loss: 0.406\n",
      "[3,    10] loss: 0.422\n",
      "[3,    20] loss: 0.375\n",
      "[3,    30] loss: 0.326\n",
      "[3,    40] loss: 0.383\n",
      "[4,    10] loss: 0.330\n",
      "[4,    20] loss: 0.320\n",
      "[4,    30] loss: 0.280\n",
      "[4,    40] loss: 0.309\n",
      "[5,    10] loss: 0.288\n",
      "[5,    20] loss: 0.264\n",
      "[5,    30] loss: 0.228\n",
      "[5,    40] loss: 0.218\n",
      "[6,    10] loss: 0.210\n",
      "[6,    20] loss: 0.189\n",
      "[6,    30] loss: 0.178\n",
      "[6,    40] loss: 0.174\n",
      "[7,    10] loss: 0.176\n",
      "[7,    20] loss: 0.180\n",
      "[7,    30] loss: 0.166\n",
      "[7,    40] loss: 0.132\n",
      "[8,    10] loss: 0.148\n",
      "[8,    20] loss: 0.150\n",
      "[8,    30] loss: 0.127\n",
      "[8,    40] loss: 0.112\n",
      "[9,    10] loss: 0.124\n",
      "[9,    20] loss: 0.115\n",
      "[9,    30] loss: 0.103\n",
      "[9,    40] loss: 0.088\n",
      "[10,    10] loss: 0.069\n",
      "[10,    20] loss: 0.054\n",
      "[10,    30] loss: 0.053\n",
      "[10,    40] loss: 0.058\n",
      "[11,    10] loss: 0.051\n",
      "[11,    20] loss: 0.046\n",
      "[11,    30] loss: 0.038\n",
      "[11,    40] loss: 0.032\n",
      "[12,    10] loss: 0.019\n",
      "[12,    20] loss: 0.019\n",
      "[12,    30] loss: 0.020\n",
      "[12,    40] loss: 0.052\n",
      "[13,    10] loss: 0.185\n",
      "[13,    20] loss: 0.211\n",
      "[13,    30] loss: 0.140\n",
      "[13,    40] loss: 0.130\n",
      "[14,    10] loss: 0.162\n",
      "[14,    20] loss: 0.123\n",
      "[14,    30] loss: 0.092\n",
      "[14,    40] loss: 0.075\n",
      "[15,    10] loss: 0.046\n",
      "[15,    20] loss: 0.038\n",
      "[15,    30] loss: 0.029\n",
      "[15,    40] loss: 0.029\n",
      "[16,    10] loss: 0.033\n",
      "[16,    20] loss: 0.034\n",
      "[16,    30] loss: 0.019\n",
      "[16,    40] loss: 0.019\n",
      "[17,    10] loss: 0.012\n",
      "[17,    20] loss: 0.006\n",
      "[17,    30] loss: 0.008\n",
      "[17,    40] loss: 0.008\n",
      "[18,    10] loss: 0.006\n",
      "[18,    20] loss: 0.006\n",
      "[18,    30] loss: 0.006\n",
      "[18,    40] loss: 0.010\n",
      "[19,    10] loss: 0.040\n",
      "[19,    20] loss: 0.038\n",
      "[19,    30] loss: 0.041\n",
      "[19,    40] loss: 0.019\n",
      "[20,    10] loss: 0.011\n",
      "[20,    20] loss: 0.010\n",
      "[20,    30] loss: 0.007\n",
      "[20,    40] loss: 0.005\n",
      "[21,    10] loss: 0.003\n",
      "[21,    20] loss: 0.002\n",
      "[21,    30] loss: 0.002\n",
      "[21,    40] loss: 0.003\n",
      "[22,    10] loss: 0.001\n",
      "[22,    20] loss: 0.001\n",
      "[22,    30] loss: 0.001\n",
      "[22,    40] loss: 0.002\n",
      "[23,    10] loss: 0.001\n",
      "[23,    20] loss: 0.001\n",
      "[23,    30] loss: 0.001\n",
      "[23,    40] loss: 0.013\n",
      "[24,    10] loss: 0.058\n",
      "[24,    20] loss: 0.034\n",
      "[24,    30] loss: 0.020\n",
      "[24,    40] loss: 0.024\n",
      "[25,    10] loss: 0.026\n",
      "[25,    20] loss: 0.027\n",
      "[25,    30] loss: 0.025\n",
      "[25,    40] loss: 0.030\n",
      "[26,    10] loss: 0.121\n",
      "[26,    20] loss: 0.124\n",
      "[26,    30] loss: 0.102\n",
      "[26,    40] loss: 0.069\n",
      "[27,    10] loss: 0.054\n",
      "[27,    20] loss: 0.052\n",
      "[27,    30] loss: 0.044\n",
      "[27,    40] loss: 0.027\n",
      "[28,    10] loss: 0.015\n",
      "[28,    20] loss: 0.008\n",
      "[28,    30] loss: 0.007\n",
      "[28,    40] loss: 0.007\n",
      "[29,    10] loss: 0.005\n",
      "[29,    20] loss: 0.002\n",
      "[29,    30] loss: 0.003\n",
      "[29,    40] loss: 0.005\n",
      "[30,    10] loss: 0.002\n",
      "[30,    20] loss: 0.004\n",
      "[30,    30] loss: 0.003\n",
      "[30,    40] loss: 0.003\n",
      "[31,    10] loss: 0.001\n",
      "[31,    20] loss: 0.001\n",
      "[31,    30] loss: 0.001\n",
      "[31,    40] loss: 0.002\n",
      "[32,    10] loss: 0.001\n",
      "[32,    20] loss: 0.001\n",
      "[32,    30] loss: 0.001\n",
      "[32,    40] loss: 0.001\n",
      "[33,    10] loss: 0.001\n",
      "[33,    20] loss: 0.001\n",
      "[33,    30] loss: 0.001\n",
      "[33,    40] loss: 0.005\n",
      "[34,    10] loss: 0.024\n",
      "[34,    20] loss: 0.022\n",
      "[34,    30] loss: 0.011\n",
      "[34,    40] loss: 0.015\n",
      "[35,    10] loss: 0.021\n",
      "[35,    20] loss: 0.016\n",
      "[35,    30] loss: 0.010\n",
      "[35,    40] loss: 0.008\n",
      "[36,    10] loss: 0.009\n",
      "[36,    20] loss: 0.006\n",
      "[36,    30] loss: 0.005\n",
      "[36,    40] loss: 0.004\n",
      "[37,    10] loss: 0.002\n",
      "[37,    20] loss: 0.001\n",
      "[37,    30] loss: 0.001\n",
      "[37,    40] loss: 0.005\n",
      "[38,    10] loss: 0.017\n",
      "[38,    20] loss: 0.011\n",
      "[38,    30] loss: 0.009\n",
      "[38,    40] loss: 0.004\n",
      "[39,    10] loss: 0.002\n",
      "[39,    20] loss: 0.001\n",
      "[39,    30] loss: 0.001\n",
      "[39,    40] loss: 0.001\n",
      "[40,    10] loss: 0.001\n",
      "[40,    20] loss: 0.001\n",
      "[40,    30] loss: 0.001\n",
      "[40,    40] loss: 0.002\n",
      "[41,    10] loss: 0.001\n",
      "[41,    20] loss: 0.001\n",
      "[41,    30] loss: 0.001\n",
      "[41,    40] loss: 0.013\n",
      "[42,    10] loss: 0.054\n",
      "[42,    20] loss: 0.042\n",
      "[42,    30] loss: 0.037\n",
      "[42,    40] loss: 0.059\n",
      "[43,    10] loss: 0.181\n",
      "[43,    20] loss: 0.117\n",
      "[43,    30] loss: 0.080\n",
      "[43,    40] loss: 0.073\n",
      "[44,    10] loss: 0.026\n",
      "[44,    20] loss: 0.018\n",
      "[44,    30] loss: 0.013\n",
      "[44,    40] loss: 0.020\n",
      "[45,    10] loss: 0.028\n",
      "[45,    20] loss: 0.026\n",
      "[45,    30] loss: 0.018\n",
      "[45,    40] loss: 0.010\n",
      "[46,    10] loss: 0.005\n",
      "[46,    20] loss: 0.005\n",
      "[46,    30] loss: 0.005\n",
      "[46,    40] loss: 0.005\n",
      "[47,    10] loss: 0.002\n",
      "[47,    20] loss: 0.003\n",
      "[47,    30] loss: 0.002\n",
      "[47,    40] loss: 0.001\n",
      "[48,    10] loss: 0.001\n",
      "[48,    20] loss: 0.001\n",
      "[48,    30] loss: 0.001\n",
      "[48,    40] loss: 0.001\n",
      "[49,    10] loss: 0.001\n",
      "[49,    20] loss: 0.001\n",
      "[49,    30] loss: 0.001\n",
      "[49,    40] loss: 0.001\n",
      "[50,    10] loss: 0.001\n",
      "[50,    20] loss: 0.001\n",
      "[50,    30] loss: 0.001\n",
      "[50,    40] loss: 0.003\n",
      "[51,    10] loss: 0.004\n",
      "[51,    20] loss: 0.004\n",
      "[51,    30] loss: 0.004\n",
      "[51,    40] loss: 0.002\n",
      "[52,    10] loss: 0.001\n",
      "[52,    20] loss: 0.001\n",
      "[52,    30] loss: 0.001\n",
      "[52,    40] loss: 0.002\n",
      "[53,    10] loss: 0.001\n",
      "[53,    20] loss: 0.001\n",
      "[53,    30] loss: 0.001\n",
      "[53,    40] loss: 0.001\n",
      "[54,    10] loss: 0.001\n",
      "[54,    20] loss: 0.000\n",
      "[54,    30] loss: 0.001\n",
      "[54,    40] loss: 0.000\n",
      "[55,    10] loss: 0.000\n",
      "[55,    20] loss: 0.001\n",
      "[55,    30] loss: 0.000\n",
      "[55,    40] loss: 0.000\n",
      "[56,    10] loss: 0.000\n",
      "[56,    20] loss: 0.000\n",
      "[56,    30] loss: 0.000\n",
      "[56,    40] loss: 0.001\n",
      "[57,    10] loss: 0.000\n",
      "[57,    20] loss: 0.000\n",
      "[57,    30] loss: 0.000\n",
      "[57,    40] loss: 0.000\n",
      "[58,    10] loss: 0.000\n",
      "[58,    20] loss: 0.000\n",
      "[58,    30] loss: 0.000\n",
      "[58,    40] loss: 0.000\n",
      "[59,    10] loss: 0.000\n",
      "[59,    20] loss: 0.000\n",
      "[59,    30] loss: 0.000\n",
      "[59,    40] loss: 0.001\n",
      "[60,    10] loss: 0.002\n",
      "[60,    20] loss: 0.002\n",
      "[60,    30] loss: 0.001\n",
      "[60,    40] loss: 0.001\n",
      "[61,    10] loss: 0.000\n",
      "[61,    20] loss: 0.000\n",
      "[61,    30] loss: 0.000\n",
      "[61,    40] loss: 0.001\n",
      "[62,    10] loss: 0.000\n",
      "[62,    20] loss: 0.000\n",
      "[62,    30] loss: 0.000\n",
      "[62,    40] loss: 0.000\n",
      "[63,    10] loss: 0.000\n",
      "[63,    20] loss: 0.000\n",
      "[63,    30] loss: 0.000\n",
      "[63,    40] loss: 0.000\n",
      "[64,    10] loss: 0.000\n",
      "[64,    20] loss: 0.000\n",
      "[64,    30] loss: 0.000\n",
      "[64,    40] loss: 0.008\n",
      "[65,    10] loss: 0.011\n",
      "[65,    20] loss: 0.006\n",
      "[65,    30] loss: 0.005\n",
      "[65,    40] loss: 0.005\n",
      "[66,    10] loss: 0.004\n",
      "[66,    20] loss: 0.001\n",
      "[66,    30] loss: 0.001\n",
      "[66,    40] loss: 0.001\n",
      "[67,    10] loss: 0.000\n",
      "[67,    20] loss: 0.001\n",
      "[67,    30] loss: 0.001\n",
      "[67,    40] loss: 0.002\n",
      "[68,    10] loss: 0.001\n",
      "[68,    20] loss: 0.002\n",
      "[68,    30] loss: 0.001\n",
      "[68,    40] loss: 0.001\n",
      "[69,    10] loss: 0.001\n",
      "[69,    20] loss: 0.001\n",
      "[69,    30] loss: 0.001\n",
      "[69,    40] loss: 0.001\n",
      "[70,    10] loss: 0.000\n",
      "[70,    20] loss: 0.001\n",
      "[70,    30] loss: 0.000\n",
      "[70,    40] loss: 0.001\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 75 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 95 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   6\n",
      "[1,    10] loss: 1.428\n",
      "[1,    20] loss: 0.654\n",
      "[1,    30] loss: 0.535\n",
      "[1,    40] loss: 0.531\n",
      "[2,    10] loss: 0.452\n",
      "[2,    20] loss: 0.439\n",
      "[2,    30] loss: 0.418\n",
      "[2,    40] loss: 0.401\n",
      "[3,    10] loss: 0.457\n",
      "[3,    20] loss: 0.408\n",
      "[3,    30] loss: 0.346\n",
      "[3,    40] loss: 0.322\n",
      "[4,    10] loss: 0.348\n",
      "[4,    20] loss: 0.290\n",
      "[4,    30] loss: 0.281\n",
      "[4,    40] loss: 0.245\n",
      "[5,    10] loss: 0.235\n",
      "[5,    20] loss: 0.223\n",
      "[5,    30] loss: 0.204\n",
      "[5,    40] loss: 0.187\n",
      "[6,    10] loss: 0.146\n",
      "[6,    20] loss: 0.151\n",
      "[6,    30] loss: 0.158\n",
      "[6,    40] loss: 0.140\n",
      "[7,    10] loss: 0.109\n",
      "[7,    20] loss: 0.105\n",
      "[7,    30] loss: 0.099\n",
      "[7,    40] loss: 0.103\n",
      "[8,    10] loss: 0.183\n",
      "[8,    20] loss: 0.167\n",
      "[8,    30] loss: 0.124\n",
      "[8,    40] loss: 0.124\n",
      "[9,    10] loss: 0.158\n",
      "[9,    20] loss: 0.136\n",
      "[9,    30] loss: 0.101\n",
      "[9,    40] loss: 0.083\n",
      "[10,    10] loss: 0.061\n",
      "[10,    20] loss: 0.055\n",
      "[10,    30] loss: 0.048\n",
      "[10,    40] loss: 0.051\n",
      "[11,    10] loss: 0.047\n",
      "[11,    20] loss: 0.044\n",
      "[11,    30] loss: 0.034\n",
      "[11,    40] loss: 0.029\n",
      "[12,    10] loss: 0.019\n",
      "[12,    20] loss: 0.015\n",
      "[12,    30] loss: 0.012\n",
      "[12,    40] loss: 0.012\n",
      "[13,    10] loss: 0.017\n",
      "[13,    20] loss: 0.021\n",
      "[13,    30] loss: 0.011\n",
      "[13,    40] loss: 0.029\n",
      "[14,    10] loss: 0.149\n",
      "[14,    20] loss: 0.148\n",
      "[14,    30] loss: 0.127\n",
      "[14,    40] loss: 0.149\n",
      "[15,    10] loss: 0.326\n",
      "[15,    20] loss: 0.251\n",
      "[15,    30] loss: 0.201\n",
      "[15,    40] loss: 0.152\n",
      "[16,    10] loss: 0.085\n",
      "[16,    20] loss: 0.071\n",
      "[16,    30] loss: 0.069\n",
      "[16,    40] loss: 0.067\n",
      "[17,    10] loss: 0.069\n",
      "[17,    20] loss: 0.072\n",
      "[17,    30] loss: 0.043\n",
      "[17,    40] loss: 0.048\n",
      "[18,    10] loss: 0.041\n",
      "[18,    20] loss: 0.043\n",
      "[18,    30] loss: 0.022\n",
      "[18,    40] loss: 0.024\n",
      "[19,    10] loss: 0.013\n",
      "[19,    20] loss: 0.014\n",
      "[19,    30] loss: 0.009\n",
      "[19,    40] loss: 0.007\n",
      "[20,    10] loss: 0.005\n",
      "[20,    20] loss: 0.003\n",
      "[20,    30] loss: 0.004\n",
      "[20,    40] loss: 0.005\n",
      "[21,    10] loss: 0.003\n",
      "[21,    20] loss: 0.003\n",
      "[21,    30] loss: 0.003\n",
      "[21,    40] loss: 0.004\n",
      "[22,    10] loss: 0.003\n",
      "[22,    20] loss: 0.004\n",
      "[22,    30] loss: 0.003\n",
      "[22,    40] loss: 0.003\n",
      "[23,    10] loss: 0.002\n",
      "[23,    20] loss: 0.002\n",
      "[23,    30] loss: 0.002\n",
      "[23,    40] loss: 0.005\n",
      "[24,    10] loss: 0.012\n",
      "[24,    20] loss: 0.011\n",
      "[24,    30] loss: 0.006\n",
      "[24,    40] loss: 0.017\n",
      "[25,    10] loss: 0.105\n",
      "[25,    20] loss: 0.075\n",
      "[25,    30] loss: 0.047\n",
      "[25,    40] loss: 0.041\n",
      "[26,    10] loss: 0.028\n",
      "[26,    20] loss: 0.019\n",
      "[26,    30] loss: 0.021\n",
      "[26,    40] loss: 0.029\n",
      "[27,    10] loss: 0.061\n",
      "[27,    20] loss: 0.054\n",
      "[27,    30] loss: 0.038\n",
      "[27,    40] loss: 0.021\n",
      "[28,    10] loss: 0.011\n",
      "[28,    20] loss: 0.008\n",
      "[28,    30] loss: 0.006\n",
      "[28,    40] loss: 0.015\n",
      "[29,    10] loss: 0.041\n",
      "[29,    20] loss: 0.042\n",
      "[29,    30] loss: 0.017\n",
      "[29,    40] loss: 0.021\n",
      "[30,    10] loss: 0.013\n",
      "[30,    20] loss: 0.009\n",
      "[30,    30] loss: 0.005\n",
      "[30,    40] loss: 0.005\n",
      "[31,    10] loss: 0.003\n",
      "[31,    20] loss: 0.003\n",
      "[31,    30] loss: 0.003\n",
      "[31,    40] loss: 0.002\n",
      "[32,    10] loss: 0.001\n",
      "[32,    20] loss: 0.001\n",
      "[32,    30] loss: 0.001\n",
      "[32,    40] loss: 0.005\n",
      "[33,    10] loss: 0.019\n",
      "[33,    20] loss: 0.011\n",
      "[33,    30] loss: 0.010\n",
      "[33,    40] loss: 0.007\n",
      "[34,    10] loss: 0.005\n",
      "[34,    20] loss: 0.003\n",
      "[34,    30] loss: 0.003\n",
      "[34,    40] loss: 0.003\n",
      "[35,    10] loss: 0.001\n",
      "[35,    20] loss: 0.001\n",
      "[35,    30] loss: 0.001\n",
      "[35,    40] loss: 0.010\n",
      "[36,    10] loss: 0.018\n",
      "[36,    20] loss: 0.027\n",
      "[36,    30] loss: 0.017\n",
      "[36,    40] loss: 0.008\n",
      "[37,    10] loss: 0.007\n",
      "[37,    20] loss: 0.005\n",
      "[37,    30] loss: 0.003\n",
      "[37,    40] loss: 0.003\n",
      "[38,    10] loss: 0.002\n",
      "[38,    20] loss: 0.002\n",
      "[38,    30] loss: 0.002\n",
      "[38,    40] loss: 0.001\n",
      "[39,    10] loss: 0.001\n",
      "[39,    20] loss: 0.001\n",
      "[39,    30] loss: 0.001\n",
      "[39,    40] loss: 0.005\n",
      "[40,    10] loss: 0.016\n",
      "[40,    20] loss: 0.010\n",
      "[40,    30] loss: 0.007\n",
      "[40,    40] loss: 0.005\n",
      "[41,    10] loss: 0.004\n",
      "[41,    20] loss: 0.005\n",
      "[41,    30] loss: 0.002\n",
      "[41,    40] loss: 0.013\n",
      "[42,    10] loss: 0.031\n",
      "[42,    20] loss: 0.027\n",
      "[42,    30] loss: 0.020\n",
      "[42,    40] loss: 0.023\n",
      "[43,    10] loss: 0.041\n",
      "[43,    20] loss: 0.029\n",
      "[43,    30] loss: 0.019\n",
      "[43,    40] loss: 0.038\n",
      "[44,    10] loss: 0.125\n",
      "[44,    20] loss: 0.066\n",
      "[44,    30] loss: 0.051\n",
      "[44,    40] loss: 0.038\n",
      "[45,    10] loss: 0.023\n",
      "[45,    20] loss: 0.010\n",
      "[45,    30] loss: 0.010\n",
      "[45,    40] loss: 0.009\n",
      "[46,    10] loss: 0.006\n",
      "[46,    20] loss: 0.004\n",
      "[46,    30] loss: 0.006\n",
      "[46,    40] loss: 0.005\n",
      "[47,    10] loss: 0.004\n",
      "[47,    20] loss: 0.004\n",
      "[47,    30] loss: 0.003\n",
      "[47,    40] loss: 0.003\n",
      "[48,    10] loss: 0.001\n",
      "[48,    20] loss: 0.001\n",
      "[48,    30] loss: 0.001\n",
      "[48,    40] loss: 0.001\n",
      "[49,    10] loss: 0.001\n",
      "[49,    20] loss: 0.001\n",
      "[49,    30] loss: 0.001\n",
      "[49,    40] loss: 0.002\n",
      "[50,    10] loss: 0.001\n",
      "[50,    20] loss: 0.001\n",
      "[50,    30] loss: 0.001\n",
      "[50,    40] loss: 0.005\n",
      "[51,    10] loss: 0.010\n",
      "[51,    20] loss: 0.011\n",
      "[51,    30] loss: 0.007\n",
      "[51,    40] loss: 0.004\n",
      "[52,    10] loss: 0.002\n",
      "[52,    20] loss: 0.002\n",
      "[52,    30] loss: 0.002\n",
      "[52,    40] loss: 0.001\n",
      "[53,    10] loss: 0.001\n",
      "[53,    20] loss: 0.001\n",
      "[53,    30] loss: 0.001\n",
      "[53,    40] loss: 0.001\n",
      "[54,    10] loss: 0.001\n",
      "[54,    20] loss: 0.001\n",
      "[54,    30] loss: 0.000\n",
      "[54,    40] loss: 0.002\n",
      "[55,    10] loss: 0.001\n",
      "[55,    20] loss: 0.002\n",
      "[55,    30] loss: 0.001\n",
      "[55,    40] loss: 0.001\n",
      "[56,    10] loss: 0.001\n",
      "[56,    20] loss: 0.000\n",
      "[56,    30] loss: 0.000\n",
      "[56,    40] loss: 0.029\n",
      "[57,    10] loss: 0.151\n",
      "[57,    20] loss: 0.095\n",
      "[57,    30] loss: 0.064\n",
      "[57,    40] loss: 0.108\n",
      "[58,    10] loss: 0.342\n",
      "[58,    20] loss: 0.218\n",
      "[58,    30] loss: 0.127\n",
      "[58,    40] loss: 0.112\n",
      "[59,    10] loss: 0.088\n",
      "[59,    20] loss: 0.071\n",
      "[59,    30] loss: 0.042\n",
      "[59,    40] loss: 0.037\n",
      "[60,    10] loss: 0.018\n",
      "[60,    20] loss: 0.013\n",
      "[60,    30] loss: 0.013\n",
      "[60,    40] loss: 0.022\n",
      "[61,    10] loss: 0.028\n",
      "[61,    20] loss: 0.021\n",
      "[61,    30] loss: 0.013\n",
      "[61,    40] loss: 0.012\n",
      "[62,    10] loss: 0.005\n",
      "[62,    20] loss: 0.004\n",
      "[62,    30] loss: 0.005\n",
      "[62,    40] loss: 0.018\n",
      "[63,    10] loss: 0.028\n",
      "[63,    20] loss: 0.025\n",
      "[63,    30] loss: 0.019\n",
      "[63,    40] loss: 0.017\n",
      "[64,    10] loss: 0.004\n",
      "[64,    20] loss: 0.006\n",
      "[64,    30] loss: 0.003\n",
      "[64,    40] loss: 0.003\n",
      "[65,    10] loss: 0.002\n",
      "[65,    20] loss: 0.002\n",
      "[65,    30] loss: 0.002\n",
      "[65,    40] loss: 0.005\n",
      "[66,    10] loss: 0.018\n",
      "[66,    20] loss: 0.009\n",
      "[66,    30] loss: 0.005\n",
      "[66,    40] loss: 0.007\n",
      "[67,    10] loss: 0.004\n",
      "[67,    20] loss: 0.002\n",
      "[67,    30] loss: 0.002\n",
      "[67,    40] loss: 0.003\n",
      "[68,    10] loss: 0.002\n",
      "[68,    20] loss: 0.004\n",
      "[68,    30] loss: 0.001\n",
      "[68,    40] loss: 0.003\n",
      "[69,    10] loss: 0.001\n",
      "[69,    20] loss: 0.001\n",
      "[69,    30] loss: 0.001\n",
      "[69,    40] loss: 0.002\n",
      "[70,    10] loss: 0.001\n",
      "[70,    20] loss: 0.001\n",
      "[70,    30] loss: 0.001\n",
      "[70,    40] loss: 0.001\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 71 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 91 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 98 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   7\n",
      "[1,    10] loss: 1.390\n",
      "[1,    20] loss: 0.618\n",
      "[1,    30] loss: 0.539\n",
      "[1,    40] loss: 0.490\n",
      "[2,    10] loss: 0.438\n",
      "[2,    20] loss: 0.394\n",
      "[2,    30] loss: 0.394\n",
      "[2,    40] loss: 0.371\n",
      "[3,    10] loss: 0.339\n",
      "[3,    20] loss: 0.306\n",
      "[3,    30] loss: 0.288\n",
      "[3,    40] loss: 0.302\n",
      "[4,    10] loss: 0.260\n",
      "[4,    20] loss: 0.237\n",
      "[4,    30] loss: 0.231\n",
      "[4,    40] loss: 0.222\n",
      "[5,    10] loss: 0.214\n",
      "[5,    20] loss: 0.185\n",
      "[5,    30] loss: 0.176\n",
      "[5,    40] loss: 0.201\n",
      "[6,    10] loss: 0.284\n",
      "[6,    20] loss: 0.243\n",
      "[6,    30] loss: 0.215\n",
      "[6,    40] loss: 0.185\n",
      "[7,    10] loss: 0.163\n",
      "[7,    20] loss: 0.165\n",
      "[7,    30] loss: 0.147\n",
      "[7,    40] loss: 0.125\n",
      "[8,    10] loss: 0.096\n",
      "[8,    20] loss: 0.092\n",
      "[8,    30] loss: 0.075\n",
      "[8,    40] loss: 0.080\n",
      "[9,    10] loss: 0.081\n",
      "[9,    20] loss: 0.072\n",
      "[9,    30] loss: 0.058\n",
      "[9,    40] loss: 0.058\n",
      "[10,    10] loss: 0.052\n",
      "[10,    20] loss: 0.050\n",
      "[10,    30] loss: 0.038\n",
      "[10,    40] loss: 0.035\n",
      "[11,    10] loss: 0.055\n",
      "[11,    20] loss: 0.059\n",
      "[11,    30] loss: 0.045\n",
      "[11,    40] loss: 0.077\n",
      "[12,    10] loss: 0.245\n",
      "[12,    20] loss: 0.191\n",
      "[12,    30] loss: 0.156\n",
      "[12,    40] loss: 0.132\n",
      "[13,    10] loss: 0.116\n",
      "[13,    20] loss: 0.094\n",
      "[13,    30] loss: 0.061\n",
      "[13,    40] loss: 0.095\n",
      "[14,    10] loss: 0.162\n",
      "[14,    20] loss: 0.145\n",
      "[14,    30] loss: 0.105\n",
      "[14,    40] loss: 0.074\n",
      "[15,    10] loss: 0.046\n",
      "[15,    20] loss: 0.043\n",
      "[15,    30] loss: 0.032\n",
      "[15,    40] loss: 0.029\n",
      "[16,    10] loss: 0.021\n",
      "[16,    20] loss: 0.017\n",
      "[16,    30] loss: 0.014\n",
      "[16,    40] loss: 0.011\n",
      "[17,    10] loss: 0.006\n",
      "[17,    20] loss: 0.009\n",
      "[17,    30] loss: 0.007\n",
      "[17,    40] loss: 0.013\n",
      "[18,    10] loss: 0.043\n",
      "[18,    20] loss: 0.028\n",
      "[18,    30] loss: 0.015\n",
      "[18,    40] loss: 0.013\n",
      "[19,    10] loss: 0.006\n",
      "[19,    20] loss: 0.006\n",
      "[19,    30] loss: 0.004\n",
      "[19,    40] loss: 0.007\n",
      "[20,    10] loss: 0.015\n",
      "[20,    20] loss: 0.015\n",
      "[20,    30] loss: 0.009\n",
      "[20,    40] loss: 0.008\n",
      "[21,    10] loss: 0.005\n",
      "[21,    20] loss: 0.003\n",
      "[21,    30] loss: 0.003\n",
      "[21,    40] loss: 0.004\n",
      "[22,    10] loss: 0.002\n",
      "[22,    20] loss: 0.002\n",
      "[22,    30] loss: 0.002\n",
      "[22,    40] loss: 0.003\n",
      "[23,    10] loss: 0.002\n",
      "[23,    20] loss: 0.002\n",
      "[23,    30] loss: 0.001\n",
      "[23,    40] loss: 0.005\n",
      "[24,    10] loss: 0.010\n",
      "[24,    20] loss: 0.007\n",
      "[24,    30] loss: 0.004\n",
      "[24,    40] loss: 0.005\n",
      "[25,    10] loss: 0.004\n",
      "[25,    20] loss: 0.003\n",
      "[25,    30] loss: 0.003\n",
      "[25,    40] loss: 0.034\n",
      "[26,    10] loss: 0.136\n",
      "[26,    20] loss: 0.128\n",
      "[26,    30] loss: 0.118\n",
      "[26,    40] loss: 0.091\n",
      "[27,    10] loss: 0.079\n",
      "[27,    20] loss: 0.062\n",
      "[27,    30] loss: 0.049\n",
      "[27,    40] loss: 0.058\n",
      "[28,    10] loss: 0.095\n",
      "[28,    20] loss: 0.076\n",
      "[28,    30] loss: 0.045\n",
      "[28,    40] loss: 0.057\n",
      "[29,    10] loss: 0.109\n",
      "[29,    20] loss: 0.077\n",
      "[29,    30] loss: 0.055\n",
      "[29,    40] loss: 0.027\n",
      "[30,    10] loss: 0.017\n",
      "[30,    20] loss: 0.011\n",
      "[30,    30] loss: 0.010\n",
      "[30,    40] loss: 0.010\n",
      "[31,    10] loss: 0.005\n",
      "[31,    20] loss: 0.005\n",
      "[31,    30] loss: 0.003\n",
      "[31,    40] loss: 0.004\n",
      "[32,    10] loss: 0.005\n",
      "[32,    20] loss: 0.003\n",
      "[32,    30] loss: 0.002\n",
      "[32,    40] loss: 0.002\n",
      "[33,    10] loss: 0.002\n",
      "[33,    20] loss: 0.001\n",
      "[33,    30] loss: 0.001\n",
      "[33,    40] loss: 0.002\n",
      "[34,    10] loss: 0.001\n",
      "[34,    20] loss: 0.002\n",
      "[34,    30] loss: 0.001\n",
      "[34,    40] loss: 0.006\n",
      "[35,    10] loss: 0.019\n",
      "[35,    20] loss: 0.010\n",
      "[35,    30] loss: 0.006\n",
      "[35,    40] loss: 0.008\n",
      "[36,    10] loss: 0.014\n",
      "[36,    20] loss: 0.010\n",
      "[36,    30] loss: 0.006\n",
      "[36,    40] loss: 0.004\n",
      "[37,    10] loss: 0.002\n",
      "[37,    20] loss: 0.002\n",
      "[37,    30] loss: 0.002\n",
      "[37,    40] loss: 0.002\n",
      "[38,    10] loss: 0.001\n",
      "[38,    20] loss: 0.001\n",
      "[38,    30] loss: 0.001\n",
      "[38,    40] loss: 0.001\n",
      "[39,    10] loss: 0.001\n",
      "[39,    20] loss: 0.001\n",
      "[39,    30] loss: 0.001\n",
      "[39,    40] loss: 0.001\n",
      "[40,    10] loss: 0.001\n",
      "[40,    20] loss: 0.001\n",
      "[40,    30] loss: 0.001\n",
      "[40,    40] loss: 0.004\n",
      "[41,    10] loss: 0.002\n",
      "[41,    20] loss: 0.003\n",
      "[41,    30] loss: 0.001\n",
      "[41,    40] loss: 0.003\n",
      "[42,    10] loss: 0.001\n",
      "[42,    20] loss: 0.002\n",
      "[42,    30] loss: 0.001\n",
      "[42,    40] loss: 0.004\n",
      "[43,    10] loss: 0.003\n",
      "[43,    20] loss: 0.005\n",
      "[43,    30] loss: 0.003\n",
      "[43,    40] loss: 0.002\n",
      "[44,    10] loss: 0.001\n",
      "[44,    20] loss: 0.001\n",
      "[44,    30] loss: 0.001\n",
      "[44,    40] loss: 0.004\n",
      "[45,    10] loss: 0.010\n",
      "[45,    20] loss: 0.010\n",
      "[45,    30] loss: 0.004\n",
      "[45,    40] loss: 0.003\n",
      "[46,    10] loss: 0.002\n",
      "[46,    20] loss: 0.001\n",
      "[46,    30] loss: 0.001\n",
      "[46,    40] loss: 0.005\n",
      "[47,    10] loss: 0.015\n",
      "[47,    20] loss: 0.010\n",
      "[47,    30] loss: 0.013\n",
      "[47,    40] loss: 0.010\n",
      "[48,    10] loss: 0.013\n",
      "[48,    20] loss: 0.012\n",
      "[48,    30] loss: 0.006\n",
      "[48,    40] loss: 0.026\n",
      "[49,    10] loss: 0.087\n",
      "[49,    20] loss: 0.082\n",
      "[49,    30] loss: 0.046\n",
      "[49,    40] loss: 0.049\n",
      "[50,    10] loss: 0.035\n",
      "[50,    20] loss: 0.019\n",
      "[50,    30] loss: 0.014\n",
      "[50,    40] loss: 0.014\n",
      "[51,    10] loss: 0.022\n",
      "[51,    20] loss: 0.022\n",
      "[51,    30] loss: 0.011\n",
      "[51,    40] loss: 0.026\n",
      "[52,    10] loss: 0.080\n",
      "[52,    20] loss: 0.073\n",
      "[52,    30] loss: 0.034\n",
      "[52,    40] loss: 0.032\n",
      "[53,    10] loss: 0.018\n",
      "[53,    20] loss: 0.011\n",
      "[53,    30] loss: 0.008\n",
      "[53,    40] loss: 0.016\n",
      "[54,    10] loss: 0.047\n",
      "[54,    20] loss: 0.036\n",
      "[54,    30] loss: 0.018\n",
      "[54,    40] loss: 0.024\n",
      "[55,    10] loss: 0.052\n",
      "[55,    20] loss: 0.041\n",
      "[55,    30] loss: 0.028\n",
      "[55,    40] loss: 0.019\n",
      "[56,    10] loss: 0.014\n",
      "[56,    20] loss: 0.018\n",
      "[56,    30] loss: 0.010\n",
      "[56,    40] loss: 0.014\n",
      "[57,    10] loss: 0.018\n",
      "[57,    20] loss: 0.017\n",
      "[57,    30] loss: 0.012\n",
      "[57,    40] loss: 0.057\n",
      "[58,    10] loss: 0.149\n",
      "[58,    20] loss: 0.113\n",
      "[58,    30] loss: 0.073\n",
      "[58,    40] loss: 0.075\n",
      "[59,    10] loss: 0.082\n",
      "[59,    20] loss: 0.070\n",
      "[59,    30] loss: 0.052\n",
      "[59,    40] loss: 0.044\n",
      "[60,    10] loss: 0.033\n",
      "[60,    20] loss: 0.030\n",
      "[60,    30] loss: 0.022\n",
      "[60,    40] loss: 0.019\n",
      "[61,    10] loss: 0.020\n",
      "[61,    20] loss: 0.010\n",
      "[61,    30] loss: 0.009\n",
      "[61,    40] loss: 0.014\n",
      "[62,    10] loss: 0.015\n",
      "[62,    20] loss: 0.017\n",
      "[62,    30] loss: 0.009\n",
      "[62,    40] loss: 0.007\n",
      "[63,    10] loss: 0.003\n",
      "[63,    20] loss: 0.003\n",
      "[63,    30] loss: 0.002\n",
      "[63,    40] loss: 0.004\n",
      "[64,    10] loss: 0.001\n",
      "[64,    20] loss: 0.002\n",
      "[64,    30] loss: 0.002\n",
      "[64,    40] loss: 0.005\n",
      "[65,    10] loss: 0.004\n",
      "[65,    20] loss: 0.006\n",
      "[65,    30] loss: 0.002\n",
      "[65,    40] loss: 0.003\n",
      "[66,    10] loss: 0.001\n",
      "[66,    20] loss: 0.001\n",
      "[66,    30] loss: 0.001\n",
      "[66,    40] loss: 0.001\n",
      "[67,    10] loss: 0.001\n",
      "[67,    20] loss: 0.001\n",
      "[67,    30] loss: 0.001\n",
      "[67,    40] loss: 0.006\n",
      "[68,    10] loss: 0.016\n",
      "[68,    20] loss: 0.013\n",
      "[68,    30] loss: 0.007\n",
      "[68,    40] loss: 0.010\n",
      "[69,    10] loss: 0.010\n",
      "[69,    20] loss: 0.008\n",
      "[69,    30] loss: 0.006\n",
      "[69,    40] loss: 0.004\n",
      "[70,    10] loss: 0.002\n",
      "[70,    20] loss: 0.002\n",
      "[70,    30] loss: 0.001\n",
      "[70,    40] loss: 0.097\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 68 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 87 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 96 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   8\n",
      "[1,    10] loss: 1.423\n",
      "[1,    20] loss: 0.659\n",
      "[1,    30] loss: 0.548\n",
      "[1,    40] loss: 0.472\n",
      "[2,    10] loss: 0.505\n",
      "[2,    20] loss: 0.472\n",
      "[2,    30] loss: 0.410\n",
      "[2,    40] loss: 0.384\n",
      "[3,    10] loss: 0.355\n",
      "[3,    20] loss: 0.351\n",
      "[3,    30] loss: 0.311\n",
      "[3,    40] loss: 0.314\n",
      "[4,    10] loss: 0.283\n",
      "[4,    20] loss: 0.270\n",
      "[4,    30] loss: 0.226\n",
      "[4,    40] loss: 0.238\n",
      "[5,    10] loss: 0.208\n",
      "[5,    20] loss: 0.196\n",
      "[5,    30] loss: 0.184\n",
      "[5,    40] loss: 0.204\n",
      "[6,    10] loss: 0.231\n",
      "[6,    20] loss: 0.233\n",
      "[6,    30] loss: 0.184\n",
      "[6,    40] loss: 0.166\n",
      "[7,    10] loss: 0.135\n",
      "[7,    20] loss: 0.107\n",
      "[7,    30] loss: 0.119\n",
      "[7,    40] loss: 0.124\n",
      "[8,    10] loss: 0.168\n",
      "[8,    20] loss: 0.149\n",
      "[8,    30] loss: 0.136\n",
      "[8,    40] loss: 0.104\n",
      "[9,    10] loss: 0.071\n",
      "[9,    20] loss: 0.062\n",
      "[9,    30] loss: 0.058\n",
      "[9,    40] loss: 0.049\n",
      "[10,    10] loss: 0.039\n",
      "[10,    20] loss: 0.029\n",
      "[10,    30] loss: 0.028\n",
      "[10,    40] loss: 0.024\n",
      "[11,    10] loss: 0.016\n",
      "[11,    20] loss: 0.018\n",
      "[11,    30] loss: 0.015\n",
      "[11,    40] loss: 0.023\n",
      "[12,    10] loss: 0.082\n",
      "[12,    20] loss: 0.068\n",
      "[12,    30] loss: 0.065\n",
      "[12,    40] loss: 0.074\n",
      "[13,    10] loss: 0.136\n",
      "[13,    20] loss: 0.108\n",
      "[13,    30] loss: 0.071\n",
      "[13,    40] loss: 0.061\n",
      "[14,    10] loss: 0.037\n",
      "[14,    20] loss: 0.035\n",
      "[14,    30] loss: 0.024\n",
      "[14,    40] loss: 0.020\n",
      "[15,    10] loss: 0.010\n",
      "[15,    20] loss: 0.007\n",
      "[15,    30] loss: 0.007\n",
      "[15,    40] loss: 0.007\n",
      "[16,    10] loss: 0.004\n",
      "[16,    20] loss: 0.004\n",
      "[16,    30] loss: 0.003\n",
      "[16,    40] loss: 0.026\n",
      "[17,    10] loss: 0.219\n",
      "[17,    20] loss: 0.171\n",
      "[17,    30] loss: 0.111\n",
      "[17,    40] loss: 0.080\n",
      "[18,    10] loss: 0.049\n",
      "[18,    20] loss: 0.047\n",
      "[18,    30] loss: 0.028\n",
      "[18,    40] loss: 0.022\n",
      "[19,    10] loss: 0.015\n",
      "[19,    20] loss: 0.012\n",
      "[19,    30] loss: 0.009\n",
      "[19,    40] loss: 0.019\n",
      "[20,    10] loss: 0.047\n",
      "[20,    20] loss: 0.048\n",
      "[20,    30] loss: 0.035\n",
      "[20,    40] loss: 0.045\n",
      "[21,    10] loss: 0.175\n",
      "[21,    20] loss: 0.129\n",
      "[21,    30] loss: 0.109\n",
      "[21,    40] loss: 0.074\n",
      "[22,    10] loss: 0.035\n",
      "[22,    20] loss: 0.021\n",
      "[22,    30] loss: 0.023\n",
      "[22,    40] loss: 0.020\n",
      "[23,    10] loss: 0.009\n",
      "[23,    20] loss: 0.009\n",
      "[23,    30] loss: 0.007\n",
      "[23,    40] loss: 0.005\n",
      "[24,    10] loss: 0.003\n",
      "[24,    20] loss: 0.003\n",
      "[24,    30] loss: 0.002\n",
      "[24,    40] loss: 0.015\n",
      "[25,    10] loss: 0.018\n",
      "[25,    20] loss: 0.020\n",
      "[25,    30] loss: 0.020\n",
      "[25,    40] loss: 0.009\n",
      "[26,    10] loss: 0.007\n",
      "[26,    20] loss: 0.005\n",
      "[26,    30] loss: 0.005\n",
      "[26,    40] loss: 0.003\n",
      "[27,    10] loss: 0.003\n",
      "[27,    20] loss: 0.002\n",
      "[27,    30] loss: 0.001\n",
      "[27,    40] loss: 0.002\n",
      "[28,    10] loss: 0.001\n",
      "[28,    20] loss: 0.001\n",
      "[28,    30] loss: 0.001\n",
      "[28,    40] loss: 0.014\n",
      "[29,    10] loss: 0.052\n",
      "[29,    20] loss: 0.061\n",
      "[29,    30] loss: 0.032\n",
      "[29,    40] loss: 0.041\n",
      "[30,    10] loss: 0.088\n",
      "[30,    20] loss: 0.054\n",
      "[30,    30] loss: 0.043\n",
      "[30,    40] loss: 0.049\n",
      "[31,    10] loss: 0.084\n",
      "[31,    20] loss: 0.069\n",
      "[31,    30] loss: 0.052\n",
      "[31,    40] loss: 0.053\n",
      "[32,    10] loss: 0.048\n",
      "[32,    20] loss: 0.044\n",
      "[32,    30] loss: 0.025\n",
      "[32,    40] loss: 0.031\n",
      "[33,    10] loss: 0.049\n",
      "[33,    20] loss: 0.028\n",
      "[33,    30] loss: 0.019\n",
      "[33,    40] loss: 0.015\n",
      "[34,    10] loss: 0.007\n",
      "[34,    20] loss: 0.004\n",
      "[34,    30] loss: 0.004\n",
      "[34,    40] loss: 0.008\n",
      "[35,    10] loss: 0.011\n",
      "[35,    20] loss: 0.009\n",
      "[35,    30] loss: 0.009\n",
      "[35,    40] loss: 0.049\n",
      "[36,    10] loss: 0.040\n",
      "[36,    20] loss: 0.044\n",
      "[36,    30] loss: 0.027\n",
      "[36,    40] loss: 0.027\n",
      "[37,    10] loss: 0.009\n",
      "[37,    20] loss: 0.008\n",
      "[37,    30] loss: 0.005\n",
      "[37,    40] loss: 0.005\n",
      "[38,    10] loss: 0.002\n",
      "[38,    20] loss: 0.002\n",
      "[38,    30] loss: 0.002\n",
      "[38,    40] loss: 0.008\n",
      "[39,    10] loss: 0.011\n",
      "[39,    20] loss: 0.008\n",
      "[39,    30] loss: 0.005\n",
      "[39,    40] loss: 0.006\n",
      "[40,    10] loss: 0.007\n",
      "[40,    20] loss: 0.004\n",
      "[40,    30] loss: 0.003\n",
      "[40,    40] loss: 0.004\n",
      "[41,    10] loss: 0.002\n",
      "[41,    20] loss: 0.002\n",
      "[41,    30] loss: 0.001\n",
      "[41,    40] loss: 0.002\n",
      "[42,    10] loss: 0.001\n",
      "[42,    20] loss: 0.001\n",
      "[42,    30] loss: 0.001\n",
      "[42,    40] loss: 0.001\n",
      "[43,    10] loss: 0.001\n",
      "[43,    20] loss: 0.001\n",
      "[43,    30] loss: 0.001\n",
      "[43,    40] loss: 0.001\n",
      "[44,    10] loss: 0.001\n",
      "[44,    20] loss: 0.000\n",
      "[44,    30] loss: 0.001\n",
      "[44,    40] loss: 0.001\n",
      "[45,    10] loss: 0.001\n",
      "[45,    20] loss: 0.001\n",
      "[45,    30] loss: 0.001\n",
      "[45,    40] loss: 0.001\n",
      "[46,    10] loss: 0.000\n",
      "[46,    20] loss: 0.001\n",
      "[46,    30] loss: 0.000\n",
      "[46,    40] loss: 0.010\n",
      "[47,    10] loss: 0.046\n",
      "[47,    20] loss: 0.019\n",
      "[47,    30] loss: 0.019\n",
      "[47,    40] loss: 0.011\n",
      "[48,    10] loss: 0.005\n",
      "[48,    20] loss: 0.002\n",
      "[48,    30] loss: 0.003\n",
      "[48,    40] loss: 0.002\n",
      "[49,    10] loss: 0.001\n",
      "[49,    20] loss: 0.001\n",
      "[49,    30] loss: 0.001\n",
      "[49,    40] loss: 0.001\n",
      "[50,    10] loss: 0.001\n",
      "[50,    20] loss: 0.001\n",
      "[50,    30] loss: 0.001\n",
      "[50,    40] loss: 0.001\n",
      "[51,    10] loss: 0.001\n",
      "[51,    20] loss: 0.001\n",
      "[51,    30] loss: 0.001\n",
      "[51,    40] loss: 0.001\n",
      "[52,    10] loss: 0.000\n",
      "[52,    20] loss: 0.000\n",
      "[52,    30] loss: 0.001\n",
      "[52,    40] loss: 0.000\n",
      "[53,    10] loss: 0.000\n",
      "[53,    20] loss: 0.000\n",
      "[53,    30] loss: 0.000\n",
      "[53,    40] loss: 0.001\n",
      "[54,    10] loss: 0.000\n",
      "[54,    20] loss: 0.000\n",
      "[54,    30] loss: 0.000\n",
      "[54,    40] loss: 0.013\n",
      "[55,    10] loss: 0.024\n",
      "[55,    20] loss: 0.035\n",
      "[55,    30] loss: 0.018\n",
      "[55,    40] loss: 0.014\n",
      "[56,    10] loss: 0.004\n",
      "[56,    20] loss: 0.002\n",
      "[56,    30] loss: 0.002\n",
      "[56,    40] loss: 0.002\n",
      "[57,    10] loss: 0.001\n",
      "[57,    20] loss: 0.001\n",
      "[57,    30] loss: 0.001\n",
      "[57,    40] loss: 0.001\n",
      "[58,    10] loss: 0.001\n",
      "[58,    20] loss: 0.001\n",
      "[58,    30] loss: 0.001\n",
      "[58,    40] loss: 0.001\n",
      "[59,    10] loss: 0.000\n",
      "[59,    20] loss: 0.001\n",
      "[59,    30] loss: 0.001\n",
      "[59,    40] loss: 0.048\n",
      "[60,    10] loss: 0.221\n",
      "[60,    20] loss: 0.147\n",
      "[60,    30] loss: 0.111\n",
      "[60,    40] loss: 0.113\n",
      "[61,    10] loss: 0.124\n",
      "[61,    20] loss: 0.091\n",
      "[61,    30] loss: 0.058\n",
      "[61,    40] loss: 0.066\n",
      "[62,    10] loss: 0.103\n",
      "[62,    20] loss: 0.083\n",
      "[62,    30] loss: 0.055\n",
      "[62,    40] loss: 0.063\n",
      "[63,    10] loss: 0.085\n",
      "[63,    20] loss: 0.056\n",
      "[63,    30] loss: 0.036\n",
      "[63,    40] loss: 0.077\n",
      "[64,    10] loss: 0.087\n",
      "[64,    20] loss: 0.053\n",
      "[64,    30] loss: 0.039\n",
      "[64,    40] loss: 0.020\n",
      "[65,    10] loss: 0.009\n",
      "[65,    20] loss: 0.008\n",
      "[65,    30] loss: 0.005\n",
      "[65,    40] loss: 0.014\n",
      "[66,    10] loss: 0.020\n",
      "[66,    20] loss: 0.019\n",
      "[66,    30] loss: 0.008\n",
      "[66,    40] loss: 0.007\n",
      "[67,    10] loss: 0.004\n",
      "[67,    20] loss: 0.003\n",
      "[67,    30] loss: 0.002\n",
      "[67,    40] loss: 0.002\n",
      "[68,    10] loss: 0.002\n",
      "[68,    20] loss: 0.002\n",
      "[68,    30] loss: 0.001\n",
      "[68,    40] loss: 0.021\n",
      "[69,    10] loss: 0.072\n",
      "[69,    20] loss: 0.033\n",
      "[69,    30] loss: 0.027\n",
      "[69,    40] loss: 0.013\n",
      "[70,    10] loss: 0.005\n",
      "[70,    20] loss: 0.005\n",
      "[70,    30] loss: 0.004\n",
      "[70,    40] loss: 0.009\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 88 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 96 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "training on data set   9\n",
      "[1,    10] loss: 1.393\n",
      "[1,    20] loss: 0.633\n",
      "[1,    30] loss: 0.512\n",
      "[1,    40] loss: 0.493\n",
      "[2,    10] loss: 0.531\n",
      "[2,    20] loss: 0.458\n",
      "[2,    30] loss: 0.422\n",
      "[2,    40] loss: 0.471\n",
      "[3,    10] loss: 0.415\n",
      "[3,    20] loss: 0.353\n",
      "[3,    30] loss: 0.322\n",
      "[3,    40] loss: 0.283\n",
      "[4,    10] loss: 0.242\n",
      "[4,    20] loss: 0.259\n",
      "[4,    30] loss: 0.221\n",
      "[4,    40] loss: 0.250\n",
      "[5,    10] loss: 0.280\n",
      "[5,    20] loss: 0.257\n",
      "[5,    30] loss: 0.214\n",
      "[5,    40] loss: 0.193\n",
      "[6,    10] loss: 0.162\n",
      "[6,    20] loss: 0.148\n",
      "[6,    30] loss: 0.141\n",
      "[6,    40] loss: 0.144\n",
      "[7,    10] loss: 0.152\n",
      "[7,    20] loss: 0.130\n",
      "[7,    30] loss: 0.123\n",
      "[7,    40] loss: 0.139\n",
      "[8,    10] loss: 0.142\n",
      "[8,    20] loss: 0.130\n",
      "[8,    30] loss: 0.125\n",
      "[8,    40] loss: 0.094\n",
      "[9,    10] loss: 0.105\n",
      "[9,    20] loss: 0.087\n",
      "[9,    30] loss: 0.076\n",
      "[9,    40] loss: 0.063\n",
      "[10,    10] loss: 0.037\n",
      "[10,    20] loss: 0.039\n",
      "[10,    30] loss: 0.027\n",
      "[10,    40] loss: 0.030\n",
      "[11,    10] loss: 0.021\n",
      "[11,    20] loss: 0.016\n",
      "[11,    30] loss: 0.017\n",
      "[11,    40] loss: 0.013\n",
      "[12,    10] loss: 0.008\n",
      "[12,    20] loss: 0.008\n",
      "[12,    30] loss: 0.006\n",
      "[12,    40] loss: 0.006\n",
      "[13,    10] loss: 0.004\n",
      "[13,    20] loss: 0.003\n",
      "[13,    30] loss: 0.004\n",
      "[13,    40] loss: 0.003\n",
      "[14,    10] loss: 0.003\n",
      "[14,    20] loss: 0.002\n",
      "[14,    30] loss: 0.002\n",
      "[14,    40] loss: 0.002\n",
      "[15,    10] loss: 0.001\n",
      "[15,    20] loss: 0.002\n",
      "[15,    30] loss: 0.002\n",
      "[15,    40] loss: 0.017\n",
      "[16,    10] loss: 0.190\n",
      "[16,    20] loss: 0.133\n",
      "[16,    30] loss: 0.128\n",
      "[16,    40] loss: 0.144\n",
      "[17,    10] loss: 0.152\n",
      "[17,    20] loss: 0.131\n",
      "[17,    30] loss: 0.092\n",
      "[17,    40] loss: 0.091\n",
      "[18,    10] loss: 0.073\n",
      "[18,    20] loss: 0.059\n",
      "[18,    30] loss: 0.033\n",
      "[18,    40] loss: 0.037\n",
      "[19,    10] loss: 0.055\n",
      "[19,    20] loss: 0.038\n",
      "[19,    30] loss: 0.037\n",
      "[19,    40] loss: 0.019\n",
      "[20,    10] loss: 0.012\n",
      "[20,    20] loss: 0.010\n",
      "[20,    30] loss: 0.007\n",
      "[20,    40] loss: 0.011\n",
      "[21,    10] loss: 0.012\n",
      "[21,    20] loss: 0.008\n",
      "[21,    30] loss: 0.009\n",
      "[21,    40] loss: 0.007\n",
      "[22,    10] loss: 0.004\n",
      "[22,    20] loss: 0.004\n",
      "[22,    30] loss: 0.003\n",
      "[22,    40] loss: 0.016\n",
      "[23,    10] loss: 0.049\n",
      "[23,    20] loss: 0.058\n",
      "[23,    30] loss: 0.047\n",
      "[23,    40] loss: 0.028\n",
      "[24,    10] loss: 0.017\n",
      "[24,    20] loss: 0.016\n",
      "[24,    30] loss: 0.009\n",
      "[24,    40] loss: 0.011\n",
      "[25,    10] loss: 0.040\n",
      "[25,    20] loss: 0.025\n",
      "[25,    30] loss: 0.019\n",
      "[25,    40] loss: 0.012\n",
      "[26,    10] loss: 0.006\n",
      "[26,    20] loss: 0.004\n",
      "[26,    30] loss: 0.003\n",
      "[26,    40] loss: 0.005\n",
      "[27,    10] loss: 0.007\n",
      "[27,    20] loss: 0.006\n",
      "[27,    30] loss: 0.004\n",
      "[27,    40] loss: 0.003\n",
      "[28,    10] loss: 0.002\n",
      "[28,    20] loss: 0.001\n",
      "[28,    30] loss: 0.001\n",
      "[28,    40] loss: 0.001\n",
      "[29,    10] loss: 0.001\n",
      "[29,    20] loss: 0.001\n",
      "[29,    30] loss: 0.001\n",
      "[29,    40] loss: 0.001\n",
      "[30,    10] loss: 0.001\n",
      "[30,    20] loss: 0.001\n",
      "[30,    30] loss: 0.001\n",
      "[30,    40] loss: 0.001\n",
      "[31,    10] loss: 0.001\n",
      "[31,    20] loss: 0.001\n",
      "[31,    30] loss: 0.001\n",
      "[31,    40] loss: 0.001\n",
      "[32,    10] loss: 0.001\n",
      "[32,    20] loss: 0.001\n",
      "[32,    30] loss: 0.001\n",
      "[32,    40] loss: 0.001\n",
      "[33,    10] loss: 0.001\n",
      "[33,    20] loss: 0.001\n",
      "[33,    30] loss: 0.001\n",
      "[33,    40] loss: 0.001\n",
      "[34,    10] loss: 0.000\n",
      "[34,    20] loss: 0.000\n",
      "[34,    30] loss: 0.000\n",
      "[34,    40] loss: 0.000\n",
      "[35,    10] loss: 0.000\n",
      "[35,    20] loss: 0.000\n",
      "[35,    30] loss: 0.000\n",
      "[35,    40] loss: 0.001\n",
      "[36,    10] loss: 0.000\n",
      "[36,    20] loss: 0.000\n",
      "[36,    30] loss: 0.000\n",
      "[36,    40] loss: 0.005\n",
      "[37,    10] loss: 0.014\n",
      "[37,    20] loss: 0.009\n",
      "[37,    30] loss: 0.006\n",
      "[37,    40] loss: 0.004\n",
      "[38,    10] loss: 0.002\n",
      "[38,    20] loss: 0.004\n",
      "[38,    30] loss: 0.002\n",
      "[38,    40] loss: 0.005\n",
      "[39,    10] loss: 0.011\n",
      "[39,    20] loss: 0.008\n",
      "[39,    30] loss: 0.005\n",
      "[39,    40] loss: 0.023\n",
      "[40,    10] loss: 0.169\n",
      "[40,    20] loss: 0.172\n",
      "[40,    30] loss: 0.103\n",
      "[40,    40] loss: 0.096\n",
      "[41,    10] loss: 0.045\n",
      "[41,    20] loss: 0.037\n",
      "[41,    30] loss: 0.022\n",
      "[41,    40] loss: 0.020\n",
      "[42,    10] loss: 0.009\n",
      "[42,    20] loss: 0.006\n",
      "[42,    30] loss: 0.006\n",
      "[42,    40] loss: 0.005\n",
      "[43,    10] loss: 0.004\n",
      "[43,    20] loss: 0.002\n",
      "[43,    30] loss: 0.002\n",
      "[43,    40] loss: 0.003\n",
      "[44,    10] loss: 0.003\n",
      "[44,    20] loss: 0.004\n",
      "[44,    30] loss: 0.002\n",
      "[44,    40] loss: 0.003\n",
      "[45,    10] loss: 0.002\n",
      "[45,    20] loss: 0.002\n",
      "[45,    30] loss: 0.002\n",
      "[45,    40] loss: 0.002\n",
      "[46,    10] loss: 0.001\n",
      "[46,    20] loss: 0.001\n",
      "[46,    30] loss: 0.001\n",
      "[46,    40] loss: 0.001\n",
      "[47,    10] loss: 0.001\n",
      "[47,    20] loss: 0.001\n",
      "[47,    30] loss: 0.001\n",
      "[47,    40] loss: 0.001\n",
      "[48,    10] loss: 0.001\n",
      "[48,    20] loss: 0.001\n",
      "[48,    30] loss: 0.001\n",
      "[48,    40] loss: 0.001\n",
      "[49,    10] loss: 0.001\n",
      "[49,    20] loss: 0.001\n",
      "[49,    30] loss: 0.001\n",
      "[49,    40] loss: 0.001\n",
      "[50,    10] loss: 0.001\n",
      "[50,    20] loss: 0.001\n",
      "[50,    30] loss: 0.001\n",
      "[50,    40] loss: 0.002\n",
      "[51,    10] loss: 0.001\n",
      "[51,    20] loss: 0.001\n",
      "[51,    30] loss: 0.001\n",
      "[51,    40] loss: 0.001\n",
      "[52,    10] loss: 0.001\n",
      "[52,    20] loss: 0.001\n",
      "[52,    30] loss: 0.001\n",
      "[52,    40] loss: 0.002\n",
      "[53,    10] loss: 0.003\n",
      "[53,    20] loss: 0.003\n",
      "[53,    30] loss: 0.001\n",
      "[53,    40] loss: 0.004\n",
      "[54,    10] loss: 0.014\n",
      "[54,    20] loss: 0.007\n",
      "[54,    30] loss: 0.005\n",
      "[54,    40] loss: 0.005\n",
      "[55,    10] loss: 0.003\n",
      "[55,    20] loss: 0.002\n",
      "[55,    30] loss: 0.001\n",
      "[55,    40] loss: 0.001\n",
      "[56,    10] loss: 0.001\n",
      "[56,    20] loss: 0.001\n",
      "[56,    30] loss: 0.001\n",
      "[56,    40] loss: 0.001\n",
      "[57,    10] loss: 0.001\n",
      "[57,    20] loss: 0.000\n",
      "[57,    30] loss: 0.001\n",
      "[57,    40] loss: 0.001\n",
      "[58,    10] loss: 0.000\n",
      "[58,    20] loss: 0.001\n",
      "[58,    30] loss: 0.001\n",
      "[58,    40] loss: 0.002\n",
      "[59,    10] loss: 0.002\n",
      "[59,    20] loss: 0.001\n",
      "[59,    30] loss: 0.001\n",
      "[59,    40] loss: 0.009\n",
      "[60,    10] loss: 0.031\n",
      "[60,    20] loss: 0.014\n",
      "[60,    30] loss: 0.015\n",
      "[60,    40] loss: 0.010\n",
      "[61,    10] loss: 0.044\n",
      "[61,    20] loss: 0.021\n",
      "[61,    30] loss: 0.013\n",
      "[61,    40] loss: 0.011\n",
      "[62,    10] loss: 0.008\n",
      "[62,    20] loss: 0.004\n",
      "[62,    30] loss: 0.004\n",
      "[62,    40] loss: 0.008\n",
      "[63,    10] loss: 0.018\n",
      "[63,    20] loss: 0.015\n",
      "[63,    30] loss: 0.009\n",
      "[63,    40] loss: 0.006\n",
      "[64,    10] loss: 0.004\n",
      "[64,    20] loss: 0.003\n",
      "[64,    30] loss: 0.002\n",
      "[64,    40] loss: 0.001\n",
      "[65,    10] loss: 0.001\n",
      "[65,    20] loss: 0.001\n",
      "[65,    30] loss: 0.001\n",
      "[65,    40] loss: 0.017\n",
      "[66,    10] loss: 0.065\n",
      "[66,    20] loss: 0.037\n",
      "[66,    30] loss: 0.023\n",
      "[66,    40] loss: 0.020\n",
      "[67,    10] loss: 0.008\n",
      "[67,    20] loss: 0.006\n",
      "[67,    30] loss: 0.004\n",
      "[67,    40] loss: 0.004\n",
      "[68,    10] loss: 0.002\n",
      "[68,    20] loss: 0.001\n",
      "[68,    30] loss: 0.001\n",
      "[68,    40] loss: 0.002\n",
      "[69,    10] loss: 0.001\n",
      "[69,    20] loss: 0.001\n",
      "[69,    30] loss: 0.001\n",
      "[69,    40] loss: 0.025\n",
      "[70,    10] loss: 0.056\n",
      "[70,    20] loss: 0.053\n",
      "[70,    30] loss: 0.022\n",
      "[70,    40] loss: 0.026\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 train images: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 1: 44 %\n",
      "Accuracy of the network on the 10000 test dataset 2: 67 %\n",
      "Accuracy of the network on the 10000 test dataset 3: 84 %\n",
      "Accuracy of the network on the 10000 test dataset 4: 93 %\n",
      "Accuracy of the network on the 10000 test dataset 5: 97 %\n",
      "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
      "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loss_all=[]\n",
    "\n",
    "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
    "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
    "\n",
    "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
    "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbZaQekCfVjN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "ouBomi5DfVjR",
    "outputId": "f2ec7506-9544-4a6c-adc8-2e6d23c7eaa9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOy9d3hU17W//+6Z0Uga9QpCBZBANIGE\nEL2Dg417N3YSx3HNvXFiJw6xE/umOF/nOj87TmLHFdtxr+Sa4IoTg2mmgyyKAEkgkECol1EbTdm/\nP85ISKgNYkajkfb7POcZzTl79l5zHph19l5rfbaQUqJQKBSKoY3O2wYoFAqFwvsoZ6BQKBQK5QwU\nCoVCoZyBQqFQKFDOQKFQKBSAwdsG9IXo6Gg5atQob5uhUCgUPsWePXsqpJQxXV3zSWcwatQodu/e\n7W0zFAqFwqcQQpzo7ppaJlIoFAqFcgYKhUKhUM5AoVAoFPhozEAxNLFarRQXF9Pc3OxtU3yagIAA\nEhIS8PPz87YpigGEcgYKn6G4uJiQkBBGjRqFEMLb5vgkUkoqKyspLi5m9OjR3jZHMYBQy0QKn6G5\nuZmoqCjlCC4AIQRRUVFqdqXohHIGCp9COYILR91DRVcMKWew92Q1//t5Lg6Hku1WKBSK9gwpZ3C4\nxMyLG49xqqbJ26YoBgG/+93vePLJJ3tss2bNGg4dOuTWcQsLC3nnnXe6vX7JJZcQHh7O5Zdf7tZx\nFYObIeUMxg4LBiC/rN7LliiGCt5wBitXruTNN99065iKwY9HnYEQ4lUhRJkQ4kA3178rhMgRQuwX\nQnwjhEj3pD1jYzVnkFdm9uQwikHMY489RmpqKvPmzePIkSNt51etWsX06dNJT0/nuuuuo7GxkW++\n+Ya1a9eycuVKMjIyKCgo6LIdwIcffkhaWhrp6eksWLAAALvdzsqVK5k+fTpTpkzhxRdfBOChhx5i\n8+bNZGRk8Je//KWTjUuXLiUkJKQf7oZiMOHp1NLXgL8Db3Rz/TiwUEpZLYRYDrwEzPSUMeEmIzEh\n/uSVqpmBr/P7jw9y6HSdW/ucOCKU314xqdvre/bs4b333iM7OxubzUZmZibTpk0D4Nprr+Wuu+4C\n4JFHHuGVV17hJz/5CVdeeSWXX345119/PQDh4eFdtnv00UdZt24d8fHx1NTUAPDKK68QFhbGrl27\nsFgszJ07l2XLlvH444/z5JNP8sknn7j1+yuGNh51BlLKTUKIUT1c/6bd2+1AgiftAW12kKeWiRR9\nYPPmzVxzzTWYTCYArrzyyrZrBw4c4JFHHqGmpob6+nouvvjiLvvort3cuXO57bbbuPHGG7n22msB\n+PLLL8nJyWH16tUA1NbWkpeXh9Fo9OTXVAxRBlLR2R3A554eZGxsMP/cewoppUqx82F6eoL3Brfd\ndhtr1qwhPT2d1157ja+//vq82r3wwgvs2LGDTz/9lGnTprFnzx6klDzzzDOdHEt3fSsUF8KACCAL\nIRajOYMHe2hztxBitxBid3l5eZ/HGjMshHqLjZJaVXSjOD8WLFjAmjVraGpqwmw28/HHH7ddM5vN\nxMXFYbVaefvtt9vOh4SEYDabe21XUFDAzJkzefTRR4mJiaGoqIiLL76Y559/HqvVCsDRo0dpaGjo\n1KdC4Q687gyEEFOAl4GrpJSV3bWTUr4kpcySUmbFxHS5N4NLpLYFkdVSkeL8yMzM5KabbiI9PZ3l\ny5czffr0tmt/+MMfmDlzJnPnzmX8+PFt51esWMETTzzB1KlTKSgo6LbdypUrmTx5MmlpacyZM4f0\n9HTuvPNOJk6cSGZmJmlpadxzzz3YbDamTJmCXq8nPT29ywDy/PnzueGGG/jqq69ISEhg3bp1nr0x\nikGBkNKzBVjOmMEnUsq0Lq4lAeuBW8+JH/RIVlaW7OvmNlUNLWT+4d88ctkE7pyf3Kc+FN4hNzeX\nCRMmeNuMQYG6l0MTIcQeKWVWV9c8GjMQQrwLLAKihRDFwG8BPwAp5QvAb4Ao4Dnn+r2tO0PdRWSQ\nkaggo6o1UCgUinZ4Opvo5l6u3wnc6UkbumKMyihSKBSKDng9ZuANxg4LJq/UjKeXyBQKhcJXGJrO\nIDaEumYb5WaLt01RKBSKAcHQdAZOjaKjqhJZoVAogKHqDGI13RalUaRQKBQaQ9IZRAcbCTf5qSCy\n4oIYiBLW2dnZzJ49m0mTJjFlyhTef/99t46tGLwMSWcghGBsbDD5aplI4WH62xmYTCbeeOMNDh48\nyBdffMH999/fJnynUPTEkHQGAGNiQzhapjKKFOfHQJewTk1NZezYsQCMGDGC2NhYLkS+RTF0GEhC\ndf3K2Nhg3m20UtnQQnSwv7fNUZwvnz8EZ/a7t8/hk2H5491e9jUJ6507d9LS0kJKSsoF3xrF4GfI\nOoPUYVoQ+WipWTkDhUv4koR1SUkJ3//+93n99dfR6YbsAoDiPBiyzqD9FphzUqK9bI3ivOnhCd4b\nDCQJ67q6Oi677DIee+wxZs2a5YZvpxgKDNlHhtgQf0ICDGrXM4XL+IKEdUtLC9dccw233npr29KU\nQuEKQ3Zm0JpRpGoNFK7SXsI6Nja2SwnrmJgYZs6c2fZjvWLFCu666y6efvppVq9e3W27lStXkpeX\nh5SSpUuXkp6ezpQpUygsLCQzMxMpJTExMaxZs6aDhPVtt93Gz372szY7PvjgAzZt2kRlZSWvvfYa\nAK+99hoZGRn9d6MUPonHJaw9wYVIWLfnwdU5fHW4lN2PfMcNVik8jZJddh/qXg5NepKwHrLLRKDF\nDSrqW6hqaPG2KQqFQuFVhrgz0DKK1N4GCoViqDO0nYFzC8zPD5RgtTu8bI1CoVB4jyHtDOLCArho\nQiz/2FrIsr9s4rP9JaoiWaFQDEmGtDMQQrDq1ixe+UEWfnrBf7+9l6uf+4adx6u8bZpCoVD0K0PL\nGViboPxIh1NCCJZOGMbn9y3gieunUFbXzE0vbeOpL49gd6hZgkKhGBoMLWew62V4dgY0dVZx1OsE\nN2Qlsv6BRVyfmcDT6/P53ss7KDM3e8FQhS8wECWsT5w4QWZmJhkZGUyaNIkXXnjBrWMrBi9DyxmE\nJWqvNSe7bRJo1PPEDek8cf0U9hVVc+nftvBNfkU/GagYbPS3M4iLi2Pbtm1kZ2ezY8cOHn/8cU6f\nPu3W8RWDk6HlDMKTtNfaol6b3pCVyL9+PI+wQAPfe2UHj6zZr+oRFANewtpoNOLvrwkvWiwWHA6V\nJadwjaElR9HqDHqYGbRn3PAQ1t47jyfWHeHN7SdYm32a+y5K5dbZI/HTDy0/OtD4084/cbjqsFv7\nHB85ngdnPNjtdV+RsC4qKuKyyy4jPz+fJ554ghEjRrjtHikGLx79RRNCvCqEKBNCHOjmuhBCPC2E\nyBdC5AghMj1pD6Yo8DNBTe8zg1aC/A387spJfH7ffNITw/nDJ4e4+K+bVMbREKS9hHVoaGgnCev5\n8+czefJk3n77bQ4ePNhlH921a5WwXrVqFXa7HdAkrN944w0yMjKYOXMmlZWV5OXl9WpnYmIiOTk5\n5Ofn8/rrr1NaWuqGb68Y7Hh6ZvAa8HfgjW6uLwfGOo+ZwPPOV88ghBY3qHVtZtCe1GEhvHH7DNYf\nLuP3Hx/iztd3se5nC4gLC/SAoYre6OkJ3hsMJAnrVkaMGEFaWhqbN29WCqaKXvHozEBKuQno6RH6\nKuANqbEdCBdCxHnSJsKTXF4mOpfWNNQ3bp+BzSF54INvcaj00yGDL0hYFxcX09TUBEB1dTVbtmxh\n3Lhxbr0PisGJtxe+44H2azbFznOeIzzxvJaJumJUdBD/c/lEvimo5NWtx91kmGKg017Cevny5V1K\nWM+dO5fx48e3nV+xYgVPPPEEU6dOpaCgoNt2K1euZPLkyaSlpTFnzhzS09O58847mThxIpmZmaSl\npXHPPfdgs9k6SFifG0DOzc1l5syZpKens3DhQn7xi18wefJkz98chc/jcQlrIcQo4BMpZVoX1z4B\nHpdSbnG+/wp4UErZSZ9aCHE3cDdAUlLStBMnTvTNoM1PwVe/h1+dAv/gvvUBSCm56409bMor5+N7\n5zFueEif+1K4hpJddh/qXg5NBrKE9Skgsd37BOe5TkgpX5JSZkkps2JiYvo+4nmkl/aEEILHr5tM\naICB+97bh8Vmv6D+FAqFwpt42xmsBW51ZhXNAmqllCUeHfE800t7IjrYnz9dN4XDZ8w89eXRC+5P\noVAovIVHs4mEEO8Ci4BoIUQx8FvAD0BK+QLwGXApkA80Aj/0pD2AW50BwNIJw7hlZhIvbT7GkvGx\nzEyOcku/CoVC0Z941BlIKW/u5boEfuxJGzoRFAt64wUvE7XnkcsmsCWvgpWrc/j8vvkE+Q+tWj6F\nQuH7eHuZqP/R6bRaAzfNDABMRgNP3pBOUXUjj3/u3qpYhUKh6A+GnjMAt6SXnsuM0ZH8cM5o3tx+\ngq1K2E6hUPgYQ9MZuHlm0MovLxlHcnQQv1ydg7nZ6vb+FQOLgShh3UpdXR0JCQnce++9bh1bMXgZ\nms4gfCQ0lIHVvXsVBPjpefLGdEpqm3js01y39q3wTbzlDP7nf/6nTf1UoXCFIeoMnKUNtcVu7zoz\nKYK7FiTz3q4int2QT2md2hxnMDHQJaxBU1ctLS1l2bJl/XBHFIOFoZn20pZeegKix7i9+59dlMr+\n4lqeWHeEJ788wszRkVyRPoJL0+KICDK6fbyhyJk//hFLrnuD9f4TxjP817/u9rovSFg7HA4eeOAB\n3nrrLf7zn/+49f4oBjdDyhlUNFVwvPY400LjtSmRG9NL2xPgp+edu2aRX1bPJzmnWfvtaR7+6ABP\nrDvCh/fMZuwwJV3hi7SXsAY6SVg/8sgj1NTUUF9f30lptLd2rRLWN954I9deey2gSVjn5OSwevVq\nAGpra8nLy8No7P6B4rnnnuPSSy8lISHBLd9ZMXQYUs7gi+Nf8Kddf2LzDRsI1xk8EkRuz5jYYO6/\nKJX7lo4lp7iWO17fzQ9e3cn//fdchocFeHTswU5PT/DeYKBIWG/bto3Nmzfz3HPPUV9fT0tLC8HB\nwTz++ONu+qaKwcqQihlEBEQAUGWtg9ARbk8v7Q4hBOmJ4bz2w+nUNlm57R87qW1S2Ua+hi9IWL/9\n9tucPHmSwsJCnnzySW699VblCBQuMbScgb/mDGqaayCs7/sa9JW0+DBe+P408svquefN3Urczsfw\nBQlrhaKveFzC2hNkZWXJ3bs7qVz3Sm5lLjd+ciN/XfRXlu5dDcc3ws/dm/bnCmv2neL+97O5bEoc\nz6yYik4n+t0GX0TJLrsPdS+HJgNZwrpfaVsmslRp6aXmErC19LsdV0+N51fLx/NpTglrvz3d7+Mr\nFArFuQxJZ1DdXK2ll0oH1HW5fYLHuXtBMjEh/nx1uMwr4ysUCkV7hpQz8Nf7YzKYNGcQ1lp41j9B\n5HMRQrBgbAyb88qxq32UFQqFlxlSzgC02UG1pfpsFXI/B5Hbs3BcDDWNVnKKa7xmg0KhUMBQdAb+\nEdrMIDQBEP2WXtoV88dEIwRsPFruNRsUCoUChqIzCHA6A4MRQuK8tkwEEBFkZEpCuHIGCoXC6wxN\nZ2Cp1t6E93+twbksTI3h26Iaqhv6P6tJcWEMVAlrvV5PRkYGGRkZHSQzFIqeGHrOwLlMJKV0bnLj\nfWfgkLBFbYgzKPGGMwgMDCQ7O5vs7GzWrl3r1rEVg5ch5Qwadu4k86092FqaabI1aTODulPg8F4l\ncHpCGGGBfmxSS0U+gS9IWCsUfWFICdW1HDtG3LpsQlL1VFuqMYUlgsOmFZ+FeUfl0aDXMW9sNBuP\nliOlRAhVjewKmz84SkVRvVv7jE4MZv6Nqd1e9wUJa4Dm5maysrIwGAw89NBDXH311W67R4rBy5By\nBvpwregstFErPItvn17qJWcA2lLRpzklHD5jZkJcqNfsUPSML0hYA5w4cYL4+HiOHTvGkiVLmDx5\nMikpKRf8/RWDm6HlDCI0ZxDSKJ1VyCO1CzVFMNJ7di1MjQG0FFPlDFyjpyd4bzBQJKwB4uPjAUhO\nTmbRokXs27dPOQNFrwypmIEhst3MwFKtzQaEDsq9u1/xsNAAxg8PYeMRFTcYyPiChHV1dTUWiwWA\niooKtm7dysSJE916HxSDE487AyHEJUKII0KIfCHEQ11cTxJCbBBC7BNC5AghLvWULa0zg9Ampz6R\nXyCMXggH/g+8rN66MDWG3SeqqLfYvGqHont8QcI6NzeXrKws0tPTWbx4MQ899JByBgqX8KiEtRBC\nDxwFvgMUA7uAm6WUh9q1eQnYJ6V8XggxEfhMSjmqp377KmEtrVYOT57C6vkGQn50B/dPux++fQ8+\nugduXwdJs867T3fxTX4Ft7y8g1W3ZvGdicO8ZsdARskuuw91L4cmFyxhLYT4XyFEqBDCIIRYJ4Qo\nFULc4sJHZwD5UspjUsoW4D3gqnPaSKB1oTwM8Jimc2ODg+qETKIt/mcLz8ZfDn4mzSl4kWmjIjAZ\n9Ww8qlRMFQpF/+PqMtFyKWUdcDnaj/V44EEXPhcPtNd7KHaea8/vgO8JIYqBz4CfdNWREOJuIcRu\nIcTu8vK+ra3n7y1jb8rthFlCqGqu0k76B2sO4eBHYLP0qV934G/Qs2BsDJ/tP0NTi9oBTaFQ9C+u\nOoPWrKNLgQ+llNVoT/Tu4GbgNSllgrP/N4UQneySUr4kpcySUmbFxMT0aaAzeZux1PyNwJYgbevL\nVqbcBM01kPdl376Bm7h93miqGlr4cI/39JIUCsXQxFVn8LkQ4gAwE/i3ECIacOUx+hSQ2O59gvNc\ne+4APgCQUm4DAoBoF+06L/yC/LEbjeht7ZaJAJIXQVAs5LzviWFdZvqoCKYmhbNq8zFsdodXbVEo\nFEMLl5yBlHIlsASYJqW0Ak3AtS58dBcwVggxWghhBFYA54qlnASWAgghJqA5A4/kWFa0NNKYkoZN\nBpxdJgLQG2Dy9XB0HTRVd9+BhxFC8KOFKRRVNfHZgTNes0OhUAw9XA0gXws0SSltzvTQfwC9rtVI\nKW3AvcA6IBf4QEp5UAjxqBCitXzzAeAuIcS3wLvAbdJDKU7hEeEANBuMmC11WB3Wsxen3AT2Fji4\nxhNDu8x3JgwjOSaIFzcW4MlML4VCoWiPq8tEv5NSmoUQc9DW9d8GXnDlg1LKz6SUqVLKFCnlY85z\nv5FSrnX+fUhKOVdKmS6lzJBSemzhPtxoIzKqCEuAP4EtUGupPXsxLh2ix3l9qUinE9yzIJmDp+uU\nkukAZ6BKWJ88eZJly5YxYcIEJk6cSGFhoVvHVwxOXHUGrektlwMvSin/Bfh7xiTPodMdYNKkr3GE\nGwhppONSkRCQfhOc3AbVhV6zEeDqqfHEhvjz4sZjXrVDceF4wxnceuutrFy5ktzcXHbu3ElsbKxb\nx1cMTlx1BiVCiGfR1vw/c67/+5yURWiIJkYnwhyENNExowhg8g3aa86H/WxZR/wNem6fN5ot+RXs\nL67t/QOKfmOgS1gfOnQIm83Gd77zHQCCg4PbhPUUip5wVajuRrTloWeklNVCiBFAJ2mJgU5IWCKU\ngy7YSmijpMpS1bFBeBKMnAvZb8O8+0Hv5x1DgVtmJvHs+nxe3FTA32/J9JodA5UNr71E2Qn3zpxi\nRyaz+La7u73uCxLWR48eJTw8nGuvvZbjx49z0UUX8fjjj6PX6916rxSDD1ezieqBg8AiIcSPgAgp\n5ecetcwDmEK1ejedydImY92J2fdC9XHY81r/GncOoQF+3DIric/2l7DjWKVXbVFotJewDg0N7SRh\nPX/+fCZPnszbb7/NwYMHu+yju3atEtarVq3CbtdWZb/88kveeOMNMjIymDlzJpWVleTl5fVoo81m\nY/PmzTz55JPs2rWLY8eO8dprr7nnBigGNS7NDIQQ9wL/DbSm2nwghHhWSvmcxyzzAAGhmuaP3tTc\n9TIRwLjlMGo+bPijtmwUGN7PVp7lRwtS+M+hUn742i7+cdt0ZiZHec2WgUZPT/DeYKBIWCckJJCR\nkUFycjIAV199Ndu3b+eOO+5wx9dUDGJcXfe/G5ghpfy1lPLXaMVnP/KcWZ7BGBKOw+aHPqCJGIux\nYwC5FSHg4se0eoNNT/S/ke2ICDLy7t2zGBEeyG3/2MV2NUPwKr4gYT19+nRqampolWxZv369Ui1V\nuISrzkAALe3eW53nfAph0GFvCUIf0Eik5Zwq5PbEpUPGd2HHi1Dl3Yye2JAA3r1rFgkRgfzwH7vY\nVqAcgrfwBQlrvV7Pk08+ydKlS5k8eTJSyrYYhULREy5JWAshfommIfRP56lrgHellD0nWXuIvkpY\nA3z2fwuxSCv29wL54s7RvHzxy103rCuBZ6bBmKVw05sXYK17KDdbuGXVdoqqG3nrjplkjYr0tkn9\njpJddh/qXg5NLljCWkr5/wH3AI3O40fecgQXTEswBv8m/C2BnbOJ2hMap2UU5a6Fwq39Z183xIT4\n8+7dswgL9OP5rwu8bY5CoRhk9OgMnHsYhAohQoHDwMvO44jznM8hbKEYjU1IR2DX2UTtmX0vhMbD\nul+Dw/vCcdHB/iwZH8vOwirsDiVVoVAo3EdvM4ODwAHna+vfB9r97XMY7BEYDFZa/P2oaa7pWf/H\naIKlv4WSbDjyaf8Z2QOzkqMwN9s4dLrO26YoFIpBRI/OQEqZKKVMcr62/t36Pqm1nRBifE/9DCSM\nOm2t3RoqcNitmK1dZ2W0Mfl6CImDfW/1g3W9M8uZXqoyixQKhTtxl6RE90IpA4yggDgAbOGS4KZu\nCs/ao9ND+grI+zeYS/vBwp4ZFhpAcnSQcgYKhcKtuMsZ+Eyaaas+EeF0X4V8Lum3gLR7XdG0lZnJ\nUew8ruIGCoXCfbjLGfjMr1JEVAoAuhArIa7MDABiUiFhOmS/AwNgj4FZyZGYLTYOnlYidt5kIEpY\nb9iwgYyMjLYjICCANWu8u0eHwjfwOeXRCyU4PA4pBXpTC6GNsvvCs3PJ+C6U58LpvZ410AVmq7iB\nz9DfzmDx4sVkZ2eTnZ3N+vXrMZlMLFu2zK3jKwYn7nIG9t6bDAz0Qf5YWwLRBza7vkwEkHYtGAK0\n2YGXiQ0NIDkmiO3HeqiTUHiEgS5h3Z7Vq1ezfPlyJWGtcAlXheqmdHG6FiiSUjqklNO7uD4g0QX5\n4WgJwhDQRESzwXVnEBAGE66A/R/CssfAL8CzhvbCrOQoPs4+jc3uwKAfchM8aj4uoOV0g1v7NI4I\nIvyKlG6v+4KEdXvee+89fv7zn1/wfVEMDVz9FXkF2AO8AbwJ7Ab+BeQJIZZ6yDaPoDcZcFi0KuTo\nFqPry0QAGbdAcy0c+cxzBrrIrOQoZ9xA1Rv0F74gYd1KSUkJ+/fv76R4qlB0h6ub2xQCd0gpcwCE\nEJOB/wF+DawGMjxinQcQfnpNkiLkDBHNYex3dWYAMHohhCZom9+kXes5I11gVrJWL7H9WCXpid6T\n2fYWPT3Be4OBImHdygcffMA111yDn5/3NmhS+BauzgwmtDoCACnlfmCilDLfM2Z5GEsIfsZmgiwB\nri8Twdmag4L1UHfac/a5QGxIACkxqt6gP/EFCetW3n33XW6++WZ3fXXFEMBVZ3BYCPGMEGKu83ja\nec4fsHnQPo+gs4YhhETvd57LRKAtFUkHfPueZ4w7D2YlR7GrsBqb3fu6SUMBX5CwBi3bqKioiIUL\nF3r2higGFa5KWJuAnwDznKe2As8AzUCwlLJfE94vRMIaYN2zD2OY8B6Nb0/i/y0pZcd3d5xfB/+4\nTNsa86f7wODfZzsulE9yTnPvO/tY8+O5ZAyBpSIlu+w+1L0cmrhDwrpRSvknKeUVzuNxKWWDlNLe\nmyMQQlwihDgihMgXQjzUTZsbhRCHhBAHhRAez900Si1P3xoCjbZGLHbL+XWw4BdQdwr2vuEB61xn\n5mhVb6BQKNyDq6mls4DfAiPbf0ZKmdrL5/TAs8B3gGJglxBirZTyULs2Y4FfAXOllNVCiNjz/hbn\nSZBxGGbAHurAaJVUN1czPGi46x0kL4KkObD5zzD1e+AX6CFLeyYmxJ8xscFsza9gyfhYCisaOFnV\nyJnaZn4wZxSJkSq/XKFQuIar2UT/AH6Jll56PgVmM4B8KeUxACHEe8BVQPuSzLuAZ6WU1QBSyrLz\n6L9PhAYnYAYcoY62wrPzcgZCwOJfw+uXw57XYNZ/ecrUXpmdHMWb20+w7C+bOpzX6wS/ulQtAygU\nCtdw1RnUSSk/7r1ZJ+KBonbvi4GZ57RJBRBCbAX0wO+klF+c25EQ4m7gboCkpKRzL58XETHDOGHz\nQxdsI6TmPKqQ2zN6PoyaD5ufgswfaHsfeIF7FiYzIjyQEeEBjIoKYmSUiXve3MOW/Aqv2KNQKHwT\nV7OJ1gsh/lcIMV0IMaX1cJMNBmAssAhtn+VVQohO0VAp5UtSyiwpZVZMTMwFDRgcHUGLxYQ+qIXQ\nJtnz9pc9sfhhaCiDXd3so9wPJESY+K9FKVyVEU96YjjhJiPzxkRz8HQdVQ0tXrNLoVD4Fq46g3nO\n4ym0GMCzwN9d+NwpILHd+wTnufYUA2ullFYp5XHgKJpz8Bj6YCN2qwnD+eoTncvI2ZCyBLb+FSz1\n2jmHHQ6ugdevhEP/cp/R58HcsdEAfFOgZgcKhcI1XM0mmt/FscCFj+4CxgohRgshjMAKYO05bdag\nzQoQQkSjLRsdc/kb9AGdyU+TpAhoJqYlgMNVh/ve2aJfQ2MlbHsW9r4Jz86AD38AxzfC1qfdZ/R5\nMCU+jJAAA1vylDPwJANRwhrgl7/8JZMmTWLChAn89Kc/7XlrV4XCSY/OQAhxs/P1p10dvXUupbQB\n9wLrgFzgAynlQSHEo0KIVmGXdUClEOIQsAFYKaX0aK6kLshPk6QwNjFRn8Cm4k3YHX0UXk2cDmOX\nwdd/hLX3gp8Jrv8HLHkETu2GmpPuNd4FDHods5Oj2JxXoX4IvEx/O4NvvvmGrVu3kpOTw4EDB9i1\naxcbN2506/iKwUlvM4MI52tMN0evSCk/k1KmSilTpJSPOc/9Rkq51vm3lFL+XEo5UUo5WUrp8dJe\nncmAsISiN9gYpQujxlLDt8Lmf2AAACAASURBVOXf9r3DZY/B5Bvhe/+EezZpukVp12nXvLRUNG9s\nNKdqmjhZ1eiV8QcrA13CWghBc3MzLS0tWCwWrFYrw4YN66e7o/BleswmklI+53z9n/4xp3/QGfXo\nWkIACNfpMOgMfF30NZnDMvvWYUwqXLeq47nIZBg+RYsfzPnJBVp8/swdo8UNtuRXMDIqqN/H9zSf\nf/45Z86ccWufw4cPZ/ny5d1e9wUJ69mzZ7N48WLi4uKQUnLvvfeqSmOFS7gUMxBCRAshfimEeE4I\n8VLr4WnjPIneEgZAo6xnxvAZbCja4P5BJl3tXCoq6r2tm0mODmJEWICKG7gRX5Cwzs/PJzc3l+Li\nYk6dOsX69evZvHmzm+6AYjDjap3Bv4DtwBZ8aFeznjDYNGdQp29mYcJC/nfn/3K89jijw0a7b5CJ\nV8NXj2pLRXPudV+/LiCEYO6YaL48VIrdIdHrRL+O72l6eoL3BgNFwvqjjz5i1qxZBAcHA9p92rZt\nG/Pnz3fH11QMYlxNLQ2SUj4gpXxHSvl+6+FRyzyMn13bD6AxwMKixEUAbCxyc6AtKkVbKjrknQ3J\n542NprbJysHT/aojOGjxBQnrpKQkNm7ciM1mw2q1snHjRrVMpHAJV53B50KIQbWrdqAhEodDhzXI\nSpxpOOMixnluqah4F9QWu7/vXpiTcjZuoLhwfEHC+vrrryclJYXJkyeTnp5Oeno6V1xxhedvjsLn\ncVXCuhoIAxqBFkCgJQJFeta8rrlQCWuA/c+t5+So+7CdDOeym9byQuHbrNq/io03biQ8wI1y0JUF\n8EwmXPxHmP1j9/XrIpf8dRORQUbeuWtWv4/tbpTssvtQ93JocsES1kA04IfmEGKc7y9ME8LLhESE\nYm0JhGAr9uoaFicuxiEdbD7l5mBbVAoMnwwHP3Jvvy4yb0w0uwuraWoZFKEehULhIXorOmuVhZjU\nzeGzhMSEY20xoTe1YK+uYkLUBGIDYz2zVDTRe0tF88ZG02J3sPtEH/WXFArFkKC3mUHrZjTPdnG4\nok00YPGPDMbeEoQhsBl7dTU6oWNB4gK2ntpKi93NAm+TrtFevVCANmN0JH56oeIGCoWiR3p0BlLK\nO5yvfdUmGrAYgv2dkhQWrNXaD+XixMU02hrZdWaXewdrWyrq/6wik9FAZlIE2wrUbmiKoYXdIZUc\ny3ngaswAIcR4IcS1QohbWg9PGuZpdEF+YAlBCGgq0Qp5ZgyfQaAh0DNLRWnXQfFOqMh3f9+9MCEu\nlOPlDeo/hmLIYLM7mPv4et7Z2f/aYL6KqxXIjwAvAS8Ay4G/Atd70C6PozMZ0LdohWfVG9fiaGgg\nwBDA7LjZbCjagEM63Dtg+s0g9LDvTff26wKJkSbMFhs1jdZ+H1uh8AYnqxo5U9dMXmm9t03xGVyd\nGdwELAZKpJTfB9IBnxa80QX5YXBKUlhFHdXvaTV0y0Yto6yxjD2le9w7YMhwSL0Est8Be//+KCc5\n90JWonXuZaBKWD/44IOkpaWRlpbG++/7dG1on8kr05xAdaPa4MlVXHUGTVJKO2ATQoQAZ4CRnjPL\n8wg/HX7NTlHWzBQqX30VR1MTixMXE2gI5LPjn7l/0MxbtZ3Rjnba1dOjKGfgPfrbGXz66afs3buX\n7OxsduzYwZNPPkldXZ1bx/cF8tucgZoNu4qrzmCfcyvKV4HdwE7n4bMIITC0hCKlQM4Yi72ykpoP\nP8TkZ2JJ0hK+LPwSq7uf4MdcBCEjYO8b7u23FxIjAwHlDNzBQJewPnToEAsWLMBgMBAUFMSUKVP4\n4ov+ffgYCBwt1eQ6atTMwGV6FaoTQgi0TeprgGeFEOuAUCnlXo9b52GMIgBzQzj+YSVEzZhB5aqX\nCb/pJi4bfRmfHvuUzac2syRpifsG1Btg6ndh85+1moOwBPf13QMmo4HoYH+KBpEzOHr0D5jrc93a\nZ0jwBFJTu1dr9wUJ6/T0dH7/+9/zwAMP0NjYyIYNG5g4caJb75Mv0BorUHEy1+l1ZiC1FJR/t3uf\nPxgcAYC/nxGzOYoW22Gi/utH2MrLqfnnP5k1YhYR/hGeWSqa+j2QDtj3du9t3UhSZKCaGVwgviBh\nvWzZMi699FLmzJnDzTffzOzZs9Hr9W66A76B3SEpKFcxg/PFVQnrbCHEVCnlPo9a08+YAgMwm6OJ\ni8tHTBlGYGYmlS+tIvz667l41MV8lP8R9S31BBuD3TdoxChIXqxlFS34Bej65z9qUqSJ3Seq+2Ws\n/qCnJ3hvMFAkrAEefvhhHn74YQBuueUWUlNTL/Tr+RRFVY1YbA7iwwM5VdOEze7AoHc5i37I0psc\nRauzmArsEkIcEULsFULsE0L4/OwgKCQIc52m7FlX9y3R//3f2M6cofajNVyWfBkWu4WvTn7l/oEz\nb4XaIjjmgXqGbkiKNHG6pgmr3c0ps0MIX5CwttvtVFZqBYY5OTnk5OSwbNmgEhzuldZMohmjNR3N\nmia1VOQKvc0MdgKZwJW9tPNJgiJDcZREY7f7UVf3LcPn/paA9ClUvvoKU274nPjgeD47/hlXjbnK\nvQOPvwwCI7VA8piL3Nt3NyRGmnBIOF3TNCi3wewP2ktYx8bGdilhHRMTw8yZM9t+rFesWMFdd93F\n008/zerVq7ttt3LlSvLy8pBSsnTpUtLT05kyZQqFhYVkZmYipSQmJoY1a9Z0kLC+7bbb+NnPftZm\nh9VqbdvIJjQ0lLfeeguDwdUFgMFBXpl2T6ePiuSjfaeoaWwhOtjfy1b5AFLKbg9gX0/XvXVMmzZN\nuoPKf+fJ1x55Vq5ePUdu336FlFLKqnffk4fGjZfNeXnyb3v+Jqe8PkWWN5a7ZbwOfPFrKX8fJWVd\nifv77oLtBRVy5IOfyE1Hy/plPE9w6NAhb5swaBjM9/L+9/bJ2X/8j9x4pEyOfPATufN4pbdNGjAA\nu2U3v6u9LaTFCCF+3t3heVflWYwRwcQ5IqgzR1Nffxi7vZnghVpaX/3GTVyefDkO6eCL4x5Izcv8\nASDh+Tmw7TmwWdw/RjuSolStgWJokFdmZsywECJMRgCqG1QQ2RV6cwZ6IBgI6ebwaQzBRuIcEZjN\n0SDsmM0H8YuLwz81lfqNG0kOT2Z85HjPZBXFpMId/9YE7Nb9Cp6ZplUnOzyz78CwkACMep1yBopB\njcMhyS+rZ2xsMOEmP0Cll7pKb86gREr5qJTy910drgwghLjEGXjOF0I81EO764QQUgjR5S48nkAX\n5Ee4NNFUqwWRj+dqeyAHL1xI49692M1mLht9Gfsr9nOi7oT7DYjPhFv/Bd9fA0HRsOa/4KN73D8O\noNMJEiICfb7WQCqxvQtmMN/D4uommq0OUoe1cwZNambgCr05A3EhnQsh9Gh7HywHJgI3CyE6VcA4\nJS7uA3ZcyHjni87kh0AwPDiF5iYTRQXaDCB44QKw2WjY+g2XJl+KXuj5Z94/PWdIymK4a4OWZXRo\nLVibPDJMYqTJp2cGAQEBVFZWDuofM08jpaSyspKAgABvm+IRWoPHY2JDCPY3YNAJJUnhIr2lGSy9\nwP5nAPlSymMAQoj3gKuAc8Va/gD8CVh5geOdF7og7esnx47ilDmG0JBi8ncfJCUjA11oKPWbNjHi\nkotZlLiINXlruDfjXox6o2eMEQImXKVlGJ34BsZc6K3vTFKkiX0nfbfWICEhgeLiYsrLy71tik8T\nEBBAQkL/VL/3N61ppWNigxFCEG4yKkkKF+nRGUgpL3SvxHigqN37YmBm+wZCiEwgUUr5qRCiW2cg\nhLgbuBsgKSnpAs1y9mnUo4/wZ1iDgcNEExN7gvVvPUVK5iqC582lftMmpMPBjeNu5KuTX/HliS+5\nPPlyt4zdJSPngN4IBes95gzqmm3UNloJc06hfQk/Pz9Gjx7tbTMUA5ijpWaGhfoTFqj9+44w+VHd\noGYGruDVsjwhhA54Cnigt7ZSypeklFlSyqyYmBh3jU/ghCgCT9ixWrUnJWHMZ/dnnxK0YAH2igqa\nD+UyK24WI0NH8v5hD8sBG02QNBsKPFOMlniB6qXfFFSw72Q1dodaplEMTPLL6kkddja3JcJkVJIU\nLuJpZ3AKSGz3PsF5rpUQIA34WghRCMwC1vZnEDlgQiTCJokMmIyUOoKGG9j24duY5s4FIajf+DU6\noeOG1BvILs/mSNWR3ju9EFIWQ9lBMJ9xe9cXImVd22Tley/v4JrnviHr//2b+97bx5p9p6hV67GK\nAUJrJtGY2LPyMeEmP5VN5CKedga7gLFCiNFCCCOwAljbelFKWSuljJZSjpJSjgK2A1dKKXd72K42\n/EeHIfz1xNliqa8PJ2wkWJvrKa0oI2DyZOo3bQLg6jFX46/354MjH3jWoBSnSuqxr93e9YVIWReU\n1+OQcPvc0SweF8uWvArufz+bG178xt1mKhR94lRNE40tdsbGqplBX/CoM5BS2oB7gXVALvCBlPKg\nEOJRIcSAkLgQBh0B4yKILvPHbI7GP7wEBOz7YhPBCxfQnLMfW1UVYf5hXDLqEj459gn1LR7cSm/Y\nZDBFa3EDNxMS4EdkkLFvzsAZmLt19kieuimDXQ9fxA/njiK/rJ4Wm9I7Unif1g1tUoedMzNosqoM\nNBfweMxASvmZlDJVSpkipXzMee43Usq1XbRd1J+zglYCJ0QRWm/E0hSPEBaCh0VwImc3wQsWgpQ0\nbN4MwE3jbqLR1sgnxzrryLsNnQ6SF2lxAw/8A06MNPWp1qCgvAGjXkdChDa70OkEk0aE4ZDaE5lC\n4W3OppW2dwZGWmwOmqyeKeYcTChdVyBgXARCJwhxTAAgPj2clsZSyjChj46mfqO2VJQWncaEyAm8\nf+R9zz5ppCzRtscs7VoT/0JI6mOtQUF5PaOiTR2kgNV2moqBxNHSemJC/Ak3nU3/jnBmzalag95R\nzgCt+Mw4MozompFYrUaixmr/cHZ9/DXB8+dTv2UL0mZDCMFN424ivyaffWUe3NohZbH26oGloqTI\nsxrv50NBeT0pMR33dRgergNdo3IGigFBXll9hyUioM0xKH2i3lHOwEnghEhia4Ixm6Npth/GaIqm\n+PA+/OcuwFFXR+NObcvn5aOXE+IXwqr9q3BID62Vh46AmPEecgYm7A5JSW2zy5+x2h2crGzs5Aze\nPfoiwaNe8HmJC4XvI6Ukv9TcIXgMZ2cGKqOod5QzcBIwIVLTKaofgcNRxOisidgtJynyG40hJoay\nv/4N6XBg8jPx46k/ZsupLTz/7fOeMyhlCZzc5nZpir7UGpyobMTmkKTEdtwH4YT5BMJYzvGKGrfa\nqFCcL6drm2losXeIFwBEBDlnBiqjqFeUM3DiF2PCL8aEvzkNgJHTAwA72V/tJPpnP6c5J4c65wbk\nt4y/hWvGXMML377gGXlr0JyBrVlzCG6kL+v8rVka584MKpsrQUiO157q6mMKRb+RV6oFj9sXnAHt\nlEuVM+gN5QzaETAhkmGl47Ba/WmUhzAYA6gpOUhz+kIC0tIo+/NTOBobEULwyKxHyIzN5JGtj3Cw\nwv2B3g7SFG4kLiwQg06clzNo3Vw8+RxnUNWsqZWcaTilUvcUXqWwogGA0dEdZ6/hga0zA7VM1BvK\nGbQjcEIUI22x1NaMoM68g1EZmThsxzmw6RTDfv0rbKWlVL7yKgBGvZGnFj1FZEAkP13/U8oay9xr\njDEIEme6XZpC75Sybu8Mmq12Hv34EB9/e7rLzxSU1zM8NIBg/7NSVlJKKpu0vXYtVKj/bAqvUmq2\nYNAJooI6CkkaDTqCjHoVM3AB5QzaYUwKxd/kj7FpEjpdHUmZ8UhHA0e3H0CkphF66XIqX3kFa0kJ\nAFGBUTyz5BnMVjP3b7gfq93N/+BSlkDpAbdLUyRGmih2OoPaRiu3vrqTV7ce5/mvC7psX1De0Cle\nUG+tx+rQvq/OWMWJyga32ugtKuotqm7CBymrsxAT4o9O11l1XymXuoZyBu0QekHA+EhiT08DwBJQ\niBACa3M+BzedJvaBB8DhoOypv7R9ZlzkOB6d8yj7K/a7vxht/GXa67fvubXb1lqDktombnjxG7JP\n1jA7OYrcM3Wd/tNIKTlWVs+Yc+MFzlkBgPCrGhTppadqmrjimS3c/Ua/1z0qLpAyczOxIV1veh8R\n5KcCyC6gnME5mKbGklg3kqbGcGrqthGXOgGD4SQ5XxcjYocTefsPqfv4Yxr37Gn7zMWjLmZC5ARe\n3v8ydnduWxkzDpLmwN7XweG+NNaJphresv2Ce/6+hpKaZl67fTo/+04qUsKO4x1Vy8vNFswWGymx\nXccL/PX+6IxVPp9eWllv4fuv7KCktplj5Q0qBuJjlJstxIR0vWGPpk+klol6QzmDc/BPCccvIhBh\nHodef4zE9MlY6k/TWFPFke1niL7rLgwxMZz47vfIW7yEoh/fS8Vzz3GvYyEnzSdZV7jOvQZNuw2q\njkHhJrd1Ocl+hEm6E8x0ZPP+PbOZkxJNemIYAX46th+r7NC2x0wiYFLUJPTGap+eGZibrdz2j12c\nqm7iivQRNFntVKoiJZ+izGwhNrTrmYFaJnIN5QzOQegEQVnDiTyVjk5nR0bWAhAYXET2f4oQgSZG\nvvM2sStXYsrMpOX4cSr+/iwxD/6d2c0J7i9Gm3gVBITDntfc1uU4k5aGd9+EeiaOCAXA36Bn2sgI\nth/rODNozSQ61xlUNWntpsRMAV0Txyp9c/exZqudu9/Yw6GSOp7/XiZXpY8A8PmZzlCixeagqqGl\n+2Uik5+aGbiAcgZdYJo2jBHVmTgcOqrNO4hOHImOPGpKGzmeU4ExMZGoO24n/s9PkvLZp6T8+0sA\nbq1LI78mnw1FbswA8guAjFsg9xOod88PbmCTFpAOrtzf4fys0VHkltR1KN0vKG8gyKhn2DlPXa0z\ng7RorS6jyFzsFtv6Eykl9723j23HKvnzDeksGT+srSivqFoFkX2FinoLALHdLBOFm4zUNVvVpky9\noJxBFxjC/QkaE4ejbiSIXJJnzqXqVD6mUAvZ/z7Zqb0xIQH/sWNIOFRBYkgiL+W85N4152m3gcMK\n2W+5p79a5w936SGwnpWlmJ0SBXSMGxSU15Pi3E+2PZVNlYT7hzMydKT2vuUMFptvKUMeKTWz7mAp\nP/9OKldPjQdoU2VVMwPfoczc6gy6nxlICXVNanbQE8oZdIMpazihZVMICqpGRmlPHJHDT1FSUMuZ\nY7Wd2gfNnUfz7j3cNfZWDlUe4pvTbtz0JWYcjJwLe9wUSK47BTqD5mDKzhbMTUkIJ9BP3yFuUFDW\nWaAOtAByVEAU8cHaj6jOUMUpH3uazj6pyWhcPiWu7VyQv4GoICPF1coZ+AplddoDTfcxg1blUhU3\n6AnlDLohcEIksQ0zADhTsZm4seOoKcnG32Rg35edZwdB8+chrVYWV8QyPGg4L+a86P7ZQfVxOL7x\nwvuqOw2j5mt/n9rbdtpo0JE1KqLNGTRYbJyubSYlJqhTF5XNlUQGRhJiDCHIEIowVnHCx56m952s\nISzQr1PVakKkiaIq33JsQ5mzM4Pul4lAVSH3hnIG3SAMOiLHZyEtQdgdOYzMmk3FyUKSM/Qc+7ac\nmtKOP3ymrCxEQADN32zn9rTb2Ve2j1cPvMquM7soqS+58JTTCVdCYMSFB5JtLVBfplU3m6LhdHaH\ny7OSozh8xkxlvYXjzhL/nmYGAAnB8ej8fC+9NLuohozE8E5LYIkRgRSpmYHPUGa2IAREBxu7vB7h\ndAYqo6hnlDPogeDpIzBVTiI8/Ay1ej+E0KEjD71eR/ZXRR3a6vz9Mc2YTsPmzVwz5hpSwlL4696/\ncvu621n2z2VMf3s6d315Fw3WPlbq+gVA+i1w+BPtx7yvmE8DEsLiYcRUON1xX4ZZydoP/M7jVWfT\nSmM7O4PKpkoiAyIBGBmWqKWXVg7wH9CcD+D5eWBtxtxs5WiZmYzE8E7NEiNNnK5pUgFHH6Hc3ExU\nkLHDxkvtURvcuIZyBj3gF2siSszCaGymqu5lkubEcGzfJlJnxHJkWwlN9R2fNILnzaOlsBDdmQpW\nX7maz6/9nFXLVvGb2b9hxfgV7Dqzi19t/lXfU0+n3QYOG2S/0/cvVefUHwp1OoPyXGg5+yM+JSEM\nk1HPtmOVFJTXo9cJRkaZOnRhsVuot9YTFeicGYQkIPyqOVHpwb2h3UHhFijdDwf+yf7iWqSEqUld\nOIMIE1a75Eyd63s+KLyHJkXR9RIRnF0mUjODnlHOoBeGj70cY8V4hg3LJSLtaxIv3Yhf0i/xj8zh\n4KaOwm5B87R1+IYtWzDoDCSEJDArbhY3pN7AL6f/kpXTV7KhaAN/3/f3vhkTkwqxk+D4+RWgdYhd\ntMpNhyVAfCZIB5zJabvsp9eRNSqS7U5nkBRpwt+g79Bfa41B68wgPjgehJ3jNSV9+FL9SK1zNrfj\nBfadrAboZmagMop8iTKzpdtMIoDQAAN6nVAB5F5QzqAXQjJGMebYbynZ+VMO515MxaFYWlpOET/7\nXfZ/XYTd6qDpQAVnntyNPiYevxEjqN+ypcu+bhl/C9eNvY5V+1fx6bFP+2ZQ/FQoyYbzCE5/9eoL\n/POPv9He1DnTSkNHQFyG9nenpaJIjpbWs+dEdZfB41YpiraYQUiC1k1D8cCWcagtBkMAnMnBnLeV\n5OigDvvltpIY4aw1UM7AJ+hJlwhACEF4oJ9SLu0F5Qx6QWfUE371GKaYx1BeHktdw3JK98WgM5Zh\nF8fI31RM1YdHsVU00XywkqD582ncth1p7fwPTwjBwzMfZtqwafxm62/YX76/ixF7IS4DGivPPuX2\ngt1m4/CWryk6tB+H3a7NDPzDwD8EQuMgJK6TM5jtjBuU1lm6DB63Fpy1LhMlBicC0CIqBq6Mg5RQ\nUwTpNyMDwsg8836XswKAEeGBCKEKz3wBu0NSUd/SbVppK+Em5Qx6QzkDFwgcH0ni5NEkOqKpdOgo\nO2IABLHjD2D/Sksz1Yf705RTTtC8uTgaGmjKzu6yLz+9H39Z9BdiTDHct+E+Kpoqzs+YEZna6+mu\n+z+X4kMHsDQ2YLdaqSkt0WIGYfHt+uscRE6LDyPIqC0NdekMnIqlrctEw4OHI9ChG8jqpY1VYGuC\nmHGYJ9zMEsd25sZaumxqNOiICw1ok/lWDFyqGlqwO2S3aaWthJuMapmoFzzuDIQQlwghjggh8oUQ\nD3Vx/edCiENCiBwhxFdCiJGetqkvhF+RQqYumRabDUtwErIxjvDhewl1SOSsEZimDcNyvJaAtGlg\nMFC/ZWu3fUUERPC3xX+jvKmctQVrz8+QYZO0grES15xB/u7tbX9XFJ3QlolCz3EGFXnQXNd2qjVu\nAHTaxwDOzgxanYGfzo/owFh0xqqBm1FU66wNCUtkV8y16JDMr/242+YJkSaVXuoDlJmdBWc9LBOB\n0idyBY86AyGEHngWWA5MBG4WQkw8p9k+IEtKOQVYDfx/nrSpr+hDjIxdPpV4eyT2yHjMh8Ox+hdy\n3L+EzUcKOBlchZSSlsJmAjPSadi8ucf+xkWOY3TYaHad2dV2rmn/fhr37uvhU2gpprETOj3Nd4WU\nkoLdOxg5ZSoIQcXJQm2ZKHTE2UYjMgEJJd92+OyicTH4G3SMiem4pyxoMYNAQyAmv7NZRkkhCej8\nBrB6aasER1gCWytD2CAziTn6bgc5jvYkRqjCM1+greCs12UipVzaG56eGcwA8qWUx6SULcB7wFXt\nG0gpN0gpW39BtgMJHrapzwRNH8706Em0YKfcMhWA3ITVHKnZzD/X/YvKSAuNORUEz5tH86FD2Cor\ne+xvxvAZ7C3di9VhRTocnPrZzzn1wAO9B2HjMrRlol7alRUew1xZzrg584kYHkfFiePQWKFlErUy\nousg8q2zR/HVAwsJc+Zot6eyqbIteNxKUmgiev8BvExU44yxhCeRXVTNlsjrEI0VcPD/umyeGBlI\nqbnZ5/SWhhrldV1XH0sp+ezvf6YwR/t3rc0MlDPoCU87g3igfaSz2HmuO+4APu/qghDibiHEbiHE\n7vJy78glC51g0opZjHTEUN0SQKM5nOTRtYTVTUIndJwMr6alsJbAzLkANGztfqkIIGt4Fo22RnIr\nc2naswdrcTG2khIsubk9GzIiA5qqoKazLEZ7CnZvByFIyZxBVMJIKk4e0y60XyYKioawpE7OQNsr\nuWN9QSutUhTtiQ+OB72Zwqrqnm33FrVF4GeixS+MA6fr0KcsgpjxsOPFLp1qYoQJKfE5vaW+0rh3\nL9ZSN+/j3Q+0LhPFnLNMVFZ4jNzNG8jdtB7QZgbNVgfNVuXcu2PABJCFEN8DsoAnuroupXxJSpkl\npcyKiYnpX+Pa4TcsiBU33sQNWYuoO2JA6AqYkpmAnyWCI1UnkFLiaIlAHxNN3bove+xr+rDpAOw8\ns5OaNWsQgYEgBOav1vdsxAhtVtJb3CB/9w5GpE7AFBZOdNJIasrKsTlExwAyaM7FhWWnVtpLUbTS\nml56srWOYaBRWwRhieSeMdNiczB1ZCTMuFu7hye3d2o+lKSspdXKyTvupPyZp71tynlTZrYQGmAg\nwK9jLczxfdrWpaXHtX29I9r0idTsoDs87QxOAYnt3ic4z3VACHER8DBwpZSy6xSPAURgWjSjLpuJ\nvW40IBk1PZ9QEUdDcwOloY00Hagk7Iorqd+4EVtVVbf9RAVGMSZ8DPtObsf8xTpCL7mEwKlTMa/v\n3hnUrC2g8cxwLYjcww94XXkZ5YXHGJM1E4DoxFFIKalqMXWcGYDmXKqPQ5NrT/XtpShaaVUvrWw5\nMzCfvmqKIDyR7CJNqTQjKRzSV0BQDPz7N51mB0Op8Kz56FFkUxPNBw9525TzpqzOQmxo50yiVmdQ\ndaoYq6W5TZJCpZd2j6edwS5grBBitBDCCKwAOqTPCCGmAi+iOQKfmafqdHomzl6Bpc6PiqrPWPGT\nZQh07DEX0VJYR8iyaurgbAAAIABJREFUK8Bmo+6TT3rsJ2tYFobNe3A0NBB29dWELF2CJTcX66nO\nT9iOJhv135zGvLUUYif2mF6av3sHAClZswCITtSStCosQZ2dQXxrumrvswO7w06NpaatxqCV1pmB\nzq+S4oH4NF1bDGEJ7DtZTWyIPyPCAsAYBBf9Dop3arpF7RgWEoBRrxsSGUXNOVoFuiU/H9niW0/O\nXRWcNZnrKMk7QszI0UjpoPxEYTvlUt/6fv2JR52BlNIG3AusA3KBD6SUB4UQjwohrnQ2ewIIBj4U\nQmQLIc4z19J7TF68jPriCMwNuwmL1jF27FgqAyqRSI5ts+A/eTI1H63psY8ZcTOYnW1BDo/GND2L\n4CVLADCv77xbWstJLf3TeroBW9QcKMmmsdbCyUOdA9UFu7cTOSKByBHaD3/48Dj0OkGFPRKM58QC\n4tK1VxecQY2lBod0dJoZRAVEYdQFoPOrbhO4GzC0NDoD54mdlUrTb9Eyqv79G7CY2z6i0wniIwIp\nHgIZRU05zuJHqxVLfr53jTlPupKiKPx2L1I6mHXtTVqb4wVtexqomUH3eDxmIKX8TEqZKqVMkVI+\n5jz3GynlWuffF0kph0kpM5zHlT33OHDwN5mIilqK0Dk4dfJTJk9Jo0U2Uxxgxl5Qw9FJt9Kcm0tz\nDwHhqSQxuVBSNDcFodPhP3o0xtGjqd/QeanIcuJsLUCzfQY0VZP96QE+fvpbytpfa6inOPcAKdNn\ntZ3TGwxEBgsqrF1U3QZGQGRyh70NuqNNiuKcmYEQgsSQePwCqvmm4DwL6TxNnTbLqg+Io7CykalJ\nEWev6XRw6RNQfwY2dQxXJQwRKeumnByMo0YB0Jx72LvGnAdSSs0ZnLNMdHzfbgJDwxgzYzYBIaGU\nHi9QMQMXGDABZF9l6qJ7sDbpOXb4XVJTU9Hr9ZSOaCDKoONUqYmCsddT/X8fdd/Buk3ogP9MtLWd\nClm6hIadu7DX1XVo2nLSjF9cEIaYQJoqhwNQ5tyIZsfa/5+98w6PozoX93tmtu+q9y7ZsuQiy72D\nqSY4QAi9pnBJSH4khPSEJDchNwmEBAikkFxuEgglhgRIsKk24AYGW7LlLqtXq0vb+86c3x8ry002\nhuDYhH2fR4+0s2dH35ydmW/OV1vHxrXV1aJr2pi/4AAZ1hBDwfFrvlO0IO5IfZdw1bFSFEc4kAGK\nkoqw2dxsaDw10V7HZDTqqjEUV4RHlaEonAszb4C3H4Khg0/GRem2/3ifgeb1EmltJfniixE223Ef\nXE43PMEYkZh+2MpA1zXadmyjbMZsFEUlp2xiYmVwgiSUwb9Iel4R0jeBqLKXgG8vkyZNotnViY5k\n7pQ0OgvOYXuNn5Ytb7Pm/35L7BCbrJQS9z//yfCkbNbJBqJa/ER1nHsexGL4NhxMXJO6JNLpxVSS\njGVqBuH9Eo0UBvvBbDPQuWeEnua4c7S5djO2lFTyyisPkzVTdeINSvyeAZqb7yESOcS8VLwwbkoZ\naeV4HKhYOp4yKEgqIKYM0z7sP70ykUcTzra6HSgiXqb7KM6/E4xWeOW7YwqxKM2GMxDFF44dPf4/\nhNCePSAl1pkzsVRWfqiUwXhhpX3NTYS8HspmzQUgu2wiQ10dGNCxGlWch9TO0nWdTZs2EQz+55sC\nT4SEMvgAKJ/yRUCybceV5Bc8hdHcwHBuiOxwjImlgpbsxaz69f3seP1VXvz1L9FHu56Fdu0i0tqK\n+vHzCGkhdg3FbbfWGdWoGRn43nh97H9E+/zIiIa5JBnr1AzQJSPW64hEDcy7uAxbsonNz7cSi0Zo\n317LhNnzEcohX28kQKYSN990tjxHR+fD7Nj5BTQtfkE9ui3AKyyFzrePe6xHFqk7lEJHIVEZRKh+\n1jedRqsDdxcIhU0DJiZlJ2E3G44e48iGs74DzWug8VXg5EcUdXm7TnmV1+COuPPYWjUNy5QphOvr\nkR9En+1/A+O1u2zbXosQCiUz4kEROWUT0bUYw10dR5Wk6OjoYPXq1bx5jCrDHzUSyuADYFL1J4m1\n3kTvllyk3kvV9DcYmvIjRsxrWHJRJYRewVNQSnjyXJpq3uG1Pz40tioQZjOTr7oZgWBL3xYAhKri\nOOds3OvX8fU1X2EoODTmPDYVJ2EqSkKxG/FH4mag/IkpzFleSk+Ti91rtxMJBimbOftwIT09ZJrj\nXdacI9sQQsXj2c7evd8kHA7R0TPIXlGJ7Dg65v5QRkIjGISBJNPRZSoOhJfmZAROL1ORqwuZlM/2\n/V5mFI2zKjjA/FsgYxKsuws4uaWsdw3u4uPPfZw/7vrjMcfEYl76B146qQojuGsnppIS1NRUzFMm\nowcCRLtOrCLuqWasLtEhpSja6mrJq5iM1RE/P7PLJgLxfINUmwl38ODKoKcn3o9k69atRD5kUVQn\ng4Qy+IA46/r/h6uhkIE3z8XruYpQVKOv6k+8030JhsVDYDETRZI161x2vf4q639zP54XXyLpvPNI\nyyygMr2S2r7asf155lci/EH63nqd39T9hkiHF8VhRE23IBSBZUo6SiADlSjp1iGmnZGPI93M1lfi\nIaX5FVMOF9Czn2RjGKPJSDDUjMMxhfLy7zIw+DJ79vwEKSUe6cDZvpPjcSDHQBFHnzrlqeUAlBWM\n8HbLMFHtNHnCdHcTceTjDESpLhy/bDUABhNUXw29OyEwclITzzb1bALgN3W/YUP3+M2K9u9fwe7d\nt+HzvQfTTSwCr/34hFujhnbuwjKjGgDLlHjZsA+LqWhgrBRFXBn4XU76W5uZMGoiAkjNzsVktTHQ\n1kKa/fCVQU9PD6qqEgqF2Lnz+Of9R4GEMviAsKemseTaT9G5cxc2fQ5ba5fTsu9CfGEDkyq2cMaM\np8nLaaXHaSJTZLL1rbW02I2kf/YzQDzfYPvgdiJahC5PF1/x/omIUXBlXzH/aPoHvrYhTCXJYyGR\n1qkZKFKhzDyIOrgD1agw76IyPANtZKSWYE89PPQTz/540/D8PHRDL0lJ0yguupmCghtwup4iN7cR\ngHZXDPzHrqk0Eho5qhTFAYqSi6g0VKKZ6vGFY2zrOE1KU7g7GVKzAZhxPGUAULIEkND5Nmk2I3aT\nelJWBrX9tZSllDE5fTLf2fAd2txtR41xubcCMOI8flmTw2jfCG/eDzV/eteh0b4+YgMDWKfHlYG5\nYhIYDIT2fjiUwaA3jNWo4hg1+7Vtj89X2SHKQCgK2WUTRp3Ih5ex7unpoaKigtzcXDZv3nzKTXan\nmoQy+ACZsWw5ORPKaV7zAmazBaergKzN3yKn+xuYQlbKK9/ClrMDT8p1pKeWsDcrhQERf3qenzuf\nsBbm9c7X+fyazxNUNcyLFlC1L0iRyENx6ZiKD5pmTBNT0KSkwCTH8gMqF+ZSYk3m/LRr8b15RNLa\naJmIzPJcFGOEJMc0hBBUTPoh0ehkyidtITdngDaKoGvzMY9xvCJ1B2htbaWqqYpAfz+qGmPDB+Q3\n0CMaI39vJDr0Pp7QdQ08PbTH0jGpCpW5R5u3DqNgDqhm6NgUD5dNt9H9AYeXRvUoOwZ3sDBvIQ+e\n8yAm1cRX3vgK3sjBPAcpJW53/HsdGXkPyuDAd7fv+MmOEA8pBbBWTwdAMZkwT5z44VkZeMNkJ5vH\nHpDa6mqxp6WTVVJ22LicsnIGO9rIshnoc4eIajrBYBCn00l+fj4LFixgcHCQ1tbjB0/8p5NQBh8g\niqJy/s23EnSOMM1hxt62B5uIktYyk+qCB1F8yUya8jZpU734uRSzLYWtL8TDTmfnzEYguGPjHThD\nTn5//u/JvfATaH393OH8OAB7rS1j/8vniTIQkyQbspH745nI7poO5qafA4DznSP6EXu6wZZBclH8\nKzeIolGZDbS2nIemZVI2cTvtFCI7ju1EHg4dXYriADt2xMtgZ/oyqCjpZ0Pj+8s3WN+1nrs23zX2\npBZudhHY2o/nlaOfnt8Vbx/oMfb6U5ian4zJ8C6nvNESDzVtjzsVC09CKeu9w3sJxoLMzZlLniOP\ne8+6l25vN9/b+D10GX84CIW6iEaHMRhScLlq0fUTq9ISaNnOc8M/Y6jbCyPHn6/Qzp1gNGKectCk\naJk8mVD9Xla1rOLpfU+//4P8N3Bo9rHrlZdp3bSB0qnTDyYUjpJTNpFYNMLslCiBiEZN28iYvyA/\nP5+qqipsNhubNx/7IeijQEIZfMDkllcw4/zlNL35BhGfl4LLZiPDGoqxkllLH8VkCpJavoLy2Xlo\n+lTatm/D1ddLijmFqRlTUYTCr8/9NVWZVaRcfBGW6mry33YRQ+OergfGwk8HO730RXUUaSPa7STc\n0It/ZTfOcB/NMR/KSIiY85Ba/Z4eSC7AlOpD6hAcjl9E4XCYgQEPRuNFGAz9iJQAw207xju0eG2j\n0Mi4kUTRaJT60SfK3GAuqemt7O5xM+x776Wmfrf9d6zYt4La/rgPJdQSD5kN7h4m2u9/bzsbDSut\nddmZMV5I6XiULIG+nRByU5QeTzw7oJiklDQP+PCE3n+8+gHf0JycOQDMy53Ht+d/m3Xd6/jTrrh5\n58CqoLjoJnQ9iNt9As2MtBh7GjPpjU5lX/Bc2Hf8PtvBnbuwTJ6MYjIR02Ns7N7IGnMz2tAw97xy\nBz/d/FManY3v+zhPNvHs43gkUes/niWmKOSKg5FiMXcYqelkl8V9WUVyBLNBYU19/2HKwGg0Mnfu\nXBobGxk5Ti2x/3QSyuAkcMa1nyY1J48Fl11NzrxKjEVJ+Db1kJJcja4vx2rdzrSPt5NVthAQ1L0a\nv2jvPvNu/nrRX1mQF48SEiYThb+6HzW1FAK9dDib+VtjvIbOYKeXgVH/rDf4cYb+speoIcymkecx\nzyiOb996iBPRvT/ex8DYR9htwtndB0Bvb3wFkZ93KaqaRH5eA+397nGbvvijfsJaeNyVQWNjI5FI\nhBkzZmDRLAT8TUgJbza/t9VBi6uF+pG4Unl0z6MA+PYO44pJdEXgeeM9RrqM9opuiaYf33l8KCWL\nQerQtYWiNBuBiMbj73Twtae3M/+u1zn//vVc+ftNuIPvTyEc8BccqlSvrbyWpYVLeaL+CaJ6FLe7\nDlW1U1j4KUA5Ib+B1rOT3b54OZMObfFxTUVS0wjt3o11+nRieozrX7yeW1+/lfW2+Hzdm3crJsXE\n3xv+fsLHFe3tpfsrtxMd+PeUGBv0hMlKMiMjETpbmxFSktIUXw3pwRj999XifKaJtPx8DGYzzs5W\nlpRn8tqoMkhLS8NqjYcPz5s3D0VRPtKrg4QyOAlYHA7+64H/ZcnVNwLgWJxPbChIuNnF1CnfxOdN\np6n5Ryy+qhTFWM7O11YTDYfGHIqHYsjJQ00vQ++p57tvZfLQ9odwh90Mdnqw59rYk9nHSiUZqTvZ\n5n4Ke3EOfzb8AVdM4q7tO7gjTzck5xMINRFxJcVbYHIwvK6wcAL5eVeSmdlJhzFj3DpFR5aiiLS3\n438nfvHs3LkTh8PBeeedB4DRJUhNdrH+PYaYrmpZhSpUrqm8hg3dG2jtbUI4w/RGddojOsGdg0QH\n34MNf1QZ9MjM44eVHkrR/HhV2PY3Kc2MRxT98Pk9rG8cZOGEDL55QQVtQ35ueaz2PTe/iekx6gbq\nmJsz97DtQgiumHQFI6ER3u55G7dnG8nJ1RiNqSQnz8A5suld99365l4CejrFlXZc4UxcrR3HjCoK\nt7SgBwJYZ1Sztmst9SP1fHvet3noCy8BUD5oYFnpMl5ofYFA9MTm2/Xsc3hXr2bg3ntPaPy/QjCi\n4Q3HyE42E9i6lQGrkYyoTuStTchIhOCuIWREJ1A3QGiPk+ySuBP5/Ck5dI0E6ezeT37+wY5/SUlJ\nTJs2jbq6OkKh8bvf/aeTUAYniUMTvmzTM1EcRrwbusnPLaK3dzm67sOj30fh1DOJRQLsfP3ownQA\n0V4/6ALb7AlMf7ufWdvc3PXOXfR1Ohky7+Jt3x461EGGirfQ1d/DLqWRd8LraMWDwRUm5g5D2Ach\nN5GkdMLhPlQKD1MGycnJOBwOCgtvQCiSQG4EOU7y2aG9jyPd3bTfcCOdN93EwIsv0dTURFVVFcnJ\nyWRkZ5AbyKWsuJuNTUMnHKWhS50X215kcf5ibp15K2bVzIYNrwGgZVtp9MeQQuBd+x5WB64uAmoy\nitnBhEzHiX3GZI8Xr+vYxNJJWTxwzUxe/MoZ1H7/fH5z3Sy+fO4k7r1qBpvbRvj633ag6ycehdIw\n0oA/6j9KGQCcWXAmqeZUXmr+Bz7fPlKS430r0tMW4fHuJBbzHvWZQ9m1XSXZOMjSG+MO4Y7wbGh4\nadyxoV3xBEfL9Goe3/s4hY5Crp98PZa0DIyFhYTq67m64mp8UR+vtr96QsfmffVVMBjwrFxFYNu7\n17n6VzjY+9jCwJrVeK1myhYsQff7CWzdir9uAEOmFWOhA9c/m8grqmCgrZVzKzMxE8Xv9RymDAAW\nLlxIJBLhnXfeQdNOwzLsJ5mEMvg3IAwKSUsLCTe7GPzDDiYWLKG9fQaDg68y+woQagZb/vn8uDfN\nA8XpMr94NbZ58/jiq4KuN7fTb61h2L+fCy64gNTUVDbLInSpsNc+yJcrr2N9SjyBzbm5N+4vALy2\n+P7ttskMdXUidZ3e3t6xi8JmK8NgmEFGXgeDzeOsDA6UooiY6LrlC8hYDMu0adT87/+i6zrV1fEQ\nxamVU8kIZ6CYGhj0hqnvPf5N7AC1fbX0+fu4ZOIlpFvSuXTipVj2qcSkpOqKSWROSqVLlwS2DxAb\nPkGnrrubPjKZXpCCooh3H3+AksXQsw2DFuSTswqYln/45y+dWcAdyyfz4s5efvbS4dE3/qifZc8s\n45nGZ44+xlE/yNzco5WBUTVyYemFtAy8gZQaKSnxxMH09CVIqeF0HtuEMdjpodedw/TSTlKybKTl\n2mjXzoD68U1FwR07UZKTaXJ4qRuo44YpN6Aq8QYxlilTCNXvZVb2LCamTORvDX8bdx+HEm5tJdzU\nRNZXvoIhN5e+n/4UeRJvqAezj8201cTnpfKyKxAmE9433ibS5sY2K5v0qyrQwxrFvklEwyFMgRHm\nxaOMj1IGBQUFlJWVsW7dOu677z5WrlxJc3PzR0YxJJTBv4mkpYWkXz8ZzRki9x1Jd9cUFFFM3/CD\nFFefQcDdzb63j3YSRjo8qKlmDOk2Cu6/j4EJ5UwJLUAXMRxTMli8eDHz5s2jd3AYzWxlmdnPF2qf\no3BRKm5NMrSlK24iAryGuPM1I2sO0VCQ/S1NDA8PH3ZRFBV9CrM5QFOoFY4oSzAcGsYQkxi+fz/R\nri6Kfvsbiv7vYTrLJ5Lk85EeiJsTysvLEQhCwwMgIiccYrqqdRV2o51ziuIRUZ+a+immBssYljoF\nFWnMuqCEencUicC7vvuE9qm7OuP+ghM1ER2g9AzQY9C15ZhDblk6gc8uLuVPb7bxx40HwxJXt6+m\nz9/Hb+t+SzB2uNKq7aulOKmYbFv2uPu8ZOIlFBjiN7qUlJmjv2ehKBZGnMc2Fe1a3YBBhJgyP+7P\nKZ2eSU9wEpGWzRDyHPWgEdy1C2tVFY/vexK70c4nyz859p5l6hSiHZ3ofj9XVV7F7uHd1A8fP9zU\n+2p89ZBy6SfI+fa3CO+tx/XMs8f9zL/CgYSzLM8AvZEANrOV7IrJ2BYtJLgv/vBhm5mFMcdOygWl\nmAZUShzT6G9roSotPhfGpKN9X9dffz1XX301EyZMYPfu3TzxxBPce++9vPbaa3iOKBz5n0ZCGfwb\nsVVnkfO1OeRPLSFdT6Zj9xxCoS6mfyIIwsTGvx598UQ6PZhKkgEImM1smj0LcwjShmbyV+8feWT3\nI1iLrUipEcrO4bZLfw/ONr6442FahBOHVxLoGVUG+gAWSwEl0xZiMJl56hc/BSA746Ajs7TkEiJh\nO57sAAw3HSbLcGCIW1/UiW3bQd7dd2ObNw+/otCfnExp/wBdt9xCtH+AwsJCDCYDmf4MSgt7eW5b\nN7F3yUYOxoKs6VjDspJlBIdGaHh7I5n+TNKxsMfaRkgPUTwtHXu+nV5F4N/aT8wVQvNHCe4ZxvVS\nK4N/3s3QX/YwvGIfzmebcK1qIToSo1vPGDfZTEpJOHyMaKeiBSAU6Dj2DVgIwX9fPJXlVbn87KV6\ntnXGk+xWta4iyZTEcGj4sKdqTdfYOrB13FXBAaZnTmeq3YxHWjEa46W2FcVMauo8nMdQBiFflMY6\nD5WW9ZgnzQegZHoGulToDEzlnz/7Li//9r6x8XowSLixEW3KBFa3r+bySZfjMB00oZknx/1W4YYG\nLp5wMWbVzN8bj3Ykx5yhseguz6ursc6ahTEnh6Tly7HNncvgr36F5nYf81j/FQ6Yiexb32HIYaV0\nxiyEEDjOOhslqRJDjglDRtw57DijAFNJErMzzme4oYN04cOtW9jUdvTN3Wg0MnXqVK688kq+9a1v\nce2111JaWspbb73FAw88wHPPPTfmZ/tPI6EM/s2oDhMZN0yhanoV7cMOLMPV9Pb/iaLqmXgHd9FY\nczA2POYKo7kjmEeTzd588000KSkyzyXFO8z3a3K5f+v93LbxNhTfCDgyieTOg5vXYLVnYVbXIIRg\n7ZZ4xJAv3E6SYyppeQXcdP/vSS2PX/Sv/+Yedqx5GV3TUBQDemQ+pjQPvtZVh8me++QbnLFXkvW1\nr5Fy8UUA7N69G4BFX/oSmstN1xe+AMEQ5RPLyQvmUVHaTWO/jyc3dx53XtZ1rcMf9XNx2cW8+OAv\neeGBe6h/Nr78fz1tLSubVyKEYNYFJeweDiOlpP/BOnp/8g7Dj+/F91YPui+C5goT7fYS3DeC7+1e\nXIHb2K9nMqMolVgsRktLCxs2bGDFihXcd9993H333bz44ovoRxZnsyRDbjV0HD+KR1UEv7xqBrnJ\nFu54dhcdrm5q+mr49NRPszBvIX/e/ecxB2yTqwlvxDuuv+BQSkw6+wIRenwHbzrpaYvx+5sIh/uP\nGr/3rR40TTA9dX28Ax6QOzEFs9XAFmcZLY2d1L+5jr7mRqSu435+JWgab6cOoaNz/eTrDz/0qaNl\nKfbWk2JO4cLSC3mx9UX80YNhvVLTGfy/XQz8fgehvS2E9+0j+cKPAXElmfOD76N5PAz+5rfHPdb3\ny4A3jFEVDG5aR8ygMmHx0rjs0xaiJhcg9IMrR6EI0q+uRFUMpDYn4RsZJGBI4rX6o+fyUIxGI5Mn\nT+aaa67htttuY968eezbt4+HH36Yxx57jI6OjpNybKeKhDI4RcxethBFUWhon4GmBSifNwhorH74\nKfZu3E9gzxBDf4o7+UwTUnG73WzdupWZM2cSDKeRmWdhyvoObm2dgOqLYh0cQAe2b98eb2P5+bUs\nvaAKj6aTPFxJY2o2gWA7SUnTAEjOyiZlQgUOu52MnFxe++Pv+PtPvk/I7yO34AZ0XaF54KDzMbR3\nL5Nf2MOWuclk3PL5se07d+6koKCA/IULKHzwAcINDQw++CAVkyqwaBYGnfUsLk/nvtUNjPiPXQxs\nVcsqcu25ZPYJ+lubMJjMBBtbiAKxco3H9j6GpmuUz81GTTXTbTViLk0m+cJSsr5YTfpVyYS2/Iq0\ny7PI/dY88r+/gLRlVqJyAumGKTTWvcOvfvUrHn/8cd544w2GhoaYMGEC1dXV1NTUjK8QSs+A7tpx\nw2wPxWE28D+XVtHQ7+XHax8D4uaeL838EiOhEZ5uiCdvHcgvOJ4yCAY7McgA7WGFF1oP2vvT05cA\nMOI83LGv65Ld6/dTYG8lY0IeMbeHri99maFf3EOqvZtedw8Tk92Y7XY2PvQArZd8gr4778RYVsoj\nxs2cW3TuWMvSAxiys1HT08cyka+qvIpALMCLrQfzFvw1/WgjIWRYY+S5eO/kpGXLxt63TJ5M2rXX\n4FyxglDDwVyF6GAA75v7ke/B6T4eA54whWbo7mpHACXVcZNaZL+Mr5J3vHz4MWVYGcjux6Qn4fF4\nyMnNY2PT0An3605PT2f58uV8/etfZ9myZfT39/PII4/wl7/85T9GKSSUwSkiNTWVyy+/nE4/+AZn\n4BSvsLD6LOyhVsLPbWbk8Xq0qE7Gp6diyrOzceNGpJTMmbGAoDdK0fKFOM4+m7OfaeEX6mdQw0Fy\ns7LYsmVL/KamGrCe80W8SQolWjqP5sT9AgeUAcQjiYqKi7nmznu48Nav0dO4j7/9+A7ysiYwNFTM\nsKWL7u4n0PUwgw89RMii8vblFWMZngMDA/T39485jh1nnknaddfifPJJCkdvrMqwwufPceCPaPzy\n1QYAdP1wpTAUHGJTzyY+Xvpx3nn2KZKzclh2y3fIMuXiUZ18uurTdHo7WdW6ClVVmHl+Mdv2B4id\nWUjy2UWYSpIZuOfnBGpr6b3je2MlmH3WdjYYaxgSI6xfv56CggKuu+46vvOd73Dbbbdx+eWXc9ll\nl3HGGWewdevWoxVCyRLQwrB/67t+n8um5rC8KofNQ2uoSp9FgaOAmdkzWZK/hEd2P0IgGqC2v5YC\nRwF5jrxj7sftiTvurUnTWdWyaszW73BMwWhMw3lEaYr2nUN4R0JMN/4dihfRf/fd+NatY/Dpp+mr\nfwoUOwUdBkp6B+nc34nHqJJ/771su+/T9AkvN0698SgZhBCjmchxZVCdWU1FWgXPND6DlBIZ1fC8\n3ompJJmkpYXovlSs8y/AeIRDNvO221AdDvp/Ei+EqPmjDP1pN+4XWvFv7j3q/74XBrwhFrpaGbCb\nyckvwmJ3IHVJYPsgisVLYOs7aC7XYZ+puPEc6qPxY1qUlU8wqr3nrnwWi4UlS5Zw++23c8EFFzAw\nMMAjjzzCihUrxhzNmtdLzw9+gP+d41cAPt1IKINTSFVVFcuXL2d3azm6NKKWv82ygk+TZLBRO7SG\nZ/auZFNNO337B9m2bRuzZs0i4opHfGSXppB/7y8xTyij52/PoBoMLDrjDJxOJy0tB8tWpC8qRhGC\nCcEcABxJcRNM6KsGAAAgAElEQVTAobVZhBBMO+s8LvvOD3H29fDSfT9huGcWUZ+NhsYf8daGM+mL\nrebNM5Owpx90fNbU1MQ/O+2ggsn66ldR09Px3/ML0rPSyQ3m0hnaxmcWlfJUTSdbdv+BDRvn4HQe\nvFBeaXsFTWosCE+ir7mRBZddRbA/jSRjGp0D26iKljA7ezZ3bb6LNncb5XPT0aMbefmhP6FrOr71\n6wnW1WFfvJhAbS3OJ/+K0+nkT2t206Y4mawV8PmP3cj1119PZWXlWKJRt7ebS5+/lIaMhvEVQnE8\nKfDdTEUHuHqJjmIaYqS/eizc9NaZt+IMO/nrvr+ytX/rWNbxsYgnmzk4s/Rq2j3t7B6Km+GEUEhL\nW8SIc9OYgtBiOptXtpKULCkzb8E34MCzchWZX7iFrptvJGKUmOzLGbHPYrISwmg0sv+cJSRdtJwn\nGlcwNWMqs7NnjyuHpXo64cZGgtu3I4TgqoqrqB+p54XWF/Bu6kH3Rkj5WCmWSgU9MIKx7BKkdvjT\nviEtjaxvfJ1AbS2ufzzPyIp9aL4IxgIH7pfbiI28v3j+rpEAO7vdzOzZicdmYeLiMwEIt7rQPRHs\nc3JB0/C9efj3lpKdQ6DSBBIq6gIUGQ28Vv/+EuRMJhOLFy/m9ttv55xzzqGhoWHsga33e9/D/cyz\ndN78OUYee/wDLYDn97eetIJ6CWVwilmwYAGLFl1Ae8c0/Fk7kMsGyf3mXKxzs4lFdrL9pZ/x6IOP\ngoRF8xcz2OlFCMgodKA6HBQ9/DCuZAcpLi/Zb7yBw+Fgy5aDETAlZxbg0yVTTBb8MYWawbhT+NB0\n/AOUzpjN1f99F2G/H1+fTt2uS5i204/a7cVztUbJxSNMFPELuKamhpqaGubMmYPDcdD5qCYnk/Od\n7xDatYsJGmSGMnlsx2OcPytIVdYArr5foWkhdu66lUCgDX/Uz6q9LzHdPoP2V9aRlJnF1KXn4qyL\nX6Reo5PXHv4tdy/+GWbVzI+e+zpP/ehrRHw1jHS9waoHH2fwgQcxFhVR9LNvYj/zTPrvv5+Vf/87\nUurcLP/KXFsVhs2ew0wTUT3Ktzd8m05PJ3/Y+QdWipUsXLyQrVu38tRTT9HY2EjMlAw5005YGbzZ\n9yoGYaahtZS/rK4hHA5TnVXNGQVn8Icdf8AVdr2rv8DjriM5uZoLyi7EpJhYdYjfJj1tMeFwH4FA\n3K9Ut6aTkR4/S6fXgy7o+/2zmMrKGJoxjb1vrmPB5deSXzEVT+WZTJq3h5nzp9Hw9kae3/IUbe42\nbpxy41F1fA6Q8dnPYszLo/v2rxIbHOTiCRczMWUiP13/P/SuqcdfrGMqS8a39jXCu/+GDJvxv3O0\nYzX1yiuxzpyJ69ndhJtdpH2ynIxPTQEhcD7beNwbm9QlseEgoUYnge0D6BENTyjKfz1ag9R1Mobj\nDz1lc+JO80DdIMKsknzBDNT0dHxrj87d0e0OLIAahntUK2v39r+nPJEjMZlMnHXWWVRVVbFhwwYa\nHn4Y75rXyPzKbTjOPpv+u+6i9wc/YKh/LQMDJ5avcYCenh7Wr18/tuLweHaxpeYTdHT84X3LezzU\nO++886Ts+GTy8MMP33nLLbecajE+MEpLS+npMRLTNuE2rMOSlMOc8/6LyUuW0rp3N06bgsljpHuT\nDWd/AHuqmRnnxgvNSbOZDauepSQ7D+uzzyNzctjr948V31INCp3NLiIZT2PzFbJlZS4CM0OuHjo6\n27nwwgsxGo1jsiRlZDJxzgL2bXgDnzmJpshMJvylA68yF5GpkWPcwWC/hZde3kpFRQWf/OQnUZTD\nnynMFZMIbNtKeFsdbUVFhO1hVnQ+wu2TthOKKsRSH8CqraWrbyV3v7GZeW/dSFFDFu7ejeROWk40\nnIXYN0KSVSX3mmrqXl5JsiWFikAOlldbCcoIV37rTrr39dHXuAExDJPPTcNS821sRWa2ObPZ7XBQ\nGOpGCYyQd/GtRLf2Y8qzY8yOZxQ/uO1BXm1/lV+e9Utm58xmxb4VNIkmLp50Mc31zdTV1bFlyxaG\njPkY+raTtuSziNE4/PEIa2F+8NYPOD/pXKa6NAKdu2loamZ61TQmpk8cKyPyrXnfIsU8fpirpgVo\nav4pOTmXkJt5Nk2uJtZ1rePy8suxGCwYjSl0df8FozEVEalm9R/3MGFmJnPE/9Jca6Z1KELfOUvY\n+sar5EwoZ/mtXyPoi9G4I8i0CX3kD69mmzOP2s7NpEybyDfnfRNVjH9MisWCbf58nE8+SaBuG1mf\nvIKrplzDrKYJZPRY+Xraz3mu73lmrajD6jBgW/AxAnUD2OfkoJgP7lMIgbBPIubMQyhdZHxqEYrF\ngGIz4N/Ui5pswlR4sJKsjGp4XuvE9Xwz7pfa8L3ZQ6BugODuYYZ2dPLz+ja29AZ5ZGkq3W+vJpaa\nwtmfvQUZ1XE+24R1eia26mzCzS341q0j46abxhJApZSsXr2aopIiBurfYZqpgmhEI5hvZ2L2CSYk\nHoPS0lLqampobW+numwCeT/8b5KXLwcp6d3yF9pLnmdg6EVstgk4HJXvur+BgQEeffRRmpubcbvd\nlJamULf9U6iqncrKH2Mw2N+XnD/+8Y9777zzzofHey+xMjgNEEJw0UWXEov+PzweOy2tP2LNa8vw\nRtvJWLQUIQSG/lrQNuIZCpBbljz22f7WZnQtRsXNt5D7k/+h5K23MGgaf/71r6n9+jfouvVLpO/4\nDRHHfqzeYq6QZfQ+38fm13ZjNTkwGc1HyZOeX8j1l3ydRZEiPDGFly5czj7PpezYejlBVwEe/z1M\nmuTlyiuvRFWPvpkIIcj97x+S0duLUUrOV8/nMzkOHGKQlcNV/GRNlM3aEiKhPj5p78ORqWC11WEw\nJeMazqR+5x8xTXoJ8yQr5fMWUrHwDDb9/Un2v7wRpTKHpxa1sC+5n+t/dAeqSKY+18D/0sCnJkxm\nreqlrnoqJq+bkdZ+3ukpoKlzLYZMC57XOpG65K39b/HI7ke4quIqPlb6MW6YcgMPL3uYkfAIdw/c\nzcLrFo6ZlOrdZp7QLuK53/43Edexo09e3vky0zunY64zk2nS2KPn09/fz/898hcmJU3inKJzKE4q\nptBReMx9eDw748lmo5nHN065kUA0wPUvXU+ruxWrtZjMzPNpa/8Nm17/CYqqYTZu4aHVMVZp2ewu\nyqa7u4OyGbO5+PZvo6gqpdPjYcMdlT8n4jDTnu+ktNvCj6d8D6NiPKYsbW1tbO7vJ+vHPyZYu5X+\ne36BCOgU7LFjqcrgv5Z9EfOAG2NDB6vLvLjOUpFRHfdLbejBGHogih6IEu704NvkQxg8eP5xF4Ft\ncZ+IfX4u5vJU3C+2jRVUDLe66H+wDu/aLtQ0C4HZRtZU1XFH8a+5N/NJvG4PX+rUuaxwNc2v3cNQ\nko2yGbOJdPsYeboBGdawzYqbMR1nn43u8RyWCe31evH5fJSVTyLvY9Np9tRxA2ZaH9/D75/bicfX\nzuDga3h9+445L1JKBtpbCbgP90eYgkHmbqnBnZpK48eXx5WgomC56Xxctxkw9AvMHWb27vkmI+9S\nWsTpdPL4449jMBiYP38+u3bV8tam69Gifgpqb0fbe3LMROLD2NBh7ty5sra29t0Hfgjp7e1l27bf\noRr+gcEQprengsysqzH3hal7eRVFVfOoPu8CXH2dDLa30tPcgG94iP/38BPYUlIJ7t7D3nvuYW1B\nPn6TiTk9vUxU99NzTQNpj6WQbvoKhvSJPGnYALFUikyzOPez08idEH9aDbe5ca1siZfBUGBEc/Oi\nsYawCrOVOvaKyVRWvUlS8iDllk9TOu+rYB6/R8Dgr3/NxjfW0nlOFtOnv05PeDK/GOgE3UJKOJmb\nPUspnvsYNvUcdjzXStUlOUTVHegyHvtvjmVQPec3qGoFr/7hQSYvOYvyxUu46dWbaHG18JPhs7A+\nvpnNEzLQVBMblrspbykmVcsmtWUXs1o66KwqYMArmT/9Usp8kzFeXcDVTTeRbklnxUUrsBgO9s/d\n79vPV9d+lX0j+7hi0hV8Y+43sGJi05N380aHJFu4uGzpPDrciwj5o6Rk2cAaYl/ndvY17UFTNC44\n+wKmlU9k7aoXeandT6HJgz09iy987nqkKkmzpB3zu29v/z0trfey9MytGI3xvIjtA9u5fe3tRLUo\n9519Hwty57J549cIaC/jaSugdU0S+X4vWQGY+Yf/I6O84jDTj5SSx763iaA3QkRGENEAEfcjGCzT\nmXPRZ5l7USkW+0GloOs6GzduZO2oiSUnJ4dzR5zojz+Oevk3sOoV5NycDwaVoRV/xf3YE3zvK6m0\nOgLcHf0W1U0lRx2Xkmwi6+ZK2q+5DDU5mbJnn0EYjcRGQvQ/sA1TkQNDhhX/lj7UdDMti9085H6E\nXUO7KNQKWTS8CN2v40flHKWS6kAew70baDJ4KC9aTGYoHWFWcSzKI/mCUoQi0Hw+mpeeBUYjGTd9\nFuu1V/B4zbP0vdXHxddezMyJM/jbTz5PXokXY5qHiKMHaQiNzhmk2j/G9Fk/wWw+mIPTXb+bt55+\ngu763Zjtds7+9OeZdtZ5oOt0fu5zBLfVsefrX2N3ezs333wzmZkKNbVXIITC9LRf0PfN79F3Qw96\nrom58/9O0qj/7lB8Ph9//vOfCQQC3HTTTWRlZbBmzRWohj3oO26kWr+YtKsrMBcnH/XZE0EIsVVK\nOa6t8qQrAyHEhcCDgAr8UUr58yPeNwOPAXOAYeAaKWX78fb5n6wMDuB297Kt7odo2lqEkCQlVRFz\nTqTmqb3E/PEyvak5eWSVllFaPZvq8y887POhUIh//vOf7Nu3j5mz/CQlPce0ir8R64yyffUWahlh\nfrScsnABA5rAUZlGgdVArH4ENcVMykVl1PzhcezWOaQaNV5RaxgyBrEa4EJ7gIGC7SgZrWS3zCVZ\nCWHJtmDILkfJqsKUNQuTNRslZqblukvpuqmdkHCwdftF2HJT2Gbew5yt16NFwLZwBVOKR9s+6kZG\n+grp6JuCaogwueItTOYg+focKpc8hGLLBKDH18O1/7iCn/3ahc8BbWfOJdjgQqbn4cvJJ+xroyaz\nhV+ucGOOaPTMncGuqJ+Pl3yBiEXwVMYr3HTplygvOXq5HtbCPLT9IR7d8yj5ShafM1+KtS+MYjex\ntXOImG4g2T0Zq8GK09BJ2DKIQCE3nE+Omo5DddHfVUcw6sMbHcGXmU8sMwfpSOFrX/g0ackZSCkZ\n6vLR3+4hpzSZ9AITbnctTc0/R9dDLFq4BgDP0CDD3Z0ELTo/3HMXzb5Wvi0m41o/i9Qp/yRn9iBK\nq4nsX0t2XHELb5QupdsZYMAbZk5JGpfMyOf8Kdn01zv559rX2Ovew9KUYtSdq+kLRFBMUzE7ypn/\niaXMXFZOKBTkueeeo6WlhclZEyjRsljn3Iau65wZmsgEUYTWs4FA7eOgxxWOpTSD9Ifv4fe9G3im\n/lmWeRcyxVFJcXIJxUlFpBlTsU3LxJydhPe11+j+8m1kffMbZH7ucwD43unB9c8WENA/PcRd5t/T\n6GukLKmUZdrHGNk7gqZaqAlmM9fSh1ELM6dHpzr9fASCbmMfG/N3svzSq6nKn374NVBfT9+DDxBc\nt4G+rGTWLZkFlhzemvg8nynMIF92oEcFwUE71kApWaEZ2P0lOLPexlu6DhEzk7b/MjJCF9I2tJ2m\nkVU4iqPkV6Xj6vbRVRMlLZZL9YALtbGFvJ/9DMtFH+ehhx7CYtGZMnUlmjbCQP9n2N+tY1QUynv2\nYl+yESXJxrwzXsRmK0JKSbTbh2t7D8/Uv8pIyMN1F15B6bRyGvf+lB7vE4w0nsOevkKuvOxyqkbb\nlL4fTpkyEEKoQCOwDOgGaoDrpJR7DxlzK1AtpfyiEOJa4DIp5TXH2+9HQRkcIBzup7//Bfr6V+L1\n7gYEJjUfm6MEm70Yq6UAszkHVXWgqjZU1YqiWohGRgiG9tPcXIPbvRGLxcvbm64BDj41ZhtinNlT\nTLKjAlVR0aSkOazTFIohRQRdN6HTwCXXnoV4fZi9eid5sRQylTR0NUzXzF8Ryjj2khopELoBKTTE\n03Po0vPoKEhGFwJVCgwSTFIlt3gPXk1lYLCEzFgmlVo+ZquFxqRdWLLXkZPTRtTjQPU4MCgKBqGi\n+QMoPX4sWTacUz5NW80gA/4IImpBeObhSWvDozQzbTBCWasXGQvgLchlTtZy7Mb4KihiCKIUxDNV\nhdWIajUizAacw73s3raBodZmYlqEmBEMUcBkxV1QRNRkAClRUZiqFVIdK8WK6eBhI5FqhICjhZGU\nrbgy9mJIH0BVNUJBByFvBkFPNtFAOo7MdpKz2zAYIui6Snv7PIZ6JyODfmTAj6JpqLEYBk2iSomq\nGTHGwlgiESqy9xH4hBP8BprCE9G0HFTyUGUuzcNenJoX3RgiKc1DMNbMAvU8FpgWYW/rIOSy4A4H\nGA67cMU8eI0qw2lGokJncaySCi2PiDJCSOtkg9nJgAKVfpWMvhZcUQ82k5cJacNUJLejKho+ezG7\n8s9glYgx2NmPZSBKhttAkl8iFYEvWWUoNcqsfU5mtbjxpwk8ZTa00nRSs6azz7KNkL2H3CQj6dYo\noOP1ptHjymePczKzzPlM3vgS27KKcGXkYg9EKDM6yPrMEu7adjcD4QFumHoD2bZsdKkTkzG8YS9v\nbd9I+UAa6YYkzKqfcuMuLFMGkRJ6mlRcoSq8BgNtkR5cWpD5scUURwtJUcNYKtejZ7WjhFLQTT5Q\n4o5cJWBBt4RAgajThr89C2toKmp2JgI/IWUEPXMXdruTPTvPwz+QBmE/mtGEbnNgszqZOeMVlKgF\n6ZxJ2JeGz29nIGwgZPSwyOrA7hgmlNxOML2BtIELqJj7U/624Xn279/PjTfeSFlZ2TgX3LtzKpXB\nIuBOKeXHRl/fASClvPuQMa+OjnlbCGEA+oAseRzBPkrK4FD8/hb6B17C72sgGOomFNpPNPpuzTgU\nDIYMkPOw2z+D2WzGZDJht9vJy89j1+BOtj37R8rWRJBBCyFdoqnW0Z8wU++9iimT5hIbCeFe3U44\n4setD9PjbKKleSvhUA9ptgyy7Plk2XKwW+xoJh8xkxvN5EUzeXAMzsIxGLeDe0SQJrWXMNH4jx4g\nqofI8EWYsH+I5MH96L4+ZChexsBvt9F1QTrmJX0oxhhSCkAgpcBgiKCqRycNSamArqLrCkgFECAF\nEglSQ0hQhEBBRREqhyrIw3cU3y5G55HRMzKMhkBgkAq6HiMmo+jEMCg6ikGCUYIix/Zh8haiuYoZ\nikkU+yBG+xAWmxtF0YmErXiGi4kMlaMOT0LRTQSUKEERISgiBAiji2Nfo+lp+8nLa8Ri9WG2eMed\njxNByvicKlJBShmPsJGjhywFUihIoRw1VwIJQh78fRxZD6AIHaEcPS4WM+L3p+L3pyFjgmRLL/Z0\nb3xOj0DTVHRdHZP9wDlx4P8fkMdgiB4mk9QVPC0LGd7zcSLhrHjJEamj6FEUPYqqRxH6gTnUsRZv\nwV62HdVXgsNbQXZgMslaJjGTG29OLd6cLQTTGuGI4xYRB2n115LZvwQxOmdSSlyKnwa1h9607ZRW\nbsRq9Y07ZzKmoLvS0Pvz2LcJNF1HKkYCxeWk+ELc/rtfv+s8j8epVAZXAhdKKT83+vpTwAIp5ZcP\nGbN7dEz36OuW0TFDR+zrFuAWgOLi4jn/KVl//yqaFiAcHkTTAmh6AC0WQNeDGI3pmM15mM3ZKMdx\nFB5ASokmNRShoIgTjyvQdY1oKEQ4EMDV72a4dRg9oEEYZEQDrxdFD2NQIxiUMAYliMUYIjPXiGrU\nkJpAiwpkajkSEx5fEKc7gDcYJoYgJgUxCVEpCFusBIxGYiIK0REitkxsqUaSbV7sZjfJFi/pdgUp\no0ipEY2EiUU1pK6j6zq6riE1HS0SIxYMEfH5CAe8oOvEW1ELBHElYVSNKIixG51BCSOJoRFDkzF0\nYhgNhtFIKokBDWtaGia7A1WCqoOdNJJlLsFhGyNNBjQXGHUP0aiOP6rhM/sx+5IwoAAqUWEHYULF\nFL/5AjoQMWpE1SgRNUzYEMGYk4k1N4uoFsMf9hOIBeKSyShS9SJUF0ZFwYgBIwZUFHQhiRljRJUI\nMRlGlxGEAihgkhr2mA+DSaKpKromQJMogN0o0GMRYrEw0VgITcr4ygfG5EMqyNGbMVJHKCqqooDQ\n4zdlCRoCqQs0XUfXQIsa0KJGYlEDsYiBqMeG5rVg0HSUqI4pqmEU8Zu7MdWHKd2HJcWC2WpBNUpi\nIkhURJFCR0cihY5EH1NeBx4AjJodazQFJWaHqA2C6Rh1G6qiocoYMhRCiwpiMYWYphCLKeiMKj0h\nCKLjFTpRk0LUADEdHNKMUQHUCFKNoZhdKPY+QnoQvx4kFNOIahqqxYRVJJEUzCU5nIMpZkVDoklB\nTIsRiPlwxfoxGJxYbT4sdj962EBoJJmw0zpqipOj4bdy7O/UCel85n/uOeFr9FCOpwwM4208HZFS\nPgw8DPGVwSkW57RBVW3YbEc77d4rQggM4r2fDoqiYrbZMdvsJGdmUTyt/F+SIwko+Jf2cPphBk6w\nx1qCBKeMkx1auh8oOuR14ei2cceMmolSiDuSEyRIkCDBv4mTrQxqgElCiDIhhAm4Flh5xJiVwGdG\n/74SeON4/oIECRIkSPDBc1LNRFLKmBDiy8CrxENL/yyl3COE+B+gVkq5EvgT8LgQohkYIa4wEiRI\nkCDBv5GT7jOQUr4EvHTEth8e8ncIuOpky5EgQYIECY5NohxFggQJEiRIKIMECRIkSJBQBgkSJEiQ\ngIQySJAgQYIEfEirlgohBoH3m4KcCby3Xnenlg+bvPDhkzkh78klIe/J5b3IWyKlzBrvjQ+lMvhX\nEELUHisd+3TkwyYvfPhkTsh7cknIe3L5oORNmIkSJEiQIEFCGSRIkCBBgo+mMhi3/+dpzIdNXvjw\nyZyQ9+SSkPfk8oHI+5HzGSRIkCBBgqP5KK4MEiRIkCDBESSUQYIECRIk+GgpAyHEhUKIBiFEsxDi\nu6daniMRQvxZCDEw2v3twLZ0IcQaIUTT6O+0UynjoQghioQQa4UQe4UQe4QQt49uPy1lFkJYhBBb\nhBA7RuX98ej2MiHE5tHz4unRcuunDUIIVQhRJ4R4YfT1aSuvEKJdCLFLCLFdCFE7uu20PB8AhBCp\nQohnhBD7hBD1QohFp7m8laNze+DHI4T46gch80dGGQghVOB3wHJgKnCdEGLqqZXqKB4FLjxi23eB\n16WUk4DXR1+fLsSAb0gppwILgS+NzunpKnMYOFdKOQOYCVwohFgI3AP8SkpZDjiBm0+hjONxO1B/\nyOvTXd5zpJQzD4l9P13PB4AHgVeklJOBGcTn+bSVV0rZMDq3M4E5QAD4Bx+EzFLKj8QPsAh49ZDX\ndwB3nGq5xpGzFNh9yOsGIG/07zyg4VTLeBzZnweWfRhkBmzANmAB8exNw3jnyan+Id4d8HXgXOAF\n4h3pT2d524HMI7adlucD8a6KbYwG0pzu8o4j/wXAWx+UzB+ZlQHx1rpdh7zu5sPRbjdHStk7+ncf\nkHMqhTkWQohSYBawmdNY5lGTy3ZgAFgDtAAuKWVsdMjpdl48AHyb0d7zQAant7wSWC2E2CqEuGV0\n2+l6PpQBg8Ajo2a4Pwoh7Jy+8h7JtcCK0b//ZZk/SsrgQ4+Mq/3TLhZYCOEAngW+KqX0HPre6Saz\nlFKT8SV2ITAfmHyKRTomQoiLgQEp5dZTLct74Awp5Wzi5tgvCSGWHvrmaXY+GIDZwO+llLMAP0eY\nV04zeccY9RN9Avj7ke+9X5k/SspgP1B0yOvC0W2nO/1CiDyA0d8Dp1iewxBCGIkrgiellM+Nbj6t\nZQaQUrqAtcTNLKlCiANd/06n82IJ8Anx/9u7gxCpyzCO499fZrJYmJl0sVhC6RBJSXQQD4Kn7Fgg\n4kHCSx6yU1gEnTp5CNrqUqcgyYOgSAep1oigSKNysQSLEDK0MihYCBH5dXifzf9uuzShu/OH+X1g\nmHfeGf48A//hmff9zzyPdB44RNsqeo3+xovtn+v+V9pe9mP093y4AFyw/UU9PkxLDn2Nt+tx4Cvb\nv9TjG455lJLBKWBD/RLjNtoS69iQYxrEMWB3jXfT9uV7QZJoPazP2n6181QvY5a0VtKdNR6jXd84\nS0sKT9XLehOv7Rdtr7M9TjtfT9jeRU/jlbRS0h0zY9qe9hl6ej7YvgT8JOmBmtoGfEdP451jJ9e3\niOBmxDzsiyBLfMFlO3COtk/80rDjmSe+94CLwFXat5Y9tD3iSeB74CPgrmHH2Yl3C205OgV8U7ft\nfY0Z2Ah8XfGeAV6u+fuBk8APtGX3imHHOk/sW4H3+xxvxXW6bt/OfMb6ej5UbA8DX9Y5cRRY3ed4\nK+aVwO/Aqs7cDcecchQRETFS20QREbGAJIOIiEgyiIiIJIOIiCDJICIiSDKImEXStTlVIW9akTJJ\n492KtBF9cut/vyRipPzlVq4iYqRkZRAxgKrTf6Bq9Z+UtL7mxyWdkDQlaVLSfTV/j6Qj1TvhtKTN\ndahlkt6ufgof1D+hkbSv+kJMSTo0pLcZIyzJIGK2sTnbRDs6z/1p+yHgDVo1UYDXgXdsbwQOAhM1\nPwF84tY7YRPtH7kAG4A3bT8I/AE8WfMvAI/UcZ5ZrDcXsZD8AzmiQ9K07dvnmT9Pa4zzYxXnu2R7\njaTLtDryV2v+ou27Jf0GrLN9pXOMceBDtwYkSNoPLLf9iqTjwDStJMJR29OL/FYjZsnKIGJwXmD8\nf1zpjK9x/brdE7ROfJuAU52qpBFLIskgYnA7Ovef1/gzWkVRgF3ApzWeBPbCPw11Vi10UEm3APfa\n/hjYT+vA9a/VScRiyrePiNnGqhPajOO2Z35eulrSFO3b/c6ae5bWKet5Wtesp2v+OeAtSXtoK4C9\ntIq086URr3UAAABGSURBVFkGvFsJQ8CEW7+FiCWTawYRA6hrBo/avjzsWCIWQ7aJIiIiK4OIiMjK\nICIiSDKIiAiSDCIigiSDiIggySAiIoC/AQ+dp/iKDeSjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i,j in enumerate(train_loss_all):\n",
    "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
    "    \n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Training_loss\")\n",
    "\n",
    "plt.legend()\n",
    "fig.savefig(\"Figure.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qlY1RknNfVjb",
    "outputId": "4726a79b-6d8c-47c9-a3c3-13d40ff4d5d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uMVKmRRhfVjd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Exp_2_Attention_models_on_9_datasets_made_from_10k_mosaic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
