{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CIFAR_mosaic_attention_visualisation_100epoch_trend_Plots.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjG64ra4aFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f185e36a-d9c8-460e-b1b5-586b943bf1f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8-7SARDZErK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKp2zTlx2PPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4068653c-c63b-41b4-b72c-faaa41e0d361"
      },
      "source": [
        "a = torch.arange(2*3*1*1)\n",
        "print(a.shape,a)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6]) tensor([0, 1, 2, 3, 4, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmQ0YrSm2YKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "47ca5fb0-c51a-456b-df5e-23578affc90d"
      },
      "source": [
        "torch.reshape(a, (2,3,1,1))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0]],\n",
              "\n",
              "         [[1]],\n",
              "\n",
              "         [[2]]],\n",
              "\n",
              "\n",
              "        [[[3]],\n",
              "\n",
              "         [[4]],\n",
              "\n",
              "         [[5]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTkC12__23oO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "77e3d72a-4fa6-4dcb-e9d5-1b51f2347b95"
      },
      "source": [
        "torch.reshape(a, (2*3,1,1))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0]],\n",
              "\n",
              "        [[1]],\n",
              "\n",
              "        [[2]],\n",
              "\n",
              "        [[3]],\n",
              "\n",
              "        [[4]],\n",
              "\n",
              "        [[5]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acRFqJNrZErV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4b16ae9c-7083-4c2e-b693-c7139d0e7641"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data/cifar-10-python.tar.gz\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V_JUhwCeZErk",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'horse','ship', 'truck'}\n",
        "\n",
        "background_classes = {'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog'}\n",
        "\n",
        "# print(type(foreground_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cYuA7_81ZEro",
        "colab": {}
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M6nKOvp8ZEsF",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hS0soEiqZEsK",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "desired_num = 30000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSO9SFE25Lrk",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n",
        "batch = 250\n",
        "msd = MosaicDataset(mosaic_list_of_images, mosaic_label , fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip0BY-Oclf8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=9*3, out_channels=32, kernel_size=3, padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=0)\n",
        "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=0)\n",
        "    self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=0)\n",
        "    self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(32,track_running_stats=False)\n",
        "    self.batch_norm2 = nn.BatchNorm2d(128,track_running_stats=False)\n",
        "    self.dropout1 = nn.Dropout2d(p=0.05)\n",
        "    self.dropout2 = nn.Dropout2d(p=0.1)\n",
        "    self.fc1 = nn.Linear(128,64)\n",
        "    self.fc2 = nn.Linear(64, 32)\n",
        "    self.fc3 = nn.Linear(32, 3)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(self.batch_norm1(x))\n",
        "\n",
        "    x = (F.relu(self.conv2(x)))\n",
        "    x = self.pool(x)\n",
        "    \n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(self.batch_norm2(x))\n",
        "\n",
        "    x = (F.relu(self.conv4(x)))\n",
        "    x = self.pool(x)\n",
        "    x = self.dropout1(x)\n",
        "\n",
        "    x = self.conv5(x)\n",
        "    x = F.relu(self.batch_norm2(x))\n",
        "\n",
        "    x = (F.relu(self.conv6(x)))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = self.dropout2(x)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.dropout2(x)\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKDBpz7zgz8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "block_net = CNN()\n",
        "block_net = block_net.to(\"cuda\").double()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn3oPpfH3-P3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "add5b302-dbae-4a88-fbec-91b2abea60d2"
      },
      "source": [
        "block_net"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(27, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
              "  (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
              "  (dropout1): Dropout2d(p=0.05, inplace=False)\n",
              "  (dropout2): Dropout2d(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1yVWgR4vFhe",
        "colab": {}
      },
      "source": [
        "test_images =[]        #list of mosaic images, each mosaic image is saved as laist of 9 images\n",
        "fore_idx_test =[]                   #list of indexes at which foreground image is present in a mosaic image                \n",
        "test_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(10000):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx_test.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  test_images.append(image_list)\n",
        "  test_label.append(label)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xRq1cUvgvLeH",
        "colab": {}
      },
      "source": [
        "test_data = MosaicDataset(test_images,test_label,fore_idx_test)\n",
        "test_loader = DataLoader( test_data,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Zq1FF44fD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(block_net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFfAJZkcZEsY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d5b83f4-4f2c-437c-a1e5-c3d4761af5e6"
      },
      "source": [
        "nos_epochs = 300\n",
        "acti = []\n",
        "loss_epoch = []\n",
        "cnt=0\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  ep_lossi = []\n",
        "  \n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels , fore_idx = data\n",
        "    # zero the parameter gradients\n",
        "    # print(inputs.shape)\n",
        "    inputs = torch.reshape(inputs,(batch,9*3,32,32))\n",
        "    # print(inputs.shape)\n",
        "    inputs , labels , fore_idx = inputs.to(\"cuda\") , labels.to(\"cuda\"), fore_idx.to(\"cuda\")\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    outputs = block_net(inputs)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = criterion(outputs, labels) \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini_batch = 40\n",
        "    if i % mini_batch == mini_batch-1:    # print every 50 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / mini_batch))\n",
        "      ep_lossi.append(running_loss/mini_batch) # loss per minibatch\n",
        "      running_loss = 0.0\n",
        "\n",
        "\n",
        "  loss_epoch.append(np.mean(ep_lossi))\n",
        "  if(np.mean(ep_lossi) <= 0.005):\n",
        "    break;\n",
        "  cnt=cnt+1    \n",
        "    \n",
        "print('Finished Training')\n",
        "print(cnt, len(loss_epoch))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    40] loss: 1.099\n",
            "[1,    80] loss: 1.097\n",
            "[1,   120] loss: 1.094\n",
            "[2,    40] loss: 1.085\n",
            "[2,    80] loss: 1.081\n",
            "[2,   120] loss: 1.072\n",
            "[3,    40] loss: 1.061\n",
            "[3,    80] loss: 1.063\n",
            "[3,   120] loss: 1.058\n",
            "[4,    40] loss: 1.046\n",
            "[4,    80] loss: 1.050\n",
            "[4,   120] loss: 1.046\n",
            "[5,    40] loss: 1.031\n",
            "[5,    80] loss: 1.032\n",
            "[5,   120] loss: 1.037\n",
            "[6,    40] loss: 1.021\n",
            "[6,    80] loss: 1.007\n",
            "[6,   120] loss: 1.027\n",
            "[7,    40] loss: 0.994\n",
            "[7,    80] loss: 0.994\n",
            "[7,   120] loss: 1.013\n",
            "[8,    40] loss: 0.973\n",
            "[8,    80] loss: 0.980\n",
            "[8,   120] loss: 0.982\n",
            "[9,    40] loss: 0.927\n",
            "[9,    80] loss: 0.962\n",
            "[9,   120] loss: 0.970\n",
            "[10,    40] loss: 0.889\n",
            "[10,    80] loss: 0.930\n",
            "[10,   120] loss: 0.943\n",
            "[11,    40] loss: 0.851\n",
            "[11,    80] loss: 0.924\n",
            "[11,   120] loss: 0.915\n",
            "[12,    40] loss: 0.807\n",
            "[12,    80] loss: 0.854\n",
            "[12,   120] loss: 0.881\n",
            "[13,    40] loss: 0.758\n",
            "[13,    80] loss: 0.836\n",
            "[13,   120] loss: 0.828\n",
            "[14,    40] loss: 0.681\n",
            "[14,    80] loss: 0.770\n",
            "[14,   120] loss: 0.792\n",
            "[15,    40] loss: 0.626\n",
            "[15,    80] loss: 0.720\n",
            "[15,   120] loss: 0.730\n",
            "[16,    40] loss: 0.565\n",
            "[16,    80] loss: 0.648\n",
            "[16,   120] loss: 0.701\n",
            "[17,    40] loss: 0.531\n",
            "[17,    80] loss: 0.613\n",
            "[17,   120] loss: 0.639\n",
            "[18,    40] loss: 0.471\n",
            "[18,    80] loss: 0.530\n",
            "[18,   120] loss: 0.562\n",
            "[19,    40] loss: 0.405\n",
            "[19,    80] loss: 0.474\n",
            "[19,   120] loss: 0.520\n",
            "[20,    40] loss: 0.363\n",
            "[20,    80] loss: 0.422\n",
            "[20,   120] loss: 0.483\n",
            "[21,    40] loss: 0.321\n",
            "[21,    80] loss: 0.376\n",
            "[21,   120] loss: 0.426\n",
            "[22,    40] loss: 0.286\n",
            "[22,    80] loss: 0.340\n",
            "[22,   120] loss: 0.349\n",
            "[23,    40] loss: 0.260\n",
            "[23,    80] loss: 0.301\n",
            "[23,   120] loss: 0.322\n",
            "[24,    40] loss: 0.224\n",
            "[24,    80] loss: 0.287\n",
            "[24,   120] loss: 0.311\n",
            "[25,    40] loss: 0.196\n",
            "[25,    80] loss: 0.232\n",
            "[25,   120] loss: 0.262\n",
            "[26,    40] loss: 0.178\n",
            "[26,    80] loss: 0.214\n",
            "[26,   120] loss: 0.240\n",
            "[27,    40] loss: 0.180\n",
            "[27,    80] loss: 0.175\n",
            "[27,   120] loss: 0.210\n",
            "[28,    40] loss: 0.158\n",
            "[28,    80] loss: 0.167\n",
            "[28,   120] loss: 0.210\n",
            "[29,    40] loss: 0.144\n",
            "[29,    80] loss: 0.168\n",
            "[29,   120] loss: 0.180\n",
            "[30,    40] loss: 0.122\n",
            "[30,    80] loss: 0.143\n",
            "[30,   120] loss: 0.171\n",
            "[31,    40] loss: 0.111\n",
            "[31,    80] loss: 0.140\n",
            "[31,   120] loss: 0.167\n",
            "[32,    40] loss: 0.107\n",
            "[32,    80] loss: 0.132\n",
            "[32,   120] loss: 0.135\n",
            "[33,    40] loss: 0.105\n",
            "[33,    80] loss: 0.117\n",
            "[33,   120] loss: 0.132\n",
            "[34,    40] loss: 0.084\n",
            "[34,    80] loss: 0.098\n",
            "[34,   120] loss: 0.118\n",
            "[35,    40] loss: 0.083\n",
            "[35,    80] loss: 0.100\n",
            "[35,   120] loss: 0.118\n",
            "[36,    40] loss: 0.074\n",
            "[36,    80] loss: 0.090\n",
            "[36,   120] loss: 0.096\n",
            "[37,    40] loss: 0.075\n",
            "[37,    80] loss: 0.081\n",
            "[37,   120] loss: 0.084\n",
            "[38,    40] loss: 0.071\n",
            "[38,    80] loss: 0.074\n",
            "[38,   120] loss: 0.084\n",
            "[39,    40] loss: 0.064\n",
            "[39,    80] loss: 0.081\n",
            "[39,   120] loss: 0.094\n",
            "[40,    40] loss: 0.064\n",
            "[40,    80] loss: 0.070\n",
            "[40,   120] loss: 0.084\n",
            "[41,    40] loss: 0.067\n",
            "[41,    80] loss: 0.070\n",
            "[41,   120] loss: 0.076\n",
            "[42,    40] loss: 0.061\n",
            "[42,    80] loss: 0.068\n",
            "[42,   120] loss: 0.073\n",
            "[43,    40] loss: 0.057\n",
            "[43,    80] loss: 0.067\n",
            "[43,   120] loss: 0.078\n",
            "[44,    40] loss: 0.044\n",
            "[44,    80] loss: 0.052\n",
            "[44,   120] loss: 0.063\n",
            "[45,    40] loss: 0.052\n",
            "[45,    80] loss: 0.058\n",
            "[45,   120] loss: 0.066\n",
            "[46,    40] loss: 0.047\n",
            "[46,    80] loss: 0.054\n",
            "[46,   120] loss: 0.054\n",
            "[47,    40] loss: 0.049\n",
            "[47,    80] loss: 0.048\n",
            "[47,   120] loss: 0.059\n",
            "[48,    40] loss: 0.050\n",
            "[48,    80] loss: 0.049\n",
            "[48,   120] loss: 0.060\n",
            "[49,    40] loss: 0.051\n",
            "[49,    80] loss: 0.049\n",
            "[49,   120] loss: 0.054\n",
            "[50,    40] loss: 0.040\n",
            "[50,    80] loss: 0.050\n",
            "[50,   120] loss: 0.060\n",
            "[51,    40] loss: 0.042\n",
            "[51,    80] loss: 0.049\n",
            "[51,   120] loss: 0.059\n",
            "[52,    40] loss: 0.038\n",
            "[52,    80] loss: 0.046\n",
            "[52,   120] loss: 0.050\n",
            "[53,    40] loss: 0.033\n",
            "[53,    80] loss: 0.041\n",
            "[53,   120] loss: 0.053\n",
            "[54,    40] loss: 0.034\n",
            "[54,    80] loss: 0.035\n",
            "[54,   120] loss: 0.037\n",
            "[55,    40] loss: 0.031\n",
            "[55,    80] loss: 0.032\n",
            "[55,   120] loss: 0.039\n",
            "[56,    40] loss: 0.032\n",
            "[56,    80] loss: 0.039\n",
            "[56,   120] loss: 0.046\n",
            "[57,    40] loss: 0.031\n",
            "[57,    80] loss: 0.030\n",
            "[57,   120] loss: 0.037\n",
            "[58,    40] loss: 0.031\n",
            "[58,    80] loss: 0.034\n",
            "[58,   120] loss: 0.036\n",
            "[59,    40] loss: 0.027\n",
            "[59,    80] loss: 0.029\n",
            "[59,   120] loss: 0.028\n",
            "[60,    40] loss: 0.023\n",
            "[60,    80] loss: 0.029\n",
            "[60,   120] loss: 0.033\n",
            "[61,    40] loss: 0.034\n",
            "[61,    80] loss: 0.030\n",
            "[61,   120] loss: 0.037\n",
            "[62,    40] loss: 0.027\n",
            "[62,    80] loss: 0.041\n",
            "[62,   120] loss: 0.049\n",
            "[63,    40] loss: 0.027\n",
            "[63,    80] loss: 0.030\n",
            "[63,   120] loss: 0.033\n",
            "[64,    40] loss: 0.028\n",
            "[64,    80] loss: 0.029\n",
            "[64,   120] loss: 0.033\n",
            "[65,    40] loss: 0.025\n",
            "[65,    80] loss: 0.030\n",
            "[65,   120] loss: 0.033\n",
            "[66,    40] loss: 0.030\n",
            "[66,    80] loss: 0.028\n",
            "[66,   120] loss: 0.032\n",
            "[67,    40] loss: 0.024\n",
            "[67,    80] loss: 0.028\n",
            "[67,   120] loss: 0.036\n",
            "[68,    40] loss: 0.025\n",
            "[68,    80] loss: 0.027\n",
            "[68,   120] loss: 0.024\n",
            "[69,    40] loss: 0.019\n",
            "[69,    80] loss: 0.023\n",
            "[69,   120] loss: 0.038\n",
            "[70,    40] loss: 0.030\n",
            "[70,    80] loss: 0.032\n",
            "[70,   120] loss: 0.032\n",
            "[71,    40] loss: 0.020\n",
            "[71,    80] loss: 0.028\n",
            "[71,   120] loss: 0.027\n",
            "[72,    40] loss: 0.022\n",
            "[72,    80] loss: 0.022\n",
            "[72,   120] loss: 0.032\n",
            "[73,    40] loss: 0.036\n",
            "[73,    80] loss: 0.032\n",
            "[73,   120] loss: 0.033\n",
            "[74,    40] loss: 0.022\n",
            "[74,    80] loss: 0.022\n",
            "[74,   120] loss: 0.026\n",
            "[75,    40] loss: 0.020\n",
            "[75,    80] loss: 0.023\n",
            "[75,   120] loss: 0.025\n",
            "[76,    40] loss: 0.017\n",
            "[76,    80] loss: 0.023\n",
            "[76,   120] loss: 0.021\n",
            "[77,    40] loss: 0.017\n",
            "[77,    80] loss: 0.021\n",
            "[77,   120] loss: 0.019\n",
            "[78,    40] loss: 0.015\n",
            "[78,    80] loss: 0.018\n",
            "[78,   120] loss: 0.021\n",
            "[79,    40] loss: 0.020\n",
            "[79,    80] loss: 0.024\n",
            "[79,   120] loss: 0.023\n",
            "[80,    40] loss: 0.016\n",
            "[80,    80] loss: 0.024\n",
            "[80,   120] loss: 0.023\n",
            "[81,    40] loss: 0.020\n",
            "[81,    80] loss: 0.020\n",
            "[81,   120] loss: 0.024\n",
            "[82,    40] loss: 0.018\n",
            "[82,    80] loss: 0.021\n",
            "[82,   120] loss: 0.019\n",
            "[83,    40] loss: 0.019\n",
            "[83,    80] loss: 0.018\n",
            "[83,   120] loss: 0.018\n",
            "[84,    40] loss: 0.015\n",
            "[84,    80] loss: 0.019\n",
            "[84,   120] loss: 0.022\n",
            "[85,    40] loss: 0.018\n",
            "[85,    80] loss: 0.021\n",
            "[85,   120] loss: 0.021\n",
            "[86,    40] loss: 0.018\n",
            "[86,    80] loss: 0.017\n",
            "[86,   120] loss: 0.020\n",
            "[87,    40] loss: 0.017\n",
            "[87,    80] loss: 0.019\n",
            "[87,   120] loss: 0.022\n",
            "[88,    40] loss: 0.017\n",
            "[88,    80] loss: 0.020\n",
            "[88,   120] loss: 0.022\n",
            "[89,    40] loss: 0.012\n",
            "[89,    80] loss: 0.016\n",
            "[89,   120] loss: 0.013\n",
            "[90,    40] loss: 0.011\n",
            "[90,    80] loss: 0.013\n",
            "[90,   120] loss: 0.018\n",
            "[91,    40] loss: 0.015\n",
            "[91,    80] loss: 0.023\n",
            "[91,   120] loss: 0.022\n",
            "[92,    40] loss: 0.020\n",
            "[92,    80] loss: 0.017\n",
            "[92,   120] loss: 0.017\n",
            "[93,    40] loss: 0.015\n",
            "[93,    80] loss: 0.019\n",
            "[93,   120] loss: 0.017\n",
            "[94,    40] loss: 0.013\n",
            "[94,    80] loss: 0.013\n",
            "[94,   120] loss: 0.016\n",
            "[95,    40] loss: 0.019\n",
            "[95,    80] loss: 0.015\n",
            "[95,   120] loss: 0.019\n",
            "[96,    40] loss: 0.013\n",
            "[96,    80] loss: 0.018\n",
            "[96,   120] loss: 0.017\n",
            "[97,    40] loss: 0.017\n",
            "[97,    80] loss: 0.018\n",
            "[97,   120] loss: 0.019\n",
            "[98,    40] loss: 0.015\n",
            "[98,    80] loss: 0.019\n",
            "[98,   120] loss: 0.016\n",
            "[99,    40] loss: 0.016\n",
            "[99,    80] loss: 0.013\n",
            "[99,   120] loss: 0.014\n",
            "[100,    40] loss: 0.012\n",
            "[100,    80] loss: 0.016\n",
            "[100,   120] loss: 0.015\n",
            "[101,    40] loss: 0.016\n",
            "[101,    80] loss: 0.014\n",
            "[101,   120] loss: 0.013\n",
            "[102,    40] loss: 0.009\n",
            "[102,    80] loss: 0.013\n",
            "[102,   120] loss: 0.019\n",
            "[103,    40] loss: 0.014\n",
            "[103,    80] loss: 0.013\n",
            "[103,   120] loss: 0.015\n",
            "[104,    40] loss: 0.010\n",
            "[104,    80] loss: 0.013\n",
            "[104,   120] loss: 0.015\n",
            "[105,    40] loss: 0.010\n",
            "[105,    80] loss: 0.012\n",
            "[105,   120] loss: 0.014\n",
            "[106,    40] loss: 0.010\n",
            "[106,    80] loss: 0.011\n",
            "[106,   120] loss: 0.014\n",
            "[107,    40] loss: 0.009\n",
            "[107,    80] loss: 0.013\n",
            "[107,   120] loss: 0.019\n",
            "[108,    40] loss: 0.018\n",
            "[108,    80] loss: 0.018\n",
            "[108,   120] loss: 0.017\n",
            "[109,    40] loss: 0.013\n",
            "[109,    80] loss: 0.013\n",
            "[109,   120] loss: 0.014\n",
            "[110,    40] loss: 0.012\n",
            "[110,    80] loss: 0.012\n",
            "[110,   120] loss: 0.014\n",
            "[111,    40] loss: 0.009\n",
            "[111,    80] loss: 0.010\n",
            "[111,   120] loss: 0.016\n",
            "[112,    40] loss: 0.014\n",
            "[112,    80] loss: 0.015\n",
            "[112,   120] loss: 0.018\n",
            "[113,    40] loss: 0.017\n",
            "[113,    80] loss: 0.021\n",
            "[113,   120] loss: 0.018\n",
            "[114,    40] loss: 0.013\n",
            "[114,    80] loss: 0.016\n",
            "[114,   120] loss: 0.014\n",
            "[115,    40] loss: 0.010\n",
            "[115,    80] loss: 0.012\n",
            "[115,   120] loss: 0.014\n",
            "[116,    40] loss: 0.013\n",
            "[116,    80] loss: 0.011\n",
            "[116,   120] loss: 0.011\n",
            "[117,    40] loss: 0.009\n",
            "[117,    80] loss: 0.009\n",
            "[117,   120] loss: 0.013\n",
            "[118,    40] loss: 0.010\n",
            "[118,    80] loss: 0.008\n",
            "[118,   120] loss: 0.011\n",
            "[119,    40] loss: 0.008\n",
            "[119,    80] loss: 0.012\n",
            "[119,   120] loss: 0.014\n",
            "[120,    40] loss: 0.010\n",
            "[120,    80] loss: 0.009\n",
            "[120,   120] loss: 0.008\n",
            "[121,    40] loss: 0.008\n",
            "[121,    80] loss: 0.008\n",
            "[121,   120] loss: 0.007\n",
            "[122,    40] loss: 0.007\n",
            "[122,    80] loss: 0.005\n",
            "[122,   120] loss: 0.006\n",
            "[123,    40] loss: 0.007\n",
            "[123,    80] loss: 0.008\n",
            "[123,   120] loss: 0.010\n",
            "[124,    40] loss: 0.008\n",
            "[124,    80] loss: 0.011\n",
            "[124,   120] loss: 0.010\n",
            "[125,    40] loss: 0.008\n",
            "[125,    80] loss: 0.008\n",
            "[125,   120] loss: 0.013\n",
            "[126,    40] loss: 0.009\n",
            "[126,    80] loss: 0.008\n",
            "[126,   120] loss: 0.012\n",
            "[127,    40] loss: 0.011\n",
            "[127,    80] loss: 0.014\n",
            "[127,   120] loss: 0.011\n",
            "[128,    40] loss: 0.015\n",
            "[128,    80] loss: 0.013\n",
            "[128,   120] loss: 0.016\n",
            "[129,    40] loss: 0.008\n",
            "[129,    80] loss: 0.012\n",
            "[129,   120] loss: 0.012\n",
            "[130,    40] loss: 0.008\n",
            "[130,    80] loss: 0.010\n",
            "[130,   120] loss: 0.014\n",
            "[131,    40] loss: 0.010\n",
            "[131,    80] loss: 0.010\n",
            "[131,   120] loss: 0.009\n",
            "[132,    40] loss: 0.009\n",
            "[132,    80] loss: 0.011\n",
            "[132,   120] loss: 0.009\n",
            "[133,    40] loss: 0.008\n",
            "[133,    80] loss: 0.009\n",
            "[133,   120] loss: 0.010\n",
            "[134,    40] loss: 0.008\n",
            "[134,    80] loss: 0.009\n",
            "[134,   120] loss: 0.009\n",
            "[135,    40] loss: 0.007\n",
            "[135,    80] loss: 0.009\n",
            "[135,   120] loss: 0.008\n",
            "[136,    40] loss: 0.006\n",
            "[136,    80] loss: 0.008\n",
            "[136,   120] loss: 0.009\n",
            "[137,    40] loss: 0.008\n",
            "[137,    80] loss: 0.007\n",
            "[137,   120] loss: 0.007\n",
            "[138,    40] loss: 0.010\n",
            "[138,    80] loss: 0.012\n",
            "[138,   120] loss: 0.011\n",
            "[139,    40] loss: 0.004\n",
            "[139,    80] loss: 0.005\n",
            "[139,   120] loss: 0.006\n",
            "Finished Training\n",
            "138 139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVKAlUQP7sEN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6cad1d45-74e5-4bcf-c650-906eb9d0e417"
      },
      "source": [
        "cnt, epoch"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(138, 138)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO-TqZzilzE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"cnn_6layer\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMpD1Nfe4ZSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(block_net.state_dict(),\"/content/drive/My Drive/Research/block_net/block_net_\" + model + \"_epoch\" + str(cnt) + \".pt\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxwOAPq35dKK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "55d95d2c-4b67-4900-cc59-7eb675aa3727"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in train_loader:\n",
        "        inputs , labels , fore_idx = data\n",
        "        inputs = torch.reshape(inputs,(batch,9*3,32,32))\n",
        "        inputs , labels , fore_idx = inputs.to(\"cuda\") , labels.to(\"cuda\"), fore_idx.to(\"cuda\")\n",
        "        outputs = block_net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the %d train images: %d %%' % (total,  100 * correct / total))\n",
        "print(total,correct)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 30000 train images: 99 %\n",
            "30000 29941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH2mAQty8ure",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ae945da6-aa55-4f2b-b610-62c94a364b4e"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "out = []\n",
        "pred = []\n",
        "block_net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs , labels , fore_idx = data\n",
        "        inputs = torch.reshape(inputs,(batch,9*3,32,32))\n",
        "        inputs , labels , fore_idx = inputs.to(\"cuda\") , labels.to(\"cuda\"), fore_idx.to(\"cuda\")\n",
        "        outputs = block_net(inputs)\n",
        "        out.append(labels.cpu().numpy())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        pred.append(predicted.cpu().numpy())\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total))\n",
        "print(total,correct)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 41 %\n",
            "10000 4122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTovyT879m7F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "7100b7cf-7965-4270-8b0d-346ec260cf88"
      },
      "source": [
        "plt.plot(loss_epoch)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('CE loss')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'CE loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPJM-foIClcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}