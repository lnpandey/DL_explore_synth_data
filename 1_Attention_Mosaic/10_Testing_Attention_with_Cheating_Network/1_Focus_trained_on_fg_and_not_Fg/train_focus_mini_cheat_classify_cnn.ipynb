{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_focus_mini_cheat_classify_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e83986106a774c61bb09e10dccf56a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08026fd3ff9a4482be0cbda362e3dd18",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3e62e2ad5bea42c3afa9d66adbe90874",
              "IPY_MODEL_955fc7b878454c0d9f65a4f5eea0fbb3"
            ]
          }
        },
        "08026fd3ff9a4482be0cbda362e3dd18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e62e2ad5bea42c3afa9d66adbe90874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e81875753364266a24034b2b974b459",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5fd5663ff834e229c0ce72e268787dd"
          }
        },
        "955fc7b878454c0d9f65a4f5eea0fbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_852a0d4840cc4ce0a453e5a516b94c4f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:06&lt;00:00, 27598828.87it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f43962613f63492cb6f1f88e3e3532f1"
          }
        },
        "9e81875753364266a24034b2b974b459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5fd5663ff834e229c0ce72e268787dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "852a0d4840cc4ce0a453e5a516b94c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f43962613f63492cb6f1f88e3e3532f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjG64ra4aFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "06ea8d98-1d49-4616-d5e7-2a86e5d8a984"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8-7SARDZErK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acRFqJNrZErV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "e83986106a774c61bb09e10dccf56a19",
            "08026fd3ff9a4482be0cbda362e3dd18",
            "3e62e2ad5bea42c3afa9d66adbe90874",
            "955fc7b878454c0d9f65a4f5eea0fbb3",
            "9e81875753364266a24034b2b974b459",
            "a5fd5663ff834e229c0ce72e268787dd",
            "852a0d4840cc4ce0a453e5a516b94c4f",
            "f43962613f63492cb6f1f88e3e3532f1"
          ]
        },
        "outputId": "4f2a8825-46d3-44e1-ba49-5037ca0e62df"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e83986106a774c61bb09e10dccf56a19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh5DXuAV1tp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "fg1,fg2,fg3 = 0,1,2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V_JUhwCeZErk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79ed9db3-214d-445b-ab0a-1a470bf4ac10"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW9MkktGysAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]- fg1  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWxkp87fNwnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desired_num = 30000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJuGak6_zXgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label, fore_idx):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    self.fore_idx = fore_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx], self.fore_idx[idx]\n",
        "\n",
        "batch = 125\n",
        "msd = MosaicDataset(mosaic_list_of_images, mosaic_label , fore_idx)\n",
        "train_loader = DataLoader( msd,batch_size= batch ,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLRfCVv5JceE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAaEnEZiJgdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1       \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "        #self.conv1 = nn.Conv2d(3,self.f0,1)\n",
        "        #self.conv3 = nn.Conv2d(3,self.f1,3,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)        \n",
        "        x = torch.cat((x1,x3),dim=1)   \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbWIRolVJstC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,2,3,pad=0)\n",
        "        self.pool = nn.MaxPool2d(3,stride=2,padding=0)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x,x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaEn6DZrJ2q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,96,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(96,32,32)\n",
        "        self.incept2 = inception_module(64,32,48)\n",
        "        \n",
        "        self.downsample1 = downsample_module(80,80)\n",
        "        \n",
        "        self.incept3 = inception_module(160,112,48)\n",
        "        self.incept4 = inception_module(160,96,64)\n",
        "        self.incept5 = inception_module(160,80,80)\n",
        "        self.incept6 = inception_module(160,48,96)\n",
        "        \n",
        "        self.downsample2 = downsample_module(144,96)\n",
        "        \n",
        "        self.incept7 = inception_module(240,176,60)\n",
        "        self.incept8 = inception_module(236,176,60)\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(5)\n",
        "        self.linear1 = nn.Linear(236,10)\n",
        "        self.linear2 = nn.Linear(10,1)\n",
        "\n",
        "    def forward(self,z):  #y is avg image #z batch of list of 9 images\n",
        "        y = torch.zeros([batch,3, 32,32], dtype=torch.float64)\n",
        "        a = torch.zeros([batch,9],dtype=torch.float64)\n",
        "        y = y.to(\"cuda\")\n",
        "        a = a.to(\"cuda\")\n",
        "        \n",
        "        for i in range(9):\n",
        "            a[:,i] = self.helper(z[:,i])[:,0]\n",
        "\n",
        "        a = F.softmax(a,dim=1)\n",
        "\n",
        "        x1 = a[:,0]\n",
        "        torch.mul(x1[:,None,None,None],z[:,0])\n",
        "\n",
        "        for i in range(9):            \n",
        "          x1 = a[:,i]          \n",
        "          y = y + torch.mul(x1[:,None,None,None],z[:,i])\n",
        "\n",
        "        return a, y\n",
        "\n",
        "    def helper(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        #act1 = x\n",
        "        \n",
        "        x = self.incept1.forward(x)\n",
        "        #act2 = x\n",
        "        \n",
        "        x = self.incept2.forward(x)\n",
        "        #act3 = x\n",
        "        \n",
        "        x,act4 = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        #act5 = x\n",
        "        \n",
        "        x = self.incept4.forward(x)\n",
        "        #act6 = x\n",
        "        \n",
        "        x = self.incept5.forward(x)\n",
        "        #act7 = x\n",
        "        \n",
        "        x = self.incept6.forward(x)\n",
        "        #act8 = x\n",
        "        \n",
        "        x,act9 = self.downsample2.forward(x)\n",
        "        \n",
        "        x = self.incept7.forward(x)\n",
        "        #act10 = x\n",
        "        x = self.incept8.forward(x)\n",
        "        #act11 = x\n",
        "        #print(x.shape)\n",
        "        x = self.pool(x)\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1,1*1*236)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x) \n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvXR1zV5n4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "focus_net = Focus().double()\n",
        "focus_net = focus_net.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYdCXceZzSk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Classify_net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classify_net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 12, 5)\n",
        "    self.conv3 = nn.Conv2d(12,20,5)\n",
        "    self.fc1 = nn.Linear(20 * 6 * 6, 120)\n",
        "    self.fc2 = nn.Linear(120, 80)\n",
        "    self.fc3 = nn.Linear(80, 40)\n",
        "    self.fc4 = nn.Linear(40,10)\n",
        "    self.fc5 = nn.Linear(10,3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = (F.relu(self.conv2(x)))\n",
        "    x = (F.relu(self.conv3(x)))\n",
        "    x = x.view(-1, 20 * 6 * 6)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = self.fc5(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYplUGazU9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classify = Classify_net().double()\n",
        "classify = classify.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZob1uGT6fTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9be21909-84fb-4efb-b316-bbd2aec1b321"
      },
      "source": [
        "classify.load_state_dict( torch.load(\"/content/drive/My Drive/Research/Cheating_data/Classify_net_weights/classify_net\"+\"cnn\"+\".pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uE2ecgApdwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for params in classify.parameters():\n",
        "  params.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0rkwoqLpya8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "712ac328-7974-4fc6-9f84-91a43c5198b6"
      },
      "source": [
        "for params in classify.parameters():\n",
        "  print(params)\n",
        "  break;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[[-1.6423e-01,  1.7392e-01,  4.9897e-01,  6.2711e-01,  1.2254e-01],\n",
            "          [ 2.4849e-02,  2.6886e-02,  2.0595e-01,  1.3431e-01, -3.6509e-01],\n",
            "          [-2.9908e-01, -3.2545e-01, -2.9668e-01, -4.8356e-01, -4.4245e-01],\n",
            "          [-2.4543e-01, -1.0781e-01, -1.7590e-02, -4.7907e-02,  1.1002e-02],\n",
            "          [ 3.0143e-02,  2.1096e-01,  2.2314e-01,  9.1594e-02,  3.0036e-01]],\n",
            "\n",
            "         [[-5.8768e-02,  1.3975e-01,  4.1226e-01,  2.7912e-01, -2.2264e-01],\n",
            "          [ 6.7200e-02,  6.1176e-02,  1.6692e-01, -1.2179e-01, -4.3682e-01],\n",
            "          [-8.4766e-02, -3.5242e-01, -4.1281e-01, -4.5495e-01, -6.1057e-01],\n",
            "          [-1.2020e-01, -4.3536e-02,  9.0031e-02,  1.8540e-02,  1.1777e-01],\n",
            "          [ 2.2654e-01,  4.0664e-01,  4.5058e-01,  2.0680e-01,  1.6883e-01]],\n",
            "\n",
            "         [[ 2.7177e-02,  1.1304e-01,  2.0884e-01,  3.8100e-01, -4.7785e-02],\n",
            "          [ 2.1222e-01,  2.1781e-01,  1.4267e-01,  1.3736e-01, -2.5502e-01],\n",
            "          [-2.2782e-01, -3.3184e-01, -3.4972e-01, -2.0986e-01, -3.0194e-01],\n",
            "          [-3.0035e-01, -2.5346e-01, -7.7255e-02, -1.4948e-01,  6.1638e-02],\n",
            "          [ 9.3163e-02,  6.1395e-02,  2.0359e-01,  2.1181e-01,  2.5278e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 6.5681e-02, -1.5056e-01,  9.0953e-03,  2.0223e-02,  2.4469e-01],\n",
            "          [-1.3985e-01, -2.4540e-01,  1.6952e-02,  5.2645e-02, -7.8292e-02],\n",
            "          [-1.1181e-01, -5.7720e-02,  7.4450e-02, -1.9589e-01, -2.4692e-01],\n",
            "          [-6.2860e-03, -5.2753e-02,  2.0188e-01, -2.9059e-01, -8.0349e-02],\n",
            "          [ 6.8332e-02,  8.8080e-02,  4.6145e-02, -1.7064e-01, -1.2418e-02]],\n",
            "\n",
            "         [[-2.2013e-02, -2.8078e-01, -1.1229e-02,  4.4819e-02,  4.4218e-02],\n",
            "          [ 4.0202e-02, -1.6499e-01,  3.6491e-02, -5.4469e-02, -1.2856e-03],\n",
            "          [-1.2152e-01, -9.5895e-02,  1.1583e-01, -2.0966e-03, -2.7728e-01],\n",
            "          [ 8.0812e-02,  2.6461e-02,  1.9787e-01, -1.0818e-01, -3.6917e-02],\n",
            "          [ 1.3628e-01,  1.5299e-01,  9.3982e-02, -1.6524e-01, -2.2284e-01]],\n",
            "\n",
            "         [[ 1.7658e-01,  9.2664e-03, -7.1308e-02,  1.6181e-01,  2.4907e-01],\n",
            "          [ 2.0972e-02,  9.6402e-02,  1.6920e-01,  8.8409e-02,  1.7378e-01],\n",
            "          [ 8.4983e-02,  1.1137e-01,  2.7581e-01,  8.0090e-02,  5.2451e-02],\n",
            "          [ 1.4949e-01,  2.0285e-01,  3.1227e-01, -4.4864e-02,  2.4089e-02],\n",
            "          [ 1.9170e-02,  1.5360e-01,  1.3319e-01, -2.4949e-01,  9.5405e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4423e-02,  1.6213e-01,  5.6353e-02, -8.1757e-02, -1.5494e-01],\n",
            "          [-1.9736e-01,  2.3330e-01,  1.0901e-01,  8.5555e-02, -1.3112e-01],\n",
            "          [-1.5935e-01, -1.2445e-01,  3.2754e-01,  1.7633e-01, -5.0295e-02],\n",
            "          [ 6.3163e-02, -6.2180e-01, -1.5769e-01,  2.6880e-01,  3.9964e-01],\n",
            "          [ 4.9961e-01, -7.3943e-02, -3.6360e-01, -2.0237e-01, -4.0563e-02]],\n",
            "\n",
            "         [[-1.2804e-01,  2.5620e-01,  4.1600e-04,  1.4213e-02, -7.5500e-02],\n",
            "          [-1.6325e-01,  8.8658e-02,  1.9674e-01,  1.6510e-01,  1.8134e-01],\n",
            "          [-3.7243e-01, -3.2098e-01,  4.1795e-01,  2.7611e-01,  1.8044e-01],\n",
            "          [-1.4689e-01, -7.2630e-01, -9.8340e-02,  3.3257e-01,  3.9563e-01],\n",
            "          [ 2.2614e-01, -2.8384e-01, -4.7233e-01, -1.0463e-01,  8.6083e-02]],\n",
            "\n",
            "         [[ 4.1378e-02,  1.2955e-01, -1.0026e-01, -7.1011e-02, -7.8706e-02],\n",
            "          [-1.5284e-01,  1.2037e-01,  3.1181e-01,  1.9723e-02, -2.4620e-01],\n",
            "          [-6.1976e-02, -2.0366e-01,  4.0043e-01,  2.6546e-01, -1.1003e-01],\n",
            "          [ 5.8510e-02, -5.8234e-01,  3.4838e-02,  3.7670e-01,  2.5495e-01],\n",
            "          [ 3.5556e-01, -1.7375e-01, -2.6572e-01, -2.0503e-01, -1.0463e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2520e-01,  4.3267e-02, -9.5696e-02, -1.2479e-01,  2.5971e-01],\n",
            "          [ 1.6034e-01,  2.0203e-01, -2.3667e-01, -1.0040e-01,  7.7242e-02],\n",
            "          [ 1.6602e-01,  2.3048e-01, -1.3979e-01, -3.2015e-02,  2.4991e-02],\n",
            "          [ 3.8007e-02, -7.6374e-02, -1.6255e-01, -6.2367e-02,  1.7916e-01],\n",
            "          [ 8.7964e-02,  7.8229e-02,  3.4996e-02,  2.4115e-02, -4.3097e-02]],\n",
            "\n",
            "         [[ 1.8631e-01, -1.5977e-01, -3.8426e-01, -2.7585e-01,  5.4369e-02],\n",
            "          [ 3.4873e-01,  1.0126e-01, -3.2835e-01, -2.3376e-01, -2.8728e-02],\n",
            "          [ 3.5719e-01,  7.2389e-02, -5.0347e-02, -1.5016e-01, -1.5081e-01],\n",
            "          [ 1.0276e-01, -1.5496e-01, -6.2544e-02,  1.2827e-01,  2.1669e-03],\n",
            "          [ 6.5527e-02,  4.4503e-02, -5.9577e-02,  7.7762e-03,  2.0233e-01]],\n",
            "\n",
            "         [[ 4.2626e-01,  3.2905e-02, -4.1481e-01, -2.6850e-01,  8.0124e-02],\n",
            "          [ 4.7984e-01,  1.5668e-01, -3.6327e-01, -3.6918e-01, -1.8513e-01],\n",
            "          [ 4.2830e-01,  2.4397e-01, -2.7261e-01, -3.2915e-01, -3.8945e-01],\n",
            "          [ 1.5234e-01,  1.1090e-01, -5.3101e-02, -4.9351e-02, -2.0419e-01],\n",
            "          [ 1.9267e-01, -6.5243e-03,  1.8175e-02, -1.9445e-01, -1.9047e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0933e-01,  1.9894e-01,  2.0863e-01,  1.4587e-01,  1.4252e-01],\n",
            "          [ 1.3082e-01,  1.8711e-01,  3.0731e-01,  2.3098e-01,  3.2525e-01],\n",
            "          [-5.6671e-02, -1.8695e-02,  2.9559e-01,  3.8488e-01,  2.6518e-01],\n",
            "          [-2.5806e-01, -1.0353e-01,  2.3439e-01,  2.7585e-01,  2.7159e-02],\n",
            "          [-2.2996e-01, -3.1788e-01,  6.6445e-02,  1.0409e-02, -3.1845e-02]],\n",
            "\n",
            "         [[ 5.7803e-02,  7.8317e-02,  5.5037e-02,  7.0606e-02, -8.8129e-02],\n",
            "          [-1.2100e-01, -1.1819e-01, -1.7574e-01, -2.4201e-02, -6.2360e-03],\n",
            "          [-1.1220e-01, -6.8520e-02, -1.1957e-01, -8.2811e-02, -1.8721e-01],\n",
            "          [ 1.6525e-02, -5.3492e-02, -1.3703e-01, -1.6919e-01, -9.6235e-02],\n",
            "          [ 1.4995e-02, -8.2921e-02, -1.7411e-04,  3.9596e-02, -1.7593e-02]],\n",
            "\n",
            "         [[ 1.0244e-01, -4.9267e-02, -1.1532e-01, -9.2471e-02,  1.1093e-02],\n",
            "          [-4.4159e-02, -1.0172e-01, -2.7992e-01, -2.8586e-01, -2.1208e-01],\n",
            "          [ 2.3867e-02, -7.5483e-02, -1.8529e-01, -3.7971e-01, -3.0519e-01],\n",
            "          [ 2.8825e-01, -4.9825e-02, -6.1514e-02, -7.4702e-02, -6.9733e-02],\n",
            "          [ 3.2700e-01,  2.7033e-01,  1.0413e-01, -8.5814e-03,  6.5381e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 9.0600e-02,  8.7857e-02,  2.2667e-02, -1.0188e-01, -1.5937e-01],\n",
            "          [ 1.0145e-01, -7.6480e-02,  2.7598e-02, -1.7486e-01,  9.3050e-02],\n",
            "          [ 1.4969e-01,  1.2853e-02, -1.3022e-01, -6.7699e-03,  1.0338e-01],\n",
            "          [-1.0866e-01, -2.4374e-01, -2.2317e-01,  1.6494e-01,  3.3576e-01],\n",
            "          [-1.4334e-01,  1.2542e-02,  1.4377e-01,  2.9420e-01, -1.3865e-02]],\n",
            "\n",
            "         [[ 2.7987e-02,  9.3128e-02,  1.8035e-01,  9.7535e-02,  1.6398e-01],\n",
            "          [ 1.2930e-01, -1.1232e-02,  7.3424e-03, -1.7674e-01,  4.4670e-02],\n",
            "          [ 1.1415e-01,  7.6541e-02, -1.4470e-01, -2.9108e-02,  3.1982e-01],\n",
            "          [ 8.4122e-02, -2.4692e-01, -1.5448e-01,  1.9075e-01,  3.2732e-01],\n",
            "          [ 9.0571e-02, -3.4855e-02,  2.5406e-01,  3.3015e-01,  1.4373e-01]],\n",
            "\n",
            "         [[-4.5869e-02, -1.5669e-01, -1.9491e-01, -2.1801e-01, -8.4690e-02],\n",
            "          [-6.1831e-02, -1.2969e-01, -1.4524e-01, -1.4771e-01,  3.0580e-01],\n",
            "          [-2.0756e-01, -3.4834e-01, -2.0460e-01, -2.0300e-01,  2.8467e-01],\n",
            "          [-1.7694e-01, -5.1840e-01, -4.2233e-01,  1.6855e-01,  4.4407e-01],\n",
            "          [-2.8926e-01, -3.0749e-01, -3.3492e-02,  3.2682e-01,  3.3132e-01]]]],\n",
            "       device='cuda:0', dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l789TLMP9zJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images =[]        #list of mosaic images, each mosaic image is saved as laist of 9 images\n",
        "fore_idx_test =[]                   #list of indexes at which foreground image is present in a mosaic image                \n",
        "test_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(10000):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx_test.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  test_images.append(image_list)\n",
        "  test_label.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBzV9dKS5po7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = MosaicDataset(test_images,test_label,fore_idx_test)\n",
        "test_loader = DataLoader( test_data,batch_size= batch ,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5g3geNJ5zEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion_focus = nn.CrossEntropyLoss()\n",
        "optimizer_focus = optim.SGD(focus_net.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFfAJZkcZEsY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b6cdfa5-e606-4ee5-f090-46c19d31468f"
      },
      "source": [
        "nos_epochs = 1000\n",
        "focus_true_pred_true =0\n",
        "focus_false_pred_true =0\n",
        "focus_true_pred_false =0\n",
        "focus_false_pred_false =0\n",
        "\n",
        "argmax_more_than_half = 0\n",
        "argmax_less_than_half =0\n",
        "\n",
        "col1=[]\n",
        "col2=[]\n",
        "col3=[]\n",
        "col4=[]\n",
        "col5=[]\n",
        "col6=[]\n",
        "col7=[]\n",
        "col8=[]\n",
        "col9=[]\n",
        "col10=[]\n",
        "col11=[]\n",
        "col12=[]\n",
        "col13=[]\n",
        "\n",
        "for epoch in range(nos_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "  focus_true_pred_true =0\n",
        "  focus_false_pred_true =0\n",
        "  focus_true_pred_false =0\n",
        "  focus_false_pred_false =0\n",
        "  \n",
        "  argmax_more_than_half = 0\n",
        "  argmax_less_than_half =0\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  epoch_loss = []\n",
        "  cnt=0\n",
        "\n",
        "  iteration = desired_num // batch\n",
        "  \n",
        "  #training data set\n",
        "  \n",
        "  for i, data in  enumerate(train_loader):\n",
        "    inputs , labels , fore_idx = data\n",
        "    inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    # zero the parameter gradients\n",
        "    \n",
        "    optimizer_focus.zero_grad()\n",
        "    \n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "#     print(outputs)\n",
        "#     print(outputs.shape,labels.shape , torch.argmax(outputs, dim=1))\n",
        "\n",
        "    loss = criterion_focus(outputs, labels) \n",
        "    loss.backward()\n",
        "    optimizer_focus.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    mini = 60\n",
        "    if cnt % mini == mini-1:    # print every 40 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %(epoch + 1, cnt + 1, running_loss / mini))\n",
        "      epoch_loss.append(running_loss/mini)\n",
        "      running_loss = 0.0\n",
        "    cnt=cnt+1\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "      for j in range (batch):\n",
        "        focus = torch.argmax(alphas[j])\n",
        "\n",
        "        if(alphas[j][focus] >= 0.5):\n",
        "          argmax_more_than_half +=1\n",
        "        else:\n",
        "          argmax_less_than_half +=1\n",
        "\n",
        "        if(focus == fore_idx[j] and predicted[j] == labels[j]):\n",
        "          focus_true_pred_true += 1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] == labels[j]):\n",
        "          focus_false_pred_true +=1\n",
        "\n",
        "        elif(focus == fore_idx[j] and predicted[j] != labels[j]):\n",
        "          focus_true_pred_false +=1\n",
        "\n",
        "        elif(focus != fore_idx[j] and predicted[j] != labels[j]):\n",
        "          focus_false_pred_false +=1\n",
        "\n",
        "  if(np.mean(epoch_loss) <= 0.03):\n",
        "      break;\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    col1.append(epoch)\n",
        "    col2.append(argmax_more_than_half)\n",
        "    col3.append(argmax_less_than_half)\n",
        "    col4.append(focus_true_pred_true)\n",
        "    col5.append(focus_false_pred_true)\n",
        "    col6.append(focus_true_pred_false)\n",
        "    col7.append(focus_false_pred_false)\n",
        "  \n",
        "    #************************************************************************\n",
        "    #testing data set  \n",
        "    with torch.no_grad():\n",
        "      focus_true_pred_true =0\n",
        "      focus_false_pred_true =0\n",
        "      focus_true_pred_false =0\n",
        "      focus_false_pred_false =0\n",
        "\n",
        "      argmax_more_than_half = 0\n",
        "      argmax_less_than_half =0\n",
        "      for data in test_loader:\n",
        "        inputs, labels , fore_idx = data\n",
        "        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "        alphas, avg_images = focus_net(inputs)\n",
        "        outputs = classify(avg_images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        for j in range (batch):\n",
        "          focus = torch.argmax(alphas[j])\n",
        "\n",
        "          if(alphas[j][focus] >= 0.5):\n",
        "            argmax_more_than_half +=1\n",
        "          else:\n",
        "            argmax_less_than_half +=1\n",
        "\n",
        "          if(focus == fore_idx[j] and predicted[j] == labels[j]):\n",
        "            focus_true_pred_true += 1\n",
        "\n",
        "          elif(focus != fore_idx[j] and predicted[j] == labels[j]):\n",
        "            focus_false_pred_true +=1\n",
        "\n",
        "          elif(focus == fore_idx[j] and predicted[j] != labels[j]):\n",
        "            focus_true_pred_false +=1\n",
        "\n",
        "          elif(focus != fore_idx[j] and predicted[j] != labels[j]):\n",
        "            focus_false_pred_false +=1\n",
        "      \n",
        "    col8.append(argmax_more_than_half)\n",
        "    col9.append(argmax_less_than_half)\n",
        "    col10.append(focus_true_pred_true)\n",
        "    col11.append(focus_false_pred_true)\n",
        "    col12.append(focus_true_pred_false)\n",
        "    col13.append(focus_false_pred_false)\n",
        "    # torch.save(fore_net.state_dict(),\"/content/drive/My Drive/Research/CIFAR Mosaic/weights/model_epoch\"+str(epoch)+\".pt\")\n",
        "    \n",
        "print('Finished Training')\n",
        "# torch.save(fore_net.state_dict(),\"/content/drive/My Drive/Research/CIFAR Mosaic/weights/model_epoch\"+str(nos_epochs)+\".pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,    60] loss: 7.710\n",
            "[1,   120] loss: 7.790\n",
            "[1,   180] loss: 6.556\n",
            "[1,   240] loss: 6.651\n",
            "[2,    60] loss: 7.195\n",
            "[2,   120] loss: 7.249\n",
            "[2,   180] loss: 7.144\n",
            "[2,   240] loss: 7.343\n",
            "[3,    60] loss: 7.363\n",
            "[3,   120] loss: 7.259\n",
            "[3,   180] loss: 7.287\n",
            "[3,   240] loss: 7.187\n",
            "[4,    60] loss: 7.262\n",
            "[4,   120] loss: 7.171\n",
            "[4,   180] loss: 7.382\n",
            "[4,   240] loss: 7.283\n",
            "[5,    60] loss: 7.282\n",
            "[5,   120] loss: 7.200\n",
            "[5,   180] loss: 7.286\n",
            "[5,   240] loss: 7.328\n",
            "[6,    60] loss: 7.169\n",
            "[6,   120] loss: 7.307\n",
            "[6,   180] loss: 7.322\n",
            "[6,   240] loss: 7.298\n",
            "[7,    60] loss: 7.272\n",
            "[7,   120] loss: 7.197\n",
            "[7,   180] loss: 7.316\n",
            "[7,   240] loss: 7.311\n",
            "[8,    60] loss: 7.227\n",
            "[8,   120] loss: 7.195\n",
            "[8,   180] loss: 7.285\n",
            "[8,   240] loss: 7.390\n",
            "[9,    60] loss: 7.394\n",
            "[9,   120] loss: 7.220\n",
            "[9,   180] loss: 7.262\n",
            "[9,   240] loss: 7.221\n",
            "[10,    60] loss: 7.250\n",
            "[10,   120] loss: 7.332\n",
            "[10,   180] loss: 7.273\n",
            "[10,   240] loss: 7.241\n",
            "[11,    60] loss: 7.141\n",
            "[11,   120] loss: 7.328\n",
            "[11,   180] loss: 7.359\n",
            "[11,   240] loss: 7.269\n",
            "[12,    60] loss: 7.233\n",
            "[12,   120] loss: 7.339\n",
            "[12,   180] loss: 7.324\n",
            "[12,   240] loss: 7.201\n",
            "[13,    60] loss: 7.198\n",
            "[13,   120] loss: 7.114\n",
            "[13,   180] loss: 7.386\n",
            "[13,   240] loss: 7.399\n",
            "[14,    60] loss: 7.274\n",
            "[14,   120] loss: 7.188\n",
            "[14,   180] loss: 7.289\n",
            "[14,   240] loss: 7.345\n",
            "[15,    60] loss: 7.233\n",
            "[15,   120] loss: 7.259\n",
            "[15,   180] loss: 7.326\n",
            "[15,   240] loss: 7.278\n",
            "[16,    60] loss: 7.334\n",
            "[16,   120] loss: 7.264\n",
            "[16,   180] loss: 7.253\n",
            "[16,   240] loss: 7.245\n",
            "[17,    60] loss: 7.313\n",
            "[17,   120] loss: 7.177\n",
            "[17,   180] loss: 7.360\n",
            "[17,   240] loss: 7.246\n",
            "[18,    60] loss: 7.259\n",
            "[18,   120] loss: 7.315\n",
            "[18,   180] loss: 7.251\n",
            "[18,   240] loss: 7.270\n",
            "[19,    60] loss: 7.199\n",
            "[19,   120] loss: 7.327\n",
            "[19,   180] loss: 7.322\n",
            "[19,   240] loss: 7.249\n",
            "[20,    60] loss: 7.230\n",
            "[20,   120] loss: 7.332\n",
            "[20,   180] loss: 7.351\n",
            "[20,   240] loss: 7.183\n",
            "[21,    60] loss: 7.310\n",
            "[21,   120] loss: 7.253\n",
            "[21,   180] loss: 7.263\n",
            "[21,   240] loss: 7.271\n",
            "[22,    60] loss: 7.254\n",
            "[22,   120] loss: 7.294\n",
            "[22,   180] loss: 7.184\n",
            "[22,   240] loss: 7.364\n",
            "[23,    60] loss: 7.232\n",
            "[23,   120] loss: 7.359\n",
            "[23,   180] loss: 7.232\n",
            "[23,   240] loss: 7.273\n",
            "[24,    60] loss: 7.240\n",
            "[24,   120] loss: 7.399\n",
            "[24,   180] loss: 7.176\n",
            "[24,   240] loss: 7.282\n",
            "[25,    60] loss: 7.425\n",
            "[25,   120] loss: 7.168\n",
            "[25,   180] loss: 7.212\n",
            "[25,   240] loss: 7.292\n",
            "[26,    60] loss: 7.387\n",
            "[26,   120] loss: 7.309\n",
            "[26,   180] loss: 7.187\n",
            "[26,   240] loss: 7.213\n",
            "[27,    60] loss: 7.187\n",
            "[27,   120] loss: 7.380\n",
            "[27,   180] loss: 7.254\n",
            "[27,   240] loss: 7.275\n",
            "[28,    60] loss: 7.253\n",
            "[28,   120] loss: 7.300\n",
            "[28,   180] loss: 7.305\n",
            "[28,   240] loss: 7.238\n",
            "[29,    60] loss: 7.285\n",
            "[29,   120] loss: 7.289\n",
            "[29,   180] loss: 7.289\n",
            "[29,   240] loss: 7.233\n",
            "[30,    60] loss: 7.417\n",
            "[30,   120] loss: 7.289\n",
            "[30,   180] loss: 7.216\n",
            "[30,   240] loss: 7.175\n",
            "[31,    60] loss: 7.260\n",
            "[31,   120] loss: 7.283\n",
            "[31,   180] loss: 7.185\n",
            "[31,   240] loss: 7.369\n",
            "[32,    60] loss: 7.305\n",
            "[32,   120] loss: 7.236\n",
            "[32,   180] loss: 7.208\n",
            "[32,   240] loss: 7.349\n",
            "[33,    60] loss: 7.255\n",
            "[33,   120] loss: 7.136\n",
            "[33,   180] loss: 7.470\n",
            "[33,   240] loss: 7.236\n",
            "[34,    60] loss: 7.337\n",
            "[34,   120] loss: 7.376\n",
            "[34,   180] loss: 7.197\n",
            "[34,   240] loss: 7.187\n",
            "[35,    60] loss: 7.401\n",
            "[35,   120] loss: 7.318\n",
            "[35,   180] loss: 7.139\n",
            "[35,   240] loss: 7.240\n",
            "[36,    60] loss: 7.158\n",
            "[36,   120] loss: 7.258\n",
            "[36,   180] loss: 7.415\n",
            "[36,   240] loss: 7.267\n",
            "[37,    60] loss: 7.263\n",
            "[37,   120] loss: 7.249\n",
            "[37,   180] loss: 7.190\n",
            "[37,   240] loss: 7.395\n",
            "[38,    60] loss: 7.256\n",
            "[38,   120] loss: 7.234\n",
            "[38,   180] loss: 7.294\n",
            "[38,   240] loss: 7.313\n",
            "[39,    60] loss: 7.298\n",
            "[39,   120] loss: 7.267\n",
            "[39,   180] loss: 7.288\n",
            "[39,   240] loss: 7.244\n",
            "[40,    60] loss: 7.364\n",
            "[40,   120] loss: 7.289\n",
            "[40,   180] loss: 7.268\n",
            "[40,   240] loss: 7.177\n",
            "[41,    60] loss: 7.393\n",
            "[41,   120] loss: 7.141\n",
            "[41,   180] loss: 7.374\n",
            "[41,   240] loss: 7.189\n",
            "[42,    60] loss: 7.251\n",
            "[42,   120] loss: 7.259\n",
            "[42,   180] loss: 7.248\n",
            "[42,   240] loss: 7.339\n",
            "[43,    60] loss: 7.316\n",
            "[43,   120] loss: 7.422\n",
            "[43,   180] loss: 7.178\n",
            "[43,   240] loss: 7.180\n",
            "[44,    60] loss: 7.252\n",
            "[44,   120] loss: 7.293\n",
            "[44,   180] loss: 7.252\n",
            "[44,   240] loss: 7.300\n",
            "[45,    60] loss: 7.282\n",
            "[45,   120] loss: 7.229\n",
            "[45,   180] loss: 7.363\n",
            "[45,   240] loss: 7.222\n",
            "[46,    60] loss: 7.259\n",
            "[46,   120] loss: 7.266\n",
            "[46,   180] loss: 7.201\n",
            "[46,   240] loss: 7.370\n",
            "[47,    60] loss: 7.226\n",
            "[47,   120] loss: 7.227\n",
            "[47,   180] loss: 7.231\n",
            "[47,   240] loss: 7.413\n",
            "[48,    60] loss: 7.320\n",
            "[48,   120] loss: 7.324\n",
            "[48,   180] loss: 7.321\n",
            "[48,   240] loss: 7.131\n",
            "[49,    60] loss: 7.313\n",
            "[49,   120] loss: 7.164\n",
            "[49,   180] loss: 7.246\n",
            "[49,   240] loss: 7.374\n",
            "[50,    60] loss: 7.226\n",
            "[50,   120] loss: 7.361\n",
            "[50,   180] loss: 7.228\n",
            "[50,   240] loss: 7.281\n",
            "[51,    60] loss: 7.298\n",
            "[51,   120] loss: 7.095\n",
            "[51,   180] loss: 7.306\n",
            "[51,   240] loss: 7.398\n",
            "[52,    60] loss: 7.275\n",
            "[52,   120] loss: 7.218\n",
            "[52,   180] loss: 7.340\n",
            "[52,   240] loss: 7.264\n",
            "[53,    60] loss: 7.178\n",
            "[53,   120] loss: 7.284\n",
            "[53,   180] loss: 7.390\n",
            "[53,   240] loss: 7.245\n",
            "[54,    60] loss: 7.364\n",
            "[54,   120] loss: 7.250\n",
            "[54,   180] loss: 7.298\n",
            "[54,   240] loss: 7.183\n",
            "[55,    60] loss: 7.223\n",
            "[55,   120] loss: 7.383\n",
            "[55,   180] loss: 7.235\n",
            "[55,   240] loss: 7.254\n",
            "[56,    60] loss: 7.394\n",
            "[56,   120] loss: 7.182\n",
            "[56,   180] loss: 7.231\n",
            "[56,   240] loss: 7.290\n",
            "[57,    60] loss: 7.241\n",
            "[57,   120] loss: 7.257\n",
            "[57,   180] loss: 7.333\n",
            "[57,   240] loss: 7.265\n",
            "[58,    60] loss: 7.245\n",
            "[58,   120] loss: 7.296\n",
            "[58,   180] loss: 7.319\n",
            "[58,   240] loss: 7.237\n",
            "[59,    60] loss: 7.301\n",
            "[59,   120] loss: 7.311\n",
            "[59,   180] loss: 7.144\n",
            "[59,   240] loss: 7.340\n",
            "[60,    60] loss: 7.205\n",
            "[60,   120] loss: 7.286\n",
            "[60,   180] loss: 7.247\n",
            "[60,   240] loss: 7.358\n",
            "[61,    60] loss: 7.212\n",
            "[61,   120] loss: 7.327\n",
            "[61,   180] loss: 7.272\n",
            "[61,   240] loss: 7.286\n",
            "[62,    60] loss: 7.349\n",
            "[62,   120] loss: 7.432\n",
            "[62,   180] loss: 7.134\n",
            "[62,   240] loss: 7.182\n",
            "[63,    60] loss: 7.393\n",
            "[63,   120] loss: 7.210\n",
            "[63,   180] loss: 7.383\n",
            "[63,   240] loss: 7.111\n",
            "[64,    60] loss: 7.264\n",
            "[64,   120] loss: 7.209\n",
            "[64,   180] loss: 7.257\n",
            "[64,   240] loss: 7.365\n",
            "[65,    60] loss: 7.210\n",
            "[65,   120] loss: 7.212\n",
            "[65,   180] loss: 7.397\n",
            "[65,   240] loss: 7.278\n",
            "[66,    60] loss: 7.226\n",
            "[66,   120] loss: 7.289\n",
            "[66,   180] loss: 7.327\n",
            "[66,   240] loss: 7.254\n",
            "[67,    60] loss: 7.427\n",
            "[67,   120] loss: 7.204\n",
            "[67,   180] loss: 7.305\n",
            "[67,   240] loss: 7.161\n",
            "[68,    60] loss: 7.307\n",
            "[68,   120] loss: 7.368\n",
            "[68,   180] loss: 7.203\n",
            "[68,   240] loss: 7.217\n",
            "[69,    60] loss: 7.223\n",
            "[69,   120] loss: 7.239\n",
            "[69,   180] loss: 7.409\n",
            "[69,   240] loss: 7.226\n",
            "[70,    60] loss: 7.362\n",
            "[70,   120] loss: 7.135\n",
            "[70,   180] loss: 7.268\n",
            "[70,   240] loss: 7.332\n",
            "[71,    60] loss: 7.272\n",
            "[71,   120] loss: 7.350\n",
            "[71,   180] loss: 7.195\n",
            "[71,   240] loss: 7.279\n",
            "[72,    60] loss: 7.268\n",
            "[72,   120] loss: 7.296\n",
            "[72,   180] loss: 7.175\n",
            "[72,   240] loss: 7.358\n",
            "[73,    60] loss: 7.285\n",
            "[73,   120] loss: 7.229\n",
            "[73,   180] loss: 7.308\n",
            "[73,   240] loss: 7.275\n",
            "[74,    60] loss: 7.343\n",
            "[74,   120] loss: 7.167\n",
            "[74,   180] loss: 7.199\n",
            "[74,   240] loss: 7.386\n",
            "[75,    60] loss: 7.343\n",
            "[75,   120] loss: 7.332\n",
            "[75,   180] loss: 7.229\n",
            "[75,   240] loss: 7.193\n",
            "[76,    60] loss: 7.354\n",
            "[76,   120] loss: 7.322\n",
            "[76,   180] loss: 7.300\n",
            "[76,   240] loss: 7.121\n",
            "[77,    60] loss: 7.295\n",
            "[77,   120] loss: 7.422\n",
            "[77,   180] loss: 7.223\n",
            "[77,   240] loss: 7.156\n",
            "[78,    60] loss: 7.369\n",
            "[78,   120] loss: 7.273\n",
            "[78,   180] loss: 7.186\n",
            "[78,   240] loss: 7.269\n",
            "[79,    60] loss: 7.221\n",
            "[79,   120] loss: 7.225\n",
            "[79,   180] loss: 7.291\n",
            "[79,   240] loss: 7.360\n",
            "[80,    60] loss: 7.373\n",
            "[80,   120] loss: 7.197\n",
            "[80,   180] loss: 7.251\n",
            "[80,   240] loss: 7.275\n",
            "[81,    60] loss: 7.261\n",
            "[81,   120] loss: 7.272\n",
            "[81,   180] loss: 7.366\n",
            "[81,   240] loss: 7.198\n",
            "[82,    60] loss: 7.216\n",
            "[82,   120] loss: 7.494\n",
            "[82,   180] loss: 7.198\n",
            "[82,   240] loss: 7.189\n",
            "[83,    60] loss: 7.354\n",
            "[83,   120] loss: 7.314\n",
            "[83,   180] loss: 7.227\n",
            "[83,   240] loss: 7.202\n",
            "[84,    60] loss: 7.099\n",
            "[84,   120] loss: 7.324\n",
            "[84,   180] loss: 7.233\n",
            "[84,   240] loss: 7.441\n",
            "[85,    60] loss: 7.353\n",
            "[85,   120] loss: 7.269\n",
            "[85,   180] loss: 7.262\n",
            "[85,   240] loss: 7.212\n",
            "[86,    60] loss: 7.314\n",
            "[86,   120] loss: 7.212\n",
            "[86,   180] loss: 7.348\n",
            "[86,   240] loss: 7.222\n",
            "[87,    60] loss: 7.229\n",
            "[87,   120] loss: 7.313\n",
            "[87,   180] loss: 7.190\n",
            "[87,   240] loss: 7.364\n",
            "[88,    60] loss: 7.001\n",
            "[88,   120] loss: 7.537\n",
            "[88,   180] loss: 7.394\n",
            "[88,   240] loss: 7.164\n",
            "[89,    60] loss: 7.235\n",
            "[89,   120] loss: 7.360\n",
            "[89,   180] loss: 7.191\n",
            "[89,   240] loss: 7.310\n",
            "[90,    60] loss: 7.089\n",
            "[90,   120] loss: 7.328\n",
            "[90,   180] loss: 7.369\n",
            "[90,   240] loss: 7.311\n",
            "[91,    60] loss: 7.323\n",
            "[91,   120] loss: 7.399\n",
            "[91,   180] loss: 7.154\n",
            "[91,   240] loss: 7.220\n",
            "[92,    60] loss: 7.157\n",
            "[92,   120] loss: 7.332\n",
            "[92,   180] loss: 7.294\n",
            "[92,   240] loss: 7.313\n",
            "[93,    60] loss: 7.297\n",
            "[93,   120] loss: 7.295\n",
            "[93,   180] loss: 7.251\n",
            "[93,   240] loss: 7.253\n",
            "[94,    60] loss: 7.299\n",
            "[94,   120] loss: 7.176\n",
            "[94,   180] loss: 7.344\n",
            "[94,   240] loss: 7.278\n",
            "[95,    60] loss: 7.232\n",
            "[95,   120] loss: 7.303\n",
            "[95,   180] loss: 7.256\n",
            "[95,   240] loss: 7.306\n",
            "[96,    60] loss: 7.290\n",
            "[96,   120] loss: 7.311\n",
            "[96,   180] loss: 7.257\n",
            "[96,   240] loss: 7.238\n",
            "[97,    60] loss: 7.341\n",
            "[97,   120] loss: 7.241\n",
            "[97,   180] loss: 7.262\n",
            "[97,   240] loss: 7.252\n",
            "[98,    60] loss: 7.236\n",
            "[98,   120] loss: 7.294\n",
            "[98,   180] loss: 7.262\n",
            "[98,   240] loss: 7.305\n",
            "[99,    60] loss: 7.261\n",
            "[99,   120] loss: 7.240\n",
            "[99,   180] loss: 7.355\n",
            "[99,   240] loss: 7.241\n",
            "[100,    60] loss: 7.265\n",
            "[100,   120] loss: 7.325\n",
            "[100,   180] loss: 7.234\n",
            "[100,   240] loss: 7.272\n",
            "[101,    60] loss: 7.144\n",
            "[101,   120] loss: 7.435\n",
            "[101,   180] loss: 7.365\n",
            "[101,   240] loss: 7.152\n",
            "[102,    60] loss: 7.294\n",
            "[102,   120] loss: 7.252\n",
            "[102,   180] loss: 7.276\n",
            "[102,   240] loss: 7.274\n",
            "[103,    60] loss: 7.246\n",
            "[103,   120] loss: 7.312\n",
            "[103,   180] loss: 7.310\n",
            "[103,   240] loss: 7.228\n",
            "[104,    60] loss: 7.302\n",
            "[104,   120] loss: 7.347\n",
            "[104,   180] loss: 7.222\n",
            "[104,   240] loss: 7.226\n",
            "[105,    60] loss: 7.262\n",
            "[105,   120] loss: 7.292\n",
            "[105,   180] loss: 7.269\n",
            "[105,   240] loss: 7.274\n",
            "[106,    60] loss: 7.255\n",
            "[106,   120] loss: 7.260\n",
            "[106,   180] loss: 7.393\n",
            "[106,   240] loss: 7.188\n",
            "[107,    60] loss: 7.216\n",
            "[107,   120] loss: 7.185\n",
            "[107,   180] loss: 7.431\n",
            "[107,   240] loss: 7.264\n",
            "[108,    60] loss: 7.209\n",
            "[108,   120] loss: 7.139\n",
            "[108,   180] loss: 7.386\n",
            "[108,   240] loss: 7.363\n",
            "[109,    60] loss: 7.266\n",
            "[109,   120] loss: 7.402\n",
            "[109,   180] loss: 7.063\n",
            "[109,   240] loss: 7.366\n",
            "[110,    60] loss: 7.308\n",
            "[110,   120] loss: 7.204\n",
            "[110,   180] loss: 7.301\n",
            "[110,   240] loss: 7.283\n",
            "[111,    60] loss: 7.320\n",
            "[111,   120] loss: 7.209\n",
            "[111,   180] loss: 7.299\n",
            "[111,   240] loss: 7.270\n",
            "[112,    60] loss: 7.225\n",
            "[112,   120] loss: 7.370\n",
            "[112,   180] loss: 7.214\n",
            "[112,   240] loss: 7.287\n",
            "[113,    60] loss: 7.138\n",
            "[113,   120] loss: 7.362\n",
            "[113,   180] loss: 7.386\n",
            "[113,   240] loss: 7.211\n",
            "[114,    60] loss: 7.223\n",
            "[114,   120] loss: 7.321\n",
            "[114,   180] loss: 7.230\n",
            "[114,   240] loss: 7.323\n",
            "[115,    60] loss: 7.404\n",
            "[115,   120] loss: 7.382\n",
            "[115,   180] loss: 7.212\n",
            "[115,   240] loss: 7.098\n",
            "[116,    60] loss: 7.240\n",
            "[116,   120] loss: 7.401\n",
            "[116,   180] loss: 7.168\n",
            "[116,   240] loss: 7.287\n",
            "[117,    60] loss: 7.352\n",
            "[117,   120] loss: 7.247\n",
            "[117,   180] loss: 7.341\n",
            "[117,   240] loss: 7.156\n",
            "[118,    60] loss: 7.146\n",
            "[118,   120] loss: 7.290\n",
            "[118,   180] loss: 7.330\n",
            "[118,   240] loss: 7.331\n",
            "[119,    60] loss: 7.298\n",
            "[119,   120] loss: 7.216\n",
            "[119,   180] loss: 7.244\n",
            "[119,   240] loss: 7.338\n",
            "[120,    60] loss: 7.203\n",
            "[120,   120] loss: 7.384\n",
            "[120,   180] loss: 7.243\n",
            "[120,   240] loss: 7.265\n",
            "[121,    60] loss: 7.384\n",
            "[121,   120] loss: 7.133\n",
            "[121,   180] loss: 7.167\n",
            "[121,   240] loss: 7.412\n",
            "[122,    60] loss: 7.325\n",
            "[122,   120] loss: 7.178\n",
            "[122,   180] loss: 7.178\n",
            "[122,   240] loss: 7.415\n",
            "[123,    60] loss: 7.250\n",
            "[123,   120] loss: 7.273\n",
            "[123,   180] loss: 7.406\n",
            "[123,   240] loss: 7.169\n",
            "[124,    60] loss: 7.303\n",
            "[124,   120] loss: 7.262\n",
            "[124,   180] loss: 7.268\n",
            "[124,   240] loss: 7.263\n",
            "[125,    60] loss: 7.242\n",
            "[125,   120] loss: 7.302\n",
            "[125,   180] loss: 7.097\n",
            "[125,   240] loss: 7.456\n",
            "[126,    60] loss: 7.225\n",
            "[126,   120] loss: 7.162\n",
            "[126,   180] loss: 7.328\n",
            "[126,   240] loss: 7.381\n",
            "[127,    60] loss: 7.228\n",
            "[127,   120] loss: 7.269\n",
            "[127,   180] loss: 7.254\n",
            "[127,   240] loss: 7.345\n",
            "[128,    60] loss: 7.250\n",
            "[128,   120] loss: 7.311\n",
            "[128,   180] loss: 7.338\n",
            "[128,   240] loss: 7.197\n",
            "[129,    60] loss: 7.361\n",
            "[129,   120] loss: 7.355\n",
            "[129,   180] loss: 7.081\n",
            "[129,   240] loss: 7.299\n",
            "[130,    60] loss: 7.361\n",
            "[130,   120] loss: 7.249\n",
            "[130,   180] loss: 7.254\n",
            "[130,   240] loss: 7.232\n",
            "[131,    60] loss: 7.175\n",
            "[131,   120] loss: 7.294\n",
            "[131,   180] loss: 7.377\n",
            "[131,   240] loss: 7.250\n",
            "[132,    60] loss: 7.276\n",
            "[132,   120] loss: 7.309\n",
            "[132,   180] loss: 7.259\n",
            "[132,   240] loss: 7.252\n",
            "[133,    60] loss: 7.258\n",
            "[133,   120] loss: 7.302\n",
            "[133,   180] loss: 7.207\n",
            "[133,   240] loss: 7.330\n",
            "[134,    60] loss: 7.373\n",
            "[134,   120] loss: 7.286\n",
            "[134,   180] loss: 7.269\n",
            "[134,   240] loss: 7.169\n",
            "[135,    60] loss: 7.196\n",
            "[135,   120] loss: 7.295\n",
            "[135,   180] loss: 7.205\n",
            "[135,   240] loss: 7.400\n",
            "[136,    60] loss: 7.360\n",
            "[136,   120] loss: 7.189\n",
            "[136,   180] loss: 7.212\n",
            "[136,   240] loss: 7.336\n",
            "[137,    60] loss: 7.208\n",
            "[137,   120] loss: 7.251\n",
            "[137,   180] loss: 7.324\n",
            "[137,   240] loss: 7.314\n",
            "[138,    60] loss: 7.409\n",
            "[138,   120] loss: 7.210\n",
            "[138,   180] loss: 7.323\n",
            "[138,   240] loss: 7.154\n",
            "[139,    60] loss: 7.190\n",
            "[139,   120] loss: 7.232\n",
            "[139,   180] loss: 7.403\n",
            "[139,   240] loss: 7.272\n",
            "[140,    60] loss: 7.209\n",
            "[140,   120] loss: 7.281\n",
            "[140,   180] loss: 7.253\n",
            "[140,   240] loss: 7.353\n",
            "[141,    60] loss: 7.345\n",
            "[141,   120] loss: 7.134\n",
            "[141,   180] loss: 7.295\n",
            "[141,   240] loss: 7.322\n",
            "[142,    60] loss: 7.374\n",
            "[142,   120] loss: 7.258\n",
            "[142,   180] loss: 7.247\n",
            "[142,   240] loss: 7.218\n",
            "[143,    60] loss: 7.331\n",
            "[143,   120] loss: 7.205\n",
            "[143,   180] loss: 7.324\n",
            "[143,   240] loss: 7.236\n",
            "[144,    60] loss: 7.250\n",
            "[144,   120] loss: 7.140\n",
            "[144,   180] loss: 7.447\n",
            "[144,   240] loss: 7.260\n",
            "[145,    60] loss: 7.334\n",
            "[145,   120] loss: 7.324\n",
            "[145,   180] loss: 7.130\n",
            "[145,   240] loss: 7.309\n",
            "[146,    60] loss: 7.234\n",
            "[146,   120] loss: 7.281\n",
            "[146,   180] loss: 7.324\n",
            "[146,   240] loss: 7.257\n",
            "[147,    60] loss: 7.192\n",
            "[147,   120] loss: 7.299\n",
            "[147,   180] loss: 7.283\n",
            "[147,   240] loss: 7.322\n",
            "[148,    60] loss: 7.369\n",
            "[148,   120] loss: 7.372\n",
            "[148,   180] loss: 7.185\n",
            "[148,   240] loss: 7.171\n",
            "[149,    60] loss: 7.219\n",
            "[149,   120] loss: 7.308\n",
            "[149,   180] loss: 7.305\n",
            "[149,   240] loss: 7.264\n",
            "[150,    60] loss: 7.423\n",
            "[150,   120] loss: 7.257\n",
            "[150,   180] loss: 7.242\n",
            "[150,   240] loss: 7.175\n",
            "[151,    60] loss: 7.270\n",
            "[151,   120] loss: 7.300\n",
            "[151,   180] loss: 7.235\n",
            "[151,   240] loss: 7.291\n",
            "[152,    60] loss: 7.221\n",
            "[152,   120] loss: 7.282\n",
            "[152,   180] loss: 7.276\n",
            "[152,   240] loss: 7.317\n",
            "[153,    60] loss: 7.174\n",
            "[153,   120] loss: 7.295\n",
            "[153,   180] loss: 7.438\n",
            "[153,   240] loss: 7.189\n",
            "[154,    60] loss: 7.287\n",
            "[154,   120] loss: 7.263\n",
            "[154,   180] loss: 7.277\n",
            "[154,   240] loss: 7.270\n",
            "[155,    60] loss: 7.192\n",
            "[155,   120] loss: 7.347\n",
            "[155,   180] loss: 7.275\n",
            "[155,   240] loss: 7.282\n",
            "[156,    60] loss: 7.149\n",
            "[156,   120] loss: 7.345\n",
            "[156,   180] loss: 7.244\n",
            "[156,   240] loss: 7.358\n",
            "[157,    60] loss: 7.160\n",
            "[157,   120] loss: 7.274\n",
            "[157,   180] loss: 7.349\n",
            "[157,   240] loss: 7.313\n",
            "[158,    60] loss: 7.224\n",
            "[158,   120] loss: 7.251\n",
            "[158,   180] loss: 7.287\n",
            "[158,   240] loss: 7.334\n",
            "[159,    60] loss: 7.190\n",
            "[159,   120] loss: 7.274\n",
            "[159,   180] loss: 7.335\n",
            "[159,   240] loss: 7.297\n",
            "[160,    60] loss: 7.125\n",
            "[160,   120] loss: 7.309\n",
            "[160,   180] loss: 7.342\n",
            "[160,   240] loss: 7.320\n",
            "[161,    60] loss: 7.336\n",
            "[161,   120] loss: 7.238\n",
            "[161,   180] loss: 7.247\n",
            "[161,   240] loss: 7.275\n",
            "[162,    60] loss: 7.292\n",
            "[162,   120] loss: 7.404\n",
            "[162,   180] loss: 7.170\n",
            "[162,   240] loss: 7.231\n",
            "[163,    60] loss: 7.193\n",
            "[163,   120] loss: 7.457\n",
            "[163,   180] loss: 7.176\n",
            "[163,   240] loss: 7.271\n",
            "[164,    60] loss: 7.380\n",
            "[164,   120] loss: 7.253\n",
            "[164,   180] loss: 7.236\n",
            "[164,   240] loss: 7.228\n",
            "[165,    60] loss: 7.432\n",
            "[165,   120] loss: 7.216\n",
            "[165,   180] loss: 7.149\n",
            "[165,   240] loss: 7.299\n",
            "[166,    60] loss: 7.207\n",
            "[166,   120] loss: 7.419\n",
            "[166,   180] loss: 7.285\n",
            "[166,   240] loss: 7.185\n",
            "[167,    60] loss: 7.148\n",
            "[167,   120] loss: 7.202\n",
            "[167,   180] loss: 7.456\n",
            "[167,   240] loss: 7.290\n",
            "[168,    60] loss: 7.360\n",
            "[168,   120] loss: 7.084\n",
            "[168,   180] loss: 7.306\n",
            "[168,   240] loss: 7.346\n",
            "[169,    60] loss: 7.263\n",
            "[169,   120] loss: 7.310\n",
            "[169,   180] loss: 7.388\n",
            "[169,   240] loss: 7.135\n",
            "[170,    60] loss: 7.320\n",
            "[170,   120] loss: 7.200\n",
            "[170,   180] loss: 7.231\n",
            "[170,   240] loss: 7.346\n",
            "[171,    60] loss: 7.254\n",
            "[171,   120] loss: 7.415\n",
            "[171,   180] loss: 7.212\n",
            "[171,   240] loss: 7.216\n",
            "[172,    60] loss: 7.343\n",
            "[172,   120] loss: 7.322\n",
            "[172,   180] loss: 7.331\n",
            "[172,   240] loss: 7.099\n",
            "[173,    60] loss: 7.333\n",
            "[173,   120] loss: 7.154\n",
            "[173,   180] loss: 7.259\n",
            "[173,   240] loss: 7.351\n",
            "[174,    60] loss: 7.226\n",
            "[174,   120] loss: 7.305\n",
            "[174,   180] loss: 7.278\n",
            "[174,   240] loss: 7.288\n",
            "[175,    60] loss: 7.283\n",
            "[175,   120] loss: 7.416\n",
            "[175,   180] loss: 7.159\n",
            "[175,   240] loss: 7.238\n",
            "[176,    60] loss: 7.404\n",
            "[176,   120] loss: 7.283\n",
            "[176,   180] loss: 7.237\n",
            "[176,   240] loss: 7.173\n",
            "[177,    60] loss: 7.143\n",
            "[177,   120] loss: 7.328\n",
            "[177,   180] loss: 7.415\n",
            "[177,   240] loss: 7.211\n",
            "[178,    60] loss: 7.232\n",
            "[178,   120] loss: 7.330\n",
            "[178,   180] loss: 7.220\n",
            "[178,   240] loss: 7.315\n",
            "[179,    60] loss: 7.272\n",
            "[179,   120] loss: 7.233\n",
            "[179,   180] loss: 7.281\n",
            "[179,   240] loss: 7.311\n",
            "[180,    60] loss: 7.308\n",
            "[180,   120] loss: 7.228\n",
            "[180,   180] loss: 7.272\n",
            "[180,   240] loss: 7.288\n",
            "[181,    60] loss: 7.170\n",
            "[181,   120] loss: 7.287\n",
            "[181,   180] loss: 7.353\n",
            "[181,   240] loss: 7.287\n",
            "[182,    60] loss: 7.217\n",
            "[182,   120] loss: 7.344\n",
            "[182,   180] loss: 7.350\n",
            "[182,   240] loss: 7.185\n",
            "[183,    60] loss: 7.269\n",
            "[183,   120] loss: 7.118\n",
            "[183,   180] loss: 7.347\n",
            "[183,   240] loss: 7.363\n",
            "[184,    60] loss: 7.260\n",
            "[184,   120] loss: 7.211\n",
            "[184,   180] loss: 7.370\n",
            "[184,   240] loss: 7.255\n",
            "[185,    60] loss: 7.242\n",
            "[185,   120] loss: 7.374\n",
            "[185,   180] loss: 7.279\n",
            "[185,   240] loss: 7.202\n",
            "[186,    60] loss: 7.258\n",
            "[186,   120] loss: 7.253\n",
            "[186,   180] loss: 7.224\n",
            "[186,   240] loss: 7.361\n",
            "[187,    60] loss: 7.278\n",
            "[187,   120] loss: 7.325\n",
            "[187,   180] loss: 7.274\n",
            "[187,   240] loss: 7.220\n",
            "[188,    60] loss: 7.364\n",
            "[188,   120] loss: 7.401\n",
            "[188,   180] loss: 7.145\n",
            "[188,   240] loss: 7.186\n",
            "[189,    60] loss: 7.208\n",
            "[189,   120] loss: 7.278\n",
            "[189,   180] loss: 7.189\n",
            "[189,   240] loss: 7.422\n",
            "[190,    60] loss: 7.268\n",
            "[190,   120] loss: 7.189\n",
            "[190,   180] loss: 7.239\n",
            "[190,   240] loss: 7.401\n",
            "[191,    60] loss: 7.277\n",
            "[191,   120] loss: 7.296\n",
            "[191,   180] loss: 7.336\n",
            "[191,   240] loss: 7.188\n",
            "[192,    60] loss: 7.299\n",
            "[192,   120] loss: 7.096\n",
            "[192,   180] loss: 7.418\n",
            "[192,   240] loss: 7.283\n",
            "[193,    60] loss: 7.364\n",
            "[193,   120] loss: 7.133\n",
            "[193,   180] loss: 7.297\n",
            "[193,   240] loss: 7.302\n",
            "[194,    60] loss: 7.217\n",
            "[194,   120] loss: 7.375\n",
            "[194,   180] loss: 7.229\n",
            "[194,   240] loss: 7.275\n",
            "[195,    60] loss: 7.371\n",
            "[195,   120] loss: 7.226\n",
            "[195,   180] loss: 7.126\n",
            "[195,   240] loss: 7.373\n",
            "[196,    60] loss: 7.213\n",
            "[196,   120] loss: 7.275\n",
            "[196,   180] loss: 7.323\n",
            "[196,   240] loss: 7.285\n",
            "[197,    60] loss: 7.259\n",
            "[197,   120] loss: 7.276\n",
            "[197,   180] loss: 7.230\n",
            "[197,   240] loss: 7.331\n",
            "[198,    60] loss: 7.315\n",
            "[198,   120] loss: 7.234\n",
            "[198,   180] loss: 7.327\n",
            "[198,   240] loss: 7.220\n",
            "[199,    60] loss: 7.384\n",
            "[199,   120] loss: 7.144\n",
            "[199,   180] loss: 7.214\n",
            "[199,   240] loss: 7.354\n",
            "[200,    60] loss: 7.308\n",
            "[200,   120] loss: 7.226\n",
            "[200,   180] loss: 7.142\n",
            "[200,   240] loss: 7.420\n",
            "[201,    60] loss: 7.305\n",
            "[201,   120] loss: 7.300\n",
            "[201,   180] loss: 7.186\n",
            "[201,   240] loss: 7.305\n",
            "[202,    60] loss: 7.270\n",
            "[202,   120] loss: 7.206\n",
            "[202,   180] loss: 7.331\n",
            "[202,   240] loss: 7.289\n",
            "[203,    60] loss: 7.331\n",
            "[203,   120] loss: 7.297\n",
            "[203,   180] loss: 7.208\n",
            "[203,   240] loss: 7.260\n",
            "[204,    60] loss: 7.212\n",
            "[204,   120] loss: 7.375\n",
            "[204,   180] loss: 7.271\n",
            "[204,   240] loss: 7.238\n",
            "[205,    60] loss: 7.191\n",
            "[205,   120] loss: 7.184\n",
            "[205,   180] loss: 7.396\n",
            "[205,   240] loss: 7.324\n",
            "[206,    60] loss: 7.384\n",
            "[206,   120] loss: 7.208\n",
            "[206,   180] loss: 7.234\n",
            "[206,   240] loss: 7.270\n",
            "[207,    60] loss: 7.258\n",
            "[207,   120] loss: 7.245\n",
            "[207,   180] loss: 7.341\n",
            "[207,   240] loss: 7.253\n",
            "[208,    60] loss: 7.392\n",
            "[208,   120] loss: 7.160\n",
            "[208,   180] loss: 7.269\n",
            "[208,   240] loss: 7.275\n",
            "[209,    60] loss: 7.278\n",
            "[209,   120] loss: 7.263\n",
            "[209,   180] loss: 7.301\n",
            "[209,   240] loss: 7.255\n",
            "[210,    60] loss: 7.136\n",
            "[210,   120] loss: 7.283\n",
            "[210,   180] loss: 7.280\n",
            "[210,   240] loss: 7.398\n",
            "[211,    60] loss: 7.329\n",
            "[211,   120] loss: 7.207\n",
            "[211,   180] loss: 7.338\n",
            "[211,   240] loss: 7.222\n",
            "[212,    60] loss: 7.307\n",
            "[212,   120] loss: 7.326\n",
            "[212,   180] loss: 7.090\n",
            "[212,   240] loss: 7.375\n",
            "[213,    60] loss: 7.242\n",
            "[213,   120] loss: 7.268\n",
            "[213,   180] loss: 7.281\n",
            "[213,   240] loss: 7.305\n",
            "[214,    60] loss: 7.269\n",
            "[214,   120] loss: 7.266\n",
            "[214,   180] loss: 7.293\n",
            "[214,   240] loss: 7.269\n",
            "[215,    60] loss: 7.265\n",
            "[215,   120] loss: 7.193\n",
            "[215,   180] loss: 7.409\n",
            "[215,   240] loss: 7.230\n",
            "[216,    60] loss: 7.305\n",
            "[216,   120] loss: 7.272\n",
            "[216,   180] loss: 7.313\n",
            "[216,   240] loss: 7.207\n",
            "[217,    60] loss: 7.262\n",
            "[217,   120] loss: 7.428\n",
            "[217,   180] loss: 7.336\n",
            "[217,   240] loss: 7.070\n",
            "[218,    60] loss: 7.309\n",
            "[218,   120] loss: 7.351\n",
            "[218,   180] loss: 7.328\n",
            "[218,   240] loss: 7.108\n",
            "[219,    60] loss: 7.296\n",
            "[219,   120] loss: 7.382\n",
            "[219,   180] loss: 7.312\n",
            "[219,   240] loss: 7.106\n",
            "[220,    60] loss: 7.243\n",
            "[220,   120] loss: 7.221\n",
            "[220,   180] loss: 7.240\n",
            "[220,   240] loss: 7.393\n",
            "[221,    60] loss: 7.339\n",
            "[221,   120] loss: 7.347\n",
            "[221,   180] loss: 7.184\n",
            "[221,   240] loss: 7.228\n",
            "[222,    60] loss: 7.339\n",
            "[222,   120] loss: 7.250\n",
            "[222,   180] loss: 7.210\n",
            "[222,   240] loss: 7.297\n",
            "[223,    60] loss: 7.278\n",
            "[223,   120] loss: 7.299\n",
            "[223,   180] loss: 7.245\n",
            "[223,   240] loss: 7.274\n",
            "[224,    60] loss: 7.339\n",
            "[224,   120] loss: 7.204\n",
            "[224,   180] loss: 7.156\n",
            "[224,   240] loss: 7.397\n",
            "[225,    60] loss: 7.382\n",
            "[225,   120] loss: 7.107\n",
            "[225,   180] loss: 7.260\n",
            "[225,   240] loss: 7.348\n",
            "[226,    60] loss: 7.276\n",
            "[226,   120] loss: 7.336\n",
            "[226,   180] loss: 7.235\n",
            "[226,   240] loss: 7.250\n",
            "[227,    60] loss: 7.201\n",
            "[227,   120] loss: 7.257\n",
            "[227,   180] loss: 7.325\n",
            "[227,   240] loss: 7.314\n",
            "[228,    60] loss: 7.403\n",
            "[228,   120] loss: 7.322\n",
            "[228,   180] loss: 7.080\n",
            "[228,   240] loss: 7.291\n",
            "[229,    60] loss: 7.248\n",
            "[229,   120] loss: 7.320\n",
            "[229,   180] loss: 7.301\n",
            "[229,   240] loss: 7.227\n",
            "[230,    60] loss: 7.471\n",
            "[230,   120] loss: 7.167\n",
            "[230,   180] loss: 7.102\n",
            "[230,   240] loss: 7.356\n",
            "[231,    60] loss: 7.319\n",
            "[231,   120] loss: 7.305\n",
            "[231,   180] loss: 7.261\n",
            "[231,   240] loss: 7.211\n",
            "[232,    60] loss: 7.240\n",
            "[232,   120] loss: 7.316\n",
            "[232,   180] loss: 7.224\n",
            "[232,   240] loss: 7.316\n",
            "[233,    60] loss: 7.364\n",
            "[233,   120] loss: 7.344\n",
            "[233,   180] loss: 7.103\n",
            "[233,   240] loss: 7.285\n",
            "[234,    60] loss: 7.210\n",
            "[234,   120] loss: 7.213\n",
            "[234,   180] loss: 7.311\n",
            "[234,   240] loss: 7.362\n",
            "[235,    60] loss: 7.359\n",
            "[235,   120] loss: 7.407\n",
            "[235,   180] loss: 7.208\n",
            "[235,   240] loss: 7.122\n",
            "[236,    60] loss: 7.268\n",
            "[236,   120] loss: 7.241\n",
            "[236,   180] loss: 7.336\n",
            "[236,   240] loss: 7.251\n",
            "[237,    60] loss: 7.245\n",
            "[237,   120] loss: 7.276\n",
            "[237,   180] loss: 7.328\n",
            "[237,   240] loss: 7.248\n",
            "[238,    60] loss: 7.379\n",
            "[238,   120] loss: 7.192\n",
            "[238,   180] loss: 7.201\n",
            "[238,   240] loss: 7.324\n",
            "[239,    60] loss: 7.171\n",
            "[239,   120] loss: 7.325\n",
            "[239,   180] loss: 7.254\n",
            "[239,   240] loss: 7.347\n",
            "[240,    60] loss: 7.426\n",
            "[240,   120] loss: 7.326\n",
            "[240,   180] loss: 7.138\n",
            "[240,   240] loss: 7.206\n",
            "[241,    60] loss: 7.311\n",
            "[241,   120] loss: 7.311\n",
            "[241,   180] loss: 7.243\n",
            "[241,   240] loss: 7.232\n",
            "[242,    60] loss: 7.244\n",
            "[242,   120] loss: 7.199\n",
            "[242,   180] loss: 7.298\n",
            "[242,   240] loss: 7.356\n",
            "[243,    60] loss: 7.355\n",
            "[243,   120] loss: 7.318\n",
            "[243,   180] loss: 7.163\n",
            "[243,   240] loss: 7.260\n",
            "[244,    60] loss: 7.244\n",
            "[244,   120] loss: 7.143\n",
            "[244,   180] loss: 7.315\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f7408a70d9b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfore_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VToKa651tMtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for params in classify.parameters():\n",
        "  print(params)\n",
        "  break;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIAJ3UZN8rPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(classify.state_dict(),\"/content/drive/My Drive/Research/Cheating_data/final_attention_weights/train_focus_mini_cheat_classify_cnn\"+\".pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LgQKXW-8MH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"epochs\", \"argmax > 0.5\" ,\"argmax < 0.5\", \"focus_true_pred_true\", \"focus_false_pred_true\", \"focus_true_pred_false\", \"focus_false_pred_false\" ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSKphM888Y5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.DataFrame()\n",
        "df_test = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrWoEGXZ8cBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train[columns[0]] = col1\n",
        "df_train[columns[1]] = col2\n",
        "df_train[columns[2]] = col3\n",
        "df_train[columns[3]] = col4\n",
        "df_train[columns[4]] = col5\n",
        "df_train[columns[5]] = col6\n",
        "df_train[columns[6]] = col7\n",
        "\n",
        "df_test[columns[0]] = col1\n",
        "df_test[columns[1]] = col8\n",
        "df_test[columns[2]] = col9\n",
        "df_test[columns[3]] = col10\n",
        "df_test[columns[4]] = col11\n",
        "df_test[columns[5]] = col12\n",
        "df_test[columns[6]] = col13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGJoMFcK8eTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei9HVQBZ8gn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.figure(12,12)\n",
        "plt.plot(col1,col2, label='argmax > 0.5')\n",
        "plt.plot(col1,col3, label='argmax < 0.5')\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"training data\")\n",
        "plt.title(\"On Training set\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(col1,col4, label =\"focus_true_pred_true \")\n",
        "plt.plot(col1,col5, label =\"focus_false_pred_true \")\n",
        "plt.plot(col1,col6, label =\"focus_true_pred_false \")\n",
        "plt.plot(col1,col7, label =\"focus_false_pred_false \")\n",
        "plt.title(\"On Training set\")\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"training data\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QKYVO8i8ivA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRlpgnjy8k1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.figure(12,12)\n",
        "plt.plot(col1,col8, label='argmax > 0.5')\n",
        "plt.plot(col1,col9, label='argmax < 0.5')\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Testing data\")\n",
        "plt.title(\"On Testing set\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(col1,col10, label =\"focus_true_pred_true \")\n",
        "plt.plot(col1,col11, label =\"focus_false_pred_true \")\n",
        "plt.plot(col1,col12, label =\"focus_true_pred_false \")\n",
        "plt.plot(col1,col13, label =\"focus_false_pred_false \")\n",
        "plt.title(\"On Testing set\")\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Testing data\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJEMJnUI9FP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in train_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 30000 train images: %d %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an7qmNLB-Ilb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels , fore_idx = data\n",
        "    inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n",
        "    alphas, avg_images = focus_net(inputs)\n",
        "    outputs = classify(avg_images)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total))\n",
        "print(\"total correct\", correct)\n",
        "print(\"total train set images\", total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWXci6ZVBlmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}