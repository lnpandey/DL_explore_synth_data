{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exp_1_Attention_models_on_mosaic_plots_for_all_datasets_training_loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSjG64ra4aFu",
        "outputId": "15e6d003-3a60-41ad-8dc9-f11ad9bbd127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8-7SARDZErK"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tquyEd6CrMM",
        "outputId": "9084b54b-c3f0-47e7-98ab-ba68365e122c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruTiWAOeCrMR"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWIK-x4iCrMW"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2PlOCj4CrMb"
      },
      "source": [
        "def imshow(img):\n",
        "  img = img / 2 + 0.5     # unnormalize\n",
        "  npimg = img#.numpy()\n",
        "  plt.imshow(np.transpose(npimg, axes = (1, 2, 0)))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUfgpxB3CrMj"
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOxvH0f1CrMn"
      },
      "source": [
        "desired_num = 10000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  np.random.seed(35000 + i)\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  np.random.seed(15000 + i)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  # fg = np.random.randint(0,9)\n",
        "  fg = 0\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-6BQ7ruMbZw"
      },
      "source": [
        "del foreground_data\n",
        "del foreground_label\n",
        "del background_data\n",
        "del background_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb-O3Wg1Kj7_"
      },
      "source": [
        "# path = \"/content/drive/My Drive/Research/testing classify on diff focus nets/give weightage to random image/\"\n",
        "# path = \"/content/drive/My Drive/Research/Experiments on CIFAR mosaic/Exp_1_Attention_models_on _90k_mosaic_mini_inception/weights/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX91RwMy-IP4"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  cnt = 0\n",
        "  counter = np.array([0,0,0,0,0,0,0,0,0])\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    np.random.seed(dataset_number*10000 + i)\n",
        "    give_pref = np.random.randint(0,9)\n",
        "    # print(\"outside\", give_pref,foreground_index[i])\n",
        "    for j in range(9):\n",
        "      if j == give_pref:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "\n",
        "    if give_pref == foreground_index[i] :\n",
        "      # print(\"equal are\", give_pref,foreground_index[i])\n",
        "      cnt += 1\n",
        "      counter[give_pref] += 1\n",
        "    else :\n",
        "      counter[give_pref] += 1\n",
        "\n",
        "    avg_image_dataset.append(img)\n",
        "\n",
        "  print(\"number of correct averaging happened for dataset \"+str(dataset_number)+\" is \"+str(cnt)) \n",
        "  print(\"the averaging are done as \", counter) \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGz8Y88vIZPT",
        "outputId": "05d1eb10-9096-4b4b-dc12-707d5d9127ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 9)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of correct averaging happened for dataset 1 is 1051\n",
            "the averaging are done as  [1051 1145 1064 1091 1140 1163 1109 1108 1129]\n",
            "number of correct averaging happened for dataset 2 is 1129\n",
            "the averaging are done as  [1129 1119 1083 1131 1099 1122 1116 1148 1053]\n",
            "number of correct averaging happened for dataset 3 is 1126\n",
            "the averaging are done as  [1126 1138 1125 1105 1128 1130 1077 1071 1100]\n",
            "number of correct averaging happened for dataset 4 is 1102\n",
            "the averaging are done as  [1102 1112 1104 1133 1091 1120 1119 1072 1147]\n",
            "number of correct averaging happened for dataset 5 is 1121\n",
            "the averaging are done as  [1121 1075 1104 1104 1093 1100 1141 1097 1165]\n",
            "number of correct averaging happened for dataset 6 is 1092\n",
            "the averaging are done as  [1092 1103 1125 1064 1138 1088 1135 1145 1110]\n",
            "number of correct averaging happened for dataset 7 is 1087\n",
            "the averaging are done as  [1087 1129 1083 1123 1139 1078 1120 1148 1093]\n",
            "number of correct averaging happened for dataset 8 is 1125\n",
            "the averaging are done as  [1125 1087 1080 1103 1127 1101 1126 1175 1076]\n",
            "number of correct averaging happened for dataset 9 is 1152\n",
            "the averaging are done as  [1152 1072 1110 1165 1104 1086 1115 1069 1127]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSO9SFE25Lrk"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obE1xeyRks1Q"
      },
      "source": [
        "batch = 256\n",
        "epochs = 300\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SadRzWBBZEsP"
      },
      "source": [
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgGYMG_ZZEsT"
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "        #self.conv1 = nn.Conv2d(3,self.f0,1)\n",
        "        #self.conv3 = nn.Conv2d(3,self.f1,3,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)\n",
        "        \n",
        "        x = torch.cat((x1,x3),dim=1)\n",
        "        \n",
        "    \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thkdqW91Hpju"
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,2,3,pad=0)\n",
        "        self.pool = nn.MaxPool2d(3,stride=2,padding=0)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x,x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1yVWgR4vFhe"
      },
      "source": [
        "class inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,96,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(96,32,32)\n",
        "        self.incept2 = inception_module(64,32,48)\n",
        "        \n",
        "        self.downsample1 = downsample_module(80,80)\n",
        "        \n",
        "        self.incept3 = inception_module(160,112,48)\n",
        "        self.incept4 = inception_module(160,96,64)\n",
        "        self.incept5 = inception_module(160,80,80)\n",
        "        self.incept6 = inception_module(160,48,96)\n",
        "        \n",
        "        self.downsample2 = downsample_module(144,96)\n",
        "        \n",
        "        self.incept7 = inception_module(240,176,60)\n",
        "        self.incept8 = inception_module(236,176,60)\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(5)\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.linear = nn.Linear(236,3)\n",
        "    def forward(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        #act1 = x\n",
        "        \n",
        "        x = self.incept1.forward(x)\n",
        "        #act2 = x\n",
        "        \n",
        "        x = self.incept2.forward(x)\n",
        "        #act3 = x\n",
        "        \n",
        "        x,act4 = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        #act5 = x\n",
        "        \n",
        "        x = self.incept4.forward(x)\n",
        "        #act6 = x\n",
        "        \n",
        "        x = self.incept5.forward(x)\n",
        "        #act7 = x\n",
        "        \n",
        "        x = self.incept6.forward(x)\n",
        "        #act8 = x\n",
        "        \n",
        "        x,act9 = self.downsample2.forward(x)\n",
        "        \n",
        "        x = self.incept7.forward(x)\n",
        "        #act10 = x\n",
        "        x = self.incept8.forward(x)\n",
        "        #act11 = x\n",
        "        #print(x.shape)\n",
        "        x = self.pool(x)\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1,1*1*236)\n",
        "        x = self.linear(x) \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZXXm5ECrN1"
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFfAJZkcZEsY"
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = inception_net().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 200\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "        if (np.mean(ep_lossi) <= 0.05):\n",
        "            break\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    # torch.save(inc.state_dict(),\"train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGqfP_P0CrN7",
        "outputId": "226e7f7e-ff9f-4320-f0c3-0d61ca419733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.088\n",
            "[1,    20] loss: 1.081\n",
            "[1,    30] loss: 1.068\n",
            "[1,    40] loss: 1.047\n",
            "[2,    10] loss: 1.048\n",
            "[2,    20] loss: 1.061\n",
            "[2,    30] loss: 1.056\n",
            "[2,    40] loss: 1.043\n",
            "[3,    10] loss: 1.030\n",
            "[3,    20] loss: 1.041\n",
            "[3,    30] loss: 1.033\n",
            "[3,    40] loss: 1.044\n",
            "[4,    10] loss: 1.017\n",
            "[4,    20] loss: 1.011\n",
            "[4,    30] loss: 1.010\n",
            "[4,    40] loss: 1.014\n",
            "[5,    10] loss: 0.989\n",
            "[5,    20] loss: 0.984\n",
            "[5,    30] loss: 0.985\n",
            "[5,    40] loss: 0.973\n",
            "[6,    10] loss: 0.967\n",
            "[6,    20] loss: 0.973\n",
            "[6,    30] loss: 0.956\n",
            "[6,    40] loss: 0.950\n",
            "[7,    10] loss: 0.951\n",
            "[7,    20] loss: 0.969\n",
            "[7,    30] loss: 0.954\n",
            "[7,    40] loss: 0.956\n",
            "[8,    10] loss: 0.892\n",
            "[8,    20] loss: 0.872\n",
            "[8,    30] loss: 0.847\n",
            "[8,    40] loss: 0.915\n",
            "[9,    10] loss: 0.834\n",
            "[9,    20] loss: 0.879\n",
            "[9,    30] loss: 0.906\n",
            "[9,    40] loss: 0.881\n",
            "[10,    10] loss: 0.800\n",
            "[10,    20] loss: 0.769\n",
            "[10,    30] loss: 0.706\n",
            "[10,    40] loss: 0.734\n",
            "[11,    10] loss: 0.770\n",
            "[11,    20] loss: 0.812\n",
            "[11,    30] loss: 0.849\n",
            "[11,    40] loss: 0.843\n",
            "[12,    10] loss: 0.704\n",
            "[12,    20] loss: 0.714\n",
            "[12,    30] loss: 0.678\n",
            "[12,    40] loss: 0.677\n",
            "[13,    10] loss: 0.630\n",
            "[13,    20] loss: 0.692\n",
            "[13,    30] loss: 0.651\n",
            "[13,    40] loss: 0.614\n",
            "[14,    10] loss: 0.539\n",
            "[14,    20] loss: 0.481\n",
            "[14,    30] loss: 0.431\n",
            "[14,    40] loss: 0.410\n",
            "[15,    10] loss: 0.329\n",
            "[15,    20] loss: 0.363\n",
            "[15,    30] loss: 0.348\n",
            "[15,    40] loss: 0.342\n",
            "[16,    10] loss: 0.301\n",
            "[16,    20] loss: 0.350\n",
            "[16,    30] loss: 0.339\n",
            "[16,    40] loss: 0.369\n",
            "[17,    10] loss: 0.440\n",
            "[17,    20] loss: 0.518\n",
            "[17,    30] loss: 0.498\n",
            "[17,    40] loss: 0.476\n",
            "[18,    10] loss: 0.427\n",
            "[18,    20] loss: 0.444\n",
            "[18,    30] loss: 0.413\n",
            "[18,    40] loss: 0.358\n",
            "[19,    10] loss: 0.231\n",
            "[19,    20] loss: 0.234\n",
            "[19,    30] loss: 0.186\n",
            "[19,    40] loss: 0.215\n",
            "[20,    10] loss: 0.251\n",
            "[20,    20] loss: 0.265\n",
            "[20,    30] loss: 0.216\n",
            "[20,    40] loss: 0.203\n",
            "[21,    10] loss: 0.180\n",
            "[21,    20] loss: 0.198\n",
            "[21,    30] loss: 0.165\n",
            "[21,    40] loss: 0.156\n",
            "[22,    10] loss: 0.077\n",
            "[22,    20] loss: 0.091\n",
            "[22,    30] loss: 0.070\n",
            "[22,    40] loss: 0.059\n",
            "[23,    10] loss: 0.028\n",
            "[23,    20] loss: 0.023\n",
            "[23,    30] loss: 0.018\n",
            "[23,    40] loss: 0.017\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 87 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 57 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 39 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 37 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.098\n",
            "[1,    20] loss: 1.073\n",
            "[1,    30] loss: 1.079\n",
            "[1,    40] loss: 1.052\n",
            "[2,    10] loss: 1.053\n",
            "[2,    20] loss: 1.055\n",
            "[2,    30] loss: 1.064\n",
            "[2,    40] loss: 1.060\n",
            "[3,    10] loss: 1.051\n",
            "[3,    20] loss: 1.045\n",
            "[3,    30] loss: 1.053\n",
            "[3,    40] loss: 1.035\n",
            "[4,    10] loss: 1.027\n",
            "[4,    20] loss: 1.034\n",
            "[4,    30] loss: 1.021\n",
            "[4,    40] loss: 1.019\n",
            "[5,    10] loss: 0.994\n",
            "[5,    20] loss: 1.010\n",
            "[5,    30] loss: 1.006\n",
            "[5,    40] loss: 0.975\n",
            "[6,    10] loss: 0.968\n",
            "[6,    20] loss: 0.975\n",
            "[6,    30] loss: 0.979\n",
            "[6,    40] loss: 0.952\n",
            "[7,    10] loss: 0.957\n",
            "[7,    20] loss: 0.925\n",
            "[7,    30] loss: 0.948\n",
            "[7,    40] loss: 0.940\n",
            "[8,    10] loss: 0.908\n",
            "[8,    20] loss: 0.894\n",
            "[8,    30] loss: 0.912\n",
            "[8,    40] loss: 0.953\n",
            "[9,    10] loss: 0.954\n",
            "[9,    20] loss: 0.951\n",
            "[9,    30] loss: 0.948\n",
            "[9,    40] loss: 0.929\n",
            "[10,    10] loss: 0.830\n",
            "[10,    20] loss: 0.812\n",
            "[10,    30] loss: 0.806\n",
            "[10,    40] loss: 0.814\n",
            "[11,    10] loss: 0.770\n",
            "[11,    20] loss: 0.811\n",
            "[11,    30] loss: 0.807\n",
            "[11,    40] loss: 0.802\n",
            "[12,    10] loss: 0.734\n",
            "[12,    20] loss: 0.718\n",
            "[12,    30] loss: 0.696\n",
            "[12,    40] loss: 0.726\n",
            "[13,    10] loss: 0.659\n",
            "[13,    20] loss: 0.724\n",
            "[13,    30] loss: 0.675\n",
            "[13,    40] loss: 0.674\n",
            "[14,    10] loss: 0.690\n",
            "[14,    20] loss: 0.669\n",
            "[14,    30] loss: 0.623\n",
            "[14,    40] loss: 0.606\n",
            "[15,    10] loss: 0.531\n",
            "[15,    20] loss: 0.560\n",
            "[15,    30] loss: 0.492\n",
            "[15,    40] loss: 0.475\n",
            "[16,    10] loss: 0.416\n",
            "[16,    20] loss: 0.434\n",
            "[16,    30] loss: 0.423\n",
            "[16,    40] loss: 0.418\n",
            "[17,    10] loss: 0.428\n",
            "[17,    20] loss: 0.448\n",
            "[17,    30] loss: 0.422\n",
            "[17,    40] loss: 0.386\n",
            "[18,    10] loss: 0.288\n",
            "[18,    20] loss: 0.293\n",
            "[18,    30] loss: 0.257\n",
            "[18,    40] loss: 0.270\n",
            "[19,    10] loss: 0.284\n",
            "[19,    20] loss: 0.336\n",
            "[19,    30] loss: 0.305\n",
            "[19,    40] loss: 0.277\n",
            "[20,    10] loss: 0.194\n",
            "[20,    20] loss: 0.158\n",
            "[20,    30] loss: 0.171\n",
            "[20,    40] loss: 0.132\n",
            "[21,    10] loss: 0.074\n",
            "[21,    20] loss: 0.078\n",
            "[21,    30] loss: 0.061\n",
            "[21,    40] loss: 0.058\n",
            "[22,    10] loss: 0.030\n",
            "[22,    20] loss: 0.035\n",
            "[22,    30] loss: 0.030\n",
            "[22,    40] loss: 0.022\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 85 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 39 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 36 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 1.100\n",
            "[1,    20] loss: 1.093\n",
            "[1,    30] loss: 1.080\n",
            "[1,    40] loss: 1.064\n",
            "[2,    10] loss: 1.077\n",
            "[2,    20] loss: 1.068\n",
            "[2,    30] loss: 1.073\n",
            "[2,    40] loss: 1.075\n",
            "[3,    10] loss: 1.063\n",
            "[3,    20] loss: 1.061\n",
            "[3,    30] loss: 1.048\n",
            "[3,    40] loss: 1.054\n",
            "[4,    10] loss: 1.038\n",
            "[4,    20] loss: 1.036\n",
            "[4,    30] loss: 1.048\n",
            "[4,    40] loss: 1.055\n",
            "[5,    10] loss: 1.024\n",
            "[5,    20] loss: 1.025\n",
            "[5,    30] loss: 1.022\n",
            "[5,    40] loss: 1.021\n",
            "[6,    10] loss: 1.000\n",
            "[6,    20] loss: 1.008\n",
            "[6,    30] loss: 0.988\n",
            "[6,    40] loss: 0.987\n",
            "[7,    10] loss: 0.974\n",
            "[7,    20] loss: 0.986\n",
            "[7,    30] loss: 0.975\n",
            "[7,    40] loss: 0.987\n",
            "[8,    10] loss: 0.946\n",
            "[8,    20] loss: 0.949\n",
            "[8,    30] loss: 0.913\n",
            "[8,    40] loss: 0.931\n",
            "[9,    10] loss: 0.857\n",
            "[9,    20] loss: 0.868\n",
            "[9,    30] loss: 0.866\n",
            "[9,    40] loss: 0.995\n",
            "[10,    10] loss: 0.972\n",
            "[10,    20] loss: 0.980\n",
            "[10,    30] loss: 0.964\n",
            "[10,    40] loss: 0.925\n",
            "[11,    10] loss: 0.833\n",
            "[11,    20] loss: 0.795\n",
            "[11,    30] loss: 0.751\n",
            "[11,    40] loss: 0.769\n",
            "[12,    10] loss: 0.722\n",
            "[12,    20] loss: 0.753\n",
            "[12,    30] loss: 0.764\n",
            "[12,    40] loss: 0.746\n",
            "[13,    10] loss: 0.663\n",
            "[13,    20] loss: 0.686\n",
            "[13,    30] loss: 0.703\n",
            "[13,    40] loss: 0.706\n",
            "[14,    10] loss: 0.674\n",
            "[14,    20] loss: 0.729\n",
            "[14,    30] loss: 0.687\n",
            "[14,    40] loss: 0.649\n",
            "[15,    10] loss: 0.591\n",
            "[15,    20] loss: 0.622\n",
            "[15,    30] loss: 0.587\n",
            "[15,    40] loss: 0.541\n",
            "[16,    10] loss: 0.448\n",
            "[16,    20] loss: 0.420\n",
            "[16,    30] loss: 0.413\n",
            "[16,    40] loss: 0.383\n",
            "[17,    10] loss: 0.345\n",
            "[17,    20] loss: 0.389\n",
            "[17,    30] loss: 0.377\n",
            "[17,    40] loss: 0.375\n",
            "[18,    10] loss: 0.307\n",
            "[18,    20] loss: 0.341\n",
            "[18,    30] loss: 0.304\n",
            "[18,    40] loss: 0.283\n",
            "[19,    10] loss: 0.263\n",
            "[19,    20] loss: 0.307\n",
            "[19,    30] loss: 0.260\n",
            "[19,    40] loss: 0.277\n",
            "[20,    10] loss: 0.280\n",
            "[20,    20] loss: 0.294\n",
            "[20,    30] loss: 0.293\n",
            "[20,    40] loss: 0.254\n",
            "[21,    10] loss: 0.221\n",
            "[21,    20] loss: 0.228\n",
            "[21,    30] loss: 0.228\n",
            "[21,    40] loss: 0.225\n",
            "[22,    10] loss: 0.234\n",
            "[22,    20] loss: 0.250\n",
            "[22,    30] loss: 0.254\n",
            "[22,    40] loss: 0.197\n",
            "[23,    10] loss: 0.131\n",
            "[23,    20] loss: 0.106\n",
            "[23,    30] loss: 0.092\n",
            "[23,    40] loss: 0.093\n",
            "[24,    10] loss: 0.088\n",
            "[24,    20] loss: 0.101\n",
            "[24,    30] loss: 0.078\n",
            "[24,    40] loss: 0.080\n",
            "[25,    10] loss: 0.115\n",
            "[25,    20] loss: 0.112\n",
            "[25,    30] loss: 0.111\n",
            "[25,    40] loss: 0.137\n",
            "[26,    10] loss: 0.354\n",
            "[26,    20] loss: 0.369\n",
            "[26,    30] loss: 0.398\n",
            "[26,    40] loss: 0.331\n",
            "[27,    10] loss: 0.201\n",
            "[27,    20] loss: 0.183\n",
            "[27,    30] loss: 0.156\n",
            "[27,    40] loss: 0.168\n",
            "[28,    10] loss: 0.204\n",
            "[28,    20] loss: 0.240\n",
            "[28,    30] loss: 0.187\n",
            "[28,    40] loss: 0.211\n",
            "[29,    10] loss: 0.281\n",
            "[29,    20] loss: 0.336\n",
            "[29,    30] loss: 0.305\n",
            "[29,    40] loss: 0.239\n",
            "[30,    10] loss: 0.224\n",
            "[30,    20] loss: 0.185\n",
            "[30,    30] loss: 0.146\n",
            "[30,    40] loss: 0.150\n",
            "[31,    10] loss: 0.090\n",
            "[31,    20] loss: 0.105\n",
            "[31,    30] loss: 0.084\n",
            "[31,    40] loss: 0.076\n",
            "[32,    10] loss: 0.072\n",
            "[32,    20] loss: 0.069\n",
            "[32,    30] loss: 0.056\n",
            "[32,    40] loss: 0.055\n",
            "[33,    10] loss: 0.032\n",
            "[33,    20] loss: 0.037\n",
            "[33,    30] loss: 0.037\n",
            "[33,    40] loss: 0.026\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 59 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 56 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 40 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 38 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 1.110\n",
            "[1,    20] loss: 1.091\n",
            "[1,    30] loss: 1.094\n",
            "[1,    40] loss: 1.096\n",
            "[2,    10] loss: 1.083\n",
            "[2,    20] loss: 1.084\n",
            "[2,    30] loss: 1.083\n",
            "[2,    40] loss: 1.080\n",
            "[3,    10] loss: 1.072\n",
            "[3,    20] loss: 1.069\n",
            "[3,    30] loss: 1.062\n",
            "[3,    40] loss: 1.072\n",
            "[4,    10] loss: 1.046\n",
            "[4,    20] loss: 1.059\n",
            "[4,    30] loss: 1.046\n",
            "[4,    40] loss: 1.057\n",
            "[5,    10] loss: 1.031\n",
            "[5,    20] loss: 1.032\n",
            "[5,    30] loss: 1.026\n",
            "[5,    40] loss: 1.022\n",
            "[6,    10] loss: 1.023\n",
            "[6,    20] loss: 1.022\n",
            "[6,    30] loss: 1.009\n",
            "[6,    40] loss: 1.024\n",
            "[7,    10] loss: 0.982\n",
            "[7,    20] loss: 0.988\n",
            "[7,    30] loss: 0.984\n",
            "[7,    40] loss: 1.004\n",
            "[8,    10] loss: 0.968\n",
            "[8,    20] loss: 0.996\n",
            "[8,    30] loss: 1.006\n",
            "[8,    40] loss: 1.008\n",
            "[9,    10] loss: 0.972\n",
            "[9,    20] loss: 0.951\n",
            "[9,    30] loss: 0.917\n",
            "[9,    40] loss: 0.917\n",
            "[10,    10] loss: 0.896\n",
            "[10,    20] loss: 0.910\n",
            "[10,    30] loss: 0.918\n",
            "[10,    40] loss: 0.927\n",
            "[11,    10] loss: 0.835\n",
            "[11,    20] loss: 0.822\n",
            "[11,    30] loss: 0.836\n",
            "[11,    40] loss: 0.792\n",
            "[12,    10] loss: 0.832\n",
            "[12,    20] loss: 0.845\n",
            "[12,    30] loss: 0.839\n",
            "[12,    40] loss: 0.822\n",
            "[13,    10] loss: 0.731\n",
            "[13,    20] loss: 0.734\n",
            "[13,    30] loss: 0.701\n",
            "[13,    40] loss: 0.738\n",
            "[14,    10] loss: 0.697\n",
            "[14,    20] loss: 0.759\n",
            "[14,    30] loss: 0.778\n",
            "[14,    40] loss: 0.729\n",
            "[15,    10] loss: 0.617\n",
            "[15,    20] loss: 0.602\n",
            "[15,    30] loss: 0.564\n",
            "[15,    40] loss: 0.582\n",
            "[16,    10] loss: 0.536\n",
            "[16,    20] loss: 0.550\n",
            "[16,    30] loss: 0.524\n",
            "[16,    40] loss: 0.535\n",
            "[17,    10] loss: 0.570\n",
            "[17,    20] loss: 0.652\n",
            "[17,    30] loss: 0.564\n",
            "[17,    40] loss: 0.575\n",
            "[18,    10] loss: 0.417\n",
            "[18,    20] loss: 0.410\n",
            "[18,    30] loss: 0.394\n",
            "[18,    40] loss: 0.342\n",
            "[19,    10] loss: 0.329\n",
            "[19,    20] loss: 0.358\n",
            "[19,    30] loss: 0.348\n",
            "[19,    40] loss: 0.335\n",
            "[20,    10] loss: 0.234\n",
            "[20,    20] loss: 0.246\n",
            "[20,    30] loss: 0.235\n",
            "[20,    40] loss: 0.245\n",
            "[21,    10] loss: 0.293\n",
            "[21,    20] loss: 0.321\n",
            "[21,    30] loss: 0.330\n",
            "[21,    40] loss: 0.287\n",
            "[22,    10] loss: 0.265\n",
            "[22,    20] loss: 0.231\n",
            "[22,    30] loss: 0.231\n",
            "[22,    40] loss: 0.204\n",
            "[23,    10] loss: 0.150\n",
            "[23,    20] loss: 0.153\n",
            "[23,    30] loss: 0.128\n",
            "[23,    40] loss: 0.133\n",
            "[24,    10] loss: 0.087\n",
            "[24,    20] loss: 0.095\n",
            "[24,    30] loss: 0.078\n",
            "[24,    40] loss: 0.090\n",
            "[25,    10] loss: 0.165\n",
            "[25,    20] loss: 0.180\n",
            "[25,    30] loss: 0.169\n",
            "[25,    40] loss: 0.175\n",
            "[26,    10] loss: 0.162\n",
            "[26,    20] loss: 0.176\n",
            "[26,    30] loss: 0.167\n",
            "[26,    40] loss: 0.172\n",
            "[27,    10] loss: 0.248\n",
            "[27,    20] loss: 0.267\n",
            "[27,    30] loss: 0.225\n",
            "[27,    40] loss: 0.183\n",
            "[28,    10] loss: 0.194\n",
            "[28,    20] loss: 0.194\n",
            "[28,    30] loss: 0.202\n",
            "[28,    40] loss: 0.167\n",
            "[29,    10] loss: 0.180\n",
            "[29,    20] loss: 0.192\n",
            "[29,    30] loss: 0.202\n",
            "[29,    40] loss: 0.191\n",
            "[30,    10] loss: 0.362\n",
            "[30,    20] loss: 0.477\n",
            "[30,    30] loss: 0.451\n",
            "[30,    40] loss: 0.357\n",
            "[31,    10] loss: 0.228\n",
            "[31,    20] loss: 0.216\n",
            "[31,    30] loss: 0.144\n",
            "[31,    40] loss: 0.142\n",
            "[32,    10] loss: 0.075\n",
            "[32,    20] loss: 0.075\n",
            "[32,    30] loss: 0.061\n",
            "[32,    40] loss: 0.055\n",
            "[33,    10] loss: 0.019\n",
            "[33,    20] loss: 0.024\n",
            "[33,    30] loss: 0.019\n",
            "[33,    40] loss: 0.043\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 40 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 40 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 1.102\n",
            "[1,    20] loss: 1.092\n",
            "[1,    30] loss: 1.100\n",
            "[1,    40] loss: 1.092\n",
            "[2,    10] loss: 1.080\n",
            "[2,    20] loss: 1.092\n",
            "[2,    30] loss: 1.088\n",
            "[2,    40] loss: 1.090\n",
            "[3,    10] loss: 1.077\n",
            "[3,    20] loss: 1.062\n",
            "[3,    30] loss: 1.068\n",
            "[3,    40] loss: 1.077\n",
            "[4,    10] loss: 1.057\n",
            "[4,    20] loss: 1.061\n",
            "[4,    30] loss: 1.060\n",
            "[4,    40] loss: 1.059\n",
            "[5,    10] loss: 1.053\n",
            "[5,    20] loss: 1.047\n",
            "[5,    30] loss: 1.041\n",
            "[5,    40] loss: 1.049\n",
            "[6,    10] loss: 1.022\n",
            "[6,    20] loss: 1.032\n",
            "[6,    30] loss: 1.020\n",
            "[6,    40] loss: 1.011\n",
            "[7,    10] loss: 1.003\n",
            "[7,    20] loss: 0.998\n",
            "[7,    30] loss: 0.994\n",
            "[7,    40] loss: 0.996\n",
            "[8,    10] loss: 0.979\n",
            "[8,    20] loss: 0.996\n",
            "[8,    30] loss: 0.986\n",
            "[8,    40] loss: 0.988\n",
            "[9,    10] loss: 0.932\n",
            "[9,    20] loss: 0.929\n",
            "[9,    30] loss: 0.941\n",
            "[9,    40] loss: 0.978\n",
            "[10,    10] loss: 0.949\n",
            "[10,    20] loss: 0.981\n",
            "[10,    30] loss: 0.982\n",
            "[10,    40] loss: 0.977\n",
            "[11,    10] loss: 0.930\n",
            "[11,    20] loss: 0.910\n",
            "[11,    30] loss: 0.880\n",
            "[11,    40] loss: 0.881\n",
            "[12,    10] loss: 0.807\n",
            "[12,    20] loss: 0.803\n",
            "[12,    30] loss: 0.806\n",
            "[12,    40] loss: 0.825\n",
            "[13,    10] loss: 0.798\n",
            "[13,    20] loss: 0.816\n",
            "[13,    30] loss: 0.812\n",
            "[13,    40] loss: 0.806\n",
            "[14,    10] loss: 0.765\n",
            "[14,    20] loss: 0.788\n",
            "[14,    30] loss: 0.781\n",
            "[14,    40] loss: 0.769\n",
            "[15,    10] loss: 0.718\n",
            "[15,    20] loss: 0.692\n",
            "[15,    30] loss: 0.650\n",
            "[15,    40] loss: 0.650\n",
            "[16,    10] loss: 0.598\n",
            "[16,    20] loss: 0.565\n",
            "[16,    30] loss: 0.554\n",
            "[16,    40] loss: 0.588\n",
            "[17,    10] loss: 0.614\n",
            "[17,    20] loss: 0.670\n",
            "[17,    30] loss: 0.645\n",
            "[17,    40] loss: 0.652\n",
            "[18,    10] loss: 0.502\n",
            "[18,    20] loss: 0.506\n",
            "[18,    30] loss: 0.470\n",
            "[18,    40] loss: 0.468\n",
            "[19,    10] loss: 0.441\n",
            "[19,    20] loss: 0.409\n",
            "[19,    30] loss: 0.405\n",
            "[19,    40] loss: 0.449\n",
            "[20,    10] loss: 0.446\n",
            "[20,    20] loss: 0.489\n",
            "[20,    30] loss: 0.423\n",
            "[20,    40] loss: 0.428\n",
            "[21,    10] loss: 0.430\n",
            "[21,    20] loss: 0.450\n",
            "[21,    30] loss: 0.417\n",
            "[21,    40] loss: 0.366\n",
            "[22,    10] loss: 0.274\n",
            "[22,    20] loss: 0.281\n",
            "[22,    30] loss: 0.259\n",
            "[22,    40] loss: 0.236\n",
            "[23,    10] loss: 0.161\n",
            "[23,    20] loss: 0.144\n",
            "[23,    30] loss: 0.124\n",
            "[23,    40] loss: 0.151\n",
            "[24,    10] loss: 0.197\n",
            "[24,    20] loss: 0.216\n",
            "[24,    30] loss: 0.203\n",
            "[24,    40] loss: 0.193\n",
            "[25,    10] loss: 0.256\n",
            "[25,    20] loss: 0.239\n",
            "[25,    30] loss: 0.224\n",
            "[25,    40] loss: 0.218\n",
            "[26,    10] loss: 0.165\n",
            "[26,    20] loss: 0.173\n",
            "[26,    30] loss: 0.142\n",
            "[26,    40] loss: 0.125\n",
            "[27,    10] loss: 0.162\n",
            "[27,    20] loss: 0.174\n",
            "[27,    30] loss: 0.189\n",
            "[27,    40] loss: 0.209\n",
            "[28,    10] loss: 0.398\n",
            "[28,    20] loss: 0.478\n",
            "[28,    30] loss: 0.467\n",
            "[28,    40] loss: 0.409\n",
            "[29,    10] loss: 0.364\n",
            "[29,    20] loss: 0.310\n",
            "[29,    30] loss: 0.260\n",
            "[29,    40] loss: 0.232\n",
            "[30,    10] loss: 0.190\n",
            "[30,    20] loss: 0.164\n",
            "[30,    30] loss: 0.147\n",
            "[30,    40] loss: 0.162\n",
            "[31,    10] loss: 0.119\n",
            "[31,    20] loss: 0.083\n",
            "[31,    30] loss: 0.065\n",
            "[31,    40] loss: 0.060\n",
            "[32,    10] loss: 0.032\n",
            "[32,    20] loss: 0.039\n",
            "[32,    30] loss: 0.021\n",
            "[32,    40] loss: 0.019\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 41 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 1.106\n",
            "[1,    20] loss: 1.096\n",
            "[1,    30] loss: 1.105\n",
            "[1,    40] loss: 1.090\n",
            "[2,    10] loss: 1.093\n",
            "[2,    20] loss: 1.089\n",
            "[2,    30] loss: 1.086\n",
            "[2,    40] loss: 1.079\n",
            "[3,    10] loss: 1.074\n",
            "[3,    20] loss: 1.068\n",
            "[3,    30] loss: 1.072\n",
            "[3,    40] loss: 1.081\n",
            "[4,    10] loss: 1.072\n",
            "[4,    20] loss: 1.055\n",
            "[4,    30] loss: 1.061\n",
            "[4,    40] loss: 1.058\n",
            "[5,    10] loss: 1.034\n",
            "[5,    20] loss: 1.036\n",
            "[5,    30] loss: 1.043\n",
            "[5,    40] loss: 1.032\n",
            "[6,    10] loss: 1.026\n",
            "[6,    20] loss: 1.028\n",
            "[6,    30] loss: 1.021\n",
            "[6,    40] loss: 1.018\n",
            "[7,    10] loss: 1.019\n",
            "[7,    20] loss: 1.009\n",
            "[7,    30] loss: 1.015\n",
            "[7,    40] loss: 0.995\n",
            "[8,    10] loss: 0.955\n",
            "[8,    20] loss: 0.981\n",
            "[8,    30] loss: 0.980\n",
            "[8,    40] loss: 0.990\n",
            "[9,    10] loss: 0.918\n",
            "[9,    20] loss: 0.947\n",
            "[9,    30] loss: 0.942\n",
            "[9,    40] loss: 0.940\n",
            "[10,    10] loss: 0.886\n",
            "[10,    20] loss: 0.912\n",
            "[10,    30] loss: 0.912\n",
            "[10,    40] loss: 0.909\n",
            "[11,    10] loss: 0.870\n",
            "[11,    20] loss: 0.898\n",
            "[11,    30] loss: 0.859\n",
            "[11,    40] loss: 0.863\n",
            "[12,    10] loss: 0.792\n",
            "[12,    20] loss: 0.808\n",
            "[12,    30] loss: 0.796\n",
            "[12,    40] loss: 0.804\n",
            "[13,    10] loss: 0.727\n",
            "[13,    20] loss: 0.785\n",
            "[13,    30] loss: 0.771\n",
            "[13,    40] loss: 0.788\n",
            "[14,    10] loss: 0.679\n",
            "[14,    20] loss: 0.689\n",
            "[14,    30] loss: 0.678\n",
            "[14,    40] loss: 0.662\n",
            "[15,    10] loss: 0.593\n",
            "[15,    20] loss: 0.627\n",
            "[15,    30] loss: 0.611\n",
            "[15,    40] loss: 0.610\n",
            "[16,    10] loss: 0.531\n",
            "[16,    20] loss: 0.531\n",
            "[16,    30] loss: 0.546\n",
            "[16,    40] loss: 0.574\n",
            "[17,    10] loss: 0.550\n",
            "[17,    20] loss: 0.578\n",
            "[17,    30] loss: 0.568\n",
            "[17,    40] loss: 0.555\n",
            "[18,    10] loss: 0.416\n",
            "[18,    20] loss: 0.419\n",
            "[18,    30] loss: 0.392\n",
            "[18,    40] loss: 0.431\n",
            "[19,    10] loss: 0.406\n",
            "[19,    20] loss: 0.411\n",
            "[19,    30] loss: 0.408\n",
            "[19,    40] loss: 0.425\n",
            "[20,    10] loss: 0.355\n",
            "[20,    20] loss: 0.373\n",
            "[20,    30] loss: 0.347\n",
            "[20,    40] loss: 0.370\n",
            "[21,    10] loss: 0.377\n",
            "[21,    20] loss: 0.430\n",
            "[21,    30] loss: 0.396\n",
            "[21,    40] loss: 0.371\n",
            "[22,    10] loss: 0.288\n",
            "[22,    20] loss: 0.300\n",
            "[22,    30] loss: 0.257\n",
            "[22,    40] loss: 0.251\n",
            "[23,    10] loss: 0.164\n",
            "[23,    20] loss: 0.169\n",
            "[23,    30] loss: 0.167\n",
            "[23,    40] loss: 0.188\n",
            "[24,    10] loss: 0.256\n",
            "[24,    20] loss: 0.243\n",
            "[24,    30] loss: 0.227\n",
            "[24,    40] loss: 0.302\n",
            "[25,    10] loss: 0.322\n",
            "[25,    20] loss: 0.327\n",
            "[25,    30] loss: 0.294\n",
            "[25,    40] loss: 0.254\n",
            "[26,    10] loss: 0.204\n",
            "[26,    20] loss: 0.221\n",
            "[26,    30] loss: 0.179\n",
            "[26,    40] loss: 0.212\n",
            "[27,    10] loss: 0.214\n",
            "[27,    20] loss: 0.254\n",
            "[27,    30] loss: 0.256\n",
            "[27,    40] loss: 0.260\n",
            "[28,    10] loss: 0.223\n",
            "[28,    20] loss: 0.229\n",
            "[28,    30] loss: 0.212\n",
            "[28,    40] loss: 0.180\n",
            "[29,    10] loss: 0.094\n",
            "[29,    20] loss: 0.112\n",
            "[29,    30] loss: 0.114\n",
            "[29,    40] loss: 0.134\n",
            "[30,    10] loss: 0.081\n",
            "[30,    20] loss: 0.087\n",
            "[30,    30] loss: 0.081\n",
            "[30,    40] loss: 0.088\n",
            "[31,    10] loss: 0.047\n",
            "[31,    20] loss: 0.042\n",
            "[31,    30] loss: 0.040\n",
            "[31,    40] loss: 0.052\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 40 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 42 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 1.106\n",
            "[1,    20] loss: 1.108\n",
            "[1,    30] loss: 1.098\n",
            "[1,    40] loss: 1.090\n",
            "[2,    10] loss: 1.102\n",
            "[2,    20] loss: 1.094\n",
            "[2,    30] loss: 1.095\n",
            "[2,    40] loss: 1.100\n",
            "[3,    10] loss: 1.086\n",
            "[3,    20] loss: 1.081\n",
            "[3,    30] loss: 1.073\n",
            "[3,    40] loss: 1.086\n",
            "[4,    10] loss: 1.068\n",
            "[4,    20] loss: 1.063\n",
            "[4,    30] loss: 1.058\n",
            "[4,    40] loss: 1.069\n",
            "[5,    10] loss: 1.050\n",
            "[5,    20] loss: 1.055\n",
            "[5,    30] loss: 1.048\n",
            "[5,    40] loss: 1.051\n",
            "[6,    10] loss: 1.023\n",
            "[6,    20] loss: 1.032\n",
            "[6,    30] loss: 1.033\n",
            "[6,    40] loss: 1.038\n",
            "[7,    10] loss: 1.013\n",
            "[7,    20] loss: 1.027\n",
            "[7,    30] loss: 1.021\n",
            "[7,    40] loss: 1.010\n",
            "[8,    10] loss: 1.002\n",
            "[8,    20] loss: 1.007\n",
            "[8,    30] loss: 0.995\n",
            "[8,    40] loss: 1.006\n",
            "[9,    10] loss: 0.973\n",
            "[9,    20] loss: 0.994\n",
            "[9,    30] loss: 0.998\n",
            "[9,    40] loss: 0.977\n",
            "[10,    10] loss: 0.916\n",
            "[10,    20] loss: 0.887\n",
            "[10,    30] loss: 0.900\n",
            "[10,    40] loss: 0.903\n",
            "[11,    10] loss: 0.902\n",
            "[11,    20] loss: 0.916\n",
            "[11,    30] loss: 0.910\n",
            "[11,    40] loss: 0.908\n",
            "[12,    10] loss: 0.839\n",
            "[12,    20] loss: 0.872\n",
            "[12,    30] loss: 0.868\n",
            "[12,    40] loss: 0.883\n",
            "[13,    10] loss: 0.826\n",
            "[13,    20] loss: 0.855\n",
            "[13,    30] loss: 0.833\n",
            "[13,    40] loss: 0.849\n",
            "[14,    10] loss: 0.736\n",
            "[14,    20] loss: 0.739\n",
            "[14,    30] loss: 0.772\n",
            "[14,    40] loss: 0.732\n",
            "[15,    10] loss: 0.673\n",
            "[15,    20] loss: 0.720\n",
            "[15,    30] loss: 0.752\n",
            "[15,    40] loss: 0.727\n",
            "[16,    10] loss: 0.658\n",
            "[16,    20] loss: 0.645\n",
            "[16,    30] loss: 0.649\n",
            "[16,    40] loss: 0.631\n",
            "[17,    10] loss: 0.550\n",
            "[17,    20] loss: 0.575\n",
            "[17,    30] loss: 0.584\n",
            "[17,    40] loss: 0.618\n",
            "[18,    10] loss: 0.554\n",
            "[18,    20] loss: 0.582\n",
            "[18,    30] loss: 0.615\n",
            "[18,    40] loss: 0.655\n",
            "[19,    10] loss: 0.558\n",
            "[19,    20] loss: 0.591\n",
            "[19,    30] loss: 0.538\n",
            "[19,    40] loss: 0.578\n",
            "[20,    10] loss: 0.498\n",
            "[20,    20] loss: 0.511\n",
            "[20,    30] loss: 0.509\n",
            "[20,    40] loss: 0.499\n",
            "[21,    10] loss: 0.450\n",
            "[21,    20] loss: 0.483\n",
            "[21,    30] loss: 0.553\n",
            "[21,    40] loss: 0.521\n",
            "[22,    10] loss: 0.434\n",
            "[22,    20] loss: 0.495\n",
            "[22,    30] loss: 0.448\n",
            "[22,    40] loss: 0.507\n",
            "[23,    10] loss: 0.451\n",
            "[23,    20] loss: 0.516\n",
            "[23,    30] loss: 0.547\n",
            "[23,    40] loss: 0.462\n",
            "[24,    10] loss: 0.349\n",
            "[24,    20] loss: 0.335\n",
            "[24,    30] loss: 0.345\n",
            "[24,    40] loss: 0.419\n",
            "[25,    10] loss: 0.414\n",
            "[25,    20] loss: 0.430\n",
            "[25,    30] loss: 0.437\n",
            "[25,    40] loss: 0.447\n",
            "[26,    10] loss: 0.361\n",
            "[26,    20] loss: 0.438\n",
            "[26,    30] loss: 0.413\n",
            "[26,    40] loss: 0.360\n",
            "[27,    10] loss: 0.282\n",
            "[27,    20] loss: 0.283\n",
            "[27,    30] loss: 0.310\n",
            "[27,    40] loss: 0.342\n",
            "[28,    10] loss: 0.306\n",
            "[28,    20] loss: 0.299\n",
            "[28,    30] loss: 0.281\n",
            "[28,    40] loss: 0.327\n",
            "[29,    10] loss: 0.235\n",
            "[29,    20] loss: 0.232\n",
            "[29,    30] loss: 0.246\n",
            "[29,    40] loss: 0.272\n",
            "[30,    10] loss: 0.279\n",
            "[30,    20] loss: 0.303\n",
            "[30,    30] loss: 0.276\n",
            "[30,    40] loss: 0.266\n",
            "[31,    10] loss: 0.209\n",
            "[31,    20] loss: 0.226\n",
            "[31,    30] loss: 0.239\n",
            "[31,    40] loss: 0.276\n",
            "[32,    10] loss: 0.229\n",
            "[32,    20] loss: 0.249\n",
            "[32,    30] loss: 0.257\n",
            "[32,    40] loss: 0.279\n",
            "[33,    10] loss: 0.297\n",
            "[33,    20] loss: 0.355\n",
            "[33,    30] loss: 0.331\n",
            "[33,    40] loss: 0.372\n",
            "[34,    10] loss: 0.240\n",
            "[34,    20] loss: 0.277\n",
            "[34,    30] loss: 0.265\n",
            "[34,    40] loss: 0.272\n",
            "[35,    10] loss: 0.194\n",
            "[35,    20] loss: 0.203\n",
            "[35,    30] loss: 0.192\n",
            "[35,    40] loss: 0.227\n",
            "[36,    10] loss: 0.246\n",
            "[36,    20] loss: 0.322\n",
            "[36,    30] loss: 0.266\n",
            "[36,    40] loss: 0.254\n",
            "[37,    10] loss: 0.199\n",
            "[37,    20] loss: 0.190\n",
            "[37,    30] loss: 0.197\n",
            "[37,    40] loss: 0.213\n",
            "[38,    10] loss: 0.168\n",
            "[38,    20] loss: 0.199\n",
            "[38,    30] loss: 0.196\n",
            "[38,    40] loss: 0.203\n",
            "[39,    10] loss: 0.169\n",
            "[39,    20] loss: 0.238\n",
            "[39,    30] loss: 0.185\n",
            "[39,    40] loss: 0.187\n",
            "[40,    10] loss: 0.124\n",
            "[40,    20] loss: 0.128\n",
            "[40,    30] loss: 0.147\n",
            "[40,    40] loss: 0.167\n",
            "[41,    10] loss: 0.156\n",
            "[41,    20] loss: 0.148\n",
            "[41,    30] loss: 0.164\n",
            "[41,    40] loss: 0.146\n",
            "[42,    10] loss: 0.075\n",
            "[42,    20] loss: 0.095\n",
            "[42,    30] loss: 0.116\n",
            "[42,    40] loss: 0.147\n",
            "[43,    10] loss: 0.121\n",
            "[43,    20] loss: 0.149\n",
            "[43,    30] loss: 0.136\n",
            "[43,    40] loss: 0.140\n",
            "[44,    10] loss: 0.105\n",
            "[44,    20] loss: 0.102\n",
            "[44,    30] loss: 0.127\n",
            "[44,    40] loss: 0.111\n",
            "[45,    10] loss: 0.071\n",
            "[45,    20] loss: 0.086\n",
            "[45,    30] loss: 0.085\n",
            "[45,    40] loss: 0.127\n",
            "[46,    10] loss: 0.191\n",
            "[46,    20] loss: 0.222\n",
            "[46,    30] loss: 0.235\n",
            "[46,    40] loss: 0.278\n",
            "[47,    10] loss: 0.547\n",
            "[47,    20] loss: 0.548\n",
            "[47,    30] loss: 0.417\n",
            "[47,    40] loss: 0.386\n",
            "[48,    10] loss: 0.272\n",
            "[48,    20] loss: 0.260\n",
            "[48,    30] loss: 0.221\n",
            "[48,    40] loss: 0.180\n",
            "[49,    10] loss: 0.107\n",
            "[49,    20] loss: 0.126\n",
            "[49,    30] loss: 0.124\n",
            "[49,    40] loss: 0.118\n",
            "[50,    10] loss: 0.068\n",
            "[50,    20] loss: 0.084\n",
            "[50,    30] loss: 0.068\n",
            "[50,    40] loss: 0.089\n",
            "[51,    10] loss: 0.080\n",
            "[51,    20] loss: 0.079\n",
            "[51,    30] loss: 0.072\n",
            "[51,    40] loss: 0.052\n",
            "[52,    10] loss: 0.047\n",
            "[52,    20] loss: 0.050\n",
            "[52,    30] loss: 0.041\n",
            "[52,    40] loss: 0.056\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 40 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 44 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 43 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 1.101\n",
            "[1,    20] loss: 1.100\n",
            "[1,    30] loss: 1.102\n",
            "[1,    40] loss: 1.092\n",
            "[2,    10] loss: 1.117\n",
            "[2,    20] loss: 1.096\n",
            "[2,    30] loss: 1.096\n",
            "[2,    40] loss: 1.089\n",
            "[3,    10] loss: 1.092\n",
            "[3,    20] loss: 1.081\n",
            "[3,    30] loss: 1.075\n",
            "[3,    40] loss: 1.073\n",
            "[4,    10] loss: 1.070\n",
            "[4,    20] loss: 1.066\n",
            "[4,    30] loss: 1.066\n",
            "[4,    40] loss: 1.059\n",
            "[5,    10] loss: 1.047\n",
            "[5,    20] loss: 1.052\n",
            "[5,    30] loss: 1.060\n",
            "[5,    40] loss: 1.054\n",
            "[6,    10] loss: 1.048\n",
            "[6,    20] loss: 1.025\n",
            "[6,    30] loss: 1.030\n",
            "[6,    40] loss: 1.033\n",
            "[7,    10] loss: 1.010\n",
            "[7,    20] loss: 1.015\n",
            "[7,    30] loss: 1.012\n",
            "[7,    40] loss: 0.992\n",
            "[8,    10] loss: 0.975\n",
            "[8,    20] loss: 0.973\n",
            "[8,    30] loss: 0.969\n",
            "[8,    40] loss: 0.975\n",
            "[9,    10] loss: 0.980\n",
            "[9,    20] loss: 0.986\n",
            "[9,    30] loss: 0.968\n",
            "[9,    40] loss: 0.997\n",
            "[10,    10] loss: 0.980\n",
            "[10,    20] loss: 0.975\n",
            "[10,    30] loss: 0.964\n",
            "[10,    40] loss: 0.955\n",
            "[11,    10] loss: 0.873\n",
            "[11,    20] loss: 0.876\n",
            "[11,    30] loss: 0.887\n",
            "[11,    40] loss: 0.911\n",
            "[12,    10] loss: 0.832\n",
            "[12,    20] loss: 0.860\n",
            "[12,    30] loss: 0.883\n",
            "[12,    40] loss: 0.862\n",
            "[13,    10] loss: 0.789\n",
            "[13,    20] loss: 0.823\n",
            "[13,    30] loss: 0.834\n",
            "[13,    40] loss: 0.836\n",
            "[14,    10] loss: 0.725\n",
            "[14,    20] loss: 0.748\n",
            "[14,    30] loss: 0.722\n",
            "[14,    40] loss: 0.779\n",
            "[15,    10] loss: 0.649\n",
            "[15,    20] loss: 0.718\n",
            "[15,    30] loss: 0.706\n",
            "[15,    40] loss: 0.774\n",
            "[16,    10] loss: 0.748\n",
            "[16,    20] loss: 0.767\n",
            "[16,    30] loss: 0.769\n",
            "[16,    40] loss: 0.734\n",
            "[17,    10] loss: 0.573\n",
            "[17,    20] loss: 0.590\n",
            "[17,    30] loss: 0.569\n",
            "[17,    40] loss: 0.623\n",
            "[18,    10] loss: 0.548\n",
            "[18,    20] loss: 0.600\n",
            "[18,    30] loss: 0.615\n",
            "[18,    40] loss: 0.657\n",
            "[19,    10] loss: 0.584\n",
            "[19,    20] loss: 0.602\n",
            "[19,    30] loss: 0.607\n",
            "[19,    40] loss: 0.601\n",
            "[20,    10] loss: 0.527\n",
            "[20,    20] loss: 0.589\n",
            "[20,    30] loss: 0.569\n",
            "[20,    40] loss: 0.543\n",
            "[21,    10] loss: 0.417\n",
            "[21,    20] loss: 0.448\n",
            "[21,    30] loss: 0.457\n",
            "[21,    40] loss: 0.564\n",
            "[22,    10] loss: 0.430\n",
            "[22,    20] loss: 0.485\n",
            "[22,    30] loss: 0.445\n",
            "[22,    40] loss: 0.486\n",
            "[23,    10] loss: 0.376\n",
            "[23,    20] loss: 0.409\n",
            "[23,    30] loss: 0.430\n",
            "[23,    40] loss: 0.476\n",
            "[24,    10] loss: 0.310\n",
            "[24,    20] loss: 0.354\n",
            "[24,    30] loss: 0.393\n",
            "[24,    40] loss: 0.422\n",
            "[25,    10] loss: 0.336\n",
            "[25,    20] loss: 0.350\n",
            "[25,    30] loss: 0.364\n",
            "[25,    40] loss: 0.372\n",
            "[26,    10] loss: 0.265\n",
            "[26,    20] loss: 0.292\n",
            "[26,    30] loss: 0.335\n",
            "[26,    40] loss: 0.358\n",
            "[27,    10] loss: 0.252\n",
            "[27,    20] loss: 0.266\n",
            "[27,    30] loss: 0.295\n",
            "[27,    40] loss: 0.381\n",
            "[28,    10] loss: 0.334\n",
            "[28,    20] loss: 0.372\n",
            "[28,    30] loss: 0.352\n",
            "[28,    40] loss: 0.357\n",
            "[29,    10] loss: 0.281\n",
            "[29,    20] loss: 0.318\n",
            "[29,    30] loss: 0.339\n",
            "[29,    40] loss: 0.362\n",
            "[30,    10] loss: 0.277\n",
            "[30,    20] loss: 0.307\n",
            "[30,    30] loss: 0.337\n",
            "[30,    40] loss: 0.360\n",
            "[31,    10] loss: 0.278\n",
            "[31,    20] loss: 0.323\n",
            "[31,    30] loss: 0.319\n",
            "[31,    40] loss: 0.370\n",
            "[32,    10] loss: 0.294\n",
            "[32,    20] loss: 0.319\n",
            "[32,    30] loss: 0.341\n",
            "[32,    40] loss: 0.342\n",
            "[33,    10] loss: 0.233\n",
            "[33,    20] loss: 0.250\n",
            "[33,    30] loss: 0.278\n",
            "[33,    40] loss: 0.296\n",
            "[34,    10] loss: 0.217\n",
            "[34,    20] loss: 0.255\n",
            "[34,    30] loss: 0.289\n",
            "[34,    40] loss: 0.322\n",
            "[35,    10] loss: 0.249\n",
            "[35,    20] loss: 0.303\n",
            "[35,    30] loss: 0.319\n",
            "[35,    40] loss: 0.283\n",
            "[36,    10] loss: 0.216\n",
            "[36,    20] loss: 0.233\n",
            "[36,    30] loss: 0.228\n",
            "[36,    40] loss: 0.247\n",
            "[37,    10] loss: 0.170\n",
            "[37,    20] loss: 0.216\n",
            "[37,    30] loss: 0.223\n",
            "[37,    40] loss: 0.248\n",
            "[38,    10] loss: 0.175\n",
            "[38,    20] loss: 0.184\n",
            "[38,    30] loss: 0.225\n",
            "[38,    40] loss: 0.265\n",
            "[39,    10] loss: 0.218\n",
            "[39,    20] loss: 0.226\n",
            "[39,    30] loss: 0.257\n",
            "[39,    40] loss: 0.251\n",
            "[40,    10] loss: 0.178\n",
            "[40,    20] loss: 0.182\n",
            "[40,    30] loss: 0.203\n",
            "[40,    40] loss: 0.236\n",
            "[41,    10] loss: 0.180\n",
            "[41,    20] loss: 0.176\n",
            "[41,    30] loss: 0.201\n",
            "[41,    40] loss: 0.230\n",
            "[42,    10] loss: 0.161\n",
            "[42,    20] loss: 0.176\n",
            "[42,    30] loss: 0.189\n",
            "[42,    40] loss: 0.198\n",
            "[43,    10] loss: 0.136\n",
            "[43,    20] loss: 0.145\n",
            "[43,    30] loss: 0.174\n",
            "[43,    40] loss: 0.194\n",
            "[44,    10] loss: 0.112\n",
            "[44,    20] loss: 0.158\n",
            "[44,    30] loss: 0.164\n",
            "[44,    40] loss: 0.286\n",
            "[45,    10] loss: 0.226\n",
            "[45,    20] loss: 0.273\n",
            "[45,    30] loss: 0.236\n",
            "[45,    40] loss: 0.251\n",
            "[46,    10] loss: 0.213\n",
            "[46,    20] loss: 0.197\n",
            "[46,    30] loss: 0.262\n",
            "[46,    40] loss: 0.225\n",
            "[47,    10] loss: 0.172\n",
            "[47,    20] loss: 0.211\n",
            "[47,    30] loss: 0.198\n",
            "[47,    40] loss: 0.235\n",
            "[48,    10] loss: 0.184\n",
            "[48,    20] loss: 0.213\n",
            "[48,    30] loss: 0.209\n",
            "[48,    40] loss: 0.223\n",
            "[49,    10] loss: 0.153\n",
            "[49,    20] loss: 0.172\n",
            "[49,    30] loss: 0.178\n",
            "[49,    40] loss: 0.189\n",
            "[50,    10] loss: 0.141\n",
            "[50,    20] loss: 0.168\n",
            "[50,    30] loss: 0.154\n",
            "[50,    40] loss: 0.177\n",
            "[51,    10] loss: 0.107\n",
            "[51,    20] loss: 0.121\n",
            "[51,    30] loss: 0.151\n",
            "[51,    40] loss: 0.166\n",
            "[52,    10] loss: 0.110\n",
            "[52,    20] loss: 0.120\n",
            "[52,    30] loss: 0.133\n",
            "[52,    40] loss: 0.145\n",
            "[53,    10] loss: 0.093\n",
            "[53,    20] loss: 0.113\n",
            "[53,    30] loss: 0.115\n",
            "[53,    40] loss: 0.140\n",
            "[54,    10] loss: 0.085\n",
            "[54,    20] loss: 0.114\n",
            "[54,    30] loss: 0.122\n",
            "[54,    40] loss: 0.139\n",
            "[55,    10] loss: 0.093\n",
            "[55,    20] loss: 0.122\n",
            "[55,    30] loss: 0.148\n",
            "[55,    40] loss: 0.159\n",
            "[56,    10] loss: 0.088\n",
            "[56,    20] loss: 0.109\n",
            "[56,    30] loss: 0.125\n",
            "[56,    40] loss: 0.126\n",
            "[57,    10] loss: 0.074\n",
            "[57,    20] loss: 0.089\n",
            "[57,    30] loss: 0.121\n",
            "[57,    40] loss: 0.122\n",
            "[58,    10] loss: 0.073\n",
            "[58,    20] loss: 0.092\n",
            "[58,    30] loss: 0.110\n",
            "[58,    40] loss: 0.121\n",
            "[59,    10] loss: 0.080\n",
            "[59,    20] loss: 0.108\n",
            "[59,    30] loss: 0.114\n",
            "[59,    40] loss: 0.116\n",
            "[60,    10] loss: 0.083\n",
            "[60,    20] loss: 0.101\n",
            "[60,    30] loss: 0.114\n",
            "[60,    40] loss: 0.153\n",
            "[61,    10] loss: 0.089\n",
            "[61,    20] loss: 0.093\n",
            "[61,    30] loss: 0.115\n",
            "[61,    40] loss: 0.149\n",
            "[62,    10] loss: 0.132\n",
            "[62,    20] loss: 0.171\n",
            "[62,    30] loss: 0.155\n",
            "[62,    40] loss: 0.145\n",
            "[63,    10] loss: 0.103\n",
            "[63,    20] loss: 0.124\n",
            "[63,    30] loss: 0.142\n",
            "[63,    40] loss: 0.137\n",
            "[64,    10] loss: 0.093\n",
            "[64,    20] loss: 0.109\n",
            "[64,    30] loss: 0.113\n",
            "[64,    40] loss: 0.118\n",
            "[65,    10] loss: 0.077\n",
            "[65,    20] loss: 0.099\n",
            "[65,    30] loss: 0.091\n",
            "[65,    40] loss: 0.141\n",
            "[66,    10] loss: 0.146\n",
            "[66,    20] loss: 0.169\n",
            "[66,    30] loss: 0.159\n",
            "[66,    40] loss: 0.214\n",
            "[67,    10] loss: 0.213\n",
            "[67,    20] loss: 0.247\n",
            "[67,    30] loss: 0.263\n",
            "[67,    40] loss: 0.251\n",
            "[68,    10] loss: 0.242\n",
            "[68,    20] loss: 0.247\n",
            "[68,    30] loss: 0.257\n",
            "[68,    40] loss: 0.220\n",
            "[69,    10] loss: 0.159\n",
            "[69,    20] loss: 0.180\n",
            "[69,    30] loss: 0.169\n",
            "[69,    40] loss: 0.198\n",
            "[70,    10] loss: 0.141\n",
            "[70,    20] loss: 0.132\n",
            "[70,    30] loss: 0.139\n",
            "[70,    40] loss: 0.140\n",
            "[71,    10] loss: 0.098\n",
            "[71,    20] loss: 0.102\n",
            "[71,    30] loss: 0.109\n",
            "[71,    40] loss: 0.145\n",
            "[72,    10] loss: 0.130\n",
            "[72,    20] loss: 0.141\n",
            "[72,    30] loss: 0.149\n",
            "[72,    40] loss: 0.192\n",
            "[73,    10] loss: 0.120\n",
            "[73,    20] loss: 0.174\n",
            "[73,    30] loss: 0.151\n",
            "[73,    40] loss: 0.137\n",
            "[74,    10] loss: 0.106\n",
            "[74,    20] loss: 0.106\n",
            "[74,    30] loss: 0.127\n",
            "[74,    40] loss: 0.130\n",
            "[75,    10] loss: 0.089\n",
            "[75,    20] loss: 0.095\n",
            "[75,    30] loss: 0.107\n",
            "[75,    40] loss: 0.113\n",
            "[76,    10] loss: 0.074\n",
            "[76,    20] loss: 0.106\n",
            "[76,    30] loss: 0.109\n",
            "[76,    40] loss: 0.104\n",
            "[77,    10] loss: 0.079\n",
            "[77,    20] loss: 0.087\n",
            "[77,    30] loss: 0.093\n",
            "[77,    40] loss: 0.103\n",
            "[78,    10] loss: 0.068\n",
            "[78,    20] loss: 0.065\n",
            "[78,    30] loss: 0.084\n",
            "[78,    40] loss: 0.098\n",
            "[79,    10] loss: 0.063\n",
            "[79,    20] loss: 0.073\n",
            "[79,    30] loss: 0.075\n",
            "[79,    40] loss: 0.083\n",
            "[80,    10] loss: 0.053\n",
            "[80,    20] loss: 0.064\n",
            "[80,    30] loss: 0.082\n",
            "[80,    40] loss: 0.098\n",
            "[81,    10] loss: 0.063\n",
            "[81,    20] loss: 0.067\n",
            "[81,    30] loss: 0.072\n",
            "[81,    40] loss: 0.098\n",
            "[82,    10] loss: 0.067\n",
            "[82,    20] loss: 0.080\n",
            "[82,    30] loss: 0.103\n",
            "[82,    40] loss: 0.118\n",
            "[83,    10] loss: 0.082\n",
            "[83,    20] loss: 0.095\n",
            "[83,    30] loss: 0.082\n",
            "[83,    40] loss: 0.165\n",
            "[84,    10] loss: 0.150\n",
            "[84,    20] loss: 0.203\n",
            "[84,    30] loss: 0.207\n",
            "[84,    40] loss: 0.216\n",
            "[85,    10] loss: 0.132\n",
            "[85,    20] loss: 0.125\n",
            "[85,    30] loss: 0.136\n",
            "[85,    40] loss: 0.142\n",
            "[86,    10] loss: 0.080\n",
            "[86,    20] loss: 0.098\n",
            "[86,    30] loss: 0.120\n",
            "[86,    40] loss: 0.119\n",
            "[87,    10] loss: 0.068\n",
            "[87,    20] loss: 0.084\n",
            "[87,    30] loss: 0.106\n",
            "[87,    40] loss: 0.183\n",
            "[88,    10] loss: 0.230\n",
            "[88,    20] loss: 0.311\n",
            "[88,    30] loss: 0.241\n",
            "[88,    40] loss: 0.218\n",
            "[89,    10] loss: 0.150\n",
            "[89,    20] loss: 0.146\n",
            "[89,    30] loss: 0.175\n",
            "[89,    40] loss: 0.171\n",
            "[90,    10] loss: 0.112\n",
            "[90,    20] loss: 0.139\n",
            "[90,    30] loss: 0.137\n",
            "[90,    40] loss: 0.151\n",
            "[91,    10] loss: 0.092\n",
            "[91,    20] loss: 0.117\n",
            "[91,    30] loss: 0.134\n",
            "[91,    40] loss: 0.123\n",
            "[92,    10] loss: 0.109\n",
            "[92,    20] loss: 0.125\n",
            "[92,    30] loss: 0.118\n",
            "[92,    40] loss: 0.135\n",
            "[93,    10] loss: 0.126\n",
            "[93,    20] loss: 0.147\n",
            "[93,    30] loss: 0.155\n",
            "[93,    40] loss: 0.150\n",
            "[94,    10] loss: 0.106\n",
            "[94,    20] loss: 0.103\n",
            "[94,    30] loss: 0.146\n",
            "[94,    40] loss: 0.163\n",
            "[95,    10] loss: 0.102\n",
            "[95,    20] loss: 0.130\n",
            "[95,    30] loss: 0.131\n",
            "[95,    40] loss: 0.109\n",
            "[96,    10] loss: 0.082\n",
            "[96,    20] loss: 0.086\n",
            "[96,    30] loss: 0.104\n",
            "[96,    40] loss: 0.120\n",
            "[97,    10] loss: 0.084\n",
            "[97,    20] loss: 0.117\n",
            "[97,    30] loss: 0.109\n",
            "[97,    40] loss: 0.140\n",
            "[98,    10] loss: 0.077\n",
            "[98,    20] loss: 0.089\n",
            "[98,    30] loss: 0.111\n",
            "[98,    40] loss: 0.107\n",
            "[99,    10] loss: 0.065\n",
            "[99,    20] loss: 0.080\n",
            "[99,    30] loss: 0.085\n",
            "[99,    40] loss: 0.122\n",
            "[100,    10] loss: 0.078\n",
            "[100,    20] loss: 0.084\n",
            "[100,    30] loss: 0.089\n",
            "[100,    40] loss: 0.094\n",
            "[101,    10] loss: 0.058\n",
            "[101,    20] loss: 0.075\n",
            "[101,    30] loss: 0.098\n",
            "[101,    40] loss: 0.106\n",
            "[102,    10] loss: 0.072\n",
            "[102,    20] loss: 0.081\n",
            "[102,    30] loss: 0.113\n",
            "[102,    40] loss: 0.089\n",
            "[103,    10] loss: 0.062\n",
            "[103,    20] loss: 0.074\n",
            "[103,    30] loss: 0.089\n",
            "[103,    40] loss: 0.138\n",
            "[104,    10] loss: 0.099\n",
            "[104,    20] loss: 0.134\n",
            "[104,    30] loss: 0.124\n",
            "[104,    40] loss: 0.132\n",
            "[105,    10] loss: 0.086\n",
            "[105,    20] loss: 0.122\n",
            "[105,    30] loss: 0.113\n",
            "[105,    40] loss: 0.125\n",
            "[106,    10] loss: 0.085\n",
            "[106,    20] loss: 0.102\n",
            "[106,    30] loss: 0.125\n",
            "[106,    40] loss: 0.103\n",
            "[107,    10] loss: 0.069\n",
            "[107,    20] loss: 0.089\n",
            "[107,    30] loss: 0.082\n",
            "[107,    40] loss: 0.086\n",
            "[108,    10] loss: 0.053\n",
            "[108,    20] loss: 0.070\n",
            "[108,    30] loss: 0.081\n",
            "[108,    40] loss: 0.112\n",
            "[109,    10] loss: 0.061\n",
            "[109,    20] loss: 0.100\n",
            "[109,    30] loss: 0.124\n",
            "[109,    40] loss: 0.092\n",
            "[110,    10] loss: 0.081\n",
            "[110,    20] loss: 0.075\n",
            "[110,    30] loss: 0.091\n",
            "[110,    40] loss: 0.089\n",
            "[111,    10] loss: 0.054\n",
            "[111,    20] loss: 0.066\n",
            "[111,    30] loss: 0.085\n",
            "[111,    40] loss: 0.177\n",
            "[112,    10] loss: 0.159\n",
            "[112,    20] loss: 0.190\n",
            "[112,    30] loss: 0.176\n",
            "[112,    40] loss: 0.197\n",
            "[113,    10] loss: 0.209\n",
            "[113,    20] loss: 0.237\n",
            "[113,    30] loss: 0.203\n",
            "[113,    40] loss: 0.228\n",
            "[114,    10] loss: 0.189\n",
            "[114,    20] loss: 0.240\n",
            "[114,    30] loss: 0.228\n",
            "[114,    40] loss: 0.172\n",
            "[115,    10] loss: 0.102\n",
            "[115,    20] loss: 0.111\n",
            "[115,    30] loss: 0.125\n",
            "[115,    40] loss: 0.138\n",
            "[116,    10] loss: 0.089\n",
            "[116,    20] loss: 0.104\n",
            "[116,    30] loss: 0.110\n",
            "[116,    40] loss: 0.115\n",
            "[117,    10] loss: 0.089\n",
            "[117,    20] loss: 0.119\n",
            "[117,    30] loss: 0.117\n",
            "[117,    40] loss: 0.113\n",
            "[118,    10] loss: 0.114\n",
            "[118,    20] loss: 0.127\n",
            "[118,    30] loss: 0.117\n",
            "[118,    40] loss: 0.150\n",
            "[119,    10] loss: 0.101\n",
            "[119,    20] loss: 0.123\n",
            "[119,    30] loss: 0.129\n",
            "[119,    40] loss: 0.141\n",
            "[120,    10] loss: 0.085\n",
            "[120,    20] loss: 0.109\n",
            "[120,    30] loss: 0.126\n",
            "[120,    40] loss: 0.130\n",
            "[121,    10] loss: 0.074\n",
            "[121,    20] loss: 0.106\n",
            "[121,    30] loss: 0.082\n",
            "[121,    40] loss: 0.093\n",
            "[122,    10] loss: 0.061\n",
            "[122,    20] loss: 0.084\n",
            "[122,    30] loss: 0.092\n",
            "[122,    40] loss: 0.088\n",
            "[123,    10] loss: 0.061\n",
            "[123,    20] loss: 0.064\n",
            "[123,    30] loss: 0.072\n",
            "[123,    40] loss: 0.095\n",
            "[124,    10] loss: 0.089\n",
            "[124,    20] loss: 0.103\n",
            "[124,    30] loss: 0.100\n",
            "[124,    40] loss: 0.133\n",
            "[125,    10] loss: 0.125\n",
            "[125,    20] loss: 0.135\n",
            "[125,    30] loss: 0.122\n",
            "[125,    40] loss: 0.207\n",
            "[126,    10] loss: 0.171\n",
            "[126,    20] loss: 0.166\n",
            "[126,    30] loss: 0.172\n",
            "[126,    40] loss: 0.155\n",
            "[127,    10] loss: 0.145\n",
            "[127,    20] loss: 0.151\n",
            "[127,    30] loss: 0.154\n",
            "[127,    40] loss: 0.133\n",
            "[128,    10] loss: 0.117\n",
            "[128,    20] loss: 0.115\n",
            "[128,    30] loss: 0.108\n",
            "[128,    40] loss: 0.114\n",
            "[129,    10] loss: 0.077\n",
            "[129,    20] loss: 0.089\n",
            "[129,    30] loss: 0.092\n",
            "[129,    40] loss: 0.082\n",
            "[130,    10] loss: 0.061\n",
            "[130,    20] loss: 0.057\n",
            "[130,    30] loss: 0.061\n",
            "[130,    40] loss: 0.077\n",
            "[131,    10] loss: 0.049\n",
            "[131,    20] loss: 0.056\n",
            "[131,    30] loss: 0.066\n",
            "[131,    40] loss: 0.064\n",
            "[132,    10] loss: 0.046\n",
            "[132,    20] loss: 0.066\n",
            "[132,    30] loss: 0.052\n",
            "[132,    40] loss: 0.077\n",
            "[133,    10] loss: 0.048\n",
            "[133,    20] loss: 0.071\n",
            "[133,    30] loss: 0.073\n",
            "[133,    40] loss: 0.072\n",
            "[134,    10] loss: 0.053\n",
            "[134,    20] loss: 0.067\n",
            "[134,    30] loss: 0.087\n",
            "[134,    40] loss: 0.074\n",
            "[135,    10] loss: 0.050\n",
            "[135,    20] loss: 0.067\n",
            "[135,    30] loss: 0.070\n",
            "[135,    40] loss: 0.107\n",
            "[136,    10] loss: 0.065\n",
            "[136,    20] loss: 0.102\n",
            "[136,    30] loss: 0.117\n",
            "[136,    40] loss: 0.141\n",
            "[137,    10] loss: 0.086\n",
            "[137,    20] loss: 0.128\n",
            "[137,    30] loss: 0.138\n",
            "[137,    40] loss: 0.135\n",
            "[138,    10] loss: 0.093\n",
            "[138,    20] loss: 0.121\n",
            "[138,    30] loss: 0.098\n",
            "[138,    40] loss: 0.131\n",
            "[139,    10] loss: 0.095\n",
            "[139,    20] loss: 0.112\n",
            "[139,    30] loss: 0.131\n",
            "[139,    40] loss: 0.118\n",
            "[140,    10] loss: 0.080\n",
            "[140,    20] loss: 0.089\n",
            "[140,    30] loss: 0.093\n",
            "[140,    40] loss: 0.118\n",
            "[141,    10] loss: 0.104\n",
            "[141,    20] loss: 0.116\n",
            "[141,    30] loss: 0.129\n",
            "[141,    40] loss: 0.157\n",
            "[142,    10] loss: 0.130\n",
            "[142,    20] loss: 0.142\n",
            "[142,    30] loss: 0.129\n",
            "[142,    40] loss: 0.132\n",
            "[143,    10] loss: 0.078\n",
            "[143,    20] loss: 0.085\n",
            "[143,    30] loss: 0.105\n",
            "[143,    40] loss: 0.107\n",
            "[144,    10] loss: 0.065\n",
            "[144,    20] loss: 0.079\n",
            "[144,    30] loss: 0.095\n",
            "[144,    40] loss: 0.091\n",
            "[145,    10] loss: 0.074\n",
            "[145,    20] loss: 0.073\n",
            "[145,    30] loss: 0.083\n",
            "[145,    40] loss: 0.083\n",
            "[146,    10] loss: 0.057\n",
            "[146,    20] loss: 0.058\n",
            "[146,    30] loss: 0.074\n",
            "[146,    40] loss: 0.104\n",
            "[147,    10] loss: 0.158\n",
            "[147,    20] loss: 0.173\n",
            "[147,    30] loss: 0.166\n",
            "[147,    40] loss: 0.160\n",
            "[148,    10] loss: 0.131\n",
            "[148,    20] loss: 0.126\n",
            "[148,    30] loss: 0.126\n",
            "[148,    40] loss: 0.122\n",
            "[149,    10] loss: 0.102\n",
            "[149,    20] loss: 0.102\n",
            "[149,    30] loss: 0.113\n",
            "[149,    40] loss: 0.130\n",
            "[150,    10] loss: 0.082\n",
            "[150,    20] loss: 0.083\n",
            "[150,    30] loss: 0.120\n",
            "[150,    40] loss: 0.100\n",
            "[151,    10] loss: 0.075\n",
            "[151,    20] loss: 0.091\n",
            "[151,    30] loss: 0.082\n",
            "[151,    40] loss: 0.096\n",
            "[152,    10] loss: 0.057\n",
            "[152,    20] loss: 0.070\n",
            "[152,    30] loss: 0.112\n",
            "[152,    40] loss: 0.099\n",
            "[153,    10] loss: 0.073\n",
            "[153,    20] loss: 0.074\n",
            "[153,    30] loss: 0.085\n",
            "[153,    40] loss: 0.098\n",
            "[154,    10] loss: 0.061\n",
            "[154,    20] loss: 0.053\n",
            "[154,    30] loss: 0.066\n",
            "[154,    40] loss: 0.072\n",
            "[155,    10] loss: 0.059\n",
            "[155,    20] loss: 0.063\n",
            "[155,    30] loss: 0.075\n",
            "[155,    40] loss: 0.079\n",
            "[156,    10] loss: 0.051\n",
            "[156,    20] loss: 0.076\n",
            "[156,    30] loss: 0.075\n",
            "[156,    40] loss: 0.073\n",
            "[157,    10] loss: 0.051\n",
            "[157,    20] loss: 0.055\n",
            "[157,    30] loss: 0.059\n",
            "[157,    40] loss: 0.061\n",
            "[158,    10] loss: 0.039\n",
            "[158,    20] loss: 0.056\n",
            "[158,    30] loss: 0.048\n",
            "[158,    40] loss: 0.065\n",
            "[159,    10] loss: 0.045\n",
            "[159,    20] loss: 0.064\n",
            "[159,    30] loss: 0.069\n",
            "[159,    40] loss: 0.076\n",
            "[160,    10] loss: 0.048\n",
            "[160,    20] loss: 0.086\n",
            "[160,    30] loss: 0.077\n",
            "[160,    40] loss: 0.078\n",
            "[161,    10] loss: 0.048\n",
            "[161,    20] loss: 0.053\n",
            "[161,    30] loss: 0.055\n",
            "[161,    40] loss: 0.067\n",
            "[162,    10] loss: 0.035\n",
            "[162,    20] loss: 0.043\n",
            "[162,    30] loss: 0.061\n",
            "[162,    40] loss: 0.085\n",
            "[163,    10] loss: 0.070\n",
            "[163,    20] loss: 0.076\n",
            "[163,    30] loss: 0.085\n",
            "[163,    40] loss: 0.124\n",
            "[164,    10] loss: 0.178\n",
            "[164,    20] loss: 0.222\n",
            "[164,    30] loss: 0.177\n",
            "[164,    40] loss: 0.169\n",
            "[165,    10] loss: 0.110\n",
            "[165,    20] loss: 0.139\n",
            "[165,    30] loss: 0.126\n",
            "[165,    40] loss: 0.092\n",
            "[166,    10] loss: 0.061\n",
            "[166,    20] loss: 0.078\n",
            "[166,    30] loss: 0.080\n",
            "[166,    40] loss: 0.105\n",
            "[167,    10] loss: 0.090\n",
            "[167,    20] loss: 0.096\n",
            "[167,    30] loss: 0.120\n",
            "[167,    40] loss: 0.121\n",
            "[168,    10] loss: 0.153\n",
            "[168,    20] loss: 0.175\n",
            "[168,    30] loss: 0.149\n",
            "[168,    40] loss: 0.134\n",
            "[169,    10] loss: 0.099\n",
            "[169,    20] loss: 0.101\n",
            "[169,    30] loss: 0.092\n",
            "[169,    40] loss: 0.105\n",
            "[170,    10] loss: 0.061\n",
            "[170,    20] loss: 0.081\n",
            "[170,    30] loss: 0.075\n",
            "[170,    40] loss: 0.094\n",
            "[171,    10] loss: 0.085\n",
            "[171,    20] loss: 0.074\n",
            "[171,    30] loss: 0.110\n",
            "[171,    40] loss: 0.094\n",
            "[172,    10] loss: 0.059\n",
            "[172,    20] loss: 0.077\n",
            "[172,    30] loss: 0.084\n",
            "[172,    40] loss: 0.091\n",
            "[173,    10] loss: 0.047\n",
            "[173,    20] loss: 0.068\n",
            "[173,    30] loss: 0.072\n",
            "[173,    40] loss: 0.066\n",
            "[174,    10] loss: 0.038\n",
            "[174,    20] loss: 0.060\n",
            "[174,    30] loss: 0.059\n",
            "[174,    40] loss: 0.082\n",
            "[175,    10] loss: 0.041\n",
            "[175,    20] loss: 0.063\n",
            "[175,    30] loss: 0.064\n",
            "[175,    40] loss: 0.071\n",
            "[176,    10] loss: 0.053\n",
            "[176,    20] loss: 0.064\n",
            "[176,    30] loss: 0.062\n",
            "[176,    40] loss: 0.081\n",
            "[177,    10] loss: 0.062\n",
            "[177,    20] loss: 0.072\n",
            "[177,    30] loss: 0.068\n",
            "[177,    40] loss: 0.075\n",
            "[178,    10] loss: 0.043\n",
            "[178,    20] loss: 0.050\n",
            "[178,    30] loss: 0.068\n",
            "[178,    40] loss: 0.073\n",
            "[179,    10] loss: 0.045\n",
            "[179,    20] loss: 0.054\n",
            "[179,    30] loss: 0.052\n",
            "[179,    40] loss: 0.045\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 36 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 36 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 40 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 43 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 42 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 1.113\n",
            "[1,    20] loss: 1.103\n",
            "[1,    30] loss: 1.104\n",
            "[1,    40] loss: 1.096\n",
            "[2,    10] loss: 1.093\n",
            "[2,    20] loss: 1.095\n",
            "[2,    30] loss: 1.090\n",
            "[2,    40] loss: 1.094\n",
            "[3,    10] loss: 1.078\n",
            "[3,    20] loss: 1.082\n",
            "[3,    30] loss: 1.077\n",
            "[3,    40] loss: 1.082\n",
            "[4,    10] loss: 1.064\n",
            "[4,    20] loss: 1.063\n",
            "[4,    30] loss: 1.058\n",
            "[4,    40] loss: 1.073\n",
            "[5,    10] loss: 1.058\n",
            "[5,    20] loss: 1.047\n",
            "[5,    30] loss: 1.045\n",
            "[5,    40] loss: 1.039\n",
            "[6,    10] loss: 1.026\n",
            "[6,    20] loss: 1.035\n",
            "[6,    30] loss: 1.031\n",
            "[6,    40] loss: 1.034\n",
            "[7,    10] loss: 1.006\n",
            "[7,    20] loss: 1.022\n",
            "[7,    30] loss: 1.019\n",
            "[7,    40] loss: 1.023\n",
            "[8,    10] loss: 0.976\n",
            "[8,    20] loss: 0.971\n",
            "[8,    30] loss: 0.966\n",
            "[8,    40] loss: 0.952\n",
            "[9,    10] loss: 0.951\n",
            "[9,    20] loss: 0.944\n",
            "[9,    30] loss: 0.975\n",
            "[9,    40] loss: 0.956\n",
            "[10,    10] loss: 0.926\n",
            "[10,    20] loss: 0.928\n",
            "[10,    30] loss: 0.924\n",
            "[10,    40] loss: 0.926\n",
            "[11,    10] loss: 0.897\n",
            "[11,    20] loss: 0.905\n",
            "[11,    30] loss: 0.905\n",
            "[11,    40] loss: 0.944\n",
            "[12,    10] loss: 0.906\n",
            "[12,    20] loss: 0.905\n",
            "[12,    30] loss: 0.891\n",
            "[12,    40] loss: 0.877\n",
            "[13,    10] loss: 0.769\n",
            "[13,    20] loss: 0.784\n",
            "[13,    30] loss: 0.775\n",
            "[13,    40] loss: 0.809\n",
            "[14,    10] loss: 0.809\n",
            "[14,    20] loss: 0.844\n",
            "[14,    30] loss: 0.874\n",
            "[14,    40] loss: 0.866\n",
            "[15,    10] loss: 0.767\n",
            "[15,    20] loss: 0.720\n",
            "[15,    30] loss: 0.723\n",
            "[15,    40] loss: 0.699\n",
            "[16,    10] loss: 0.606\n",
            "[16,    20] loss: 0.652\n",
            "[16,    30] loss: 0.645\n",
            "[16,    40] loss: 0.648\n",
            "[17,    10] loss: 0.513\n",
            "[17,    20] loss: 0.563\n",
            "[17,    30] loss: 0.620\n",
            "[17,    40] loss: 0.623\n",
            "[18,    10] loss: 0.532\n",
            "[18,    20] loss: 0.532\n",
            "[18,    30] loss: 0.548\n",
            "[18,    40] loss: 0.631\n",
            "[19,    10] loss: 0.644\n",
            "[19,    20] loss: 0.687\n",
            "[19,    30] loss: 0.662\n",
            "[19,    40] loss: 0.659\n",
            "[20,    10] loss: 0.529\n",
            "[20,    20] loss: 0.510\n",
            "[20,    30] loss: 0.542\n",
            "[20,    40] loss: 0.555\n",
            "[21,    10] loss: 0.483\n",
            "[21,    20] loss: 0.500\n",
            "[21,    30] loss: 0.506\n",
            "[21,    40] loss: 0.508\n",
            "[22,    10] loss: 0.381\n",
            "[22,    20] loss: 0.410\n",
            "[22,    30] loss: 0.443\n",
            "[22,    40] loss: 0.544\n",
            "[23,    10] loss: 0.483\n",
            "[23,    20] loss: 0.503\n",
            "[23,    30] loss: 0.488\n",
            "[23,    40] loss: 0.522\n",
            "[24,    10] loss: 0.435\n",
            "[24,    20] loss: 0.400\n",
            "[24,    30] loss: 0.444\n",
            "[24,    40] loss: 0.523\n",
            "[25,    10] loss: 0.463\n",
            "[25,    20] loss: 0.461\n",
            "[25,    30] loss: 0.479\n",
            "[25,    40] loss: 0.464\n",
            "[26,    10] loss: 0.347\n",
            "[26,    20] loss: 0.401\n",
            "[26,    30] loss: 0.430\n",
            "[26,    40] loss: 0.413\n",
            "[27,    10] loss: 0.349\n",
            "[27,    20] loss: 0.344\n",
            "[27,    30] loss: 0.377\n",
            "[27,    40] loss: 0.423\n",
            "[28,    10] loss: 0.318\n",
            "[28,    20] loss: 0.329\n",
            "[28,    30] loss: 0.358\n",
            "[28,    40] loss: 0.420\n",
            "[29,    10] loss: 0.346\n",
            "[29,    20] loss: 0.418\n",
            "[29,    30] loss: 0.400\n",
            "[29,    40] loss: 0.433\n",
            "[30,    10] loss: 0.363\n",
            "[30,    20] loss: 0.402\n",
            "[30,    30] loss: 0.384\n",
            "[30,    40] loss: 0.447\n",
            "[31,    10] loss: 0.366\n",
            "[31,    20] loss: 0.371\n",
            "[31,    30] loss: 0.365\n",
            "[31,    40] loss: 0.410\n",
            "[32,    10] loss: 0.314\n",
            "[32,    20] loss: 0.328\n",
            "[32,    30] loss: 0.326\n",
            "[32,    40] loss: 0.372\n",
            "[33,    10] loss: 0.288\n",
            "[33,    20] loss: 0.299\n",
            "[33,    30] loss: 0.311\n",
            "[33,    40] loss: 0.370\n",
            "[34,    10] loss: 0.297\n",
            "[34,    20] loss: 0.314\n",
            "[34,    30] loss: 0.310\n",
            "[34,    40] loss: 0.363\n",
            "[35,    10] loss: 0.282\n",
            "[35,    20] loss: 0.305\n",
            "[35,    30] loss: 0.285\n",
            "[35,    40] loss: 0.349\n",
            "[36,    10] loss: 0.237\n",
            "[36,    20] loss: 0.260\n",
            "[36,    30] loss: 0.314\n",
            "[36,    40] loss: 0.323\n",
            "[37,    10] loss: 0.222\n",
            "[37,    20] loss: 0.234\n",
            "[37,    30] loss: 0.283\n",
            "[37,    40] loss: 0.291\n",
            "[38,    10] loss: 0.220\n",
            "[38,    20] loss: 0.244\n",
            "[38,    30] loss: 0.280\n",
            "[38,    40] loss: 0.269\n",
            "[39,    10] loss: 0.201\n",
            "[39,    20] loss: 0.234\n",
            "[39,    30] loss: 0.217\n",
            "[39,    40] loss: 0.264\n",
            "[40,    10] loss: 0.181\n",
            "[40,    20] loss: 0.194\n",
            "[40,    30] loss: 0.221\n",
            "[40,    40] loss: 0.262\n",
            "[41,    10] loss: 0.220\n",
            "[41,    20] loss: 0.235\n",
            "[41,    30] loss: 0.263\n",
            "[41,    40] loss: 0.247\n",
            "[42,    10] loss: 0.176\n",
            "[42,    20] loss: 0.189\n",
            "[42,    30] loss: 0.226\n",
            "[42,    40] loss: 0.269\n",
            "[43,    10] loss: 0.171\n",
            "[43,    20] loss: 0.200\n",
            "[43,    30] loss: 0.219\n",
            "[43,    40] loss: 0.272\n",
            "[44,    10] loss: 0.184\n",
            "[44,    20] loss: 0.223\n",
            "[44,    30] loss: 0.248\n",
            "[44,    40] loss: 0.283\n",
            "[45,    10] loss: 0.180\n",
            "[45,    20] loss: 0.207\n",
            "[45,    30] loss: 0.212\n",
            "[45,    40] loss: 0.218\n",
            "[46,    10] loss: 0.166\n",
            "[46,    20] loss: 0.166\n",
            "[46,    30] loss: 0.191\n",
            "[46,    40] loss: 0.231\n",
            "[47,    10] loss: 0.169\n",
            "[47,    20] loss: 0.175\n",
            "[47,    30] loss: 0.206\n",
            "[47,    40] loss: 0.174\n",
            "[48,    10] loss: 0.146\n",
            "[48,    20] loss: 0.173\n",
            "[48,    30] loss: 0.180\n",
            "[48,    40] loss: 0.157\n",
            "[49,    10] loss: 0.139\n",
            "[49,    20] loss: 0.137\n",
            "[49,    30] loss: 0.155\n",
            "[49,    40] loss: 0.167\n",
            "[50,    10] loss: 0.121\n",
            "[50,    20] loss: 0.142\n",
            "[50,    30] loss: 0.144\n",
            "[50,    40] loss: 0.173\n",
            "[51,    10] loss: 0.122\n",
            "[51,    20] loss: 0.130\n",
            "[51,    30] loss: 0.156\n",
            "[51,    40] loss: 0.174\n",
            "[52,    10] loss: 0.137\n",
            "[52,    20] loss: 0.156\n",
            "[52,    30] loss: 0.168\n",
            "[52,    40] loss: 0.160\n",
            "[53,    10] loss: 0.131\n",
            "[53,    20] loss: 0.130\n",
            "[53,    30] loss: 0.143\n",
            "[53,    40] loss: 0.161\n",
            "[54,    10] loss: 0.134\n",
            "[54,    20] loss: 0.122\n",
            "[54,    30] loss: 0.146\n",
            "[54,    40] loss: 0.181\n",
            "[55,    10] loss: 0.142\n",
            "[55,    20] loss: 0.153\n",
            "[55,    30] loss: 0.152\n",
            "[55,    40] loss: 0.153\n",
            "[56,    10] loss: 0.128\n",
            "[56,    20] loss: 0.127\n",
            "[56,    30] loss: 0.134\n",
            "[56,    40] loss: 0.132\n",
            "[57,    10] loss: 0.114\n",
            "[57,    20] loss: 0.126\n",
            "[57,    30] loss: 0.133\n",
            "[57,    40] loss: 0.136\n",
            "[58,    10] loss: 0.105\n",
            "[58,    20] loss: 0.124\n",
            "[58,    30] loss: 0.130\n",
            "[58,    40] loss: 0.122\n",
            "[59,    10] loss: 0.112\n",
            "[59,    20] loss: 0.115\n",
            "[59,    30] loss: 0.116\n",
            "[59,    40] loss: 0.138\n",
            "[60,    10] loss: 0.112\n",
            "[60,    20] loss: 0.112\n",
            "[60,    30] loss: 0.128\n",
            "[60,    40] loss: 0.141\n",
            "[61,    10] loss: 0.120\n",
            "[61,    20] loss: 0.126\n",
            "[61,    30] loss: 0.132\n",
            "[61,    40] loss: 0.128\n",
            "[62,    10] loss: 0.107\n",
            "[62,    20] loss: 0.112\n",
            "[62,    30] loss: 0.117\n",
            "[62,    40] loss: 0.144\n",
            "[63,    10] loss: 0.114\n",
            "[63,    20] loss: 0.124\n",
            "[63,    30] loss: 0.133\n",
            "[63,    40] loss: 0.166\n",
            "[64,    10] loss: 0.130\n",
            "[64,    20] loss: 0.117\n",
            "[64,    30] loss: 0.124\n",
            "[64,    40] loss: 0.143\n",
            "[65,    10] loss: 0.115\n",
            "[65,    20] loss: 0.124\n",
            "[65,    30] loss: 0.127\n",
            "[65,    40] loss: 0.131\n",
            "[66,    10] loss: 0.110\n",
            "[66,    20] loss: 0.117\n",
            "[66,    30] loss: 0.122\n",
            "[66,    40] loss: 0.110\n",
            "[67,    10] loss: 0.108\n",
            "[67,    20] loss: 0.108\n",
            "[67,    30] loss: 0.116\n",
            "[67,    40] loss: 0.113\n",
            "[68,    10] loss: 0.098\n",
            "[68,    20] loss: 0.113\n",
            "[68,    30] loss: 0.115\n",
            "[68,    40] loss: 0.115\n",
            "[69,    10] loss: 0.105\n",
            "[69,    20] loss: 0.104\n",
            "[69,    30] loss: 0.112\n",
            "[69,    40] loss: 0.116\n",
            "[70,    10] loss: 0.111\n",
            "[70,    20] loss: 0.105\n",
            "[70,    30] loss: 0.107\n",
            "[70,    40] loss: 0.110\n",
            "[71,    10] loss: 0.107\n",
            "[71,    20] loss: 0.102\n",
            "[71,    30] loss: 0.117\n",
            "[71,    40] loss: 0.103\n",
            "[72,    10] loss: 0.104\n",
            "[72,    20] loss: 0.108\n",
            "[72,    30] loss: 0.110\n",
            "[72,    40] loss: 0.113\n",
            "[73,    10] loss: 0.102\n",
            "[73,    20] loss: 0.109\n",
            "[73,    30] loss: 0.105\n",
            "[73,    40] loss: 0.166\n",
            "[74,    10] loss: 0.207\n",
            "[74,    20] loss: 0.254\n",
            "[74,    30] loss: 0.204\n",
            "[74,    40] loss: 0.185\n",
            "[75,    10] loss: 0.145\n",
            "[75,    20] loss: 0.162\n",
            "[75,    30] loss: 0.152\n",
            "[75,    40] loss: 0.153\n",
            "[76,    10] loss: 0.121\n",
            "[76,    20] loss: 0.124\n",
            "[76,    30] loss: 0.130\n",
            "[76,    40] loss: 0.140\n",
            "[77,    10] loss: 0.115\n",
            "[77,    20] loss: 0.124\n",
            "[77,    30] loss: 0.127\n",
            "[77,    40] loss: 0.135\n",
            "[78,    10] loss: 0.109\n",
            "[78,    20] loss: 0.111\n",
            "[78,    30] loss: 0.111\n",
            "[78,    40] loss: 0.141\n",
            "[79,    10] loss: 0.105\n",
            "[79,    20] loss: 0.117\n",
            "[79,    30] loss: 0.110\n",
            "[79,    40] loss: 0.134\n",
            "[80,    10] loss: 0.107\n",
            "[80,    20] loss: 0.103\n",
            "[80,    30] loss: 0.124\n",
            "[80,    40] loss: 0.128\n",
            "[81,    10] loss: 0.101\n",
            "[81,    20] loss: 0.115\n",
            "[81,    30] loss: 0.112\n",
            "[81,    40] loss: 0.132\n",
            "[82,    10] loss: 0.106\n",
            "[82,    20] loss: 0.111\n",
            "[82,    30] loss: 0.114\n",
            "[82,    40] loss: 0.124\n",
            "[83,    10] loss: 0.103\n",
            "[83,    20] loss: 0.116\n",
            "[83,    30] loss: 0.109\n",
            "[83,    40] loss: 0.109\n",
            "[84,    10] loss: 0.114\n",
            "[84,    20] loss: 0.096\n",
            "[84,    30] loss: 0.111\n",
            "[84,    40] loss: 0.128\n",
            "[85,    10] loss: 0.107\n",
            "[85,    20] loss: 0.113\n",
            "[85,    30] loss: 0.105\n",
            "[85,    40] loss: 0.115\n",
            "[86,    10] loss: 0.111\n",
            "[86,    20] loss: 0.103\n",
            "[86,    30] loss: 0.104\n",
            "[86,    40] loss: 0.114\n",
            "[87,    10] loss: 0.105\n",
            "[87,    20] loss: 0.112\n",
            "[87,    30] loss: 0.114\n",
            "[87,    40] loss: 0.091\n",
            "[88,    10] loss: 0.100\n",
            "[88,    20] loss: 0.108\n",
            "[88,    30] loss: 0.116\n",
            "[88,    40] loss: 0.097\n",
            "[89,    10] loss: 0.108\n",
            "[89,    20] loss: 0.108\n",
            "[89,    30] loss: 0.107\n",
            "[89,    40] loss: 0.103\n",
            "[90,    10] loss: 0.107\n",
            "[90,    20] loss: 0.111\n",
            "[90,    30] loss: 0.110\n",
            "[90,    40] loss: 0.101\n",
            "[91,    10] loss: 0.108\n",
            "[91,    20] loss: 0.110\n",
            "[91,    30] loss: 0.104\n",
            "[91,    40] loss: 0.116\n",
            "[92,    10] loss: 0.118\n",
            "[92,    20] loss: 0.122\n",
            "[92,    30] loss: 0.113\n",
            "[92,    40] loss: 0.120\n",
            "[93,    10] loss: 0.115\n",
            "[93,    20] loss: 0.107\n",
            "[93,    30] loss: 0.111\n",
            "[93,    40] loss: 0.115\n",
            "[94,    10] loss: 0.120\n",
            "[94,    20] loss: 0.119\n",
            "[94,    30] loss: 0.125\n",
            "[94,    40] loss: 0.129\n",
            "[95,    10] loss: 0.117\n",
            "[95,    20] loss: 0.123\n",
            "[95,    30] loss: 0.125\n",
            "[95,    40] loss: 0.124\n",
            "[96,    10] loss: 0.110\n",
            "[96,    20] loss: 0.113\n",
            "[96,    30] loss: 0.117\n",
            "[96,    40] loss: 0.100\n",
            "[97,    10] loss: 0.101\n",
            "[97,    20] loss: 0.105\n",
            "[97,    30] loss: 0.112\n",
            "[97,    40] loss: 0.115\n",
            "[98,    10] loss: 0.100\n",
            "[98,    20] loss: 0.112\n",
            "[98,    30] loss: 0.113\n",
            "[98,    40] loss: 0.109\n",
            "[99,    10] loss: 0.102\n",
            "[99,    20] loss: 0.110\n",
            "[99,    30] loss: 0.103\n",
            "[99,    40] loss: 0.115\n",
            "[100,    10] loss: 0.102\n",
            "[100,    20] loss: 0.113\n",
            "[100,    30] loss: 0.104\n",
            "[100,    40] loss: 0.116\n",
            "[101,    10] loss: 0.106\n",
            "[101,    20] loss: 0.109\n",
            "[101,    30] loss: 0.121\n",
            "[101,    40] loss: 0.130\n",
            "[102,    10] loss: 0.101\n",
            "[102,    20] loss: 0.111\n",
            "[102,    30] loss: 0.112\n",
            "[102,    40] loss: 0.115\n",
            "[103,    10] loss: 0.103\n",
            "[103,    20] loss: 0.110\n",
            "[103,    30] loss: 0.107\n",
            "[103,    40] loss: 0.102\n",
            "[104,    10] loss: 0.114\n",
            "[104,    20] loss: 0.121\n",
            "[104,    30] loss: 0.108\n",
            "[104,    40] loss: 0.106\n",
            "[105,    10] loss: 0.112\n",
            "[105,    20] loss: 0.100\n",
            "[105,    30] loss: 0.107\n",
            "[105,    40] loss: 0.122\n",
            "[106,    10] loss: 0.129\n",
            "[106,    20] loss: 0.168\n",
            "[106,    30] loss: 0.155\n",
            "[106,    40] loss: 0.168\n",
            "[107,    10] loss: 0.138\n",
            "[107,    20] loss: 0.133\n",
            "[107,    30] loss: 0.148\n",
            "[107,    40] loss: 0.156\n",
            "[108,    10] loss: 0.169\n",
            "[108,    20] loss: 0.175\n",
            "[108,    30] loss: 0.154\n",
            "[108,    40] loss: 0.145\n",
            "[109,    10] loss: 0.132\n",
            "[109,    20] loss: 0.134\n",
            "[109,    30] loss: 0.142\n",
            "[109,    40] loss: 0.118\n",
            "[110,    10] loss: 0.106\n",
            "[110,    20] loss: 0.110\n",
            "[110,    30] loss: 0.115\n",
            "[110,    40] loss: 0.125\n",
            "[111,    10] loss: 0.116\n",
            "[111,    20] loss: 0.130\n",
            "[111,    30] loss: 0.109\n",
            "[111,    40] loss: 0.144\n",
            "[112,    10] loss: 0.113\n",
            "[112,    20] loss: 0.100\n",
            "[112,    30] loss: 0.124\n",
            "[112,    40] loss: 0.122\n",
            "[113,    10] loss: 0.112\n",
            "[113,    20] loss: 0.109\n",
            "[113,    30] loss: 0.111\n",
            "[113,    40] loss: 0.098\n",
            "[114,    10] loss: 0.102\n",
            "[114,    20] loss: 0.111\n",
            "[114,    30] loss: 0.110\n",
            "[114,    40] loss: 0.115\n",
            "[115,    10] loss: 0.098\n",
            "[115,    20] loss: 0.105\n",
            "[115,    30] loss: 0.114\n",
            "[115,    40] loss: 0.112\n",
            "[116,    10] loss: 0.101\n",
            "[116,    20] loss: 0.109\n",
            "[116,    30] loss: 0.106\n",
            "[116,    40] loss: 0.117\n",
            "[117,    10] loss: 0.105\n",
            "[117,    20] loss: 0.110\n",
            "[117,    30] loss: 0.109\n",
            "[117,    40] loss: 0.117\n",
            "[118,    10] loss: 0.107\n",
            "[118,    20] loss: 0.103\n",
            "[118,    30] loss: 0.112\n",
            "[118,    40] loss: 0.116\n",
            "[119,    10] loss: 0.107\n",
            "[119,    20] loss: 0.103\n",
            "[119,    30] loss: 0.110\n",
            "[119,    40] loss: 0.115\n",
            "[120,    10] loss: 0.101\n",
            "[120,    20] loss: 0.106\n",
            "[120,    30] loss: 0.123\n",
            "[120,    40] loss: 0.121\n",
            "[121,    10] loss: 0.119\n",
            "[121,    20] loss: 0.106\n",
            "[121,    30] loss: 0.109\n",
            "[121,    40] loss: 0.095\n",
            "[122,    10] loss: 0.103\n",
            "[122,    20] loss: 0.096\n",
            "[122,    30] loss: 0.115\n",
            "[122,    40] loss: 0.102\n",
            "[123,    10] loss: 0.102\n",
            "[123,    20] loss: 0.106\n",
            "[123,    30] loss: 0.098\n",
            "[123,    40] loss: 0.116\n",
            "[124,    10] loss: 0.103\n",
            "[124,    20] loss: 0.110\n",
            "[124,    30] loss: 0.104\n",
            "[124,    40] loss: 0.111\n",
            "[125,    10] loss: 0.100\n",
            "[125,    20] loss: 0.106\n",
            "[125,    30] loss: 0.110\n",
            "[125,    40] loss: 0.117\n",
            "[126,    10] loss: 0.110\n",
            "[126,    20] loss: 0.106\n",
            "[126,    30] loss: 0.109\n",
            "[126,    40] loss: 0.108\n",
            "[127,    10] loss: 0.098\n",
            "[127,    20] loss: 0.101\n",
            "[127,    30] loss: 0.116\n",
            "[127,    40] loss: 0.100\n",
            "[128,    10] loss: 0.099\n",
            "[128,    20] loss: 0.110\n",
            "[128,    30] loss: 0.109\n",
            "[128,    40] loss: 0.100\n",
            "[129,    10] loss: 0.100\n",
            "[129,    20] loss: 0.110\n",
            "[129,    30] loss: 0.105\n",
            "[129,    40] loss: 0.100\n",
            "[130,    10] loss: 0.107\n",
            "[130,    20] loss: 0.101\n",
            "[130,    30] loss: 0.101\n",
            "[130,    40] loss: 0.116\n",
            "[131,    10] loss: 0.106\n",
            "[131,    20] loss: 0.110\n",
            "[131,    30] loss: 0.105\n",
            "[131,    40] loss: 0.112\n",
            "[132,    10] loss: 0.107\n",
            "[132,    20] loss: 0.106\n",
            "[132,    30] loss: 0.102\n",
            "[132,    40] loss: 0.097\n",
            "[133,    10] loss: 0.100\n",
            "[133,    20] loss: 0.112\n",
            "[133,    30] loss: 0.104\n",
            "[133,    40] loss: 0.102\n",
            "[134,    10] loss: 0.101\n",
            "[134,    20] loss: 0.100\n",
            "[134,    30] loss: 0.111\n",
            "[134,    40] loss: 0.120\n",
            "[135,    10] loss: 0.105\n",
            "[135,    20] loss: 0.108\n",
            "[135,    30] loss: 0.106\n",
            "[135,    40] loss: 0.122\n",
            "[136,    10] loss: 0.126\n",
            "[136,    20] loss: 0.130\n",
            "[136,    30] loss: 0.124\n",
            "[136,    40] loss: 0.140\n",
            "[137,    10] loss: 0.244\n",
            "[137,    20] loss: 0.303\n",
            "[137,    30] loss: 0.289\n",
            "[137,    40] loss: 0.257\n",
            "[138,    10] loss: 0.184\n",
            "[138,    20] loss: 0.173\n",
            "[138,    30] loss: 0.168\n",
            "[138,    40] loss: 0.193\n",
            "[139,    10] loss: 0.134\n",
            "[139,    20] loss: 0.146\n",
            "[139,    30] loss: 0.138\n",
            "[139,    40] loss: 0.143\n",
            "[140,    10] loss: 0.119\n",
            "[140,    20] loss: 0.117\n",
            "[140,    30] loss: 0.121\n",
            "[140,    40] loss: 0.139\n",
            "[141,    10] loss: 0.291\n",
            "[141,    20] loss: 0.389\n",
            "[141,    30] loss: 0.320\n",
            "[141,    40] loss: 0.307\n",
            "[142,    10] loss: 0.204\n",
            "[142,    20] loss: 0.213\n",
            "[142,    30] loss: 0.184\n",
            "[142,    40] loss: 0.237\n",
            "[143,    10] loss: 0.206\n",
            "[143,    20] loss: 0.231\n",
            "[143,    30] loss: 0.225\n",
            "[143,    40] loss: 0.194\n",
            "[144,    10] loss: 0.171\n",
            "[144,    20] loss: 0.180\n",
            "[144,    30] loss: 0.161\n",
            "[144,    40] loss: 0.189\n",
            "[145,    10] loss: 0.128\n",
            "[145,    20] loss: 0.155\n",
            "[145,    30] loss: 0.141\n",
            "[145,    40] loss: 0.162\n",
            "[146,    10] loss: 0.124\n",
            "[146,    20] loss: 0.143\n",
            "[146,    30] loss: 0.146\n",
            "[146,    40] loss: 0.142\n",
            "[147,    10] loss: 0.119\n",
            "[147,    20] loss: 0.119\n",
            "[147,    30] loss: 0.126\n",
            "[147,    40] loss: 0.134\n",
            "[148,    10] loss: 0.103\n",
            "[148,    20] loss: 0.109\n",
            "[148,    30] loss: 0.114\n",
            "[148,    40] loss: 0.131\n",
            "[149,    10] loss: 0.108\n",
            "[149,    20] loss: 0.110\n",
            "[149,    30] loss: 0.107\n",
            "[149,    40] loss: 0.127\n",
            "[150,    10] loss: 0.144\n",
            "[150,    20] loss: 0.151\n",
            "[150,    30] loss: 0.150\n",
            "[150,    40] loss: 0.141\n",
            "[151,    10] loss: 0.126\n",
            "[151,    20] loss: 0.133\n",
            "[151,    30] loss: 0.131\n",
            "[151,    40] loss: 0.153\n",
            "[152,    10] loss: 0.173\n",
            "[152,    20] loss: 0.171\n",
            "[152,    30] loss: 0.166\n",
            "[152,    40] loss: 0.169\n",
            "[153,    10] loss: 0.132\n",
            "[153,    20] loss: 0.137\n",
            "[153,    30] loss: 0.156\n",
            "[153,    40] loss: 0.135\n",
            "[154,    10] loss: 0.118\n",
            "[154,    20] loss: 0.120\n",
            "[154,    30] loss: 0.118\n",
            "[154,    40] loss: 0.118\n",
            "[155,    10] loss: 0.106\n",
            "[155,    20] loss: 0.109\n",
            "[155,    30] loss: 0.112\n",
            "[155,    40] loss: 0.110\n",
            "[156,    10] loss: 0.104\n",
            "[156,    20] loss: 0.110\n",
            "[156,    30] loss: 0.106\n",
            "[156,    40] loss: 0.109\n",
            "[157,    10] loss: 0.111\n",
            "[157,    20] loss: 0.099\n",
            "[157,    30] loss: 0.104\n",
            "[157,    40] loss: 0.109\n",
            "[158,    10] loss: 0.108\n",
            "[158,    20] loss: 0.109\n",
            "[158,    30] loss: 0.099\n",
            "[158,    40] loss: 0.126\n",
            "[159,    10] loss: 0.105\n",
            "[159,    20] loss: 0.119\n",
            "[159,    30] loss: 0.112\n",
            "[159,    40] loss: 0.106\n",
            "[160,    10] loss: 0.106\n",
            "[160,    20] loss: 0.108\n",
            "[160,    30] loss: 0.107\n",
            "[160,    40] loss: 0.105\n",
            "[161,    10] loss: 0.110\n",
            "[161,    20] loss: 0.098\n",
            "[161,    30] loss: 0.106\n",
            "[161,    40] loss: 0.119\n",
            "[162,    10] loss: 0.108\n",
            "[162,    20] loss: 0.106\n",
            "[162,    30] loss: 0.108\n",
            "[162,    40] loss: 0.107\n",
            "[163,    10] loss: 0.100\n",
            "[163,    20] loss: 0.108\n",
            "[163,    30] loss: 0.102\n",
            "[163,    40] loss: 0.113\n",
            "[164,    10] loss: 0.107\n",
            "[164,    20] loss: 0.106\n",
            "[164,    30] loss: 0.111\n",
            "[164,    40] loss: 0.125\n",
            "[165,    10] loss: 0.101\n",
            "[165,    20] loss: 0.126\n",
            "[165,    30] loss: 0.108\n",
            "[165,    40] loss: 0.135\n",
            "[166,    10] loss: 0.121\n",
            "[166,    20] loss: 0.126\n",
            "[166,    30] loss: 0.112\n",
            "[166,    40] loss: 0.123\n",
            "[167,    10] loss: 0.104\n",
            "[167,    20] loss: 0.108\n",
            "[167,    30] loss: 0.109\n",
            "[167,    40] loss: 0.113\n",
            "[168,    10] loss: 0.103\n",
            "[168,    20] loss: 0.104\n",
            "[168,    30] loss: 0.108\n",
            "[168,    40] loss: 0.106\n",
            "[169,    10] loss: 0.102\n",
            "[169,    20] loss: 0.101\n",
            "[169,    30] loss: 0.109\n",
            "[169,    40] loss: 0.120\n",
            "[170,    10] loss: 0.098\n",
            "[170,    20] loss: 0.109\n",
            "[170,    30] loss: 0.116\n",
            "[170,    40] loss: 0.118\n",
            "[171,    10] loss: 0.111\n",
            "[171,    20] loss: 0.115\n",
            "[171,    30] loss: 0.115\n",
            "[171,    40] loss: 0.109\n",
            "[172,    10] loss: 0.103\n",
            "[172,    20] loss: 0.110\n",
            "[172,    30] loss: 0.110\n",
            "[172,    40] loss: 0.102\n",
            "[173,    10] loss: 0.103\n",
            "[173,    20] loss: 0.097\n",
            "[173,    30] loss: 0.112\n",
            "[173,    40] loss: 0.114\n",
            "[174,    10] loss: 0.096\n",
            "[174,    20] loss: 0.110\n",
            "[174,    30] loss: 0.110\n",
            "[174,    40] loss: 0.103\n",
            "[175,    10] loss: 0.110\n",
            "[175,    20] loss: 0.092\n",
            "[175,    30] loss: 0.114\n",
            "[175,    40] loss: 0.115\n",
            "[176,    10] loss: 0.106\n",
            "[176,    20] loss: 0.110\n",
            "[176,    30] loss: 0.106\n",
            "[176,    40] loss: 0.115\n",
            "[177,    10] loss: 0.113\n",
            "[177,    20] loss: 0.103\n",
            "[177,    30] loss: 0.104\n",
            "[177,    40] loss: 0.116\n",
            "[178,    10] loss: 0.107\n",
            "[178,    20] loss: 0.101\n",
            "[178,    30] loss: 0.110\n",
            "[178,    40] loss: 0.100\n",
            "[179,    10] loss: 0.101\n",
            "[179,    20] loss: 0.105\n",
            "[179,    30] loss: 0.104\n",
            "[179,    40] loss: 0.112\n",
            "[180,    10] loss: 0.098\n",
            "[180,    20] loss: 0.114\n",
            "[180,    30] loss: 0.100\n",
            "[180,    40] loss: 0.120\n",
            "[181,    10] loss: 0.101\n",
            "[181,    20] loss: 0.109\n",
            "[181,    30] loss: 0.114\n",
            "[181,    40] loss: 0.118\n",
            "[182,    10] loss: 0.101\n",
            "[182,    20] loss: 0.102\n",
            "[182,    30] loss: 0.118\n",
            "[182,    40] loss: 0.108\n",
            "[183,    10] loss: 0.107\n",
            "[183,    20] loss: 0.104\n",
            "[183,    30] loss: 0.118\n",
            "[183,    40] loss: 0.102\n",
            "[184,    10] loss: 0.115\n",
            "[184,    20] loss: 0.098\n",
            "[184,    30] loss: 0.099\n",
            "[184,    40] loss: 0.110\n",
            "[185,    10] loss: 0.098\n",
            "[185,    20] loss: 0.110\n",
            "[185,    30] loss: 0.104\n",
            "[185,    40] loss: 0.121\n",
            "[186,    10] loss: 0.107\n",
            "[186,    20] loss: 0.106\n",
            "[186,    30] loss: 0.104\n",
            "[186,    40] loss: 0.110\n",
            "[187,    10] loss: 0.099\n",
            "[187,    20] loss: 0.099\n",
            "[187,    30] loss: 0.102\n",
            "[187,    40] loss: 0.124\n",
            "[188,    10] loss: 0.099\n",
            "[188,    20] loss: 0.100\n",
            "[188,    30] loss: 0.111\n",
            "[188,    40] loss: 0.111\n",
            "[189,    10] loss: 0.113\n",
            "[189,    20] loss: 0.096\n",
            "[189,    30] loss: 0.107\n",
            "[189,    40] loss: 0.096\n",
            "[190,    10] loss: 0.113\n",
            "[190,    20] loss: 0.104\n",
            "[190,    30] loss: 0.102\n",
            "[190,    40] loss: 0.105\n",
            "[191,    10] loss: 0.104\n",
            "[191,    20] loss: 0.106\n",
            "[191,    30] loss: 0.105\n",
            "[191,    40] loss: 0.104\n",
            "[192,    10] loss: 0.102\n",
            "[192,    20] loss: 0.105\n",
            "[192,    30] loss: 0.102\n",
            "[192,    40] loss: 0.103\n",
            "[193,    10] loss: 0.093\n",
            "[193,    20] loss: 0.108\n",
            "[193,    30] loss: 0.111\n",
            "[193,    40] loss: 0.099\n",
            "[194,    10] loss: 0.101\n",
            "[194,    20] loss: 0.105\n",
            "[194,    30] loss: 0.103\n",
            "[194,    40] loss: 0.114\n",
            "[195,    10] loss: 0.113\n",
            "[195,    20] loss: 0.130\n",
            "[195,    30] loss: 0.113\n",
            "[195,    40] loss: 0.120\n",
            "[196,    10] loss: 0.107\n",
            "[196,    20] loss: 0.115\n",
            "[196,    30] loss: 0.105\n",
            "[196,    40] loss: 0.140\n",
            "[197,    10] loss: 0.262\n",
            "[197,    20] loss: 0.369\n",
            "[197,    30] loss: 0.317\n",
            "[197,    40] loss: 0.309\n",
            "[198,    10] loss: 0.273\n",
            "[198,    20] loss: 0.254\n",
            "[198,    30] loss: 0.247\n",
            "[198,    40] loss: 0.213\n",
            "[199,    10] loss: 0.142\n",
            "[199,    20] loss: 0.153\n",
            "[199,    30] loss: 0.138\n",
            "[199,    40] loss: 0.153\n",
            "[200,    10] loss: 0.115\n",
            "[200,    20] loss: 0.114\n",
            "[200,    30] loss: 0.116\n",
            "[200,    40] loss: 0.127\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 35 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 35 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 39 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 92 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VidFcG10CrN_"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6tbtiJkCrOB",
        "outputId": "b2e23a0b-8c46-4087-afbc-c60df098253c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7febac58ef28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r2iIcOUCrOE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}