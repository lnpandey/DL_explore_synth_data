{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vRukHLbtNl5P","colab_type":"code","colab":{}},"source":["###install pytorch in colab\n","# !pip3 install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n","# !pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t4wBaaNRg682","colab_type":"code","outputId":"9b8a8440-ccdc-412d-8fca-1b8a45fda689","executionInfo":{"status":"ok","timestamp":1568312649096,"user_tz":-330,"elapsed":26826,"user":{"displayName":"Shukla Avani Manishbhai cs18m052","photoUrl":"","userId":"08983259847687096575"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["##mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x2PsdapopZkQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":323},"outputId":"80f7fa2a-bdca-4883-c292-a0280951cc3a","executionInfo":{"status":"ok","timestamp":1568311512498,"user_tz":-330,"elapsed":2879,"user":{"displayName":"Shukla Avani Manishbhai cs18m052","photoUrl":"","userId":"08983259847687096575"}}},"source":["!ls '/content/drive/My Drive/DL_exp/sphere_shell_data_version3_using_uniform'"],"execution_count":14,"outputs":[{"output_type":"stream","text":["train_sp1_sh1.2_t1_16dim.csv  train_sp1_sh1.6_t1_16dim.csv\n","train_sp1_sh1.2_t1_2dim.csv   train_sp1_sh1.6_t1_2dim.csv\n","train_sp1_sh1.2_t1_32dim.csv  train_sp1_sh1.6_t1_32dim.csv\n","train_sp1_sh1.2_t1_4dim.csv   train_sp1_sh1.6_t1_4dim.csv\n","train_sp1_sh1.2_t1_64dim.csv  train_sp1_sh1.6_t1_64dim.csv\n","train_sp1_sh1.2_t1_8dim.csv   train_sp1_sh1.6_t1_8dim.csv\n","train_sp1_sh1.4_t1_16dim.csv  train_sp1_sh1.8_t1_16dim.csv\n","train_sp1_sh1.4_t1_2dim.csv   train_sp1_sh1.8_t1_2dim.csv\n","train_sp1_sh1.4_t1_32dim.csv  train_sp1_sh1.8_t1_32dim.csv\n","train_sp1_sh1.4_t1_4dim.csv   train_sp1_sh1.8_t1_4dim.csv\n","train_sp1_sh1.4_t1_64dim.csv  train_sp1_sh1.8_t1_64dim.csv\n","train_sp1_sh1.4_t1_8dim.csv   train_sp1_sh1.8_t1_8dim.csv\n","train_sp1_sh1.5_t1_16dim.csv  train_sp1_sh2_t1_16dim.csv\n","train_sp1_sh1.5_t1_2dim.csv   train_sp1_sh2_t1_2dim.csv\n","train_sp1_sh1.5_t1_32dim.csv  train_sp1_sh2_t1_32dim.csv\n","train_sp1_sh1.5_t1_4dim.csv   train_sp1_sh2_t1_4dim.csv\n","train_sp1_sh1.5_t1_64dim.csv  train_sp1_sh2_t1_64dim.csv\n","train_sp1_sh1.5_t1_8dim.csv   train_sp1_sh2_t1_8dim.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KBAG6IknNt9H","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import csv\n","import json\n","import pandas as pd\n","import torch\n","import time\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","import torch.nn.init as init"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mM0kZZ043qKk","colab_type":"code","colab":{}},"source":["base_path = '/content/drive/My Drive/DL_exp/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBKvPDiGfGn9","colab_type":"code","colab":{}},"source":["### custom dataset class for dataset. read the csv in __init__ . \n","# input : csv file path\n","# output: returns (data,label)\n","class load_dataset(Dataset):\n","    def __init__(self, data_path):\n","        self.samples = pd.read_csv(data_path).values\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        return self.samples[idx,:-1] ,self.samples[idx,-1]\n","      \n","### Multilayer Perceptron Model\n","# input : number of features (input), hidden nodes list , number of classes\n","class MultilayerPerceptron(torch.nn.Module):\n","  \n","    def __init__(self,num_features,hidden_nodes_list,num_classes):\n","        super(MultilayerPerceptron, self).__init__()\n","        \n","        num_hidden_layes = len(hidden_nodes_list)\n","        self.hidden = torch.nn.ModuleList()\n","        \n","        self.hidden.append(torch.nn.Linear(num_features, hidden_nodes_list[0]))\n","        for k in range(num_hidden_layes-1):\n","            self.hidden.append(torch.nn.Linear(hidden_nodes_list[k], hidden_nodes_list[k+1]))    \n","        self.hidden.append(torch.nn.Linear(hidden_nodes_list[num_hidden_layes-1], num_classes))\n","        \n","    # input : features\n","    # output: logits , probabilities\n","    def forward(self, x):\n","        out = x\n","        for layer in self.hidden[:-1]:\n","          out = layer(out)\n","          out = F.relu(out)\n","\n","        logits = self.hidden[-1](out)\n","        probas = F.log_softmax(logits, dim=1)\n","        return logits, probas\n","      \n","### weight initialization function\n","# use : model.apply(init_weights)\n","def init_weights(m):\n","  if isinstance(m, torch.nn.Linear):\n","    if initialisation_method=='xavier':\n","      init.xavier_uniform_(m.weight)\n","    if initialisation_method=='he':\n","      init.kaiming_uniform_(m.weight)\n","    m.bias.data.fill_(0.01)\n","    \n","### function to compute the accuracy\n","# input : model, data of type DataLoader\n","# output: cost (log loss), accuracy \n","def compute_accuracy(net, data_loader):\n","    net.eval()\n","    cost, correct_pred, num_examples = 0, 0, 0\n","    with torch.no_grad():\n","        for features, targets in data_loader:\n","            features = features.float().to(device)\n","            targets = targets.long().to(device)\n","            logits, probas = net(features)\n","            cost += F.cross_entropy(logits, targets) * targets.size(0)\n","            _, predicted_labels = torch.max(probas, 1)\n","            num_examples += targets.size(0)\n","            correct_pred += (predicted_labels == targets).sum()\n","        return cost/num_examples , correct_pred.float()/num_examples * 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IoSuX_AQv3vk","colab_type":"code","colab":{}},"source":["### Training of multiple models\n","\n","# Input: csv file path contain sphere-shell d dimension features,label data. \n","# Its name is like 'train_sp1_sh1.8_t1_2dim.csv' where sphere, shell radius, shell thickness and dimension are variable that have to specify.\n","radius_of_sphere = 1\n","radius_of_shell = 1.8\n","thickness_of_shell = 1\n","dimentions = [2,4,8,16,32,64]          # list of dimention of the data\n","# Output: upade 'uniform_result.json' file with the data, trained model details, accuracy, epoch and loss\n","# also update 'loss_uniform.json' file which have list of [{epoch},{loss}] data of trained model \n","result_folder = 'result_Hypothesis_B9/'\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","start_time = time.time()\n","\n","with open(base_path+result_folder+'loss_uniform.json', mode='a+') as readjson:\n","  try: \n","    loss = json.loads(readjson.read())\n","  except ValueError: \n","    loss = {}\n","          \n","for dim in dimentions:\n","  csv_path = base_path+'sphere_shell_data_version3_using_uniform/train_sp'+ str(radius_of_sphere) + '_sh'+ str(radius_of_shell) + '_t' + str(thickness_of_shell)+ '_' + str(dim) + 'dim.csv'\n","  dataset = load_dataset(csv_path)\n","  size = len(dataset)\n","  train_dataloader = DataLoader(dataset, batch_size=size, shuffle=True)\n","\n","\n","  tmp_h = [2,4,6,8,12,16,24,32,48,64,128,256]             #list of total number of nodes in hidden layers\n","  tmp_i = [100,200,300,400,500,600,700,800,900,1000]      #list of random seed for weight initialization\n","\n","  for t_h in tmp_h:\n","    for t_i in tmp_i:\n","\n","      #architecture\n","      num_features  = next(iter(dataset))[0].shape[0]        # Input data dimention\n","      hidden_nodes_list   = [t_h]                            # List of number of nodes at each hidden layer\n","      num_classes   = 2                                      # The number of output classes. In this case, 0 and 1\n","      \n","      # 'xavier' : Xavier Initialisation\n","      # 'he' : He Initialisation\n","      initialisation_method = 'xavier'\n","      \n","      # 'sgd' : SGD (lr) \n","      # 'sgdwm' : SGD with Momentum (lr, momentum)\n","      # 'adagrad' : AdaGrad\n","      # 'adam' : Adam\n","      # 'ngd' : Natural gradient descent\n","      # 'l1' : L1 Regularisation\n","      # 'l2' : L2 Regularisation\n","      # 'pathnorm' : PathNorm Regularisation\n","      # 'spectralnorm' : Spectral norm Regularisation\n","      optimisation_method = 'sgdwm'  \n","\n","      # Hyperparameters\n","      random_seed = t_i\n","      learning_rate = 0.05\n","            \n","      torch.manual_seed(random_seed)\n","      model = MultilayerPerceptron(num_features,hidden_nodes_list,num_classes)\n","      model.apply(init_weights)\n","      model = model.to(device)\n","\n","      if optimisation_method=='sgd' or optimisation_method=='ngd':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","      if optimisation_method=='sgdwm':\n","        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","      if optimisation_method=='adagrad':\n","        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n","      if optimisation_method=='adam':\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08)\n","\n","      model_loss = []\n","      epoch = 0\n","\n","      with torch.set_grad_enabled(False):\n","        cost,best_acc = compute_accuracy(model, train_dataloader)\n","      model_loss.append([epoch,cost.data.tolist()])\n","      \n","      count = 1\n","      prev_acc=best_acc\n","      best_epoch = epoch\n","      best_cost = cost\n","\n","      while True:\n","          model.train()\n","          for batch_idx, (features, targets) in enumerate(train_dataloader):\n","\n","              features = features.float().to(device)\n","              targets = targets.long().to(device)\n","\n","              ### FORWARD AND BACK PROP\n","              logits, probas = model(features)\n","              cost = F.cross_entropy(logits, targets)\n","              optimizer.zero_grad()\n","\n","              cost.backward()\n","\n","              ### UPDATE MODEL PARAMETERS\n","              optimizer.step()\n","\n","\n","          with torch.set_grad_enabled(False):\n","              cost,acc = compute_accuracy(model, train_dataloader)\n","              \n","          epoch+=1\n","          model_loss.append([epoch,cost.data.tolist()])\n","          \n","          # Stopping conditions\n","          if prev_acc==acc:\n","            count+=1\n","          else:\n","            prev_acc=acc\n","            count=1\n","          if (epoch>50 and best_acc-acc>=5) or count==20 or epoch==400:\n","            break\n","          if acc>best_acc:\n","            best_acc=acc\n","            best_epoch = epoch\n","            best_cost = cost\n"," \n","      print('Epoch: %03d | Accuracy: %.2f%% | Cost: %.4f' % (best_epoch,best_acc,best_cost))\n","  \n","      if len(hidden_nodes_list)==1:\n","        loss.update({'sh'+ str(radius_of_shell)+'dim'+str(dim)+'h'+str(t_h)+'i'+str(t_i):model_loss})\n","      else:\n","        loss.update({'sh'+ str(radius_of_shell)+'dim'+str(dim)+'h'+str(t_h)+'i'+str(t_i)+'l2':model_loss})\n","    \n","      with open(base_path+result_folder+'uniform_result.json', mode='a+') as readjson:\n","        try: \n","          result = json.loads(readjson.read())\n","        except ValueError: \n","          result = []  \n","       \n","      with open(base_path+result_folder+'uniform_result.json', mode='w') as feedjson:\n","        entry = {'sphere_radius': 1, 'shell_radius': radius_of_shell, 'thickness' : 1, 'dimension':dim, 'n_nodes':hidden_nodes_list, 'random_seed':t_i, 'epoch':best_epoch, 'accuracy':best_acc.data.tolist(), 'cost':best_cost.data.tolist()}\n","        result.append(entry)\n","        json.dump(result, feedjson)\n","\n","with open(base_path+result_folder+'loss_uniform.json', mode='w') as feedjson:\n","  json.dump(loss, feedjson)\n","\n","# print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2PfBT_Oo19X","colab_type":"code","colab":{}},"source":[" "],"execution_count":0,"outputs":[]}]}