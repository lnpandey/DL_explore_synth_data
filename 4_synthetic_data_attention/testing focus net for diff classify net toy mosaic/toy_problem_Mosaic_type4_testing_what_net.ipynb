{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of toy_problem_Mosaic_type4_testing_what_net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2_J4Rw2r0SQ",
        "outputId": "bc4cf7f5-cb05-42a1-b6d6-54ee3c9a4015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6fjud_Fr0Sa"
      },
      "source": [
        "# Generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqdXHO0Cr0Sd",
        "outputId": "5e55ee1b-0d02-4e8e-f86e-5370873c12e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y = np.random.randint(0,10,50000)\n",
        "idx= []\n",
        "for i in range(10):\n",
        "    print(i,sum(y==i))\n",
        "    idx.append(y==i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5115\n",
            "1 5045\n",
            "2 4974\n",
            "3 5120\n",
            "4 4950\n",
            "5 4887\n",
            "6 4850\n",
            "7 5133\n",
            "8 4880\n",
            "9 5046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddhXyODwr0Sk"
      },
      "source": [
        "x = np.zeros((50000,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyV3N2DIr0Sp"
      },
      "source": [
        "x[idx[0],:] = np.random.multivariate_normal(mean = [4,6.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[0]))\n",
        "\n",
        "x[idx[1],:] = np.random.multivariate_normal(mean = [5.5,6],cov=[[0.01,0],[0,0.01]],size=sum(idx[1]))\n",
        "\n",
        "x[idx[2],:] = np.random.multivariate_normal(mean = [4.5,4.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[2]))\n",
        "\n",
        "# x[idx[0],:] = np.random.multivariate_normal(mean = [5,5],cov=[[0.1,0],[0,0.1]],size=sum(idx[0]))\n",
        "\n",
        "# x[idx[1],:] = np.random.multivariate_normal(mean = [6,6],cov=[[0.1,0],[0,0.1]],size=sum(idx[1]))\n",
        "\n",
        "# x[idx[2],:] = np.random.multivariate_normal(mean = [5.5,6.5],cov=[[0.1,0],[0,0.1]],size=sum(idx[2]))\n",
        "\n",
        "x[idx[3],:] = np.random.multivariate_normal(mean = [3,3.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[3]))\n",
        "\n",
        "\n",
        "x[idx[4],:] = np.random.multivariate_normal(mean = [2.5,5.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[4]))\n",
        "\n",
        "x[idx[5],:] = np.random.multivariate_normal(mean = [3.5,8],cov=[[0.01,0],[0,0.01]],size=sum(idx[5]))\n",
        "\n",
        "x[idx[6],:] = np.random.multivariate_normal(mean = [5.5,8],cov=[[0.01,0],[0,0.01]],size=sum(idx[6]))\n",
        "\n",
        "x[idx[7],:] = np.random.multivariate_normal(mean = [7,6.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[7]))\n",
        "\n",
        "x[idx[8],:] = np.random.multivariate_normal(mean = [6.5,4.5],cov=[[0.01,0],[0,0.01]],size=sum(idx[8]))\n",
        "\n",
        "x[idx[9],:] = np.random.multivariate_normal(mean = [5,3],cov=[[0.01,0],[0,0.01]],size=sum(idx[9]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8Jm7YUr0St",
        "outputId": "3a285301-4140-4da9-d202-a79b19917495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "for i in range(10):\n",
        "    plt.scatter(x[idx[i],0],x[idx[i],1],label=\"class_\"+str(i))\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd3fa9e2048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfFHcZJOr0Sz"
      },
      "source": [
        "foreground_classes = {'class_0','class_1', 'class_2'}\n",
        "\n",
        "background_classes = {'class_3','class_4', 'class_5', 'class_6','class_7', 'class_8', 'class_9'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OplNpNQVr0S2",
        "outputId": "04e6af78-63ef-4170-e59d-22a78484d80b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fg_class  = np.random.randint(0,3)\n",
        "fg_idx = np.random.randint(0,9)\n",
        "\n",
        "a = []\n",
        "for i in range(9):\n",
        "    if i == fg_idx:\n",
        "        b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "    else:\n",
        "        bg_class = np.random.randint(3,10)\n",
        "        b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "        a.append(x[b])\n",
        "        print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "a = np.concatenate(a,axis=0)\n",
        "print(a.shape)\n",
        "\n",
        "print(fg_class , fg_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "background 3 present at 0\n",
            "background 6 present at 1\n",
            "background 5 present at 2\n",
            "background 8 present at 3\n",
            "background 4 present at 4\n",
            "background 6 present at 5\n",
            "foreground 0 present at 6\n",
            "background 5 present at 7\n",
            "background 4 present at 8\n",
            "(9, 2)\n",
            "0 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwZVmmRBr0S8",
        "outputId": "a17b630e-1523-4874-cc49-26301ca38582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoxzYI-ur0S_",
        "outputId": "d5241927-46d9-469b-c996-a6d197bed55e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.reshape(a,(18,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.88511685],\n",
              "       [3.30577075],\n",
              "       [5.55687964],\n",
              "       [8.08906138],\n",
              "       [3.45312375],\n",
              "       [7.93100501],\n",
              "       [6.45556666],\n",
              "       [4.41864713],\n",
              "       [2.39677238],\n",
              "       [5.39561392],\n",
              "       [5.45426384],\n",
              "       [7.99931249],\n",
              "       [3.98917172],\n",
              "       [6.51780277],\n",
              "       [3.50013407],\n",
              "       [7.93366938],\n",
              "       [2.38315422],\n",
              "       [5.5979466 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ruI0cxr0TE"
      },
      "source": [
        "a=np.reshape(a,(3,6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTUTFhJIr0TI",
        "outputId": "f643436b-75e2-4493-a08b-0433f97c64df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "plt.imshow(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd3fa4e0978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAADKCAYAAACmA/sWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANJ0lEQVR4nO3df6xf9V3H8efL0gICA6QEKu34EStxbkZYwzYxC9mGASRgIhqIboyM3AQhMt2iMBMWl5igf0y3DIcNIwO3wJYxZ9VGxMBkZPKjYPlREKhkhhZiBwxYHRte9vaPezDXyy297fd8v6f3fp6P5Jvv+fHpeb9Pmr6+p+ec7/mmqpAkLX0/MXQDkqTJMPAlqREGviQ1wsCXpEYY+JLUCANfkhoxUuAn+akktyV5sns/fBfjXkuyuXttGKWmJGnvZJT78JP8GfBCVV2d5Arg8Kr6w3nG7ayqg0foU5I0olED/3HgtKp6Nskq4JtVdeI84wx8SRrYqIH/YlUd1k0H+N7r83PGTQObgWng6qr6xi62NwVMASzL8ncetHzeM0RLQi3fb+gWxupHKzN0C2PzjkOfG7qFsXriqSOGbmGsTjjhu0O3MFYPPvQ/z1XVkfOt223qJPln4Oh5Vv3R7JmqqiS7+vQ4tqq2JzkBuD3Jw1X1H3MHVdV6YD3AofsfVb/007+1u/YWremj3vC5uKQ8MbX/0C2Mzb1nXTd0C2N1+gUXDd3CWH31S9cM3cJYrVz9zH/uat1uA7+qPrCrdUn+K8mqWad0duxiG9u796eSfBM4CXhD4EuSxmfU2zI3ABd20xcCfzt3QJLDk+zfTa8ETgUeHbGuJGkPjRr4VwOnJ3kS+EA3T5J1SV7/f+/PAZuSPAjcwcw5fANfkiZspCuHVfU88P55lm8CLu6mvw28Y5Q6kqTR+U1bSWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqRG9BH6SM5I8nmRrkivmWb9/kq906+9JclwfdSVJCzdy4CdZBlwDnAm8DbggydvmDPsI8L2q+hngz4E/HbWuJGnP9HGEfwqwtaqeqqpXgZuBc+eMORe4oZv+GvD+JOmhtiRpgfoI/GOAp2fNb+uWzTumqqaBl4Aj5m4oyVSSTUk2vfraKz20Jkl63T510baq1lfVuqpat2LZgUO3I0lLSh+Bvx1YM2t+dbds3jFJ9gMOBZ7vobYkaYH6CPz7gLVJjk+yAjgf2DBnzAbgwm76POD2qqoeakuSFmi/UTdQVdNJLgNuBZYB11fVliSfAjZV1QbgC8BfJ9kKvMDMh4IkaYJGDnyAqtoIbJyz7KpZ0z8EfqOPWpKkvbNPXbSVJI2PgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqRG9BH6SM5I8nmRrkivmWf/hJN9Nsrl7XdxHXUnSwo38I+ZJlgHXAKcD24D7kmyoqkfnDP1KVV02aj1J0t7p4wj/FGBrVT1VVa8CNwPn9rBdSVKPRj7CB44Bnp41vw141zzjfj3Je4EngN+rqqfnDkgyBUwBHHDUIeSG6R7a2zft+NohQ7cwVj978beHbmFsjv/LqaFbGK8Lhm5gvPZPH7G3OE3qou3fAcdV1S8AtwE3zDeoqtZX1bqqWrfisAMn1JoktaGPwN8OrJk1v7pb9n+q6vmq+lE3ex3wzh7qSpL2QB+Bfx+wNsnxSVYA5wMbZg9IsmrW7DnAYz3UlSTtgZFPZlXVdJLLgFuBZcD1VbUlyaeATVW1AfjdJOcA08ALwIdHrStJ2jO9XL2oqo3AxjnLrpo1fSVwZR+1JEl7x2/aSlIjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY3oJfCTXJ9kR5JHdrE+ST6bZGuSh5Kc3EddSdLC9XWE/0XgjDdZfyawtntNAZ/vqa4kaYF6CfyquhN44U2GnAvcWDPuBg5LsqqP2pKkhZnUOfxjgKdnzW/rlv0/SaaSbEqy6dUXX5lQa5LUhn3qom1Vra+qdVW1bsVhBw7djiQtKZMK/O3Amlnzq7tlkqQJmVTgbwA+1N2t827gpap6dkK1JUnAfn1sJMlNwGnAyiTbgE8CywGq6lpgI3AWsBX4AXBRH3UlSQvXS+BX1QW7WV/ApX3UkiTtnX3qoq0kaXwMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9Jjegl8JNcn2RHkkd2sf60JC8l2dy9ruqjriRp4Xr5EXPgi8DngBvfZMy3qursnupJkvZQL0f4VXUn8EIf25IkjUdfR/gL8Z4kDwLPAB+vqi1zBySZAqYAVvzk4Tz/V8dOsL3JOv6SJ4duYaxe3nLy0C2MzeZzPjN0C2P1m6vfM3QLY/Xz1//O0C2M2RW7XDOpwH8AOLaqdiY5C/gGsHbuoKpaD6wHOOiINTWh3iSpCRO5S6eqXq6qnd30RmB5kpWTqC1JmjGRwE9ydJJ006d0dZ+fRG1J0oxeTukkuQk4DViZZBvwSWA5QFVdC5wHXJJkGngFOL+qPGUjSRPUS+BX1QW7Wf85Zm7blCQNxG/aSlIjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakRBr4kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0YOfCTrElyR5JHk2xJcvk8Y5Lks0m2Jnkoycmj1pUk7Zk+fsR8GvhYVT2Q5BDg/iS3VdWjs8acCaztXu8CPt+9S5ImZOQj/Kp6tqoe6Ka/DzwGHDNn2LnAjTXjbuCwJKtGrS1JWrhez+EnOQ44CbhnzqpjgKdnzW/jjR8KJJlKsinJpukf/nefrUlS83oL/CQHA7cAH62ql/dmG1W1vqrWVdW6/Q44qK/WJEn0FPhJljMT9l+uqq/PM2Q7sGbW/OpumSRpQvq4SyfAF4DHqurTuxi2AfhQd7fOu4GXqurZUWtLkhauj7t0TgU+CDycZHO37BPAWwGq6lpgI3AWsBX4AXBRD3UlSXtg5MCvqruA7GZMAZeOWkuStPf8pq0kNcLAl6RGGPiS1AgDX5IaYeBLUiMMfElqhIEvSY0w8CWpEQa+JDXCwJekRhj4ktQIA1+SGmHgS1IjDHxJaoSBL0mNMPAlqREGviQ1wsCXpEYY+JLUiJEDP8maJHckeTTJliSXzzPmtCQvJdncva4ata4kac+M/CPmwDTwsap6IMkhwP1JbquqR+eM+1ZVnd1DPUnSXhj5CL+qnq2qB7rp7wOPAceMul1JUr9SVf1tLDkOuBN4e1W9PGv5acAtwDbgGeDjVbVlnj8/BUx1sycCj/fW3O6tBJ6bYL1Jc/8WN/dv8Zr0vh1bVUfOt6K3wE9yMPAvwJ9U1dfnrHsL8OOq2pnkLOAzVbW2l8I9SbKpqtYN3ce4uH+Lm/u3eO1L+9bLXTpJljNzBP/luWEPUFUvV9XObnojsDzJyj5qS5IWpo+7dAJ8AXisqj69izFHd+NIckpX9/lRa0uSFq6Pu3ROBT4IPJxkc7fsE8BbAarqWuA84JIk08ArwPnV58WDfqwfuoExc/8WN/dv8dpn9q3Xi7aSpH2X37SVpEYY+JLUCAMfSHJGkseTbE1yxdD99CnJ9Ul2JHlk6F7GYSGP9liskhyQ5N4kD3b79sdD9zQOSZYl+bckfz90L31L8p0kD3ePlNk0eD+tn8NPsgx4AjidmS+G3QdcMM+jIRalJO8FdgI3VtXbh+6nb0lWAatmP9oD+LWl8PfX3dl2UPf9leXAXcDlVXX3wK31KsnvA+uAtyy1x68k+Q6wrqr2iS+VeYQPpwBbq+qpqnoVuBk4d+CeelNVdwIvDN3HuCzlR3vUjJ3d7PLutaSO0JKsBn4VuG7oXlpg4M+Ew9Oz5rexRAKjNd2jPU4C7hm2k/50pzs2AzuA26pqyexb5y+APwB+PHQjY1LAPyW5v3t0zKAMfC0J3aM9bgE+Ovs5TotdVb1WVb8IrAZOSbJkTsslORvYUVX3D93LGP1yVZ0MnAlc2p1iHYyBD9uBNbPmV3fLtEjs7tEeS0FVvQjcAZwxdC89OhU4pzvPfTPwviRfGralflXV9u59B/A3zJxCHoyBP3ORdm2S45OsAM4HNgzckxZoIY/2WKySHJnksG76QGZuLPj3YbvqT1VdWVWrq+o4Zv7d3V5Vvz1wW71JclB3IwFJDgJ+BRj0brnmA7+qpoHLgFuZueD31fke3bxYJbkJ+FfgxCTbknxk6J569vqjPd436xfVzhq6qZ6sAu5I8hAzBya3VdWSu3VxCTsKuCvJg8C9wD9U1T8O2VDzt2VKUiuaP8KXpFYY+JLUCANfkhph4EtSIwx8SWqEgS9JjTDwJakR/ws6tqaQf4/kcAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbvfbwVr0TN"
      },
      "source": [
        "desired_num = 10000\n",
        "mosaic_list_of_images =[]\n",
        "mosaic_label = []\n",
        "fore_idx=[]\n",
        "for j in range(desired_num):\n",
        "    fg_class  = np.random.randint(0,3)\n",
        "    fg_idx = 0\n",
        "    a = []\n",
        "    for i in range(9):\n",
        "        if i == fg_idx:\n",
        "            b = np.random.choice(np.where(idx[fg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"foreground \"+str(fg_class)+\" present at \" + str(fg_idx))\n",
        "        else:\n",
        "            bg_class = np.random.randint(3,10)\n",
        "            b = np.random.choice(np.where(idx[bg_class]==True)[0],size=1)\n",
        "            a.append(x[b])\n",
        "#             print(\"background \"+str(bg_class)+\" present at \" + str(i))\n",
        "    a = np.concatenate(a,axis=0)\n",
        "    mosaic_list_of_images.append(np.reshape(a,(18,1)))\n",
        "    mosaic_label.append(fg_class)\n",
        "    fore_idx.append(fg_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOsFmWfMr0TR"
      },
      "source": [
        "mosaic_list_of_images = np.concatenate(mosaic_list_of_images,axis=1).T\n",
        "# print(mosaic_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2PnW7aQr0TT",
        "outputId": "66498961-8f30-489a-a77d-cf0d8505e665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(np.shape(mosaic_list_of_images))\n",
        "print(np.shape(fore_idx))\n",
        "print(np.shape(mosaic_label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 18)\n",
            "(10000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPoIwbMHx44n"
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  cnt = 0\n",
        "  counter = np.array([0,0,0,0,0,0,0,0,0])\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([18], dtype=torch.float64)\n",
        "    np.random.seed(dataset_number*10000 + i)\n",
        "    give_pref = foreground_index[i] #np.random.randint(0,9)\n",
        "    # print(\"outside\", give_pref,foreground_index[i])\n",
        "    for j in range(9):\n",
        "      if j == give_pref:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "\n",
        "    if give_pref == foreground_index[i] :\n",
        "      # print(\"equal are\", give_pref,foreground_index[i])\n",
        "      cnt += 1\n",
        "      counter[give_pref] += 1\n",
        "    else :\n",
        "      counter[give_pref] += 1\n",
        "\n",
        "    avg_image_dataset.append(img)\n",
        "\n",
        "  print(\"number of correct averaging happened for dataset \"+str(dataset_number)+\" is \"+str(cnt)) \n",
        "  print(\"the averaging are done as \", counter) \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ZAjix3x8CM",
        "outputId": "4505a8dc-0413-4ee5-c459-02f9ec77e52c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images, mosaic_label, fore_idx, 9)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of correct averaging happened for dataset 1 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 2 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 3 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 4 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 5 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 6 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 7 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 8 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n",
            "number of correct averaging happened for dataset 9 is 10000\n",
            "the averaging are done as  [10000     0     0     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL0BRf8er0TX"
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY2l62APygaV"
      },
      "source": [
        "batch = 200\n",
        "epochs = 300\n",
        "\n",
        "# training_data = avg_image_dataset_5    #just change this and training_label to desired dataset for training\n",
        "# training_label = labels_5\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVRXgwwNr0Tb"
      },
      "source": [
        "class Wherenet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Wherenet,self).__init__()\n",
        "        self.linear1 = nn.Linear(2,128)\n",
        "        self.linear2 = nn.Linear(128,256)\n",
        "        self.linear3 = nn.Linear(256,128)\n",
        "        self.linear4 = nn.Linear(128,64)\n",
        "        self.linear5 = nn.Linear(64,1)\n",
        "    def forward(self,z):\n",
        "        x = torch.zeros([batch,9],dtype=torch.float64)\n",
        "        y = torch.zeros([batch,2], dtype=torch.float64)\n",
        "        #x,y = x.to(\"cuda\"),y.to(\"cuda\")\n",
        "        for i in range(9):\n",
        "            x[:,i] = self.helper(z[:,2*i:2*i+2])[:,0]\n",
        "            #print(k[:,0].shape,x[:,i].shape)\n",
        "        x = F.softmax(x,dim=1)   # alphas\n",
        "        x1 = x[:,0]\n",
        "        for i in range(9):\n",
        "            x1 = x[:,i]          \n",
        "            #print()\n",
        "            y = y+torch.mul(x1[:,None],z[:,2*i:2*i+2])\n",
        "        return y , x \n",
        "\n",
        "    \n",
        "    def helper(self,x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = F.relu(self.linear4(x))\n",
        "        x = self.linear5(x)\n",
        "        return x\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Ek05Kxr0Te",
        "outputId": "699a153f-e7f5-4de4-b634-a28b4bc04375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "trainiter = iter(trainloader_1)\n",
        "input1,labels1 = trainiter.next()\n",
        "\n",
        "input1.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([200, 18])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxEmWZI6r0Ti",
        "outputId": "e37f5ca1-6d63-4ada-e323-2521b29dbe5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "where = Wherenet().double()\n",
        "where = where\n",
        "out_where,alphas = where(input1)\n",
        "out_where.shape,alphas.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([200, 2]), torch.Size([200, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_XeIUk0r0Tl"
      },
      "source": [
        "class Whatnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Whatnet,self).__init__()\n",
        "        self.linear1 = nn.Linear(2,128)\n",
        "        self.linear2 = nn.Linear(128,256)\n",
        "        self.linear3 = nn.Linear(256,128)\n",
        "        self.linear4 = nn.Linear(128,64)\n",
        "        self.linear5 = nn.Linear(64,3)\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = F.relu(self.linear3(x))\n",
        "        x = F.relu(self.linear4(x))\n",
        "        x = self.linear5(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35i9bIlr0Tp"
      },
      "source": [
        "# what = Whatnet().double()\n",
        "# what(out_where)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uALi25pmzQHV"
      },
      "source": [
        "def test_all(number, testloader,what, where):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            # images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            avg_inp,alphas = where(inputs)        \n",
        "            outputs = what(avg_inp)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmNprlPzTjP"
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    where = Wherenet().double()\n",
        "    what = Whatnet().double()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer_where = optim.SGD(where.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer_where = optim.Adam(where.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    # optimizer_what = optim.SGD(what.parameters(), lr=0.001, momentum=0.9)\n",
        "    optimizer_what = optim.Adam(what.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 100\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            # inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_what.zero_grad()\n",
        "            optimizer_where.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            avg_inp,alphas = where(inputs) \n",
        "            # print(avg_inp.shape)       \n",
        "            outputs = what(avg_inp)\n",
        "            \n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_what.step()\n",
        "            optimizer_where.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "        if (np.mean(ep_lossi) <= 0.05):\n",
        "            break\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    # torch.save(inc.state_dict(),\"train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            avg_inp,alphas = where(inputs)        \n",
        "            outputs = what(avg_inp)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,what, where)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQoPST5zW2t",
        "outputId": "81002f2f-cf9c-44c5-c469-b4b9ab4439d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.104\n",
            "[1,    20] loss: 1.101\n",
            "[1,    30] loss: 1.099\n",
            "[1,    40] loss: 1.100\n",
            "[1,    50] loss: 1.099\n",
            "[2,    10] loss: 1.100\n",
            "[2,    20] loss: 1.100\n",
            "[2,    30] loss: 1.099\n",
            "[2,    40] loss: 1.099\n",
            "[2,    50] loss: 1.099\n",
            "[3,    10] loss: 1.099\n",
            "[3,    20] loss: 1.099\n",
            "[3,    30] loss: 1.099\n",
            "[3,    40] loss: 1.099\n",
            "[3,    50] loss: 1.099\n",
            "[4,    10] loss: 1.099\n",
            "[4,    20] loss: 1.098\n",
            "[4,    30] loss: 1.099\n",
            "[4,    40] loss: 1.099\n",
            "[4,    50] loss: 1.099\n",
            "[5,    10] loss: 1.099\n",
            "[5,    20] loss: 1.099\n",
            "[5,    30] loss: 1.099\n",
            "[5,    40] loss: 1.099\n",
            "[5,    50] loss: 1.099\n",
            "[6,    10] loss: 1.099\n",
            "[6,    20] loss: 1.099\n",
            "[6,    30] loss: 1.099\n",
            "[6,    40] loss: 1.099\n",
            "[6,    50] loss: 1.099\n",
            "[7,    10] loss: 1.099\n",
            "[7,    20] loss: 1.098\n",
            "[7,    30] loss: 1.099\n",
            "[7,    40] loss: 1.099\n",
            "[7,    50] loss: 1.099\n",
            "[8,    10] loss: 1.099\n",
            "[8,    20] loss: 1.099\n",
            "[8,    30] loss: 1.098\n",
            "[8,    40] loss: 1.099\n",
            "[8,    50] loss: 1.099\n",
            "[9,    10] loss: 1.099\n",
            "[9,    20] loss: 1.098\n",
            "[9,    30] loss: 1.099\n",
            "[9,    40] loss: 1.099\n",
            "[9,    50] loss: 1.099\n",
            "[10,    10] loss: 1.098\n",
            "[10,    20] loss: 1.099\n",
            "[10,    30] loss: 1.098\n",
            "[10,    40] loss: 1.098\n",
            "[10,    50] loss: 1.099\n",
            "[11,    10] loss: 1.099\n",
            "[11,    20] loss: 1.098\n",
            "[11,    30] loss: 1.099\n",
            "[11,    40] loss: 1.099\n",
            "[11,    50] loss: 1.099\n",
            "[12,    10] loss: 1.098\n",
            "[12,    20] loss: 1.099\n",
            "[12,    30] loss: 1.099\n",
            "[12,    40] loss: 1.099\n",
            "[12,    50] loss: 1.099\n",
            "[13,    10] loss: 1.098\n",
            "[13,    20] loss: 1.098\n",
            "[13,    30] loss: 1.099\n",
            "[13,    40] loss: 1.099\n",
            "[13,    50] loss: 1.098\n",
            "[14,    10] loss: 1.099\n",
            "[14,    20] loss: 1.098\n",
            "[14,    30] loss: 1.099\n",
            "[14,    40] loss: 1.099\n",
            "[14,    50] loss: 1.099\n",
            "[15,    10] loss: 1.099\n",
            "[15,    20] loss: 1.098\n",
            "[15,    30] loss: 1.099\n",
            "[15,    40] loss: 1.098\n",
            "[15,    50] loss: 1.099\n",
            "[16,    10] loss: 1.098\n",
            "[16,    20] loss: 1.099\n",
            "[16,    30] loss: 1.099\n",
            "[16,    40] loss: 1.099\n",
            "[16,    50] loss: 1.099\n",
            "[17,    10] loss: 1.098\n",
            "[17,    20] loss: 1.098\n",
            "[17,    30] loss: 1.098\n",
            "[17,    40] loss: 1.099\n",
            "[17,    50] loss: 1.099\n",
            "[18,    10] loss: 1.098\n",
            "[18,    20] loss: 1.098\n",
            "[18,    30] loss: 1.099\n",
            "[18,    40] loss: 1.099\n",
            "[18,    50] loss: 1.099\n",
            "[19,    10] loss: 1.099\n",
            "[19,    20] loss: 1.099\n",
            "[19,    30] loss: 1.099\n",
            "[19,    40] loss: 1.099\n",
            "[19,    50] loss: 1.099\n",
            "[20,    10] loss: 1.098\n",
            "[20,    20] loss: 1.099\n",
            "[20,    30] loss: 1.098\n",
            "[20,    40] loss: 1.099\n",
            "[20,    50] loss: 1.099\n",
            "[21,    10] loss: 1.099\n",
            "[21,    20] loss: 1.099\n",
            "[21,    30] loss: 1.099\n",
            "[21,    40] loss: 1.098\n",
            "[21,    50] loss: 1.099\n",
            "[22,    10] loss: 1.098\n",
            "[22,    20] loss: 1.098\n",
            "[22,    30] loss: 1.098\n",
            "[22,    40] loss: 1.099\n",
            "[22,    50] loss: 1.099\n",
            "[23,    10] loss: 1.098\n",
            "[23,    20] loss: 1.098\n",
            "[23,    30] loss: 1.099\n",
            "[23,    40] loss: 1.099\n",
            "[23,    50] loss: 1.099\n",
            "[24,    10] loss: 1.099\n",
            "[24,    20] loss: 1.098\n",
            "[24,    30] loss: 1.099\n",
            "[24,    40] loss: 1.099\n",
            "[24,    50] loss: 1.098\n",
            "[25,    10] loss: 1.098\n",
            "[25,    20] loss: 1.098\n",
            "[25,    30] loss: 1.099\n",
            "[25,    40] loss: 1.099\n",
            "[25,    50] loss: 1.099\n",
            "[26,    10] loss: 1.099\n",
            "[26,    20] loss: 1.098\n",
            "[26,    30] loss: 1.098\n",
            "[26,    40] loss: 1.099\n",
            "[26,    50] loss: 1.099\n",
            "[27,    10] loss: 1.098\n",
            "[27,    20] loss: 1.099\n",
            "[27,    30] loss: 1.099\n",
            "[27,    40] loss: 1.098\n",
            "[27,    50] loss: 1.099\n",
            "[28,    10] loss: 1.099\n",
            "[28,    20] loss: 1.099\n",
            "[28,    30] loss: 1.098\n",
            "[28,    40] loss: 1.099\n",
            "[28,    50] loss: 1.098\n",
            "[29,    10] loss: 1.098\n",
            "[29,    20] loss: 1.099\n",
            "[29,    30] loss: 1.099\n",
            "[29,    40] loss: 1.099\n",
            "[29,    50] loss: 1.099\n",
            "[30,    10] loss: 1.099\n",
            "[30,    20] loss: 1.098\n",
            "[30,    30] loss: 1.099\n",
            "[30,    40] loss: 1.099\n",
            "[30,    50] loss: 1.099\n",
            "[31,    10] loss: 1.099\n",
            "[31,    20] loss: 1.098\n",
            "[31,    30] loss: 1.098\n",
            "[31,    40] loss: 1.099\n",
            "[31,    50] loss: 1.099\n",
            "[32,    10] loss: 1.098\n",
            "[32,    20] loss: 1.098\n",
            "[32,    30] loss: 1.099\n",
            "[32,    40] loss: 1.099\n",
            "[32,    50] loss: 1.099\n",
            "[33,    10] loss: 1.098\n",
            "[33,    20] loss: 1.099\n",
            "[33,    30] loss: 1.098\n",
            "[33,    40] loss: 1.099\n",
            "[33,    50] loss: 1.099\n",
            "[34,    10] loss: 1.099\n",
            "[34,    20] loss: 1.099\n",
            "[34,    30] loss: 1.098\n",
            "[34,    40] loss: 1.099\n",
            "[34,    50] loss: 1.098\n",
            "[35,    10] loss: 1.098\n",
            "[35,    20] loss: 1.099\n",
            "[35,    30] loss: 1.099\n",
            "[35,    40] loss: 1.099\n",
            "[35,    50] loss: 1.098\n",
            "[36,    10] loss: 1.099\n",
            "[36,    20] loss: 1.099\n",
            "[36,    30] loss: 1.099\n",
            "[36,    40] loss: 1.098\n",
            "[36,    50] loss: 1.099\n",
            "[37,    10] loss: 1.098\n",
            "[37,    20] loss: 1.098\n",
            "[37,    30] loss: 1.098\n",
            "[37,    40] loss: 1.099\n",
            "[37,    50] loss: 1.099\n",
            "[38,    10] loss: 1.099\n",
            "[38,    20] loss: 1.099\n",
            "[38,    30] loss: 1.098\n",
            "[38,    40] loss: 1.099\n",
            "[38,    50] loss: 1.098\n",
            "[39,    10] loss: 1.099\n",
            "[39,    20] loss: 1.098\n",
            "[39,    30] loss: 1.098\n",
            "[39,    40] loss: 1.099\n",
            "[39,    50] loss: 1.099\n",
            "[40,    10] loss: 1.098\n",
            "[40,    20] loss: 1.098\n",
            "[40,    30] loss: 1.099\n",
            "[40,    40] loss: 1.099\n",
            "[40,    50] loss: 1.099\n",
            "[41,    10] loss: 1.098\n",
            "[41,    20] loss: 1.098\n",
            "[41,    30] loss: 1.099\n",
            "[41,    40] loss: 1.100\n",
            "[41,    50] loss: 1.099\n",
            "[42,    10] loss: 1.099\n",
            "[42,    20] loss: 1.098\n",
            "[42,    30] loss: 1.099\n",
            "[42,    40] loss: 1.099\n",
            "[42,    50] loss: 1.099\n",
            "[43,    10] loss: 1.099\n",
            "[43,    20] loss: 1.099\n",
            "[43,    30] loss: 1.099\n",
            "[43,    40] loss: 1.099\n",
            "[43,    50] loss: 1.098\n",
            "[44,    10] loss: 1.099\n",
            "[44,    20] loss: 1.099\n",
            "[44,    30] loss: 1.098\n",
            "[44,    40] loss: 1.098\n",
            "[44,    50] loss: 1.098\n",
            "[45,    10] loss: 1.099\n",
            "[45,    20] loss: 1.098\n",
            "[45,    30] loss: 1.098\n",
            "[45,    40] loss: 1.099\n",
            "[45,    50] loss: 1.099\n",
            "[46,    10] loss: 1.098\n",
            "[46,    20] loss: 1.099\n",
            "[46,    30] loss: 1.099\n",
            "[46,    40] loss: 1.098\n",
            "[46,    50] loss: 1.099\n",
            "[47,    10] loss: 1.098\n",
            "[47,    20] loss: 1.098\n",
            "[47,    30] loss: 1.099\n",
            "[47,    40] loss: 1.098\n",
            "[47,    50] loss: 1.099\n",
            "[48,    10] loss: 1.099\n",
            "[48,    20] loss: 1.098\n",
            "[48,    30] loss: 1.099\n",
            "[48,    40] loss: 1.099\n",
            "[48,    50] loss: 1.099\n",
            "[49,    10] loss: 1.099\n",
            "[49,    20] loss: 1.099\n",
            "[49,    30] loss: 1.099\n",
            "[49,    40] loss: 1.098\n",
            "[49,    50] loss: 1.099\n",
            "[50,    10] loss: 1.098\n",
            "[50,    20] loss: 1.099\n",
            "[50,    30] loss: 1.098\n",
            "[50,    40] loss: 1.099\n",
            "[50,    50] loss: 1.099\n",
            "[51,    10] loss: 1.099\n",
            "[51,    20] loss: 1.099\n",
            "[51,    30] loss: 1.098\n",
            "[51,    40] loss: 1.099\n",
            "[51,    50] loss: 1.099\n",
            "[52,    10] loss: 1.098\n",
            "[52,    20] loss: 1.099\n",
            "[52,    30] loss: 1.099\n",
            "[52,    40] loss: 1.099\n",
            "[52,    50] loss: 1.098\n",
            "[53,    10] loss: 1.098\n",
            "[53,    20] loss: 1.099\n",
            "[53,    30] loss: 1.099\n",
            "[53,    40] loss: 1.099\n",
            "[53,    50] loss: 1.099\n",
            "[54,    10] loss: 1.099\n",
            "[54,    20] loss: 1.099\n",
            "[54,    30] loss: 1.099\n",
            "[54,    40] loss: 1.099\n",
            "[54,    50] loss: 1.099\n",
            "[55,    10] loss: 1.098\n",
            "[55,    20] loss: 1.098\n",
            "[55,    30] loss: 1.099\n",
            "[55,    40] loss: 1.099\n",
            "[55,    50] loss: 1.099\n",
            "[56,    10] loss: 1.098\n",
            "[56,    20] loss: 1.099\n",
            "[56,    30] loss: 1.099\n",
            "[56,    40] loss: 1.098\n",
            "[56,    50] loss: 1.099\n",
            "[57,    10] loss: 1.099\n",
            "[57,    20] loss: 1.098\n",
            "[57,    30] loss: 1.099\n",
            "[57,    40] loss: 1.099\n",
            "[57,    50] loss: 1.098\n",
            "[58,    10] loss: 1.099\n",
            "[58,    20] loss: 1.099\n",
            "[58,    30] loss: 1.099\n",
            "[58,    40] loss: 1.099\n",
            "[58,    50] loss: 1.098\n",
            "[59,    10] loss: 1.098\n",
            "[59,    20] loss: 1.098\n",
            "[59,    30] loss: 1.099\n",
            "[59,    40] loss: 1.098\n",
            "[59,    50] loss: 1.099\n",
            "[60,    10] loss: 1.099\n",
            "[60,    20] loss: 1.098\n",
            "[60,    30] loss: 1.099\n",
            "[60,    40] loss: 1.099\n",
            "[60,    50] loss: 1.099\n",
            "[61,    10] loss: 1.099\n",
            "[61,    20] loss: 1.099\n",
            "[61,    30] loss: 1.099\n",
            "[61,    40] loss: 1.098\n",
            "[61,    50] loss: 1.099\n",
            "[62,    10] loss: 1.099\n",
            "[62,    20] loss: 1.098\n",
            "[62,    30] loss: 1.099\n",
            "[62,    40] loss: 1.098\n",
            "[62,    50] loss: 1.099\n",
            "[63,    10] loss: 1.099\n",
            "[63,    20] loss: 1.098\n",
            "[63,    30] loss: 1.099\n",
            "[63,    40] loss: 1.098\n",
            "[63,    50] loss: 1.099\n",
            "[64,    10] loss: 1.099\n",
            "[64,    20] loss: 1.098\n",
            "[64,    30] loss: 1.099\n",
            "[64,    40] loss: 1.098\n",
            "[64,    50] loss: 1.099\n",
            "[65,    10] loss: 1.099\n",
            "[65,    20] loss: 1.099\n",
            "[65,    30] loss: 1.098\n",
            "[65,    40] loss: 1.098\n",
            "[65,    50] loss: 1.099\n",
            "[66,    10] loss: 1.098\n",
            "[66,    20] loss: 1.099\n",
            "[66,    30] loss: 1.099\n",
            "[66,    40] loss: 1.099\n",
            "[66,    50] loss: 1.098\n",
            "[67,    10] loss: 1.098\n",
            "[67,    20] loss: 1.099\n",
            "[67,    30] loss: 1.098\n",
            "[67,    40] loss: 1.099\n",
            "[67,    50] loss: 1.098\n",
            "[68,    10] loss: 1.098\n",
            "[68,    20] loss: 1.098\n",
            "[68,    30] loss: 1.099\n",
            "[68,    40] loss: 1.099\n",
            "[68,    50] loss: 1.098\n",
            "[69,    10] loss: 1.099\n",
            "[69,    20] loss: 1.098\n",
            "[69,    30] loss: 1.098\n",
            "[69,    40] loss: 1.099\n",
            "[69,    50] loss: 1.099\n",
            "[70,    10] loss: 1.099\n",
            "[70,    20] loss: 1.098\n",
            "[70,    30] loss: 1.099\n",
            "[70,    40] loss: 1.099\n",
            "[70,    50] loss: 1.098\n",
            "[71,    10] loss: 1.098\n",
            "[71,    20] loss: 1.098\n",
            "[71,    30] loss: 1.099\n",
            "[71,    40] loss: 1.098\n",
            "[71,    50] loss: 1.099\n",
            "[72,    10] loss: 1.099\n",
            "[72,    20] loss: 1.099\n",
            "[72,    30] loss: 1.098\n",
            "[72,    40] loss: 1.098\n",
            "[72,    50] loss: 1.098\n",
            "[73,    10] loss: 1.098\n",
            "[73,    20] loss: 1.099\n",
            "[73,    30] loss: 1.099\n",
            "[73,    40] loss: 1.099\n",
            "[73,    50] loss: 1.099\n",
            "[74,    10] loss: 1.098\n",
            "[74,    20] loss: 1.098\n",
            "[74,    30] loss: 1.099\n",
            "[74,    40] loss: 1.099\n",
            "[74,    50] loss: 1.099\n",
            "[75,    10] loss: 1.099\n",
            "[75,    20] loss: 1.099\n",
            "[75,    30] loss: 1.099\n",
            "[75,    40] loss: 1.099\n",
            "[75,    50] loss: 1.098\n",
            "[76,    10] loss: 1.098\n",
            "[76,    20] loss: 1.099\n",
            "[76,    30] loss: 1.099\n",
            "[76,    40] loss: 1.099\n",
            "[76,    50] loss: 1.098\n",
            "[77,    10] loss: 1.098\n",
            "[77,    20] loss: 1.099\n",
            "[77,    30] loss: 1.098\n",
            "[77,    40] loss: 1.099\n",
            "[77,    50] loss: 1.098\n",
            "[78,    10] loss: 1.098\n",
            "[78,    20] loss: 1.099\n",
            "[78,    30] loss: 1.099\n",
            "[78,    40] loss: 1.098\n",
            "[78,    50] loss: 1.099\n",
            "[79,    10] loss: 1.098\n",
            "[79,    20] loss: 1.098\n",
            "[79,    30] loss: 1.099\n",
            "[79,    40] loss: 1.099\n",
            "[79,    50] loss: 1.099\n",
            "[80,    10] loss: 1.099\n",
            "[80,    20] loss: 1.098\n",
            "[80,    30] loss: 1.099\n",
            "[80,    40] loss: 1.098\n",
            "[80,    50] loss: 1.099\n",
            "[81,    10] loss: 1.099\n",
            "[81,    20] loss: 1.099\n",
            "[81,    30] loss: 1.098\n",
            "[81,    40] loss: 1.099\n",
            "[81,    50] loss: 1.099\n",
            "[82,    10] loss: 1.098\n",
            "[82,    20] loss: 1.099\n",
            "[82,    30] loss: 1.098\n",
            "[82,    40] loss: 1.098\n",
            "[82,    50] loss: 1.099\n",
            "[83,    10] loss: 1.099\n",
            "[83,    20] loss: 1.099\n",
            "[83,    30] loss: 1.098\n",
            "[83,    40] loss: 1.099\n",
            "[83,    50] loss: 1.099\n",
            "[84,    10] loss: 1.099\n",
            "[84,    20] loss: 1.098\n",
            "[84,    30] loss: 1.099\n",
            "[84,    40] loss: 1.098\n",
            "[84,    50] loss: 1.099\n",
            "[85,    10] loss: 1.099\n",
            "[85,    20] loss: 1.099\n",
            "[85,    30] loss: 1.099\n",
            "[85,    40] loss: 1.099\n",
            "[85,    50] loss: 1.098\n",
            "[86,    10] loss: 1.099\n",
            "[86,    20] loss: 1.099\n",
            "[86,    30] loss: 1.098\n",
            "[86,    40] loss: 1.098\n",
            "[86,    50] loss: 1.099\n",
            "[87,    10] loss: 1.098\n",
            "[87,    20] loss: 1.099\n",
            "[87,    30] loss: 1.098\n",
            "[87,    40] loss: 1.099\n",
            "[87,    50] loss: 1.099\n",
            "[88,    10] loss: 1.098\n",
            "[88,    20] loss: 1.099\n",
            "[88,    30] loss: 1.099\n",
            "[88,    40] loss: 1.099\n",
            "[88,    50] loss: 1.098\n",
            "[89,    10] loss: 1.098\n",
            "[89,    20] loss: 1.099\n",
            "[89,    30] loss: 1.099\n",
            "[89,    40] loss: 1.098\n",
            "[89,    50] loss: 1.098\n",
            "[90,    10] loss: 1.098\n",
            "[90,    20] loss: 1.099\n",
            "[90,    30] loss: 1.099\n",
            "[90,    40] loss: 1.098\n",
            "[90,    50] loss: 1.099\n",
            "[91,    10] loss: 1.099\n",
            "[91,    20] loss: 1.098\n",
            "[91,    30] loss: 1.098\n",
            "[91,    40] loss: 1.098\n",
            "[91,    50] loss: 1.099\n",
            "[92,    10] loss: 1.098\n",
            "[92,    20] loss: 1.099\n",
            "[92,    30] loss: 1.098\n",
            "[92,    40] loss: 1.099\n",
            "[92,    50] loss: 1.099\n",
            "[93,    10] loss: 1.098\n",
            "[93,    20] loss: 1.098\n",
            "[93,    30] loss: 1.099\n",
            "[93,    40] loss: 1.099\n",
            "[93,    50] loss: 1.098\n",
            "[94,    10] loss: 1.099\n",
            "[94,    20] loss: 1.098\n",
            "[94,    30] loss: 1.099\n",
            "[94,    40] loss: 1.099\n",
            "[94,    50] loss: 1.099\n",
            "[95,    10] loss: 1.099\n",
            "[95,    20] loss: 1.099\n",
            "[95,    30] loss: 1.099\n",
            "[95,    40] loss: 1.098\n",
            "[95,    50] loss: 1.099\n",
            "[96,    10] loss: 1.099\n",
            "[96,    20] loss: 1.099\n",
            "[96,    30] loss: 1.099\n",
            "[96,    40] loss: 1.098\n",
            "[96,    50] loss: 1.099\n",
            "[97,    10] loss: 1.098\n",
            "[97,    20] loss: 1.098\n",
            "[97,    30] loss: 1.098\n",
            "[97,    40] loss: 1.099\n",
            "[97,    50] loss: 1.099\n",
            "[98,    10] loss: 1.099\n",
            "[98,    20] loss: 1.099\n",
            "[98,    30] loss: 1.099\n",
            "[98,    40] loss: 1.098\n",
            "[98,    50] loss: 1.099\n",
            "[99,    10] loss: 1.099\n",
            "[99,    20] loss: 1.098\n",
            "[99,    30] loss: 1.099\n",
            "[99,    40] loss: 1.098\n",
            "[99,    50] loss: 1.099\n",
            "[100,    10] loss: 1.098\n",
            "[100,    20] loss: 1.099\n",
            "[100,    30] loss: 1.098\n",
            "[100,    40] loss: 1.099\n",
            "[100,    50] loss: 1.099\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 33 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.103\n",
            "[1,    20] loss: 1.096\n",
            "[1,    30] loss: 1.098\n",
            "[1,    40] loss: 1.098\n",
            "[1,    50] loss: 1.099\n",
            "[2,    10] loss: 1.097\n",
            "[2,    20] loss: 1.099\n",
            "[2,    30] loss: 1.096\n",
            "[2,    40] loss: 1.097\n",
            "[2,    50] loss: 1.097\n",
            "[3,    10] loss: 1.097\n",
            "[3,    20] loss: 1.097\n",
            "[3,    30] loss: 1.096\n",
            "[3,    40] loss: 1.096\n",
            "[3,    50] loss: 1.096\n",
            "[4,    10] loss: 1.096\n",
            "[4,    20] loss: 1.095\n",
            "[4,    30] loss: 1.096\n",
            "[4,    40] loss: 1.096\n",
            "[4,    50] loss: 1.096\n",
            "[5,    10] loss: 1.096\n",
            "[5,    20] loss: 1.094\n",
            "[5,    30] loss: 1.097\n",
            "[5,    40] loss: 1.096\n",
            "[5,    50] loss: 1.096\n",
            "[6,    10] loss: 1.096\n",
            "[6,    20] loss: 1.094\n",
            "[6,    30] loss: 1.096\n",
            "[6,    40] loss: 1.096\n",
            "[6,    50] loss: 1.096\n",
            "[7,    10] loss: 1.096\n",
            "[7,    20] loss: 1.095\n",
            "[7,    30] loss: 1.096\n",
            "[7,    40] loss: 1.096\n",
            "[7,    50] loss: 1.094\n",
            "[8,    10] loss: 1.096\n",
            "[8,    20] loss: 1.096\n",
            "[8,    30] loss: 1.095\n",
            "[8,    40] loss: 1.097\n",
            "[8,    50] loss: 1.098\n",
            "[9,    10] loss: 1.094\n",
            "[9,    20] loss: 1.096\n",
            "[9,    30] loss: 1.096\n",
            "[9,    40] loss: 1.096\n",
            "[9,    50] loss: 1.095\n",
            "[10,    10] loss: 1.095\n",
            "[10,    20] loss: 1.095\n",
            "[10,    30] loss: 1.094\n",
            "[10,    40] loss: 1.096\n",
            "[10,    50] loss: 1.094\n",
            "[11,    10] loss: 1.095\n",
            "[11,    20] loss: 1.095\n",
            "[11,    30] loss: 1.094\n",
            "[11,    40] loss: 1.092\n",
            "[11,    50] loss: 1.094\n",
            "[12,    10] loss: 1.094\n",
            "[12,    20] loss: 1.094\n",
            "[12,    30] loss: 1.092\n",
            "[12,    40] loss: 1.095\n",
            "[12,    50] loss: 1.096\n",
            "[13,    10] loss: 1.092\n",
            "[13,    20] loss: 1.094\n",
            "[13,    30] loss: 1.092\n",
            "[13,    40] loss: 1.094\n",
            "[13,    50] loss: 1.094\n",
            "[14,    10] loss: 1.094\n",
            "[14,    20] loss: 1.093\n",
            "[14,    30] loss: 1.093\n",
            "[14,    40] loss: 1.093\n",
            "[14,    50] loss: 1.094\n",
            "[15,    10] loss: 1.095\n",
            "[15,    20] loss: 1.092\n",
            "[15,    30] loss: 1.092\n",
            "[15,    40] loss: 1.093\n",
            "[15,    50] loss: 1.093\n",
            "[16,    10] loss: 1.091\n",
            "[16,    20] loss: 1.093\n",
            "[16,    30] loss: 1.092\n",
            "[16,    40] loss: 1.091\n",
            "[16,    50] loss: 1.091\n",
            "[17,    10] loss: 1.092\n",
            "[17,    20] loss: 1.091\n",
            "[17,    30] loss: 1.092\n",
            "[17,    40] loss: 1.089\n",
            "[17,    50] loss: 1.090\n",
            "[18,    10] loss: 1.090\n",
            "[18,    20] loss: 1.091\n",
            "[18,    30] loss: 1.090\n",
            "[18,    40] loss: 1.090\n",
            "[18,    50] loss: 1.090\n",
            "[19,    10] loss: 1.090\n",
            "[19,    20] loss: 1.089\n",
            "[19,    30] loss: 1.088\n",
            "[19,    40] loss: 1.089\n",
            "[19,    50] loss: 1.092\n",
            "[20,    10] loss: 1.091\n",
            "[20,    20] loss: 1.089\n",
            "[20,    30] loss: 1.088\n",
            "[20,    40] loss: 1.087\n",
            "[20,    50] loss: 1.087\n",
            "[21,    10] loss: 1.086\n",
            "[21,    20] loss: 1.086\n",
            "[21,    30] loss: 1.086\n",
            "[21,    40] loss: 1.085\n",
            "[21,    50] loss: 1.089\n",
            "[22,    10] loss: 1.088\n",
            "[22,    20] loss: 1.085\n",
            "[22,    30] loss: 1.084\n",
            "[22,    40] loss: 1.085\n",
            "[22,    50] loss: 1.085\n",
            "[23,    10] loss: 1.086\n",
            "[23,    20] loss: 1.085\n",
            "[23,    30] loss: 1.084\n",
            "[23,    40] loss: 1.082\n",
            "[23,    50] loss: 1.082\n",
            "[24,    10] loss: 1.083\n",
            "[24,    20] loss: 1.083\n",
            "[24,    30] loss: 1.078\n",
            "[24,    40] loss: 1.080\n",
            "[24,    50] loss: 1.082\n",
            "[25,    10] loss: 1.080\n",
            "[25,    20] loss: 1.083\n",
            "[25,    30] loss: 1.080\n",
            "[25,    40] loss: 1.083\n",
            "[25,    50] loss: 1.078\n",
            "[26,    10] loss: 1.078\n",
            "[26,    20] loss: 1.076\n",
            "[26,    30] loss: 1.078\n",
            "[26,    40] loss: 1.076\n",
            "[26,    50] loss: 1.078\n",
            "[27,    10] loss: 1.073\n",
            "[27,    20] loss: 1.071\n",
            "[27,    30] loss: 1.074\n",
            "[27,    40] loss: 1.075\n",
            "[27,    50] loss: 1.074\n",
            "[28,    10] loss: 1.072\n",
            "[28,    20] loss: 1.071\n",
            "[28,    30] loss: 1.072\n",
            "[28,    40] loss: 1.067\n",
            "[28,    50] loss: 1.067\n",
            "[29,    10] loss: 1.070\n",
            "[29,    20] loss: 1.071\n",
            "[29,    30] loss: 1.061\n",
            "[29,    40] loss: 1.065\n",
            "[29,    50] loss: 1.065\n",
            "[30,    10] loss: 1.067\n",
            "[30,    20] loss: 1.061\n",
            "[30,    30] loss: 1.067\n",
            "[30,    40] loss: 1.067\n",
            "[30,    50] loss: 1.059\n",
            "[31,    10] loss: 1.061\n",
            "[31,    20] loss: 1.073\n",
            "[31,    30] loss: 1.063\n",
            "[31,    40] loss: 1.071\n",
            "[31,    50] loss: 1.068\n",
            "[32,    10] loss: 1.066\n",
            "[32,    20] loss: 1.059\n",
            "[32,    30] loss: 1.058\n",
            "[32,    40] loss: 1.061\n",
            "[32,    50] loss: 1.065\n",
            "[33,    10] loss: 1.068\n",
            "[33,    20] loss: 1.058\n",
            "[33,    30] loss: 1.059\n",
            "[33,    40] loss: 1.057\n",
            "[33,    50] loss: 1.064\n",
            "[34,    10] loss: 1.064\n",
            "[34,    20] loss: 1.060\n",
            "[34,    30] loss: 1.054\n",
            "[34,    40] loss: 1.062\n",
            "[34,    50] loss: 1.056\n",
            "[35,    10] loss: 1.059\n",
            "[35,    20] loss: 1.056\n",
            "[35,    30] loss: 1.062\n",
            "[35,    40] loss: 1.057\n",
            "[35,    50] loss: 1.055\n",
            "[36,    10] loss: 1.052\n",
            "[36,    20] loss: 1.057\n",
            "[36,    30] loss: 1.056\n",
            "[36,    40] loss: 1.060\n",
            "[36,    50] loss: 1.068\n",
            "[37,    10] loss: 1.058\n",
            "[37,    20] loss: 1.054\n",
            "[37,    30] loss: 1.052\n",
            "[37,    40] loss: 1.062\n",
            "[37,    50] loss: 1.054\n",
            "[38,    10] loss: 1.049\n",
            "[38,    20] loss: 1.050\n",
            "[38,    30] loss: 1.069\n",
            "[38,    40] loss: 1.057\n",
            "[38,    50] loss: 1.054\n",
            "[39,    10] loss: 1.049\n",
            "[39,    20] loss: 1.057\n",
            "[39,    30] loss: 1.053\n",
            "[39,    40] loss: 1.058\n",
            "[39,    50] loss: 1.056\n",
            "[40,    10] loss: 1.053\n",
            "[40,    20] loss: 1.059\n",
            "[40,    30] loss: 1.049\n",
            "[40,    40] loss: 1.055\n",
            "[40,    50] loss: 1.061\n",
            "[41,    10] loss: 1.056\n",
            "[41,    20] loss: 1.056\n",
            "[41,    30] loss: 1.053\n",
            "[41,    40] loss: 1.054\n",
            "[41,    50] loss: 1.052\n",
            "[42,    10] loss: 1.055\n",
            "[42,    20] loss: 1.061\n",
            "[42,    30] loss: 1.055\n",
            "[42,    40] loss: 1.051\n",
            "[42,    50] loss: 1.054\n",
            "[43,    10] loss: 1.058\n",
            "[43,    20] loss: 1.052\n",
            "[43,    30] loss: 1.055\n",
            "[43,    40] loss: 1.052\n",
            "[43,    50] loss: 1.055\n",
            "[44,    10] loss: 1.053\n",
            "[44,    20] loss: 1.048\n",
            "[44,    30] loss: 1.059\n",
            "[44,    40] loss: 1.057\n",
            "[44,    50] loss: 1.055\n",
            "[45,    10] loss: 1.055\n",
            "[45,    20] loss: 1.056\n",
            "[45,    30] loss: 1.064\n",
            "[45,    40] loss: 1.048\n",
            "[45,    50] loss: 1.052\n",
            "[46,    10] loss: 1.053\n",
            "[46,    20] loss: 1.056\n",
            "[46,    30] loss: 1.053\n",
            "[46,    40] loss: 1.057\n",
            "[46,    50] loss: 1.048\n",
            "[47,    10] loss: 1.056\n",
            "[47,    20] loss: 1.053\n",
            "[47,    30] loss: 1.052\n",
            "[47,    40] loss: 1.053\n",
            "[47,    50] loss: 1.057\n",
            "[48,    10] loss: 1.059\n",
            "[48,    20] loss: 1.055\n",
            "[48,    30] loss: 1.052\n",
            "[48,    40] loss: 1.055\n",
            "[48,    50] loss: 1.050\n",
            "[49,    10] loss: 1.045\n",
            "[49,    20] loss: 1.051\n",
            "[49,    30] loss: 1.057\n",
            "[49,    40] loss: 1.048\n",
            "[49,    50] loss: 1.067\n",
            "[50,    10] loss: 1.053\n",
            "[50,    20] loss: 1.050\n",
            "[50,    30] loss: 1.063\n",
            "[50,    40] loss: 1.056\n",
            "[50,    50] loss: 1.051\n",
            "[51,    10] loss: 1.040\n",
            "[51,    20] loss: 1.064\n",
            "[51,    30] loss: 1.055\n",
            "[51,    40] loss: 1.055\n",
            "[51,    50] loss: 1.050\n",
            "[52,    10] loss: 1.058\n",
            "[52,    20] loss: 1.054\n",
            "[52,    30] loss: 1.052\n",
            "[52,    40] loss: 1.049\n",
            "[52,    50] loss: 1.060\n",
            "[53,    10] loss: 1.058\n",
            "[53,    20] loss: 1.056\n",
            "[53,    30] loss: 1.053\n",
            "[53,    40] loss: 1.051\n",
            "[53,    50] loss: 1.052\n",
            "[54,    10] loss: 1.050\n",
            "[54,    20] loss: 1.054\n",
            "[54,    30] loss: 1.053\n",
            "[54,    40] loss: 1.050\n",
            "[54,    50] loss: 1.056\n",
            "[55,    10] loss: 1.068\n",
            "[55,    20] loss: 1.057\n",
            "[55,    30] loss: 1.051\n",
            "[55,    40] loss: 1.056\n",
            "[55,    50] loss: 1.050\n",
            "[56,    10] loss: 1.053\n",
            "[56,    20] loss: 1.054\n",
            "[56,    30] loss: 1.054\n",
            "[56,    40] loss: 1.056\n",
            "[56,    50] loss: 1.065\n",
            "[57,    10] loss: 1.052\n",
            "[57,    20] loss: 1.053\n",
            "[57,    30] loss: 1.056\n",
            "[57,    40] loss: 1.057\n",
            "[57,    50] loss: 1.056\n",
            "[58,    10] loss: 1.053\n",
            "[58,    20] loss: 1.050\n",
            "[58,    30] loss: 1.053\n",
            "[58,    40] loss: 1.060\n",
            "[58,    50] loss: 1.050\n",
            "[59,    10] loss: 1.055\n",
            "[59,    20] loss: 1.053\n",
            "[59,    30] loss: 1.047\n",
            "[59,    40] loss: 1.055\n",
            "[59,    50] loss: 1.053\n",
            "[60,    10] loss: 1.057\n",
            "[60,    20] loss: 1.049\n",
            "[60,    30] loss: 1.052\n",
            "[60,    40] loss: 1.053\n",
            "[60,    50] loss: 1.059\n",
            "[61,    10] loss: 1.052\n",
            "[61,    20] loss: 1.042\n",
            "[61,    30] loss: 1.060\n",
            "[61,    40] loss: 1.051\n",
            "[61,    50] loss: 1.066\n",
            "[62,    10] loss: 1.058\n",
            "[62,    20] loss: 1.048\n",
            "[62,    30] loss: 1.055\n",
            "[62,    40] loss: 1.052\n",
            "[62,    50] loss: 1.050\n",
            "[63,    10] loss: 1.058\n",
            "[63,    20] loss: 1.044\n",
            "[63,    30] loss: 1.053\n",
            "[63,    40] loss: 1.052\n",
            "[63,    50] loss: 1.056\n",
            "[64,    10] loss: 1.054\n",
            "[64,    20] loss: 1.056\n",
            "[64,    30] loss: 1.051\n",
            "[64,    40] loss: 1.055\n",
            "[64,    50] loss: 1.053\n",
            "[65,    10] loss: 1.053\n",
            "[65,    20] loss: 1.054\n",
            "[65,    30] loss: 1.055\n",
            "[65,    40] loss: 1.057\n",
            "[65,    50] loss: 1.054\n",
            "[66,    10] loss: 1.058\n",
            "[66,    20] loss: 1.050\n",
            "[66,    30] loss: 1.059\n",
            "[66,    40] loss: 1.050\n",
            "[66,    50] loss: 1.048\n",
            "[67,    10] loss: 1.058\n",
            "[67,    20] loss: 1.052\n",
            "[67,    30] loss: 1.055\n",
            "[67,    40] loss: 1.047\n",
            "[67,    50] loss: 1.052\n",
            "[68,    10] loss: 1.057\n",
            "[68,    20] loss: 1.050\n",
            "[68,    30] loss: 1.052\n",
            "[68,    40] loss: 1.052\n",
            "[68,    50] loss: 1.051\n",
            "[69,    10] loss: 1.057\n",
            "[69,    20] loss: 1.054\n",
            "[69,    30] loss: 1.052\n",
            "[69,    40] loss: 1.048\n",
            "[69,    50] loss: 1.060\n",
            "[70,    10] loss: 1.047\n",
            "[70,    20] loss: 1.047\n",
            "[70,    30] loss: 1.056\n",
            "[70,    40] loss: 1.058\n",
            "[70,    50] loss: 1.064\n",
            "[71,    10] loss: 1.057\n",
            "[71,    20] loss: 1.055\n",
            "[71,    30] loss: 1.057\n",
            "[71,    40] loss: 1.048\n",
            "[71,    50] loss: 1.050\n",
            "[72,    10] loss: 1.053\n",
            "[72,    20] loss: 1.054\n",
            "[72,    30] loss: 1.052\n",
            "[72,    40] loss: 1.058\n",
            "[72,    50] loss: 1.049\n",
            "[73,    10] loss: 1.051\n",
            "[73,    20] loss: 1.054\n",
            "[73,    30] loss: 1.053\n",
            "[73,    40] loss: 1.051\n",
            "[73,    50] loss: 1.056\n",
            "[74,    10] loss: 1.050\n",
            "[74,    20] loss: 1.053\n",
            "[74,    30] loss: 1.051\n",
            "[74,    40] loss: 1.055\n",
            "[74,    50] loss: 1.052\n",
            "[75,    10] loss: 1.062\n",
            "[75,    20] loss: 1.047\n",
            "[75,    30] loss: 1.056\n",
            "[75,    40] loss: 1.056\n",
            "[75,    50] loss: 1.056\n",
            "[76,    10] loss: 1.053\n",
            "[76,    20] loss: 1.055\n",
            "[76,    30] loss: 1.060\n",
            "[76,    40] loss: 1.056\n",
            "[76,    50] loss: 1.051\n",
            "[77,    10] loss: 1.053\n",
            "[77,    20] loss: 1.054\n",
            "[77,    30] loss: 1.061\n",
            "[77,    40] loss: 1.053\n",
            "[77,    50] loss: 1.054\n",
            "[78,    10] loss: 1.048\n",
            "[78,    20] loss: 1.058\n",
            "[78,    30] loss: 1.059\n",
            "[78,    40] loss: 1.051\n",
            "[78,    50] loss: 1.052\n",
            "[79,    10] loss: 1.053\n",
            "[79,    20] loss: 1.059\n",
            "[79,    30] loss: 1.052\n",
            "[79,    40] loss: 1.050\n",
            "[79,    50] loss: 1.049\n",
            "[80,    10] loss: 1.052\n",
            "[80,    20] loss: 1.050\n",
            "[80,    30] loss: 1.051\n",
            "[80,    40] loss: 1.057\n",
            "[80,    50] loss: 1.059\n",
            "[81,    10] loss: 1.053\n",
            "[81,    20] loss: 1.059\n",
            "[81,    30] loss: 1.056\n",
            "[81,    40] loss: 1.056\n",
            "[81,    50] loss: 1.039\n",
            "[82,    10] loss: 1.058\n",
            "[82,    20] loss: 1.059\n",
            "[82,    30] loss: 1.048\n",
            "[82,    40] loss: 1.044\n",
            "[82,    50] loss: 1.059\n",
            "[83,    10] loss: 1.049\n",
            "[83,    20] loss: 1.050\n",
            "[83,    30] loss: 1.055\n",
            "[83,    40] loss: 1.058\n",
            "[83,    50] loss: 1.051\n",
            "[84,    10] loss: 1.055\n",
            "[84,    20] loss: 1.057\n",
            "[84,    30] loss: 1.053\n",
            "[84,    40] loss: 1.048\n",
            "[84,    50] loss: 1.055\n",
            "[85,    10] loss: 1.053\n",
            "[85,    20] loss: 1.048\n",
            "[85,    30] loss: 1.056\n",
            "[85,    40] loss: 1.057\n",
            "[85,    50] loss: 1.048\n",
            "[86,    10] loss: 1.051\n",
            "[86,    20] loss: 1.063\n",
            "[86,    30] loss: 1.056\n",
            "[86,    40] loss: 1.052\n",
            "[86,    50] loss: 1.052\n",
            "[87,    10] loss: 1.053\n",
            "[87,    20] loss: 1.054\n",
            "[87,    30] loss: 1.054\n",
            "[87,    40] loss: 1.050\n",
            "[87,    50] loss: 1.057\n",
            "[88,    10] loss: 1.052\n",
            "[88,    20] loss: 1.053\n",
            "[88,    30] loss: 1.048\n",
            "[88,    40] loss: 1.052\n",
            "[88,    50] loss: 1.056\n",
            "[89,    10] loss: 1.051\n",
            "[89,    20] loss: 1.048\n",
            "[89,    30] loss: 1.061\n",
            "[89,    40] loss: 1.050\n",
            "[89,    50] loss: 1.053\n",
            "[90,    10] loss: 1.051\n",
            "[90,    20] loss: 1.051\n",
            "[90,    30] loss: 1.047\n",
            "[90,    40] loss: 1.053\n",
            "[90,    50] loss: 1.060\n",
            "[91,    10] loss: 1.054\n",
            "[91,    20] loss: 1.044\n",
            "[91,    30] loss: 1.052\n",
            "[91,    40] loss: 1.060\n",
            "[91,    50] loss: 1.058\n",
            "[92,    10] loss: 1.056\n",
            "[92,    20] loss: 1.054\n",
            "[92,    30] loss: 1.052\n",
            "[92,    40] loss: 1.056\n",
            "[92,    50] loss: 1.051\n",
            "[93,    10] loss: 1.051\n",
            "[93,    20] loss: 1.056\n",
            "[93,    30] loss: 1.058\n",
            "[93,    40] loss: 1.047\n",
            "[93,    50] loss: 1.049\n",
            "[94,    10] loss: 1.051\n",
            "[94,    20] loss: 1.050\n",
            "[94,    30] loss: 1.053\n",
            "[94,    40] loss: 1.047\n",
            "[94,    50] loss: 1.058\n",
            "[95,    10] loss: 1.048\n",
            "[95,    20] loss: 1.049\n",
            "[95,    30] loss: 1.054\n",
            "[95,    40] loss: 1.063\n",
            "[95,    50] loss: 1.047\n",
            "[96,    10] loss: 1.054\n",
            "[96,    20] loss: 1.047\n",
            "[96,    30] loss: 1.052\n",
            "[96,    40] loss: 1.060\n",
            "[96,    50] loss: 1.050\n",
            "[97,    10] loss: 1.053\n",
            "[97,    20] loss: 1.049\n",
            "[97,    30] loss: 1.052\n",
            "[97,    40] loss: 1.050\n",
            "[97,    50] loss: 1.059\n",
            "[98,    10] loss: 1.055\n",
            "[98,    20] loss: 1.054\n",
            "[98,    30] loss: 1.049\n",
            "[98,    40] loss: 1.052\n",
            "[98,    50] loss: 1.061\n",
            "[99,    10] loss: 1.054\n",
            "[99,    20] loss: 1.052\n",
            "[99,    30] loss: 1.059\n",
            "[99,    40] loss: 1.047\n",
            "[99,    50] loss: 1.052\n",
            "[100,    10] loss: 1.052\n",
            "[100,    20] loss: 1.055\n",
            "[100,    30] loss: 1.059\n",
            "[100,    40] loss: 1.048\n",
            "[100,    50] loss: 1.048\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 38 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 42 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 59 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 64 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 65 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 1.100\n",
            "[1,    20] loss: 1.099\n",
            "[1,    30] loss: 1.099\n",
            "[1,    40] loss: 1.098\n",
            "[1,    50] loss: 1.097\n",
            "[2,    10] loss: 1.097\n",
            "[2,    20] loss: 1.096\n",
            "[2,    30] loss: 1.097\n",
            "[2,    40] loss: 1.096\n",
            "[2,    50] loss: 1.097\n",
            "[3,    10] loss: 1.096\n",
            "[3,    20] loss: 1.095\n",
            "[3,    30] loss: 1.095\n",
            "[3,    40] loss: 1.095\n",
            "[3,    50] loss: 1.096\n",
            "[4,    10] loss: 1.095\n",
            "[4,    20] loss: 1.094\n",
            "[4,    30] loss: 1.093\n",
            "[4,    40] loss: 1.093\n",
            "[4,    50] loss: 1.093\n",
            "[5,    10] loss: 1.091\n",
            "[5,    20] loss: 1.092\n",
            "[5,    30] loss: 1.091\n",
            "[5,    40] loss: 1.090\n",
            "[5,    50] loss: 1.090\n",
            "[6,    10] loss: 1.088\n",
            "[6,    20] loss: 1.085\n",
            "[6,    30] loss: 1.090\n",
            "[6,    40] loss: 1.083\n",
            "[6,    50] loss: 1.087\n",
            "[7,    10] loss: 1.082\n",
            "[7,    20] loss: 1.080\n",
            "[7,    30] loss: 1.079\n",
            "[7,    40] loss: 1.074\n",
            "[7,    50] loss: 1.069\n",
            "[8,    10] loss: 1.063\n",
            "[8,    20] loss: 1.073\n",
            "[8,    30] loss: 1.076\n",
            "[8,    40] loss: 1.066\n",
            "[8,    50] loss: 1.054\n",
            "[9,    10] loss: 1.057\n",
            "[9,    20] loss: 1.046\n",
            "[9,    30] loss: 1.044\n",
            "[9,    40] loss: 1.036\n",
            "[9,    50] loss: 1.019\n",
            "[10,    10] loss: 1.015\n",
            "[10,    20] loss: 1.011\n",
            "[10,    30] loss: 1.014\n",
            "[10,    40] loss: 0.998\n",
            "[10,    50] loss: 1.011\n",
            "[11,    10] loss: 1.023\n",
            "[11,    20] loss: 1.007\n",
            "[11,    30] loss: 1.007\n",
            "[11,    40] loss: 1.003\n",
            "[11,    50] loss: 1.002\n",
            "[12,    10] loss: 0.998\n",
            "[12,    20] loss: 0.986\n",
            "[12,    30] loss: 1.003\n",
            "[12,    40] loss: 0.993\n",
            "[12,    50] loss: 0.987\n",
            "[13,    10] loss: 0.991\n",
            "[13,    20] loss: 0.989\n",
            "[13,    30] loss: 0.984\n",
            "[13,    40] loss: 0.981\n",
            "[13,    50] loss: 0.985\n",
            "[14,    10] loss: 0.972\n",
            "[14,    20] loss: 0.985\n",
            "[14,    30] loss: 0.988\n",
            "[14,    40] loss: 0.992\n",
            "[14,    50] loss: 0.992\n",
            "[15,    10] loss: 0.993\n",
            "[15,    20] loss: 0.995\n",
            "[15,    30] loss: 0.987\n",
            "[15,    40] loss: 0.970\n",
            "[15,    50] loss: 0.987\n",
            "[16,    10] loss: 0.959\n",
            "[16,    20] loss: 0.985\n",
            "[16,    30] loss: 0.995\n",
            "[16,    40] loss: 0.990\n",
            "[16,    50] loss: 0.990\n",
            "[17,    10] loss: 0.973\n",
            "[17,    20] loss: 0.987\n",
            "[17,    30] loss: 0.980\n",
            "[17,    40] loss: 0.994\n",
            "[17,    50] loss: 0.991\n",
            "[18,    10] loss: 0.982\n",
            "[18,    20] loss: 1.002\n",
            "[18,    30] loss: 0.985\n",
            "[18,    40] loss: 0.980\n",
            "[18,    50] loss: 0.975\n",
            "[19,    10] loss: 0.985\n",
            "[19,    20] loss: 0.979\n",
            "[19,    30] loss: 0.984\n",
            "[19,    40] loss: 0.978\n",
            "[19,    50] loss: 0.984\n",
            "[20,    10] loss: 0.987\n",
            "[20,    20] loss: 0.991\n",
            "[20,    30] loss: 0.983\n",
            "[20,    40] loss: 0.975\n",
            "[20,    50] loss: 0.977\n",
            "[21,    10] loss: 0.985\n",
            "[21,    20] loss: 0.987\n",
            "[21,    30] loss: 0.979\n",
            "[21,    40] loss: 0.997\n",
            "[21,    50] loss: 0.971\n",
            "[22,    10] loss: 0.977\n",
            "[22,    20] loss: 0.988\n",
            "[22,    30] loss: 0.975\n",
            "[22,    40] loss: 0.979\n",
            "[22,    50] loss: 0.985\n",
            "[23,    10] loss: 0.987\n",
            "[23,    20] loss: 0.984\n",
            "[23,    30] loss: 0.979\n",
            "[23,    40] loss: 0.977\n",
            "[23,    50] loss: 0.986\n",
            "[24,    10] loss: 0.988\n",
            "[24,    20] loss: 0.991\n",
            "[24,    30] loss: 0.992\n",
            "[24,    40] loss: 0.986\n",
            "[24,    50] loss: 0.979\n",
            "[25,    10] loss: 0.967\n",
            "[25,    20] loss: 0.993\n",
            "[25,    30] loss: 0.971\n",
            "[25,    40] loss: 0.993\n",
            "[25,    50] loss: 0.979\n",
            "[26,    10] loss: 0.987\n",
            "[26,    20] loss: 0.984\n",
            "[26,    30] loss: 0.979\n",
            "[26,    40] loss: 0.975\n",
            "[26,    50] loss: 0.983\n",
            "[27,    10] loss: 0.982\n",
            "[27,    20] loss: 0.983\n",
            "[27,    30] loss: 0.995\n",
            "[27,    40] loss: 0.974\n",
            "[27,    50] loss: 0.969\n",
            "[28,    10] loss: 0.961\n",
            "[28,    20] loss: 0.987\n",
            "[28,    30] loss: 0.981\n",
            "[28,    40] loss: 0.986\n",
            "[28,    50] loss: 0.991\n",
            "[29,    10] loss: 0.989\n",
            "[29,    20] loss: 0.987\n",
            "[29,    30] loss: 0.983\n",
            "[29,    40] loss: 0.973\n",
            "[29,    50] loss: 0.983\n",
            "[30,    10] loss: 0.984\n",
            "[30,    20] loss: 0.982\n",
            "[30,    30] loss: 0.990\n",
            "[30,    40] loss: 0.981\n",
            "[30,    50] loss: 0.994\n",
            "[31,    10] loss: 0.983\n",
            "[31,    20] loss: 0.984\n",
            "[31,    30] loss: 0.988\n",
            "[31,    40] loss: 0.982\n",
            "[31,    50] loss: 0.989\n",
            "[32,    10] loss: 0.995\n",
            "[32,    20] loss: 0.989\n",
            "[32,    30] loss: 0.985\n",
            "[32,    40] loss: 0.995\n",
            "[32,    50] loss: 0.975\n",
            "[33,    10] loss: 0.986\n",
            "[33,    20] loss: 0.988\n",
            "[33,    30] loss: 0.984\n",
            "[33,    40] loss: 0.980\n",
            "[33,    50] loss: 0.975\n",
            "[34,    10] loss: 0.969\n",
            "[34,    20] loss: 0.990\n",
            "[34,    30] loss: 0.980\n",
            "[34,    40] loss: 0.984\n",
            "[34,    50] loss: 0.973\n",
            "[35,    10] loss: 0.977\n",
            "[35,    20] loss: 0.987\n",
            "[35,    30] loss: 0.987\n",
            "[35,    40] loss: 0.984\n",
            "[35,    50] loss: 0.980\n",
            "[36,    10] loss: 0.996\n",
            "[36,    20] loss: 0.984\n",
            "[36,    30] loss: 0.974\n",
            "[36,    40] loss: 0.978\n",
            "[36,    50] loss: 0.972\n",
            "[37,    10] loss: 0.982\n",
            "[37,    20] loss: 0.972\n",
            "[37,    30] loss: 0.977\n",
            "[37,    40] loss: 0.988\n",
            "[37,    50] loss: 0.987\n",
            "[38,    10] loss: 0.990\n",
            "[38,    20] loss: 0.966\n",
            "[38,    30] loss: 0.977\n",
            "[38,    40] loss: 0.992\n",
            "[38,    50] loss: 0.986\n",
            "[39,    10] loss: 0.981\n",
            "[39,    20] loss: 0.989\n",
            "[39,    30] loss: 0.969\n",
            "[39,    40] loss: 0.987\n",
            "[39,    50] loss: 0.985\n",
            "[40,    10] loss: 1.001\n",
            "[40,    20] loss: 0.973\n",
            "[40,    30] loss: 0.984\n",
            "[40,    40] loss: 0.977\n",
            "[40,    50] loss: 0.983\n",
            "[41,    10] loss: 0.990\n",
            "[41,    20] loss: 0.981\n",
            "[41,    30] loss: 0.978\n",
            "[41,    40] loss: 0.982\n",
            "[41,    50] loss: 0.984\n",
            "[42,    10] loss: 0.972\n",
            "[42,    20] loss: 0.988\n",
            "[42,    30] loss: 0.984\n",
            "[42,    40] loss: 0.983\n",
            "[42,    50] loss: 0.994\n",
            "[43,    10] loss: 0.982\n",
            "[43,    20] loss: 0.984\n",
            "[43,    30] loss: 0.986\n",
            "[43,    40] loss: 0.971\n",
            "[43,    50] loss: 0.981\n",
            "[44,    10] loss: 0.984\n",
            "[44,    20] loss: 0.996\n",
            "[44,    30] loss: 0.973\n",
            "[44,    40] loss: 0.985\n",
            "[44,    50] loss: 0.973\n",
            "[45,    10] loss: 0.970\n",
            "[45,    20] loss: 0.991\n",
            "[45,    30] loss: 0.984\n",
            "[45,    40] loss: 0.983\n",
            "[45,    50] loss: 0.972\n",
            "[46,    10] loss: 0.979\n",
            "[46,    20] loss: 0.970\n",
            "[46,    30] loss: 0.985\n",
            "[46,    40] loss: 0.983\n",
            "[46,    50] loss: 0.983\n",
            "[47,    10] loss: 0.980\n",
            "[47,    20] loss: 0.989\n",
            "[47,    30] loss: 0.971\n",
            "[47,    40] loss: 0.979\n",
            "[47,    50] loss: 0.986\n",
            "[48,    10] loss: 0.979\n",
            "[48,    20] loss: 0.983\n",
            "[48,    30] loss: 0.985\n",
            "[48,    40] loss: 0.983\n",
            "[48,    50] loss: 0.977\n",
            "[49,    10] loss: 0.962\n",
            "[49,    20] loss: 0.982\n",
            "[49,    30] loss: 0.995\n",
            "[49,    40] loss: 0.978\n",
            "[49,    50] loss: 0.985\n",
            "[50,    10] loss: 0.982\n",
            "[50,    20] loss: 0.969\n",
            "[50,    30] loss: 0.986\n",
            "[50,    40] loss: 0.986\n",
            "[50,    50] loss: 0.979\n",
            "[51,    10] loss: 0.989\n",
            "[51,    20] loss: 0.983\n",
            "[51,    30] loss: 0.998\n",
            "[51,    40] loss: 0.980\n",
            "[51,    50] loss: 0.971\n",
            "[52,    10] loss: 0.996\n",
            "[52,    20] loss: 0.979\n",
            "[52,    30] loss: 0.972\n",
            "[52,    40] loss: 0.980\n",
            "[52,    50] loss: 0.982\n",
            "[53,    10] loss: 0.981\n",
            "[53,    20] loss: 0.984\n",
            "[53,    30] loss: 0.969\n",
            "[53,    40] loss: 0.990\n",
            "[53,    50] loss: 0.991\n",
            "[54,    10] loss: 0.982\n",
            "[54,    20] loss: 0.981\n",
            "[54,    30] loss: 0.985\n",
            "[54,    40] loss: 0.974\n",
            "[54,    50] loss: 0.979\n",
            "[55,    10] loss: 0.982\n",
            "[55,    20] loss: 0.974\n",
            "[55,    30] loss: 0.981\n",
            "[55,    40] loss: 0.991\n",
            "[55,    50] loss: 0.993\n",
            "[56,    10] loss: 0.991\n",
            "[56,    20] loss: 0.975\n",
            "[56,    30] loss: 0.983\n",
            "[56,    40] loss: 0.978\n",
            "[56,    50] loss: 0.997\n",
            "[57,    10] loss: 0.976\n",
            "[57,    20] loss: 0.975\n",
            "[57,    30] loss: 0.983\n",
            "[57,    40] loss: 0.975\n",
            "[57,    50] loss: 0.991\n",
            "[58,    10] loss: 0.980\n",
            "[58,    20] loss: 0.985\n",
            "[58,    30] loss: 0.975\n",
            "[58,    40] loss: 0.985\n",
            "[58,    50] loss: 0.982\n",
            "[59,    10] loss: 0.987\n",
            "[59,    20] loss: 0.968\n",
            "[59,    30] loss: 0.979\n",
            "[59,    40] loss: 0.978\n",
            "[59,    50] loss: 0.993\n",
            "[60,    10] loss: 0.977\n",
            "[60,    20] loss: 0.981\n",
            "[60,    30] loss: 0.978\n",
            "[60,    40] loss: 0.980\n",
            "[60,    50] loss: 0.980\n",
            "[61,    10] loss: 0.973\n",
            "[61,    20] loss: 0.981\n",
            "[61,    30] loss: 0.975\n",
            "[61,    40] loss: 0.982\n",
            "[61,    50] loss: 0.981\n",
            "[62,    10] loss: 0.961\n",
            "[62,    20] loss: 0.997\n",
            "[62,    30] loss: 0.989\n",
            "[62,    40] loss: 0.975\n",
            "[62,    50] loss: 0.987\n",
            "[63,    10] loss: 0.974\n",
            "[63,    20] loss: 0.981\n",
            "[63,    30] loss: 0.987\n",
            "[63,    40] loss: 0.981\n",
            "[63,    50] loss: 0.987\n",
            "[64,    10] loss: 0.976\n",
            "[64,    20] loss: 0.985\n",
            "[64,    30] loss: 0.981\n",
            "[64,    40] loss: 0.967\n",
            "[64,    50] loss: 0.985\n",
            "[65,    10] loss: 0.969\n",
            "[65,    20] loss: 0.977\n",
            "[65,    30] loss: 0.983\n",
            "[65,    40] loss: 0.984\n",
            "[65,    50] loss: 0.984\n",
            "[66,    10] loss: 0.995\n",
            "[66,    20] loss: 0.986\n",
            "[66,    30] loss: 0.978\n",
            "[66,    40] loss: 0.979\n",
            "[66,    50] loss: 0.976\n",
            "[67,    10] loss: 0.967\n",
            "[67,    20] loss: 0.973\n",
            "[67,    30] loss: 0.998\n",
            "[67,    40] loss: 0.993\n",
            "[67,    50] loss: 0.971\n",
            "[68,    10] loss: 0.982\n",
            "[68,    20] loss: 0.974\n",
            "[68,    30] loss: 0.982\n",
            "[68,    40] loss: 0.975\n",
            "[68,    50] loss: 0.996\n",
            "[69,    10] loss: 0.988\n",
            "[69,    20] loss: 0.979\n",
            "[69,    30] loss: 0.983\n",
            "[69,    40] loss: 0.975\n",
            "[69,    50] loss: 0.975\n",
            "[70,    10] loss: 0.973\n",
            "[70,    20] loss: 0.973\n",
            "[70,    30] loss: 0.989\n",
            "[70,    40] loss: 0.976\n",
            "[70,    50] loss: 0.988\n",
            "[71,    10] loss: 0.988\n",
            "[71,    20] loss: 0.988\n",
            "[71,    30] loss: 0.978\n",
            "[71,    40] loss: 0.977\n",
            "[71,    50] loss: 0.981\n",
            "[72,    10] loss: 0.989\n",
            "[72,    20] loss: 0.980\n",
            "[72,    30] loss: 0.980\n",
            "[72,    40] loss: 0.986\n",
            "[72,    50] loss: 0.980\n",
            "[73,    10] loss: 0.984\n",
            "[73,    20] loss: 0.992\n",
            "[73,    30] loss: 0.973\n",
            "[73,    40] loss: 0.974\n",
            "[73,    50] loss: 0.988\n",
            "[74,    10] loss: 0.984\n",
            "[74,    20] loss: 0.983\n",
            "[74,    30] loss: 0.979\n",
            "[74,    40] loss: 0.981\n",
            "[74,    50] loss: 0.978\n",
            "[75,    10] loss: 0.972\n",
            "[75,    20] loss: 0.977\n",
            "[75,    30] loss: 0.988\n",
            "[75,    40] loss: 0.981\n",
            "[75,    50] loss: 0.981\n",
            "[76,    10] loss: 0.991\n",
            "[76,    20] loss: 0.976\n",
            "[76,    30] loss: 0.985\n",
            "[76,    40] loss: 0.990\n",
            "[76,    50] loss: 0.981\n",
            "[77,    10] loss: 0.974\n",
            "[77,    20] loss: 0.973\n",
            "[77,    30] loss: 0.987\n",
            "[77,    40] loss: 0.985\n",
            "[77,    50] loss: 0.978\n",
            "[78,    10] loss: 0.974\n",
            "[78,    20] loss: 0.976\n",
            "[78,    30] loss: 0.985\n",
            "[78,    40] loss: 0.989\n",
            "[78,    50] loss: 0.982\n",
            "[79,    10] loss: 0.984\n",
            "[79,    20] loss: 0.978\n",
            "[79,    30] loss: 0.982\n",
            "[79,    40] loss: 0.979\n",
            "[79,    50] loss: 0.982\n",
            "[80,    10] loss: 0.979\n",
            "[80,    20] loss: 0.978\n",
            "[80,    30] loss: 0.999\n",
            "[80,    40] loss: 0.971\n",
            "[80,    50] loss: 0.989\n",
            "[81,    10] loss: 0.980\n",
            "[81,    20] loss: 0.979\n",
            "[81,    30] loss: 0.980\n",
            "[81,    40] loss: 0.983\n",
            "[81,    50] loss: 0.977\n",
            "[82,    10] loss: 0.981\n",
            "[82,    20] loss: 0.974\n",
            "[82,    30] loss: 0.994\n",
            "[82,    40] loss: 0.972\n",
            "[82,    50] loss: 0.986\n",
            "[83,    10] loss: 0.973\n",
            "[83,    20] loss: 0.970\n",
            "[83,    30] loss: 0.985\n",
            "[83,    40] loss: 0.988\n",
            "[83,    50] loss: 0.979\n",
            "[84,    10] loss: 0.984\n",
            "[84,    20] loss: 0.996\n",
            "[84,    30] loss: 0.978\n",
            "[84,    40] loss: 0.983\n",
            "[84,    50] loss: 0.976\n",
            "[85,    10] loss: 0.984\n",
            "[85,    20] loss: 0.987\n",
            "[85,    30] loss: 0.979\n",
            "[85,    40] loss: 0.977\n",
            "[85,    50] loss: 0.979\n",
            "[86,    10] loss: 0.994\n",
            "[86,    20] loss: 0.980\n",
            "[86,    30] loss: 0.966\n",
            "[86,    40] loss: 0.982\n",
            "[86,    50] loss: 0.971\n",
            "[87,    10] loss: 0.980\n",
            "[87,    20] loss: 0.968\n",
            "[87,    30] loss: 0.976\n",
            "[87,    40] loss: 0.991\n",
            "[87,    50] loss: 0.986\n",
            "[88,    10] loss: 0.981\n",
            "[88,    20] loss: 0.980\n",
            "[88,    30] loss: 0.994\n",
            "[88,    40] loss: 0.974\n",
            "[88,    50] loss: 0.974\n",
            "[89,    10] loss: 0.980\n",
            "[89,    20] loss: 0.977\n",
            "[89,    30] loss: 0.979\n",
            "[89,    40] loss: 0.984\n",
            "[89,    50] loss: 0.988\n",
            "[90,    10] loss: 0.974\n",
            "[90,    20] loss: 0.982\n",
            "[90,    30] loss: 0.968\n",
            "[90,    40] loss: 0.987\n",
            "[90,    50] loss: 0.983\n",
            "[91,    10] loss: 0.975\n",
            "[91,    20] loss: 0.979\n",
            "[91,    30] loss: 0.968\n",
            "[91,    40] loss: 0.999\n",
            "[91,    50] loss: 0.973\n",
            "[92,    10] loss: 0.973\n",
            "[92,    20] loss: 0.984\n",
            "[92,    30] loss: 0.974\n",
            "[92,    40] loss: 0.994\n",
            "[92,    50] loss: 0.977\n",
            "[93,    10] loss: 0.987\n",
            "[93,    20] loss: 1.002\n",
            "[93,    30] loss: 0.972\n",
            "[93,    40] loss: 0.980\n",
            "[93,    50] loss: 0.975\n",
            "[94,    10] loss: 0.978\n",
            "[94,    20] loss: 0.980\n",
            "[94,    30] loss: 0.982\n",
            "[94,    40] loss: 0.970\n",
            "[94,    50] loss: 0.985\n",
            "[95,    10] loss: 0.975\n",
            "[95,    20] loss: 0.964\n",
            "[95,    30] loss: 0.986\n",
            "[95,    40] loss: 0.984\n",
            "[95,    50] loss: 0.994\n",
            "[96,    10] loss: 0.970\n",
            "[96,    20] loss: 0.993\n",
            "[96,    30] loss: 0.981\n",
            "[96,    40] loss: 0.976\n",
            "[96,    50] loss: 0.982\n",
            "[97,    10] loss: 0.968\n",
            "[97,    20] loss: 0.978\n",
            "[97,    30] loss: 0.976\n",
            "[97,    40] loss: 0.975\n",
            "[97,    50] loss: 0.995\n",
            "[98,    10] loss: 0.978\n",
            "[98,    20] loss: 0.985\n",
            "[98,    30] loss: 0.982\n",
            "[98,    40] loss: 0.982\n",
            "[98,    50] loss: 0.986\n",
            "[99,    10] loss: 0.987\n",
            "[99,    20] loss: 0.977\n",
            "[99,    30] loss: 0.978\n",
            "[99,    40] loss: 0.980\n",
            "[99,    50] loss: 0.983\n",
            "[100,    10] loss: 0.975\n",
            "[100,    20] loss: 0.978\n",
            "[100,    30] loss: 0.974\n",
            "[100,    40] loss: 0.991\n",
            "[100,    50] loss: 0.984\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 53 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 50 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 41 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 1.102\n",
            "[1,    20] loss: 1.097\n",
            "[1,    30] loss: 1.094\n",
            "[1,    40] loss: 1.096\n",
            "[1,    50] loss: 1.096\n",
            "[2,    10] loss: 1.095\n",
            "[2,    20] loss: 1.093\n",
            "[2,    30] loss: 1.093\n",
            "[2,    40] loss: 1.092\n",
            "[2,    50] loss: 1.092\n",
            "[3,    10] loss: 1.091\n",
            "[3,    20] loss: 1.088\n",
            "[3,    30] loss: 1.089\n",
            "[3,    40] loss: 1.089\n",
            "[3,    50] loss: 1.090\n",
            "[4,    10] loss: 1.087\n",
            "[4,    20] loss: 1.085\n",
            "[4,    30] loss: 1.082\n",
            "[4,    40] loss: 1.082\n",
            "[4,    50] loss: 1.080\n",
            "[5,    10] loss: 1.076\n",
            "[5,    20] loss: 1.067\n",
            "[5,    30] loss: 1.063\n",
            "[5,    40] loss: 1.051\n",
            "[5,    50] loss: 1.048\n",
            "[6,    10] loss: 1.029\n",
            "[6,    20] loss: 1.008\n",
            "[6,    30] loss: 0.985\n",
            "[6,    40] loss: 0.963\n",
            "[6,    50] loss: 0.939\n",
            "[7,    10] loss: 0.923\n",
            "[7,    20] loss: 0.925\n",
            "[7,    30] loss: 0.897\n",
            "[7,    40] loss: 0.881\n",
            "[7,    50] loss: 0.892\n",
            "[8,    10] loss: 0.871\n",
            "[8,    20] loss: 0.885\n",
            "[8,    30] loss: 0.871\n",
            "[8,    40] loss: 0.869\n",
            "[8,    50] loss: 0.891\n",
            "[9,    10] loss: 0.880\n",
            "[9,    20] loss: 0.875\n",
            "[9,    30] loss: 0.868\n",
            "[9,    40] loss: 0.848\n",
            "[9,    50] loss: 0.850\n",
            "[10,    10] loss: 0.862\n",
            "[10,    20] loss: 0.874\n",
            "[10,    30] loss: 0.846\n",
            "[10,    40] loss: 0.856\n",
            "[10,    50] loss: 0.852\n",
            "[11,    10] loss: 0.857\n",
            "[11,    20] loss: 0.890\n",
            "[11,    30] loss: 0.889\n",
            "[11,    40] loss: 0.895\n",
            "[11,    50] loss: 0.857\n",
            "[12,    10] loss: 0.854\n",
            "[12,    20] loss: 0.866\n",
            "[12,    30] loss: 0.864\n",
            "[12,    40] loss: 0.849\n",
            "[12,    50] loss: 0.860\n",
            "[13,    10] loss: 0.854\n",
            "[13,    20] loss: 0.849\n",
            "[13,    30] loss: 0.853\n",
            "[13,    40] loss: 0.867\n",
            "[13,    50] loss: 0.872\n",
            "[14,    10] loss: 0.879\n",
            "[14,    20] loss: 0.857\n",
            "[14,    30] loss: 0.871\n",
            "[14,    40] loss: 0.836\n",
            "[14,    50] loss: 0.851\n",
            "[15,    10] loss: 0.857\n",
            "[15,    20] loss: 0.857\n",
            "[15,    30] loss: 0.861\n",
            "[15,    40] loss: 0.858\n",
            "[15,    50] loss: 0.854\n",
            "[16,    10] loss: 0.850\n",
            "[16,    20] loss: 0.858\n",
            "[16,    30] loss: 0.855\n",
            "[16,    40] loss: 0.869\n",
            "[16,    50] loss: 0.855\n",
            "[17,    10] loss: 0.851\n",
            "[17,    20] loss: 0.864\n",
            "[17,    30] loss: 0.873\n",
            "[17,    40] loss: 0.867\n",
            "[17,    50] loss: 0.867\n",
            "[18,    10] loss: 0.854\n",
            "[18,    20] loss: 0.844\n",
            "[18,    30] loss: 0.874\n",
            "[18,    40] loss: 0.870\n",
            "[18,    50] loss: 0.865\n",
            "[19,    10] loss: 0.845\n",
            "[19,    20] loss: 0.847\n",
            "[19,    30] loss: 0.870\n",
            "[19,    40] loss: 0.862\n",
            "[19,    50] loss: 0.834\n",
            "[20,    10] loss: 0.846\n",
            "[20,    20] loss: 0.867\n",
            "[20,    30] loss: 0.861\n",
            "[20,    40] loss: 0.838\n",
            "[20,    50] loss: 0.852\n",
            "[21,    10] loss: 0.853\n",
            "[21,    20] loss: 0.846\n",
            "[21,    30] loss: 0.872\n",
            "[21,    40] loss: 0.857\n",
            "[21,    50] loss: 0.872\n",
            "[22,    10] loss: 0.884\n",
            "[22,    20] loss: 0.863\n",
            "[22,    30] loss: 0.866\n",
            "[22,    40] loss: 0.865\n",
            "[22,    50] loss: 0.854\n",
            "[23,    10] loss: 0.871\n",
            "[23,    20] loss: 0.836\n",
            "[23,    30] loss: 0.869\n",
            "[23,    40] loss: 0.844\n",
            "[23,    50] loss: 0.867\n",
            "[24,    10] loss: 0.852\n",
            "[24,    20] loss: 0.851\n",
            "[24,    30] loss: 0.864\n",
            "[24,    40] loss: 0.843\n",
            "[24,    50] loss: 0.847\n",
            "[25,    10] loss: 0.848\n",
            "[25,    20] loss: 0.854\n",
            "[25,    30] loss: 0.868\n",
            "[25,    40] loss: 0.843\n",
            "[25,    50] loss: 0.852\n",
            "[26,    10] loss: 0.848\n",
            "[26,    20] loss: 0.859\n",
            "[26,    30] loss: 0.859\n",
            "[26,    40] loss: 0.834\n",
            "[26,    50] loss: 0.858\n",
            "[27,    10] loss: 0.870\n",
            "[27,    20] loss: 0.866\n",
            "[27,    30] loss: 0.859\n",
            "[27,    40] loss: 0.863\n",
            "[27,    50] loss: 0.865\n",
            "[28,    10] loss: 0.861\n",
            "[28,    20] loss: 0.879\n",
            "[28,    30] loss: 0.858\n",
            "[28,    40] loss: 0.859\n",
            "[28,    50] loss: 0.850\n",
            "[29,    10] loss: 0.836\n",
            "[29,    20] loss: 0.840\n",
            "[29,    30] loss: 0.875\n",
            "[29,    40] loss: 0.866\n",
            "[29,    50] loss: 0.850\n",
            "[30,    10] loss: 0.858\n",
            "[30,    20] loss: 0.859\n",
            "[30,    30] loss: 0.857\n",
            "[30,    40] loss: 0.871\n",
            "[30,    50] loss: 0.844\n",
            "[31,    10] loss: 0.856\n",
            "[31,    20] loss: 0.855\n",
            "[31,    30] loss: 0.839\n",
            "[31,    40] loss: 0.856\n",
            "[31,    50] loss: 0.854\n",
            "[32,    10] loss: 0.842\n",
            "[32,    20] loss: 0.870\n",
            "[32,    30] loss: 0.850\n",
            "[32,    40] loss: 0.849\n",
            "[32,    50] loss: 0.857\n",
            "[33,    10] loss: 0.842\n",
            "[33,    20] loss: 0.879\n",
            "[33,    30] loss: 0.841\n",
            "[33,    40] loss: 0.866\n",
            "[33,    50] loss: 0.843\n",
            "[34,    10] loss: 0.843\n",
            "[34,    20] loss: 0.864\n",
            "[34,    30] loss: 0.871\n",
            "[34,    40] loss: 0.859\n",
            "[34,    50] loss: 0.850\n",
            "[35,    10] loss: 0.849\n",
            "[35,    20] loss: 0.851\n",
            "[35,    30] loss: 0.865\n",
            "[35,    40] loss: 0.834\n",
            "[35,    50] loss: 0.854\n",
            "[36,    10] loss: 0.863\n",
            "[36,    20] loss: 0.849\n",
            "[36,    30] loss: 0.848\n",
            "[36,    40] loss: 0.835\n",
            "[36,    50] loss: 0.855\n",
            "[37,    10] loss: 0.855\n",
            "[37,    20] loss: 0.843\n",
            "[37,    30] loss: 0.877\n",
            "[37,    40] loss: 0.849\n",
            "[37,    50] loss: 0.839\n",
            "[38,    10] loss: 0.840\n",
            "[38,    20] loss: 0.859\n",
            "[38,    30] loss: 0.848\n",
            "[38,    40] loss: 0.865\n",
            "[38,    50] loss: 0.851\n",
            "[39,    10] loss: 0.853\n",
            "[39,    20] loss: 0.862\n",
            "[39,    30] loss: 0.846\n",
            "[39,    40] loss: 0.848\n",
            "[39,    50] loss: 0.870\n",
            "[40,    10] loss: 0.879\n",
            "[40,    20] loss: 0.865\n",
            "[40,    30] loss: 0.850\n",
            "[40,    40] loss: 0.840\n",
            "[40,    50] loss: 0.845\n",
            "[41,    10] loss: 0.853\n",
            "[41,    20] loss: 0.841\n",
            "[41,    30] loss: 0.839\n",
            "[41,    40] loss: 0.860\n",
            "[41,    50] loss: 0.869\n",
            "[42,    10] loss: 0.853\n",
            "[42,    20] loss: 0.854\n",
            "[42,    30] loss: 0.848\n",
            "[42,    40] loss: 0.859\n",
            "[42,    50] loss: 0.836\n",
            "[43,    10] loss: 0.851\n",
            "[43,    20] loss: 0.867\n",
            "[43,    30] loss: 0.857\n",
            "[43,    40] loss: 0.855\n",
            "[43,    50] loss: 0.842\n",
            "[44,    10] loss: 0.856\n",
            "[44,    20] loss: 0.872\n",
            "[44,    30] loss: 0.844\n",
            "[44,    40] loss: 0.849\n",
            "[44,    50] loss: 0.857\n",
            "[45,    10] loss: 0.855\n",
            "[45,    20] loss: 0.875\n",
            "[45,    30] loss: 0.858\n",
            "[45,    40] loss: 0.855\n",
            "[45,    50] loss: 0.848\n",
            "[46,    10] loss: 0.869\n",
            "[46,    20] loss: 0.861\n",
            "[46,    30] loss: 0.849\n",
            "[46,    40] loss: 0.861\n",
            "[46,    50] loss: 0.837\n",
            "[47,    10] loss: 0.855\n",
            "[47,    20] loss: 0.871\n",
            "[47,    30] loss: 0.848\n",
            "[47,    40] loss: 0.841\n",
            "[47,    50] loss: 0.880\n",
            "[48,    10] loss: 0.868\n",
            "[48,    20] loss: 0.862\n",
            "[48,    30] loss: 0.830\n",
            "[48,    40] loss: 0.862\n",
            "[48,    50] loss: 0.845\n",
            "[49,    10] loss: 0.846\n",
            "[49,    20] loss: 0.853\n",
            "[49,    30] loss: 0.847\n",
            "[49,    40] loss: 0.869\n",
            "[49,    50] loss: 0.856\n",
            "[50,    10] loss: 0.856\n",
            "[50,    20] loss: 0.848\n",
            "[50,    30] loss: 0.850\n",
            "[50,    40] loss: 0.846\n",
            "[50,    50] loss: 0.861\n",
            "[51,    10] loss: 0.836\n",
            "[51,    20] loss: 0.834\n",
            "[51,    30] loss: 0.862\n",
            "[51,    40] loss: 0.856\n",
            "[51,    50] loss: 0.858\n",
            "[52,    10] loss: 0.856\n",
            "[52,    20] loss: 0.873\n",
            "[52,    30] loss: 0.849\n",
            "[52,    40] loss: 0.853\n",
            "[52,    50] loss: 0.851\n",
            "[53,    10] loss: 0.859\n",
            "[53,    20] loss: 0.844\n",
            "[53,    30] loss: 0.857\n",
            "[53,    40] loss: 0.849\n",
            "[53,    50] loss: 0.859\n",
            "[54,    10] loss: 0.845\n",
            "[54,    20] loss: 0.851\n",
            "[54,    30] loss: 0.862\n",
            "[54,    40] loss: 0.857\n",
            "[54,    50] loss: 0.857\n",
            "[55,    10] loss: 0.842\n",
            "[55,    20] loss: 0.859\n",
            "[55,    30] loss: 0.857\n",
            "[55,    40] loss: 0.856\n",
            "[55,    50] loss: 0.857\n",
            "[56,    10] loss: 0.849\n",
            "[56,    20] loss: 0.852\n",
            "[56,    30] loss: 0.874\n",
            "[56,    40] loss: 0.883\n",
            "[56,    50] loss: 0.849\n",
            "[57,    10] loss: 0.854\n",
            "[57,    20] loss: 0.875\n",
            "[57,    30] loss: 0.855\n",
            "[57,    40] loss: 0.859\n",
            "[57,    50] loss: 0.832\n",
            "[58,    10] loss: 0.840\n",
            "[58,    20] loss: 0.856\n",
            "[58,    30] loss: 0.862\n",
            "[58,    40] loss: 0.856\n",
            "[58,    50] loss: 0.837\n",
            "[59,    10] loss: 0.852\n",
            "[59,    20] loss: 0.854\n",
            "[59,    30] loss: 0.853\n",
            "[59,    40] loss: 0.868\n",
            "[59,    50] loss: 0.840\n",
            "[60,    10] loss: 0.851\n",
            "[60,    20] loss: 0.862\n",
            "[60,    30] loss: 0.888\n",
            "[60,    40] loss: 0.877\n",
            "[60,    50] loss: 0.862\n",
            "[61,    10] loss: 0.863\n",
            "[61,    20] loss: 0.841\n",
            "[61,    30] loss: 0.860\n",
            "[61,    40] loss: 0.855\n",
            "[61,    50] loss: 0.837\n",
            "[62,    10] loss: 0.861\n",
            "[62,    20] loss: 0.849\n",
            "[62,    30] loss: 0.836\n",
            "[62,    40] loss: 0.861\n",
            "[62,    50] loss: 0.852\n",
            "[63,    10] loss: 0.859\n",
            "[63,    20] loss: 0.836\n",
            "[63,    30] loss: 0.855\n",
            "[63,    40] loss: 0.864\n",
            "[63,    50] loss: 0.846\n",
            "[64,    10] loss: 0.837\n",
            "[64,    20] loss: 0.852\n",
            "[64,    30] loss: 0.842\n",
            "[64,    40] loss: 0.852\n",
            "[64,    50] loss: 0.859\n",
            "[65,    10] loss: 0.852\n",
            "[65,    20] loss: 0.853\n",
            "[65,    30] loss: 0.856\n",
            "[65,    40] loss: 0.839\n",
            "[65,    50] loss: 0.854\n",
            "[66,    10] loss: 0.853\n",
            "[66,    20] loss: 0.845\n",
            "[66,    30] loss: 0.841\n",
            "[66,    40] loss: 0.835\n",
            "[66,    50] loss: 0.865\n",
            "[67,    10] loss: 0.843\n",
            "[67,    20] loss: 0.838\n",
            "[67,    30] loss: 0.860\n",
            "[67,    40] loss: 0.851\n",
            "[67,    50] loss: 0.844\n",
            "[68,    10] loss: 0.850\n",
            "[68,    20] loss: 0.850\n",
            "[68,    30] loss: 0.850\n",
            "[68,    40] loss: 0.857\n",
            "[68,    50] loss: 0.858\n",
            "[69,    10] loss: 0.854\n",
            "[69,    20] loss: 0.849\n",
            "[69,    30] loss: 0.849\n",
            "[69,    40] loss: 0.858\n",
            "[69,    50] loss: 0.840\n",
            "[70,    10] loss: 0.851\n",
            "[70,    20] loss: 0.850\n",
            "[70,    30] loss: 0.832\n",
            "[70,    40] loss: 0.848\n",
            "[70,    50] loss: 0.878\n",
            "[71,    10] loss: 0.858\n",
            "[71,    20] loss: 0.860\n",
            "[71,    30] loss: 0.844\n",
            "[71,    40] loss: 0.842\n",
            "[71,    50] loss: 0.852\n",
            "[72,    10] loss: 0.858\n",
            "[72,    20] loss: 0.846\n",
            "[72,    30] loss: 0.864\n",
            "[72,    40] loss: 0.844\n",
            "[72,    50] loss: 0.842\n",
            "[73,    10] loss: 0.858\n",
            "[73,    20] loss: 0.851\n",
            "[73,    30] loss: 0.836\n",
            "[73,    40] loss: 0.845\n",
            "[73,    50] loss: 0.854\n",
            "[74,    10] loss: 0.847\n",
            "[74,    20] loss: 0.864\n",
            "[74,    30] loss: 0.859\n",
            "[74,    40] loss: 0.843\n",
            "[74,    50] loss: 0.843\n",
            "[75,    10] loss: 0.853\n",
            "[75,    20] loss: 0.856\n",
            "[75,    30] loss: 0.855\n",
            "[75,    40] loss: 0.849\n",
            "[75,    50] loss: 0.856\n",
            "[76,    10] loss: 0.870\n",
            "[76,    20] loss: 0.855\n",
            "[76,    30] loss: 0.865\n",
            "[76,    40] loss: 0.836\n",
            "[76,    50] loss: 0.847\n",
            "[77,    10] loss: 0.851\n",
            "[77,    20] loss: 0.853\n",
            "[77,    30] loss: 0.847\n",
            "[77,    40] loss: 0.861\n",
            "[77,    50] loss: 0.854\n",
            "[78,    10] loss: 0.856\n",
            "[78,    20] loss: 0.839\n",
            "[78,    30] loss: 0.851\n",
            "[78,    40] loss: 0.851\n",
            "[78,    50] loss: 0.850\n",
            "[79,    10] loss: 0.833\n",
            "[79,    20] loss: 0.863\n",
            "[79,    30] loss: 0.876\n",
            "[79,    40] loss: 0.845\n",
            "[79,    50] loss: 0.854\n",
            "[80,    10] loss: 0.863\n",
            "[80,    20] loss: 0.852\n",
            "[80,    30] loss: 0.865\n",
            "[80,    40] loss: 0.848\n",
            "[80,    50] loss: 0.841\n",
            "[81,    10] loss: 0.843\n",
            "[81,    20] loss: 0.861\n",
            "[81,    30] loss: 0.875\n",
            "[81,    40] loss: 0.841\n",
            "[81,    50] loss: 0.858\n",
            "[82,    10] loss: 0.854\n",
            "[82,    20] loss: 0.864\n",
            "[82,    30] loss: 0.859\n",
            "[82,    40] loss: 0.843\n",
            "[82,    50] loss: 0.842\n",
            "[83,    10] loss: 0.848\n",
            "[83,    20] loss: 0.853\n",
            "[83,    30] loss: 0.854\n",
            "[83,    40] loss: 0.857\n",
            "[83,    50] loss: 0.841\n",
            "[84,    10] loss: 0.848\n",
            "[84,    20] loss: 0.838\n",
            "[84,    30] loss: 0.843\n",
            "[84,    40] loss: 0.839\n",
            "[84,    50] loss: 0.882\n",
            "[85,    10] loss: 0.851\n",
            "[85,    20] loss: 0.847\n",
            "[85,    30] loss: 0.873\n",
            "[85,    40] loss: 0.848\n",
            "[85,    50] loss: 0.850\n",
            "[86,    10] loss: 0.853\n",
            "[86,    20] loss: 0.841\n",
            "[86,    30] loss: 0.858\n",
            "[86,    40] loss: 0.866\n",
            "[86,    50] loss: 0.862\n",
            "[87,    10] loss: 0.859\n",
            "[87,    20] loss: 0.850\n",
            "[87,    30] loss: 0.859\n",
            "[87,    40] loss: 0.839\n",
            "[87,    50] loss: 0.844\n",
            "[88,    10] loss: 0.843\n",
            "[88,    20] loss: 0.857\n",
            "[88,    30] loss: 0.836\n",
            "[88,    40] loss: 0.873\n",
            "[88,    50] loss: 0.840\n",
            "[89,    10] loss: 0.856\n",
            "[89,    20] loss: 0.860\n",
            "[89,    30] loss: 0.847\n",
            "[89,    40] loss: 0.847\n",
            "[89,    50] loss: 0.861\n",
            "[90,    10] loss: 0.855\n",
            "[90,    20] loss: 0.863\n",
            "[90,    30] loss: 0.874\n",
            "[90,    40] loss: 0.846\n",
            "[90,    50] loss: 0.859\n",
            "[91,    10] loss: 0.858\n",
            "[91,    20] loss: 0.849\n",
            "[91,    30] loss: 0.848\n",
            "[91,    40] loss: 0.845\n",
            "[91,    50] loss: 0.847\n",
            "[92,    10] loss: 0.848\n",
            "[92,    20] loss: 0.858\n",
            "[92,    30] loss: 0.852\n",
            "[92,    40] loss: 0.861\n",
            "[92,    50] loss: 0.847\n",
            "[93,    10] loss: 0.852\n",
            "[93,    20] loss: 0.858\n",
            "[93,    30] loss: 0.848\n",
            "[93,    40] loss: 0.853\n",
            "[93,    50] loss: 0.858\n",
            "[94,    10] loss: 0.863\n",
            "[94,    20] loss: 0.853\n",
            "[94,    30] loss: 0.845\n",
            "[94,    40] loss: 0.855\n",
            "[94,    50] loss: 0.842\n",
            "[95,    10] loss: 0.845\n",
            "[95,    20] loss: 0.856\n",
            "[95,    30] loss: 0.849\n",
            "[95,    40] loss: 0.851\n",
            "[95,    50] loss: 0.840\n",
            "[96,    10] loss: 0.845\n",
            "[96,    20] loss: 0.851\n",
            "[96,    30] loss: 0.822\n",
            "[96,    40] loss: 0.843\n",
            "[96,    50] loss: 0.878\n",
            "[97,    10] loss: 0.857\n",
            "[97,    20] loss: 0.862\n",
            "[97,    30] loss: 0.839\n",
            "[97,    40] loss: 0.855\n",
            "[97,    50] loss: 0.851\n",
            "[98,    10] loss: 0.862\n",
            "[98,    20] loss: 0.838\n",
            "[98,    30] loss: 0.846\n",
            "[98,    40] loss: 0.868\n",
            "[98,    50] loss: 0.844\n",
            "[99,    10] loss: 0.846\n",
            "[99,    20] loss: 0.853\n",
            "[99,    30] loss: 0.852\n",
            "[99,    40] loss: 0.862\n",
            "[99,    50] loss: 0.852\n",
            "[100,    10] loss: 0.854\n",
            "[100,    20] loss: 0.855\n",
            "[100,    30] loss: 0.854\n",
            "[100,    40] loss: 0.851\n",
            "[100,    50] loss: 0.840\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 32 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 35 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 73 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 67 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 67 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 1.100\n",
            "[1,    20] loss: 1.094\n",
            "[1,    30] loss: 1.096\n",
            "[1,    40] loss: 1.094\n",
            "[1,    50] loss: 1.090\n",
            "[2,    10] loss: 1.086\n",
            "[2,    20] loss: 1.089\n",
            "[2,    30] loss: 1.079\n",
            "[2,    40] loss: 1.075\n",
            "[2,    50] loss: 1.062\n",
            "[3,    10] loss: 1.055\n",
            "[3,    20] loss: 1.017\n",
            "[3,    30] loss: 0.964\n",
            "[3,    40] loss: 0.885\n",
            "[3,    50] loss: 0.849\n",
            "[4,    10] loss: 0.769\n",
            "[4,    20] loss: 0.745\n",
            "[4,    30] loss: 0.710\n",
            "[4,    40] loss: 0.761\n",
            "[4,    50] loss: 0.715\n",
            "[5,    10] loss: 0.753\n",
            "[5,    20] loss: 0.711\n",
            "[5,    30] loss: 0.697\n",
            "[5,    40] loss: 0.660\n",
            "[5,    50] loss: 0.703\n",
            "[6,    10] loss: 0.699\n",
            "[6,    20] loss: 0.672\n",
            "[6,    30] loss: 0.682\n",
            "[6,    40] loss: 0.681\n",
            "[6,    50] loss: 0.668\n",
            "[7,    10] loss: 0.678\n",
            "[7,    20] loss: 0.693\n",
            "[7,    30] loss: 0.696\n",
            "[7,    40] loss: 0.680\n",
            "[7,    50] loss: 0.670\n",
            "[8,    10] loss: 0.680\n",
            "[8,    20] loss: 0.658\n",
            "[8,    30] loss: 0.680\n",
            "[8,    40] loss: 0.652\n",
            "[8,    50] loss: 0.708\n",
            "[9,    10] loss: 0.685\n",
            "[9,    20] loss: 0.689\n",
            "[9,    30] loss: 0.691\n",
            "[9,    40] loss: 0.653\n",
            "[9,    50] loss: 0.671\n",
            "[10,    10] loss: 0.679\n",
            "[10,    20] loss: 0.716\n",
            "[10,    30] loss: 0.670\n",
            "[10,    40] loss: 0.677\n",
            "[10,    50] loss: 0.668\n",
            "[11,    10] loss: 0.678\n",
            "[11,    20] loss: 0.668\n",
            "[11,    30] loss: 0.678\n",
            "[11,    40] loss: 0.681\n",
            "[11,    50] loss: 0.687\n",
            "[12,    10] loss: 0.676\n",
            "[12,    20] loss: 0.682\n",
            "[12,    30] loss: 0.665\n",
            "[12,    40] loss: 0.660\n",
            "[12,    50] loss: 0.657\n",
            "[13,    10] loss: 0.644\n",
            "[13,    20] loss: 0.671\n",
            "[13,    30] loss: 0.689\n",
            "[13,    40] loss: 0.685\n",
            "[13,    50] loss: 0.672\n",
            "[14,    10] loss: 0.700\n",
            "[14,    20] loss: 0.685\n",
            "[14,    30] loss: 0.665\n",
            "[14,    40] loss: 0.695\n",
            "[14,    50] loss: 0.658\n",
            "[15,    10] loss: 0.667\n",
            "[15,    20] loss: 0.677\n",
            "[15,    30] loss: 0.663\n",
            "[15,    40] loss: 0.662\n",
            "[15,    50] loss: 0.677\n",
            "[16,    10] loss: 0.682\n",
            "[16,    20] loss: 0.652\n",
            "[16,    30] loss: 0.675\n",
            "[16,    40] loss: 0.667\n",
            "[16,    50] loss: 0.661\n",
            "[17,    10] loss: 0.670\n",
            "[17,    20] loss: 0.648\n",
            "[17,    30] loss: 0.664\n",
            "[17,    40] loss: 0.672\n",
            "[17,    50] loss: 0.679\n",
            "[18,    10] loss: 0.698\n",
            "[18,    20] loss: 0.693\n",
            "[18,    30] loss: 0.682\n",
            "[18,    40] loss: 0.705\n",
            "[18,    50] loss: 0.654\n",
            "[19,    10] loss: 0.660\n",
            "[19,    20] loss: 0.665\n",
            "[19,    30] loss: 0.665\n",
            "[19,    40] loss: 0.709\n",
            "[19,    50] loss: 0.717\n",
            "[20,    10] loss: 0.692\n",
            "[20,    20] loss: 0.669\n",
            "[20,    30] loss: 0.701\n",
            "[20,    40] loss: 0.672\n",
            "[20,    50] loss: 0.670\n",
            "[21,    10] loss: 0.664\n",
            "[21,    20] loss: 0.662\n",
            "[21,    30] loss: 0.647\n",
            "[21,    40] loss: 0.671\n",
            "[21,    50] loss: 0.665\n",
            "[22,    10] loss: 0.649\n",
            "[22,    20] loss: 0.672\n",
            "[22,    30] loss: 0.668\n",
            "[22,    40] loss: 0.668\n",
            "[22,    50] loss: 0.649\n",
            "[23,    10] loss: 0.671\n",
            "[23,    20] loss: 0.646\n",
            "[23,    30] loss: 0.680\n",
            "[23,    40] loss: 0.662\n",
            "[23,    50] loss: 0.694\n",
            "[24,    10] loss: 0.674\n",
            "[24,    20] loss: 0.670\n",
            "[24,    30] loss: 0.659\n",
            "[24,    40] loss: 0.657\n",
            "[24,    50] loss: 0.683\n",
            "[25,    10] loss: 0.732\n",
            "[25,    20] loss: 0.720\n",
            "[25,    30] loss: 0.672\n",
            "[25,    40] loss: 0.681\n",
            "[25,    50] loss: 0.647\n",
            "[26,    10] loss: 0.680\n",
            "[26,    20] loss: 0.664\n",
            "[26,    30] loss: 0.689\n",
            "[26,    40] loss: 0.674\n",
            "[26,    50] loss: 0.665\n",
            "[27,    10] loss: 0.658\n",
            "[27,    20] loss: 0.679\n",
            "[27,    30] loss: 0.665\n",
            "[27,    40] loss: 0.673\n",
            "[27,    50] loss: 0.666\n",
            "[28,    10] loss: 0.668\n",
            "[28,    20] loss: 0.665\n",
            "[28,    30] loss: 0.683\n",
            "[28,    40] loss: 0.661\n",
            "[28,    50] loss: 0.654\n",
            "[29,    10] loss: 0.668\n",
            "[29,    20] loss: 0.690\n",
            "[29,    30] loss: 0.712\n",
            "[29,    40] loss: 0.652\n",
            "[29,    50] loss: 0.688\n",
            "[30,    10] loss: 0.663\n",
            "[30,    20] loss: 0.647\n",
            "[30,    30] loss: 0.660\n",
            "[30,    40] loss: 0.675\n",
            "[30,    50] loss: 0.668\n",
            "[31,    10] loss: 0.680\n",
            "[31,    20] loss: 0.674\n",
            "[31,    30] loss: 0.652\n",
            "[31,    40] loss: 0.648\n",
            "[31,    50] loss: 0.657\n",
            "[32,    10] loss: 0.688\n",
            "[32,    20] loss: 0.652\n",
            "[32,    30] loss: 0.666\n",
            "[32,    40] loss: 0.677\n",
            "[32,    50] loss: 0.685\n",
            "[33,    10] loss: 0.732\n",
            "[33,    20] loss: 0.712\n",
            "[33,    30] loss: 0.661\n",
            "[33,    40] loss: 0.650\n",
            "[33,    50] loss: 0.711\n",
            "[34,    10] loss: 0.681\n",
            "[34,    20] loss: 0.680\n",
            "[34,    30] loss: 0.670\n",
            "[34,    40] loss: 0.683\n",
            "[34,    50] loss: 0.652\n",
            "[35,    10] loss: 0.668\n",
            "[35,    20] loss: 0.639\n",
            "[35,    30] loss: 0.657\n",
            "[35,    40] loss: 0.680\n",
            "[35,    50] loss: 0.670\n",
            "[36,    10] loss: 0.658\n",
            "[36,    20] loss: 0.664\n",
            "[36,    30] loss: 0.662\n",
            "[36,    40] loss: 0.663\n",
            "[36,    50] loss: 0.675\n",
            "[37,    10] loss: 0.677\n",
            "[37,    20] loss: 0.684\n",
            "[37,    30] loss: 0.651\n",
            "[37,    40] loss: 0.663\n",
            "[37,    50] loss: 0.670\n",
            "[38,    10] loss: 0.651\n",
            "[38,    20] loss: 0.656\n",
            "[38,    30] loss: 0.681\n",
            "[38,    40] loss: 0.690\n",
            "[38,    50] loss: 0.654\n",
            "[39,    10] loss: 0.640\n",
            "[39,    20] loss: 0.678\n",
            "[39,    30] loss: 0.679\n",
            "[39,    40] loss: 0.685\n",
            "[39,    50] loss: 0.686\n",
            "[40,    10] loss: 0.676\n",
            "[40,    20] loss: 0.653\n",
            "[40,    30] loss: 0.687\n",
            "[40,    40] loss: 0.664\n",
            "[40,    50] loss: 0.704\n",
            "[41,    10] loss: 0.692\n",
            "[41,    20] loss: 0.689\n",
            "[41,    30] loss: 0.684\n",
            "[41,    40] loss: 0.660\n",
            "[41,    50] loss: 0.648\n",
            "[42,    10] loss: 0.656\n",
            "[42,    20] loss: 0.696\n",
            "[42,    30] loss: 0.657\n",
            "[42,    40] loss: 0.655\n",
            "[42,    50] loss: 0.663\n",
            "[43,    10] loss: 0.663\n",
            "[43,    20] loss: 0.654\n",
            "[43,    30] loss: 0.647\n",
            "[43,    40] loss: 0.661\n",
            "[43,    50] loss: 0.685\n",
            "[44,    10] loss: 0.685\n",
            "[44,    20] loss: 0.668\n",
            "[44,    30] loss: 0.696\n",
            "[44,    40] loss: 0.685\n",
            "[44,    50] loss: 0.680\n",
            "[45,    10] loss: 0.675\n",
            "[45,    20] loss: 0.669\n",
            "[45,    30] loss: 0.680\n",
            "[45,    40] loss: 0.655\n",
            "[45,    50] loss: 0.655\n",
            "[46,    10] loss: 0.669\n",
            "[46,    20] loss: 0.669\n",
            "[46,    30] loss: 0.656\n",
            "[46,    40] loss: 0.665\n",
            "[46,    50] loss: 0.669\n",
            "[47,    10] loss: 0.684\n",
            "[47,    20] loss: 0.676\n",
            "[47,    30] loss: 0.697\n",
            "[47,    40] loss: 0.661\n",
            "[47,    50] loss: 0.655\n",
            "[48,    10] loss: 0.669\n",
            "[48,    20] loss: 0.680\n",
            "[48,    30] loss: 0.647\n",
            "[48,    40] loss: 0.652\n",
            "[48,    50] loss: 0.689\n",
            "[49,    10] loss: 0.657\n",
            "[49,    20] loss: 0.655\n",
            "[49,    30] loss: 0.653\n",
            "[49,    40] loss: 0.667\n",
            "[49,    50] loss: 0.684\n",
            "[50,    10] loss: 0.657\n",
            "[50,    20] loss: 0.671\n",
            "[50,    30] loss: 0.672\n",
            "[50,    40] loss: 0.659\n",
            "[50,    50] loss: 0.675\n",
            "[51,    10] loss: 0.688\n",
            "[51,    20] loss: 0.662\n",
            "[51,    30] loss: 0.663\n",
            "[51,    40] loss: 0.678\n",
            "[51,    50] loss: 0.650\n",
            "[52,    10] loss: 0.674\n",
            "[52,    20] loss: 0.679\n",
            "[52,    30] loss: 0.664\n",
            "[52,    40] loss: 0.673\n",
            "[52,    50] loss: 0.661\n",
            "[53,    10] loss: 0.648\n",
            "[53,    20] loss: 0.668\n",
            "[53,    30] loss: 0.691\n",
            "[53,    40] loss: 0.659\n",
            "[53,    50] loss: 0.670\n",
            "[54,    10] loss: 0.650\n",
            "[54,    20] loss: 0.672\n",
            "[54,    30] loss: 0.669\n",
            "[54,    40] loss: 0.654\n",
            "[54,    50] loss: 0.647\n",
            "[55,    10] loss: 0.665\n",
            "[55,    20] loss: 0.663\n",
            "[55,    30] loss: 0.653\n",
            "[55,    40] loss: 0.666\n",
            "[55,    50] loss: 0.659\n",
            "[56,    10] loss: 0.657\n",
            "[56,    20] loss: 0.680\n",
            "[56,    30] loss: 0.678\n",
            "[56,    40] loss: 0.689\n",
            "[56,    50] loss: 0.668\n",
            "[57,    10] loss: 0.664\n",
            "[57,    20] loss: 0.671\n",
            "[57,    30] loss: 0.677\n",
            "[57,    40] loss: 0.664\n",
            "[57,    50] loss: 0.674\n",
            "[58,    10] loss: 0.671\n",
            "[58,    20] loss: 0.657\n",
            "[58,    30] loss: 0.679\n",
            "[58,    40] loss: 0.663\n",
            "[58,    50] loss: 0.657\n",
            "[59,    10] loss: 0.666\n",
            "[59,    20] loss: 0.685\n",
            "[59,    30] loss: 0.666\n",
            "[59,    40] loss: 0.657\n",
            "[59,    50] loss: 0.653\n",
            "[60,    10] loss: 0.651\n",
            "[60,    20] loss: 0.667\n",
            "[60,    30] loss: 0.669\n",
            "[60,    40] loss: 0.668\n",
            "[60,    50] loss: 0.664\n",
            "[61,    10] loss: 0.728\n",
            "[61,    20] loss: 0.686\n",
            "[61,    30] loss: 0.679\n",
            "[61,    40] loss: 0.656\n",
            "[61,    50] loss: 0.659\n",
            "[62,    10] loss: 0.649\n",
            "[62,    20] loss: 0.686\n",
            "[62,    30] loss: 0.645\n",
            "[62,    40] loss: 0.658\n",
            "[62,    50] loss: 0.672\n",
            "[63,    10] loss: 0.704\n",
            "[63,    20] loss: 0.668\n",
            "[63,    30] loss: 0.726\n",
            "[63,    40] loss: 0.692\n",
            "[63,    50] loss: 0.663\n",
            "[64,    10] loss: 0.677\n",
            "[64,    20] loss: 0.675\n",
            "[64,    30] loss: 0.638\n",
            "[64,    40] loss: 0.662\n",
            "[64,    50] loss: 0.666\n",
            "[65,    10] loss: 0.660\n",
            "[65,    20] loss: 0.652\n",
            "[65,    30] loss: 0.652\n",
            "[65,    40] loss: 0.656\n",
            "[65,    50] loss: 0.657\n",
            "[66,    10] loss: 0.652\n",
            "[66,    20] loss: 0.667\n",
            "[66,    30] loss: 0.662\n",
            "[66,    40] loss: 0.654\n",
            "[66,    50] loss: 0.658\n",
            "[67,    10] loss: 0.689\n",
            "[67,    20] loss: 0.654\n",
            "[67,    30] loss: 0.694\n",
            "[67,    40] loss: 0.657\n",
            "[67,    50] loss: 0.662\n",
            "[68,    10] loss: 0.667\n",
            "[68,    20] loss: 0.668\n",
            "[68,    30] loss: 0.638\n",
            "[68,    40] loss: 0.672\n",
            "[68,    50] loss: 0.647\n",
            "[69,    10] loss: 0.658\n",
            "[69,    20] loss: 0.663\n",
            "[69,    30] loss: 0.683\n",
            "[69,    40] loss: 0.658\n",
            "[69,    50] loss: 0.643\n",
            "[70,    10] loss: 0.654\n",
            "[70,    20] loss: 0.660\n",
            "[70,    30] loss: 0.657\n",
            "[70,    40] loss: 0.646\n",
            "[70,    50] loss: 0.689\n",
            "[71,    10] loss: 0.685\n",
            "[71,    20] loss: 0.695\n",
            "[71,    30] loss: 0.679\n",
            "[71,    40] loss: 0.661\n",
            "[71,    50] loss: 0.659\n",
            "[72,    10] loss: 0.668\n",
            "[72,    20] loss: 0.672\n",
            "[72,    30] loss: 0.696\n",
            "[72,    40] loss: 0.670\n",
            "[72,    50] loss: 0.660\n",
            "[73,    10] loss: 0.668\n",
            "[73,    20] loss: 0.672\n",
            "[73,    30] loss: 0.664\n",
            "[73,    40] loss: 0.661\n",
            "[73,    50] loss: 0.661\n",
            "[74,    10] loss: 0.665\n",
            "[74,    20] loss: 0.659\n",
            "[74,    30] loss: 0.686\n",
            "[74,    40] loss: 0.697\n",
            "[74,    50] loss: 0.679\n",
            "[75,    10] loss: 0.681\n",
            "[75,    20] loss: 0.681\n",
            "[75,    30] loss: 0.668\n",
            "[75,    40] loss: 0.662\n",
            "[75,    50] loss: 0.667\n",
            "[76,    10] loss: 0.670\n",
            "[76,    20] loss: 0.640\n",
            "[76,    30] loss: 0.677\n",
            "[76,    40] loss: 0.664\n",
            "[76,    50] loss: 0.663\n",
            "[77,    10] loss: 0.661\n",
            "[77,    20] loss: 0.653\n",
            "[77,    30] loss: 0.649\n",
            "[77,    40] loss: 0.669\n",
            "[77,    50] loss: 0.659\n",
            "[78,    10] loss: 0.673\n",
            "[78,    20] loss: 0.674\n",
            "[78,    30] loss: 0.666\n",
            "[78,    40] loss: 0.669\n",
            "[78,    50] loss: 0.647\n",
            "[79,    10] loss: 0.667\n",
            "[79,    20] loss: 0.651\n",
            "[79,    30] loss: 0.679\n",
            "[79,    40] loss: 0.669\n",
            "[79,    50] loss: 0.675\n",
            "[80,    10] loss: 0.694\n",
            "[80,    20] loss: 0.694\n",
            "[80,    30] loss: 0.666\n",
            "[80,    40] loss: 0.673\n",
            "[80,    50] loss: 0.673\n",
            "[81,    10] loss: 0.655\n",
            "[81,    20] loss: 0.671\n",
            "[81,    30] loss: 0.661\n",
            "[81,    40] loss: 0.652\n",
            "[81,    50] loss: 0.668\n",
            "[82,    10] loss: 0.672\n",
            "[82,    20] loss: 0.679\n",
            "[82,    30] loss: 0.667\n",
            "[82,    40] loss: 0.651\n",
            "[82,    50] loss: 0.669\n",
            "[83,    10] loss: 0.661\n",
            "[83,    20] loss: 0.683\n",
            "[83,    30] loss: 0.658\n",
            "[83,    40] loss: 0.662\n",
            "[83,    50] loss: 0.662\n",
            "[84,    10] loss: 0.662\n",
            "[84,    20] loss: 0.673\n",
            "[84,    30] loss: 0.651\n",
            "[84,    40] loss: 0.671\n",
            "[84,    50] loss: 0.656\n",
            "[85,    10] loss: 0.661\n",
            "[85,    20] loss: 0.678\n",
            "[85,    30] loss: 0.662\n",
            "[85,    40] loss: 0.668\n",
            "[85,    50] loss: 0.678\n",
            "[86,    10] loss: 0.654\n",
            "[86,    20] loss: 0.654\n",
            "[86,    30] loss: 0.683\n",
            "[86,    40] loss: 0.658\n",
            "[86,    50] loss: 0.663\n",
            "[87,    10] loss: 0.672\n",
            "[87,    20] loss: 0.662\n",
            "[87,    30] loss: 0.665\n",
            "[87,    40] loss: 0.704\n",
            "[87,    50] loss: 0.664\n",
            "[88,    10] loss: 0.677\n",
            "[88,    20] loss: 0.653\n",
            "[88,    30] loss: 0.664\n",
            "[88,    40] loss: 0.646\n",
            "[88,    50] loss: 0.685\n",
            "[89,    10] loss: 0.675\n",
            "[89,    20] loss: 0.678\n",
            "[89,    30] loss: 0.658\n",
            "[89,    40] loss: 0.673\n",
            "[89,    50] loss: 0.669\n",
            "[90,    10] loss: 0.665\n",
            "[90,    20] loss: 0.656\n",
            "[90,    30] loss: 0.645\n",
            "[90,    40] loss: 0.678\n",
            "[90,    50] loss: 0.666\n",
            "[91,    10] loss: 0.667\n",
            "[91,    20] loss: 0.685\n",
            "[91,    30] loss: 0.689\n",
            "[91,    40] loss: 0.658\n",
            "[91,    50] loss: 0.667\n",
            "[92,    10] loss: 0.673\n",
            "[92,    20] loss: 0.650\n",
            "[92,    30] loss: 0.662\n",
            "[92,    40] loss: 0.687\n",
            "[92,    50] loss: 0.683\n",
            "[93,    10] loss: 0.672\n",
            "[93,    20] loss: 0.649\n",
            "[93,    30] loss: 0.677\n",
            "[93,    40] loss: 0.662\n",
            "[93,    50] loss: 0.666\n",
            "[94,    10] loss: 0.673\n",
            "[94,    20] loss: 0.676\n",
            "[94,    30] loss: 0.667\n",
            "[94,    40] loss: 0.655\n",
            "[94,    50] loss: 0.657\n",
            "[95,    10] loss: 0.677\n",
            "[95,    20] loss: 0.660\n",
            "[95,    30] loss: 0.659\n",
            "[95,    40] loss: 0.639\n",
            "[95,    50] loss: 0.683\n",
            "[96,    10] loss: 0.671\n",
            "[96,    20] loss: 0.655\n",
            "[96,    30] loss: 0.670\n",
            "[96,    40] loss: 0.705\n",
            "[96,    50] loss: 0.720\n",
            "[97,    10] loss: 0.696\n",
            "[97,    20] loss: 0.681\n",
            "[97,    30] loss: 0.657\n",
            "[97,    40] loss: 0.652\n",
            "[97,    50] loss: 0.675\n",
            "[98,    10] loss: 0.684\n",
            "[98,    20] loss: 0.660\n",
            "[98,    30] loss: 0.663\n",
            "[98,    40] loss: 0.653\n",
            "[98,    50] loss: 0.681\n",
            "[99,    10] loss: 0.661\n",
            "[99,    20] loss: 0.652\n",
            "[99,    30] loss: 0.663\n",
            "[99,    40] loss: 0.710\n",
            "[99,    50] loss: 0.675\n",
            "[100,    10] loss: 0.687\n",
            "[100,    20] loss: 0.679\n",
            "[100,    30] loss: 0.671\n",
            "[100,    40] loss: 0.670\n",
            "[100,    50] loss: 0.647\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 36 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 40 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 55 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 75 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 75 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 67 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 1.100\n",
            "[1,    20] loss: 1.096\n",
            "[1,    30] loss: 1.092\n",
            "[1,    40] loss: 1.091\n",
            "[1,    50] loss: 1.090\n",
            "[2,    10] loss: 1.087\n",
            "[2,    20] loss: 1.081\n",
            "[2,    30] loss: 1.075\n",
            "[2,    40] loss: 1.060\n",
            "[2,    50] loss: 1.036\n",
            "[3,    10] loss: 0.975\n",
            "[3,    20] loss: 0.898\n",
            "[3,    30] loss: 0.782\n",
            "[3,    40] loss: 0.703\n",
            "[3,    50] loss: 0.660\n",
            "[4,    10] loss: 0.596\n",
            "[4,    20] loss: 0.559\n",
            "[4,    30] loss: 0.544\n",
            "[4,    40] loss: 0.504\n",
            "[4,    50] loss: 0.504\n",
            "[5,    10] loss: 0.521\n",
            "[5,    20] loss: 0.501\n",
            "[5,    30] loss: 0.480\n",
            "[5,    40] loss: 0.488\n",
            "[5,    50] loss: 0.467\n",
            "[6,    10] loss: 0.498\n",
            "[6,    20] loss: 0.507\n",
            "[6,    30] loss: 0.461\n",
            "[6,    40] loss: 0.472\n",
            "[6,    50] loss: 0.488\n",
            "[7,    10] loss: 0.481\n",
            "[7,    20] loss: 0.484\n",
            "[7,    30] loss: 0.487\n",
            "[7,    40] loss: 0.482\n",
            "[7,    50] loss: 0.500\n",
            "[8,    10] loss: 0.519\n",
            "[8,    20] loss: 0.473\n",
            "[8,    30] loss: 0.485\n",
            "[8,    40] loss: 0.465\n",
            "[8,    50] loss: 0.460\n",
            "[9,    10] loss: 0.478\n",
            "[9,    20] loss: 0.490\n",
            "[9,    30] loss: 0.474\n",
            "[9,    40] loss: 0.479\n",
            "[9,    50] loss: 0.520\n",
            "[10,    10] loss: 0.499\n",
            "[10,    20] loss: 0.532\n",
            "[10,    30] loss: 0.532\n",
            "[10,    40] loss: 0.495\n",
            "[10,    50] loss: 0.471\n",
            "[11,    10] loss: 0.472\n",
            "[11,    20] loss: 0.446\n",
            "[11,    30] loss: 0.476\n",
            "[11,    40] loss: 0.462\n",
            "[11,    50] loss: 0.479\n",
            "[12,    10] loss: 0.477\n",
            "[12,    20] loss: 0.484\n",
            "[12,    30] loss: 0.480\n",
            "[12,    40] loss: 0.477\n",
            "[12,    50] loss: 0.489\n",
            "[13,    10] loss: 0.472\n",
            "[13,    20] loss: 0.459\n",
            "[13,    30] loss: 0.477\n",
            "[13,    40] loss: 0.457\n",
            "[13,    50] loss: 0.455\n",
            "[14,    10] loss: 0.459\n",
            "[14,    20] loss: 0.459\n",
            "[14,    30] loss: 0.455\n",
            "[14,    40] loss: 0.461\n",
            "[14,    50] loss: 0.449\n",
            "[15,    10] loss: 0.462\n",
            "[15,    20] loss: 0.463\n",
            "[15,    30] loss: 0.475\n",
            "[15,    40] loss: 0.515\n",
            "[15,    50] loss: 0.503\n",
            "[16,    10] loss: 0.486\n",
            "[16,    20] loss: 0.497\n",
            "[16,    30] loss: 0.499\n",
            "[16,    40] loss: 0.496\n",
            "[16,    50] loss: 0.449\n",
            "[17,    10] loss: 0.473\n",
            "[17,    20] loss: 0.453\n",
            "[17,    30] loss: 0.479\n",
            "[17,    40] loss: 0.454\n",
            "[17,    50] loss: 0.450\n",
            "[18,    10] loss: 0.449\n",
            "[18,    20] loss: 0.474\n",
            "[18,    30] loss: 0.470\n",
            "[18,    40] loss: 0.523\n",
            "[18,    50] loss: 0.466\n",
            "[19,    10] loss: 0.484\n",
            "[19,    20] loss: 0.468\n",
            "[19,    30] loss: 0.463\n",
            "[19,    40] loss: 0.470\n",
            "[19,    50] loss: 0.466\n",
            "[20,    10] loss: 0.472\n",
            "[20,    20] loss: 0.437\n",
            "[20,    30] loss: 0.457\n",
            "[20,    40] loss: 0.447\n",
            "[20,    50] loss: 0.480\n",
            "[21,    10] loss: 0.476\n",
            "[21,    20] loss: 0.517\n",
            "[21,    30] loss: 0.501\n",
            "[21,    40] loss: 0.475\n",
            "[21,    50] loss: 0.471\n",
            "[22,    10] loss: 0.475\n",
            "[22,    20] loss: 0.480\n",
            "[22,    30] loss: 0.473\n",
            "[22,    40] loss: 0.466\n",
            "[22,    50] loss: 0.475\n",
            "[23,    10] loss: 0.478\n",
            "[23,    20] loss: 0.455\n",
            "[23,    30] loss: 0.467\n",
            "[23,    40] loss: 0.479\n",
            "[23,    50] loss: 0.445\n",
            "[24,    10] loss: 0.461\n",
            "[24,    20] loss: 0.455\n",
            "[24,    30] loss: 0.465\n",
            "[24,    40] loss: 0.441\n",
            "[24,    50] loss: 0.452\n",
            "[25,    10] loss: 0.451\n",
            "[25,    20] loss: 0.442\n",
            "[25,    30] loss: 0.454\n",
            "[25,    40] loss: 0.465\n",
            "[25,    50] loss: 0.479\n",
            "[26,    10] loss: 0.475\n",
            "[26,    20] loss: 0.440\n",
            "[26,    30] loss: 0.436\n",
            "[26,    40] loss: 0.464\n",
            "[26,    50] loss: 0.459\n",
            "[27,    10] loss: 0.458\n",
            "[27,    20] loss: 0.450\n",
            "[27,    30] loss: 0.450\n",
            "[27,    40] loss: 0.445\n",
            "[27,    50] loss: 0.475\n",
            "[28,    10] loss: 0.441\n",
            "[28,    20] loss: 0.473\n",
            "[28,    30] loss: 0.468\n",
            "[28,    40] loss: 0.463\n",
            "[28,    50] loss: 0.479\n",
            "[29,    10] loss: 0.463\n",
            "[29,    20] loss: 0.465\n",
            "[29,    30] loss: 0.472\n",
            "[29,    40] loss: 0.452\n",
            "[29,    50] loss: 0.455\n",
            "[30,    10] loss: 0.464\n",
            "[30,    20] loss: 0.450\n",
            "[30,    30] loss: 0.459\n",
            "[30,    40] loss: 0.451\n",
            "[30,    50] loss: 0.464\n",
            "[31,    10] loss: 0.455\n",
            "[31,    20] loss: 0.463\n",
            "[31,    30] loss: 0.456\n",
            "[31,    40] loss: 0.454\n",
            "[31,    50] loss: 0.465\n",
            "[32,    10] loss: 0.458\n",
            "[32,    20] loss: 0.462\n",
            "[32,    30] loss: 0.443\n",
            "[32,    40] loss: 0.447\n",
            "[32,    50] loss: 0.462\n",
            "[33,    10] loss: 0.435\n",
            "[33,    20] loss: 0.458\n",
            "[33,    30] loss: 0.445\n",
            "[33,    40] loss: 0.452\n",
            "[33,    50] loss: 0.509\n",
            "[34,    10] loss: 0.500\n",
            "[34,    20] loss: 0.481\n",
            "[34,    30] loss: 0.453\n",
            "[34,    40] loss: 0.469\n",
            "[34,    50] loss: 0.459\n",
            "[35,    10] loss: 0.447\n",
            "[35,    20] loss: 0.447\n",
            "[35,    30] loss: 0.455\n",
            "[35,    40] loss: 0.468\n",
            "[35,    50] loss: 0.523\n",
            "[36,    10] loss: 0.495\n",
            "[36,    20] loss: 0.489\n",
            "[36,    30] loss: 0.481\n",
            "[36,    40] loss: 0.458\n",
            "[36,    50] loss: 0.450\n",
            "[37,    10] loss: 0.471\n",
            "[37,    20] loss: 0.465\n",
            "[37,    30] loss: 0.457\n",
            "[37,    40] loss: 0.449\n",
            "[37,    50] loss: 0.462\n",
            "[38,    10] loss: 0.467\n",
            "[38,    20] loss: 0.438\n",
            "[38,    30] loss: 0.468\n",
            "[38,    40] loss: 0.473\n",
            "[38,    50] loss: 0.444\n",
            "[39,    10] loss: 0.458\n",
            "[39,    20] loss: 0.454\n",
            "[39,    30] loss: 0.453\n",
            "[39,    40] loss: 0.478\n",
            "[39,    50] loss: 0.482\n",
            "[40,    10] loss: 0.483\n",
            "[40,    20] loss: 0.464\n",
            "[40,    30] loss: 0.460\n",
            "[40,    40] loss: 0.478\n",
            "[40,    50] loss: 0.485\n",
            "[41,    10] loss: 0.471\n",
            "[41,    20] loss: 0.462\n",
            "[41,    30] loss: 0.466\n",
            "[41,    40] loss: 0.462\n",
            "[41,    50] loss: 0.480\n",
            "[42,    10] loss: 0.470\n",
            "[42,    20] loss: 0.454\n",
            "[42,    30] loss: 0.462\n",
            "[42,    40] loss: 0.456\n",
            "[42,    50] loss: 0.452\n",
            "[43,    10] loss: 0.460\n",
            "[43,    20] loss: 0.477\n",
            "[43,    30] loss: 0.484\n",
            "[43,    40] loss: 0.447\n",
            "[43,    50] loss: 0.469\n",
            "[44,    10] loss: 0.476\n",
            "[44,    20] loss: 0.480\n",
            "[44,    30] loss: 0.465\n",
            "[44,    40] loss: 0.463\n",
            "[44,    50] loss: 0.463\n",
            "[45,    10] loss: 0.460\n",
            "[45,    20] loss: 0.507\n",
            "[45,    30] loss: 0.478\n",
            "[45,    40] loss: 0.477\n",
            "[45,    50] loss: 0.492\n",
            "[46,    10] loss: 0.457\n",
            "[46,    20] loss: 0.456\n",
            "[46,    30] loss: 0.455\n",
            "[46,    40] loss: 0.456\n",
            "[46,    50] loss: 0.467\n",
            "[47,    10] loss: 0.477\n",
            "[47,    20] loss: 0.449\n",
            "[47,    30] loss: 0.452\n",
            "[47,    40] loss: 0.469\n",
            "[47,    50] loss: 0.471\n",
            "[48,    10] loss: 0.450\n",
            "[48,    20] loss: 0.465\n",
            "[48,    30] loss: 0.455\n",
            "[48,    40] loss: 0.435\n",
            "[48,    50] loss: 0.469\n",
            "[49,    10] loss: 0.482\n",
            "[49,    20] loss: 0.464\n",
            "[49,    30] loss: 0.513\n",
            "[49,    40] loss: 0.483\n",
            "[49,    50] loss: 0.460\n",
            "[50,    10] loss: 0.459\n",
            "[50,    20] loss: 0.462\n",
            "[50,    30] loss: 0.482\n",
            "[50,    40] loss: 0.494\n",
            "[50,    50] loss: 0.448\n",
            "[51,    10] loss: 0.454\n",
            "[51,    20] loss: 0.451\n",
            "[51,    30] loss: 0.459\n",
            "[51,    40] loss: 0.475\n",
            "[51,    50] loss: 0.466\n",
            "[52,    10] loss: 0.457\n",
            "[52,    20] loss: 0.451\n",
            "[52,    30] loss: 0.477\n",
            "[52,    40] loss: 0.459\n",
            "[52,    50] loss: 0.454\n",
            "[53,    10] loss: 0.449\n",
            "[53,    20] loss: 0.450\n",
            "[53,    30] loss: 0.450\n",
            "[53,    40] loss: 0.459\n",
            "[53,    50] loss: 0.441\n",
            "[54,    10] loss: 0.452\n",
            "[54,    20] loss: 0.451\n",
            "[54,    30] loss: 0.475\n",
            "[54,    40] loss: 0.437\n",
            "[54,    50] loss: 0.463\n",
            "[55,    10] loss: 0.448\n",
            "[55,    20] loss: 0.501\n",
            "[55,    30] loss: 0.491\n",
            "[55,    40] loss: 0.485\n",
            "[55,    50] loss: 0.447\n",
            "[56,    10] loss: 0.447\n",
            "[56,    20] loss: 0.449\n",
            "[56,    30] loss: 0.453\n",
            "[56,    40] loss: 0.445\n",
            "[56,    50] loss: 0.467\n",
            "[57,    10] loss: 0.446\n",
            "[57,    20] loss: 0.477\n",
            "[57,    30] loss: 0.467\n",
            "[57,    40] loss: 0.477\n",
            "[57,    50] loss: 0.469\n",
            "[58,    10] loss: 0.450\n",
            "[58,    20] loss: 0.461\n",
            "[58,    30] loss: 0.462\n",
            "[58,    40] loss: 0.459\n",
            "[58,    50] loss: 0.472\n",
            "[59,    10] loss: 0.457\n",
            "[59,    20] loss: 0.464\n",
            "[59,    30] loss: 0.448\n",
            "[59,    40] loss: 0.482\n",
            "[59,    50] loss: 0.478\n",
            "[60,    10] loss: 0.451\n",
            "[60,    20] loss: 0.453\n",
            "[60,    30] loss: 0.455\n",
            "[60,    40] loss: 0.450\n",
            "[60,    50] loss: 0.455\n",
            "[61,    10] loss: 0.449\n",
            "[61,    20] loss: 0.459\n",
            "[61,    30] loss: 0.436\n",
            "[61,    40] loss: 0.447\n",
            "[61,    50] loss: 0.476\n",
            "[62,    10] loss: 0.453\n",
            "[62,    20] loss: 0.466\n",
            "[62,    30] loss: 0.496\n",
            "[62,    40] loss: 0.464\n",
            "[62,    50] loss: 0.460\n",
            "[63,    10] loss: 0.449\n",
            "[63,    20] loss: 0.451\n",
            "[63,    30] loss: 0.464\n",
            "[63,    40] loss: 0.469\n",
            "[63,    50] loss: 0.444\n",
            "[64,    10] loss: 0.444\n",
            "[64,    20] loss: 0.447\n",
            "[64,    30] loss: 0.472\n",
            "[64,    40] loss: 0.453\n",
            "[64,    50] loss: 0.471\n",
            "[65,    10] loss: 0.460\n",
            "[65,    20] loss: 0.455\n",
            "[65,    30] loss: 0.450\n",
            "[65,    40] loss: 0.451\n",
            "[65,    50] loss: 0.446\n",
            "[66,    10] loss: 0.463\n",
            "[66,    20] loss: 0.452\n",
            "[66,    30] loss: 0.455\n",
            "[66,    40] loss: 0.438\n",
            "[66,    50] loss: 0.453\n",
            "[67,    10] loss: 0.449\n",
            "[67,    20] loss: 0.461\n",
            "[67,    30] loss: 0.459\n",
            "[67,    40] loss: 0.451\n",
            "[67,    50] loss: 0.446\n",
            "[68,    10] loss: 0.464\n",
            "[68,    20] loss: 0.459\n",
            "[68,    30] loss: 0.442\n",
            "[68,    40] loss: 0.451\n",
            "[68,    50] loss: 0.438\n",
            "[69,    10] loss: 0.446\n",
            "[69,    20] loss: 0.471\n",
            "[69,    30] loss: 0.443\n",
            "[69,    40] loss: 0.471\n",
            "[69,    50] loss: 0.468\n",
            "[70,    10] loss: 0.447\n",
            "[70,    20] loss: 0.449\n",
            "[70,    30] loss: 0.471\n",
            "[70,    40] loss: 0.466\n",
            "[70,    50] loss: 0.472\n",
            "[71,    10] loss: 0.492\n",
            "[71,    20] loss: 0.476\n",
            "[71,    30] loss: 0.462\n",
            "[71,    40] loss: 0.444\n",
            "[71,    50] loss: 0.468\n",
            "[72,    10] loss: 0.475\n",
            "[72,    20] loss: 0.455\n",
            "[72,    30] loss: 0.472\n",
            "[72,    40] loss: 0.442\n",
            "[72,    50] loss: 0.468\n",
            "[73,    10] loss: 0.461\n",
            "[73,    20] loss: 0.464\n",
            "[73,    30] loss: 0.487\n",
            "[73,    40] loss: 0.467\n",
            "[73,    50] loss: 0.483\n",
            "[74,    10] loss: 0.474\n",
            "[74,    20] loss: 0.456\n",
            "[74,    30] loss: 0.440\n",
            "[74,    40] loss: 0.464\n",
            "[74,    50] loss: 0.487\n",
            "[75,    10] loss: 0.472\n",
            "[75,    20] loss: 0.455\n",
            "[75,    30] loss: 0.458\n",
            "[75,    40] loss: 0.448\n",
            "[75,    50] loss: 0.438\n",
            "[76,    10] loss: 0.451\n",
            "[76,    20] loss: 0.456\n",
            "[76,    30] loss: 0.443\n",
            "[76,    40] loss: 0.443\n",
            "[76,    50] loss: 0.456\n",
            "[77,    10] loss: 0.460\n",
            "[77,    20] loss: 0.442\n",
            "[77,    30] loss: 0.459\n",
            "[77,    40] loss: 0.459\n",
            "[77,    50] loss: 0.458\n",
            "[78,    10] loss: 0.447\n",
            "[78,    20] loss: 0.448\n",
            "[78,    30] loss: 0.444\n",
            "[78,    40] loss: 0.460\n",
            "[78,    50] loss: 0.473\n",
            "[79,    10] loss: 0.454\n",
            "[79,    20] loss: 0.458\n",
            "[79,    30] loss: 0.455\n",
            "[79,    40] loss: 0.447\n",
            "[79,    50] loss: 0.455\n",
            "[80,    10] loss: 0.456\n",
            "[80,    20] loss: 0.436\n",
            "[80,    30] loss: 0.464\n",
            "[80,    40] loss: 0.437\n",
            "[80,    50] loss: 0.466\n",
            "[81,    10] loss: 0.452\n",
            "[81,    20] loss: 0.451\n",
            "[81,    30] loss: 0.463\n",
            "[81,    40] loss: 0.461\n",
            "[81,    50] loss: 0.456\n",
            "[82,    10] loss: 0.457\n",
            "[82,    20] loss: 0.478\n",
            "[82,    30] loss: 0.453\n",
            "[82,    40] loss: 0.449\n",
            "[82,    50] loss: 0.456\n",
            "[83,    10] loss: 0.449\n",
            "[83,    20] loss: 0.457\n",
            "[83,    30] loss: 0.478\n",
            "[83,    40] loss: 0.471\n",
            "[83,    50] loss: 0.459\n",
            "[84,    10] loss: 0.465\n",
            "[84,    20] loss: 0.453\n",
            "[84,    30] loss: 0.464\n",
            "[84,    40] loss: 0.429\n",
            "[84,    50] loss: 0.464\n",
            "[85,    10] loss: 0.442\n",
            "[85,    20] loss: 0.464\n",
            "[85,    30] loss: 0.443\n",
            "[85,    40] loss: 0.463\n",
            "[85,    50] loss: 0.454\n",
            "[86,    10] loss: 0.475\n",
            "[86,    20] loss: 0.448\n",
            "[86,    30] loss: 0.445\n",
            "[86,    40] loss: 0.461\n",
            "[86,    50] loss: 0.456\n",
            "[87,    10] loss: 0.464\n",
            "[87,    20] loss: 0.436\n",
            "[87,    30] loss: 0.461\n",
            "[87,    40] loss: 0.457\n",
            "[87,    50] loss: 0.478\n",
            "[88,    10] loss: 0.450\n",
            "[88,    20] loss: 0.448\n",
            "[88,    30] loss: 0.475\n",
            "[88,    40] loss: 0.438\n",
            "[88,    50] loss: 0.470\n",
            "[89,    10] loss: 0.469\n",
            "[89,    20] loss: 0.447\n",
            "[89,    30] loss: 0.459\n",
            "[89,    40] loss: 0.461\n",
            "[89,    50] loss: 0.462\n",
            "[90,    10] loss: 0.447\n",
            "[90,    20] loss: 0.468\n",
            "[90,    30] loss: 0.443\n",
            "[90,    40] loss: 0.449\n",
            "[90,    50] loss: 0.468\n",
            "[91,    10] loss: 0.506\n",
            "[91,    20] loss: 0.474\n",
            "[91,    30] loss: 0.449\n",
            "[91,    40] loss: 0.442\n",
            "[91,    50] loss: 0.481\n",
            "[92,    10] loss: 0.471\n",
            "[92,    20] loss: 0.478\n",
            "[92,    30] loss: 0.452\n",
            "[92,    40] loss: 0.491\n",
            "[92,    50] loss: 0.525\n",
            "[93,    10] loss: 0.466\n",
            "[93,    20] loss: 0.463\n",
            "[93,    30] loss: 0.466\n",
            "[93,    40] loss: 0.472\n",
            "[93,    50] loss: 0.455\n",
            "[94,    10] loss: 0.462\n",
            "[94,    20] loss: 0.455\n",
            "[94,    30] loss: 0.433\n",
            "[94,    40] loss: 0.442\n",
            "[94,    50] loss: 0.466\n",
            "[95,    10] loss: 0.440\n",
            "[95,    20] loss: 0.464\n",
            "[95,    30] loss: 0.438\n",
            "[95,    40] loss: 0.457\n",
            "[95,    50] loss: 0.467\n",
            "[96,    10] loss: 0.450\n",
            "[96,    20] loss: 0.444\n",
            "[96,    30] loss: 0.450\n",
            "[96,    40] loss: 0.469\n",
            "[96,    50] loss: 0.448\n",
            "[97,    10] loss: 0.468\n",
            "[97,    20] loss: 0.464\n",
            "[97,    30] loss: 0.445\n",
            "[97,    40] loss: 0.441\n",
            "[97,    50] loss: 0.443\n",
            "[98,    10] loss: 0.474\n",
            "[98,    20] loss: 0.454\n",
            "[98,    30] loss: 0.444\n",
            "[98,    40] loss: 0.443\n",
            "[98,    50] loss: 0.468\n",
            "[99,    10] loss: 0.453\n",
            "[99,    20] loss: 0.447\n",
            "[99,    30] loss: 0.480\n",
            "[99,    40] loss: 0.471\n",
            "[99,    50] loss: 0.461\n",
            "[100,    10] loss: 0.443\n",
            "[100,    20] loss: 0.467\n",
            "[100,    30] loss: 0.434\n",
            "[100,    40] loss: 0.467\n",
            "[100,    50] loss: 0.470\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 36 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 65 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 77 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 83 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 76 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 1.088\n",
            "[1,    20] loss: 1.078\n",
            "[1,    30] loss: 1.083\n",
            "[1,    40] loss: 1.068\n",
            "[1,    50] loss: 1.054\n",
            "[2,    10] loss: 1.009\n",
            "[2,    20] loss: 0.945\n",
            "[2,    30] loss: 0.812\n",
            "[2,    40] loss: 0.683\n",
            "[2,    50] loss: 0.542\n",
            "[3,    10] loss: 0.451\n",
            "[3,    20] loss: 0.402\n",
            "[3,    30] loss: 0.343\n",
            "[3,    40] loss: 0.330\n",
            "[3,    50] loss: 0.308\n",
            "[4,    10] loss: 0.302\n",
            "[4,    20] loss: 0.324\n",
            "[4,    30] loss: 0.307\n",
            "[4,    40] loss: 0.270\n",
            "[4,    50] loss: 0.288\n",
            "[5,    10] loss: 0.288\n",
            "[5,    20] loss: 0.280\n",
            "[5,    30] loss: 0.316\n",
            "[5,    40] loss: 0.271\n",
            "[5,    50] loss: 0.305\n",
            "[6,    10] loss: 0.336\n",
            "[6,    20] loss: 0.279\n",
            "[6,    30] loss: 0.301\n",
            "[6,    40] loss: 0.263\n",
            "[6,    50] loss: 0.255\n",
            "[7,    10] loss: 0.267\n",
            "[7,    20] loss: 0.262\n",
            "[7,    30] loss: 0.266\n",
            "[7,    40] loss: 0.259\n",
            "[7,    50] loss: 0.250\n",
            "[8,    10] loss: 0.283\n",
            "[8,    20] loss: 0.281\n",
            "[8,    30] loss: 0.271\n",
            "[8,    40] loss: 0.271\n",
            "[8,    50] loss: 0.261\n",
            "[9,    10] loss: 0.321\n",
            "[9,    20] loss: 0.255\n",
            "[9,    30] loss: 0.308\n",
            "[9,    40] loss: 0.332\n",
            "[9,    50] loss: 0.280\n",
            "[10,    10] loss: 0.269\n",
            "[10,    20] loss: 0.257\n",
            "[10,    30] loss: 0.272\n",
            "[10,    40] loss: 0.262\n",
            "[10,    50] loss: 0.249\n",
            "[11,    10] loss: 0.275\n",
            "[11,    20] loss: 0.284\n",
            "[11,    30] loss: 0.251\n",
            "[11,    40] loss: 0.256\n",
            "[11,    50] loss: 0.257\n",
            "[12,    10] loss: 0.263\n",
            "[12,    20] loss: 0.251\n",
            "[12,    30] loss: 0.249\n",
            "[12,    40] loss: 0.244\n",
            "[12,    50] loss: 0.246\n",
            "[13,    10] loss: 0.225\n",
            "[13,    20] loss: 0.257\n",
            "[13,    30] loss: 0.261\n",
            "[13,    40] loss: 0.268\n",
            "[13,    50] loss: 0.240\n",
            "[14,    10] loss: 0.261\n",
            "[14,    20] loss: 0.250\n",
            "[14,    30] loss: 0.289\n",
            "[14,    40] loss: 0.272\n",
            "[14,    50] loss: 0.284\n",
            "[15,    10] loss: 0.269\n",
            "[15,    20] loss: 0.245\n",
            "[15,    30] loss: 0.255\n",
            "[15,    40] loss: 0.253\n",
            "[15,    50] loss: 0.247\n",
            "[16,    10] loss: 0.227\n",
            "[16,    20] loss: 0.257\n",
            "[16,    30] loss: 0.257\n",
            "[16,    40] loss: 0.256\n",
            "[16,    50] loss: 0.301\n",
            "[17,    10] loss: 0.281\n",
            "[17,    20] loss: 0.259\n",
            "[17,    30] loss: 0.254\n",
            "[17,    40] loss: 0.249\n",
            "[17,    50] loss: 0.256\n",
            "[18,    10] loss: 0.234\n",
            "[18,    20] loss: 0.260\n",
            "[18,    30] loss: 0.254\n",
            "[18,    40] loss: 0.291\n",
            "[18,    50] loss: 0.309\n",
            "[19,    10] loss: 0.271\n",
            "[19,    20] loss: 0.259\n",
            "[19,    30] loss: 0.286\n",
            "[19,    40] loss: 0.267\n",
            "[19,    50] loss: 0.291\n",
            "[20,    10] loss: 0.297\n",
            "[20,    20] loss: 0.264\n",
            "[20,    30] loss: 0.258\n",
            "[20,    40] loss: 0.251\n",
            "[20,    50] loss: 0.245\n",
            "[21,    10] loss: 0.267\n",
            "[21,    20] loss: 0.250\n",
            "[21,    30] loss: 0.269\n",
            "[21,    40] loss: 0.235\n",
            "[21,    50] loss: 0.260\n",
            "[22,    10] loss: 0.260\n",
            "[22,    20] loss: 0.236\n",
            "[22,    30] loss: 0.249\n",
            "[22,    40] loss: 0.247\n",
            "[22,    50] loss: 0.263\n",
            "[23,    10] loss: 0.249\n",
            "[23,    20] loss: 0.259\n",
            "[23,    30] loss: 0.270\n",
            "[23,    40] loss: 0.242\n",
            "[23,    50] loss: 0.243\n",
            "[24,    10] loss: 0.229\n",
            "[24,    20] loss: 0.248\n",
            "[24,    30] loss: 0.246\n",
            "[24,    40] loss: 0.272\n",
            "[24,    50] loss: 0.288\n",
            "[25,    10] loss: 0.254\n",
            "[25,    20] loss: 0.260\n",
            "[25,    30] loss: 0.259\n",
            "[25,    40] loss: 0.245\n",
            "[25,    50] loss: 0.233\n",
            "[26,    10] loss: 0.250\n",
            "[26,    20] loss: 0.250\n",
            "[26,    30] loss: 0.251\n",
            "[26,    40] loss: 0.253\n",
            "[26,    50] loss: 0.259\n",
            "[27,    10] loss: 0.253\n",
            "[27,    20] loss: 0.243\n",
            "[27,    30] loss: 0.262\n",
            "[27,    40] loss: 0.266\n",
            "[27,    50] loss: 0.279\n",
            "[28,    10] loss: 0.262\n",
            "[28,    20] loss: 0.243\n",
            "[28,    30] loss: 0.256\n",
            "[28,    40] loss: 0.244\n",
            "[28,    50] loss: 0.265\n",
            "[29,    10] loss: 0.277\n",
            "[29,    20] loss: 0.263\n",
            "[29,    30] loss: 0.269\n",
            "[29,    40] loss: 0.251\n",
            "[29,    50] loss: 0.264\n",
            "[30,    10] loss: 0.266\n",
            "[30,    20] loss: 0.272\n",
            "[30,    30] loss: 0.279\n",
            "[30,    40] loss: 0.235\n",
            "[30,    50] loss: 0.250\n",
            "[31,    10] loss: 0.251\n",
            "[31,    20] loss: 0.244\n",
            "[31,    30] loss: 0.252\n",
            "[31,    40] loss: 0.235\n",
            "[31,    50] loss: 0.252\n",
            "[32,    10] loss: 0.251\n",
            "[32,    20] loss: 0.271\n",
            "[32,    30] loss: 0.235\n",
            "[32,    40] loss: 0.242\n",
            "[32,    50] loss: 0.239\n",
            "[33,    10] loss: 0.234\n",
            "[33,    20] loss: 0.258\n",
            "[33,    30] loss: 0.243\n",
            "[33,    40] loss: 0.248\n",
            "[33,    50] loss: 0.249\n",
            "[34,    10] loss: 0.243\n",
            "[34,    20] loss: 0.262\n",
            "[34,    30] loss: 0.272\n",
            "[34,    40] loss: 0.256\n",
            "[34,    50] loss: 0.262\n",
            "[35,    10] loss: 0.284\n",
            "[35,    20] loss: 0.250\n",
            "[35,    30] loss: 0.264\n",
            "[35,    40] loss: 0.254\n",
            "[35,    50] loss: 0.277\n",
            "[36,    10] loss: 0.279\n",
            "[36,    20] loss: 0.248\n",
            "[36,    30] loss: 0.241\n",
            "[36,    40] loss: 0.248\n",
            "[36,    50] loss: 0.258\n",
            "[37,    10] loss: 0.258\n",
            "[37,    20] loss: 0.250\n",
            "[37,    30] loss: 0.271\n",
            "[37,    40] loss: 0.243\n",
            "[37,    50] loss: 0.241\n",
            "[38,    10] loss: 0.242\n",
            "[38,    20] loss: 0.246\n",
            "[38,    30] loss: 0.247\n",
            "[38,    40] loss: 0.264\n",
            "[38,    50] loss: 0.245\n",
            "[39,    10] loss: 0.244\n",
            "[39,    20] loss: 0.279\n",
            "[39,    30] loss: 0.275\n",
            "[39,    40] loss: 0.252\n",
            "[39,    50] loss: 0.245\n",
            "[40,    10] loss: 0.232\n",
            "[40,    20] loss: 0.252\n",
            "[40,    30] loss: 0.253\n",
            "[40,    40] loss: 0.261\n",
            "[40,    50] loss: 0.248\n",
            "[41,    10] loss: 0.259\n",
            "[41,    20] loss: 0.254\n",
            "[41,    30] loss: 0.252\n",
            "[41,    40] loss: 0.248\n",
            "[41,    50] loss: 0.236\n",
            "[42,    10] loss: 0.234\n",
            "[42,    20] loss: 0.248\n",
            "[42,    30] loss: 0.253\n",
            "[42,    40] loss: 0.281\n",
            "[42,    50] loss: 0.245\n",
            "[43,    10] loss: 0.297\n",
            "[43,    20] loss: 0.254\n",
            "[43,    30] loss: 0.270\n",
            "[43,    40] loss: 0.251\n",
            "[43,    50] loss: 0.240\n",
            "[44,    10] loss: 0.238\n",
            "[44,    20] loss: 0.252\n",
            "[44,    30] loss: 0.247\n",
            "[44,    40] loss: 0.248\n",
            "[44,    50] loss: 0.245\n",
            "[45,    10] loss: 0.259\n",
            "[45,    20] loss: 0.247\n",
            "[45,    30] loss: 0.250\n",
            "[45,    40] loss: 0.271\n",
            "[45,    50] loss: 0.244\n",
            "[46,    10] loss: 0.284\n",
            "[46,    20] loss: 0.264\n",
            "[46,    30] loss: 0.267\n",
            "[46,    40] loss: 0.257\n",
            "[46,    50] loss: 0.242\n",
            "[47,    10] loss: 0.305\n",
            "[47,    20] loss: 0.281\n",
            "[47,    30] loss: 0.265\n",
            "[47,    40] loss: 0.252\n",
            "[47,    50] loss: 0.248\n",
            "[48,    10] loss: 0.249\n",
            "[48,    20] loss: 0.264\n",
            "[48,    30] loss: 0.244\n",
            "[48,    40] loss: 0.266\n",
            "[48,    50] loss: 0.238\n",
            "[49,    10] loss: 0.258\n",
            "[49,    20] loss: 0.284\n",
            "[49,    30] loss: 0.247\n",
            "[49,    40] loss: 0.243\n",
            "[49,    50] loss: 0.244\n",
            "[50,    10] loss: 0.255\n",
            "[50,    20] loss: 0.272\n",
            "[50,    30] loss: 0.256\n",
            "[50,    40] loss: 0.245\n",
            "[50,    50] loss: 0.246\n",
            "[51,    10] loss: 0.249\n",
            "[51,    20] loss: 0.229\n",
            "[51,    30] loss: 0.245\n",
            "[51,    40] loss: 0.243\n",
            "[51,    50] loss: 0.245\n",
            "[52,    10] loss: 0.252\n",
            "[52,    20] loss: 0.259\n",
            "[52,    30] loss: 0.271\n",
            "[52,    40] loss: 0.274\n",
            "[52,    50] loss: 0.253\n",
            "[53,    10] loss: 0.275\n",
            "[53,    20] loss: 0.263\n",
            "[53,    30] loss: 0.255\n",
            "[53,    40] loss: 0.278\n",
            "[53,    50] loss: 0.265\n",
            "[54,    10] loss: 0.263\n",
            "[54,    20] loss: 0.261\n",
            "[54,    30] loss: 0.277\n",
            "[54,    40] loss: 0.244\n",
            "[54,    50] loss: 0.237\n",
            "[55,    10] loss: 0.255\n",
            "[55,    20] loss: 0.247\n",
            "[55,    30] loss: 0.255\n",
            "[55,    40] loss: 0.242\n",
            "[55,    50] loss: 0.242\n",
            "[56,    10] loss: 0.232\n",
            "[56,    20] loss: 0.256\n",
            "[56,    30] loss: 0.247\n",
            "[56,    40] loss: 0.254\n",
            "[56,    50] loss: 0.287\n",
            "[57,    10] loss: 0.262\n",
            "[57,    20] loss: 0.243\n",
            "[57,    30] loss: 0.263\n",
            "[57,    40] loss: 0.253\n",
            "[57,    50] loss: 0.238\n",
            "[58,    10] loss: 0.242\n",
            "[58,    20] loss: 0.249\n",
            "[58,    30] loss: 0.271\n",
            "[58,    40] loss: 0.245\n",
            "[58,    50] loss: 0.235\n",
            "[59,    10] loss: 0.262\n",
            "[59,    20] loss: 0.245\n",
            "[59,    30] loss: 0.247\n",
            "[59,    40] loss: 0.234\n",
            "[59,    50] loss: 0.242\n",
            "[60,    10] loss: 0.249\n",
            "[60,    20] loss: 0.244\n",
            "[60,    30] loss: 0.261\n",
            "[60,    40] loss: 0.287\n",
            "[60,    50] loss: 0.265\n",
            "[61,    10] loss: 0.248\n",
            "[61,    20] loss: 0.255\n",
            "[61,    30] loss: 0.234\n",
            "[61,    40] loss: 0.252\n",
            "[61,    50] loss: 0.250\n",
            "[62,    10] loss: 0.243\n",
            "[62,    20] loss: 0.267\n",
            "[62,    30] loss: 0.258\n",
            "[62,    40] loss: 0.247\n",
            "[62,    50] loss: 0.248\n",
            "[63,    10] loss: 0.261\n",
            "[63,    20] loss: 0.232\n",
            "[63,    30] loss: 0.265\n",
            "[63,    40] loss: 0.223\n",
            "[63,    50] loss: 0.255\n",
            "[64,    10] loss: 0.274\n",
            "[64,    20] loss: 0.260\n",
            "[64,    30] loss: 0.252\n",
            "[64,    40] loss: 0.237\n",
            "[64,    50] loss: 0.249\n",
            "[65,    10] loss: 0.272\n",
            "[65,    20] loss: 0.273\n",
            "[65,    30] loss: 0.244\n",
            "[65,    40] loss: 0.245\n",
            "[65,    50] loss: 0.231\n",
            "[66,    10] loss: 0.249\n",
            "[66,    20] loss: 0.283\n",
            "[66,    30] loss: 0.294\n",
            "[66,    40] loss: 0.253\n",
            "[66,    50] loss: 0.255\n",
            "[67,    10] loss: 0.250\n",
            "[67,    20] loss: 0.264\n",
            "[67,    30] loss: 0.246\n",
            "[67,    40] loss: 0.254\n",
            "[67,    50] loss: 0.239\n",
            "[68,    10] loss: 0.230\n",
            "[68,    20] loss: 0.266\n",
            "[68,    30] loss: 0.260\n",
            "[68,    40] loss: 0.252\n",
            "[68,    50] loss: 0.255\n",
            "[69,    10] loss: 0.241\n",
            "[69,    20] loss: 0.259\n",
            "[69,    30] loss: 0.246\n",
            "[69,    40] loss: 0.252\n",
            "[69,    50] loss: 0.246\n",
            "[70,    10] loss: 0.251\n",
            "[70,    20] loss: 0.289\n",
            "[70,    30] loss: 0.263\n",
            "[70,    40] loss: 0.262\n",
            "[70,    50] loss: 0.278\n",
            "[71,    10] loss: 0.269\n",
            "[71,    20] loss: 0.242\n",
            "[71,    30] loss: 0.254\n",
            "[71,    40] loss: 0.264\n",
            "[71,    50] loss: 0.242\n",
            "[72,    10] loss: 0.239\n",
            "[72,    20] loss: 0.247\n",
            "[72,    30] loss: 0.249\n",
            "[72,    40] loss: 0.268\n",
            "[72,    50] loss: 0.326\n",
            "[73,    10] loss: 0.268\n",
            "[73,    20] loss: 0.254\n",
            "[73,    30] loss: 0.253\n",
            "[73,    40] loss: 0.259\n",
            "[73,    50] loss: 0.248\n",
            "[74,    10] loss: 0.252\n",
            "[74,    20] loss: 0.257\n",
            "[74,    30] loss: 0.242\n",
            "[74,    40] loss: 0.240\n",
            "[74,    50] loss: 0.257\n",
            "[75,    10] loss: 0.253\n",
            "[75,    20] loss: 0.245\n",
            "[75,    30] loss: 0.257\n",
            "[75,    40] loss: 0.241\n",
            "[75,    50] loss: 0.241\n",
            "[76,    10] loss: 0.231\n",
            "[76,    20] loss: 0.276\n",
            "[76,    30] loss: 0.251\n",
            "[76,    40] loss: 0.265\n",
            "[76,    50] loss: 0.252\n",
            "[77,    10] loss: 0.241\n",
            "[77,    20] loss: 0.246\n",
            "[77,    30] loss: 0.242\n",
            "[77,    40] loss: 0.254\n",
            "[77,    50] loss: 0.233\n",
            "[78,    10] loss: 0.253\n",
            "[78,    20] loss: 0.273\n",
            "[78,    30] loss: 0.274\n",
            "[78,    40] loss: 0.308\n",
            "[78,    50] loss: 0.298\n",
            "[79,    10] loss: 0.259\n",
            "[79,    20] loss: 0.254\n",
            "[79,    30] loss: 0.243\n",
            "[79,    40] loss: 0.240\n",
            "[79,    50] loss: 0.244\n",
            "[80,    10] loss: 0.237\n",
            "[80,    20] loss: 0.265\n",
            "[80,    30] loss: 0.269\n",
            "[80,    40] loss: 0.257\n",
            "[80,    50] loss: 0.241\n",
            "[81,    10] loss: 0.240\n",
            "[81,    20] loss: 0.251\n",
            "[81,    30] loss: 0.267\n",
            "[81,    40] loss: 0.242\n",
            "[81,    50] loss: 0.257\n",
            "[82,    10] loss: 0.273\n",
            "[82,    20] loss: 0.244\n",
            "[82,    30] loss: 0.255\n",
            "[82,    40] loss: 0.236\n",
            "[82,    50] loss: 0.246\n",
            "[83,    10] loss: 0.247\n",
            "[83,    20] loss: 0.257\n",
            "[83,    30] loss: 0.241\n",
            "[83,    40] loss: 0.233\n",
            "[83,    50] loss: 0.249\n",
            "[84,    10] loss: 0.256\n",
            "[84,    20] loss: 0.263\n",
            "[84,    30] loss: 0.260\n",
            "[84,    40] loss: 0.246\n",
            "[84,    50] loss: 0.247\n",
            "[85,    10] loss: 0.279\n",
            "[85,    20] loss: 0.256\n",
            "[85,    30] loss: 0.236\n",
            "[85,    40] loss: 0.246\n",
            "[85,    50] loss: 0.268\n",
            "[86,    10] loss: 0.240\n",
            "[86,    20] loss: 0.263\n",
            "[86,    30] loss: 0.240\n",
            "[86,    40] loss: 0.268\n",
            "[86,    50] loss: 0.264\n",
            "[87,    10] loss: 0.231\n",
            "[87,    20] loss: 0.239\n",
            "[87,    30] loss: 0.251\n",
            "[87,    40] loss: 0.260\n",
            "[87,    50] loss: 0.238\n",
            "[88,    10] loss: 0.254\n",
            "[88,    20] loss: 0.267\n",
            "[88,    30] loss: 0.252\n",
            "[88,    40] loss: 0.244\n",
            "[88,    50] loss: 0.234\n",
            "[89,    10] loss: 0.238\n",
            "[89,    20] loss: 0.250\n",
            "[89,    30] loss: 0.233\n",
            "[89,    40] loss: 0.245\n",
            "[89,    50] loss: 0.247\n",
            "[90,    10] loss: 0.276\n",
            "[90,    20] loss: 0.273\n",
            "[90,    30] loss: 0.304\n",
            "[90,    40] loss: 0.282\n",
            "[90,    50] loss: 0.302\n",
            "[91,    10] loss: 0.302\n",
            "[91,    20] loss: 0.250\n",
            "[91,    30] loss: 0.257\n",
            "[91,    40] loss: 0.245\n",
            "[91,    50] loss: 0.235\n",
            "[92,    10] loss: 0.234\n",
            "[92,    20] loss: 0.245\n",
            "[92,    30] loss: 0.241\n",
            "[92,    40] loss: 0.256\n",
            "[92,    50] loss: 0.248\n",
            "[93,    10] loss: 0.255\n",
            "[93,    20] loss: 0.239\n",
            "[93,    30] loss: 0.250\n",
            "[93,    40] loss: 0.237\n",
            "[93,    50] loss: 0.230\n",
            "[94,    10] loss: 0.235\n",
            "[94,    20] loss: 0.240\n",
            "[94,    30] loss: 0.271\n",
            "[94,    40] loss: 0.235\n",
            "[94,    50] loss: 0.254\n",
            "[95,    10] loss: 0.244\n",
            "[95,    20] loss: 0.236\n",
            "[95,    30] loss: 0.252\n",
            "[95,    40] loss: 0.251\n",
            "[95,    50] loss: 0.250\n",
            "[96,    10] loss: 0.256\n",
            "[96,    20] loss: 0.237\n",
            "[96,    30] loss: 0.244\n",
            "[96,    40] loss: 0.250\n",
            "[96,    50] loss: 0.250\n",
            "[97,    10] loss: 0.251\n",
            "[97,    20] loss: 0.238\n",
            "[97,    30] loss: 0.268\n",
            "[97,    40] loss: 0.246\n",
            "[97,    50] loss: 0.248\n",
            "[98,    10] loss: 0.235\n",
            "[98,    20] loss: 0.239\n",
            "[98,    30] loss: 0.262\n",
            "[98,    40] loss: 0.275\n",
            "[98,    50] loss: 0.249\n",
            "[99,    10] loss: 0.256\n",
            "[99,    20] loss: 0.290\n",
            "[99,    30] loss: 0.284\n",
            "[99,    40] loss: 0.264\n",
            "[99,    50] loss: 0.248\n",
            "[100,    10] loss: 0.253\n",
            "[100,    20] loss: 0.256\n",
            "[100,    30] loss: 0.242\n",
            "[100,    40] loss: 0.236\n",
            "[100,    50] loss: 0.268\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 32 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 64 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 76 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 83 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 1.099\n",
            "[1,    20] loss: 1.089\n",
            "[1,    30] loss: 1.085\n",
            "[1,    40] loss: 1.079\n",
            "[1,    50] loss: 1.070\n",
            "[2,    10] loss: 1.039\n",
            "[2,    20] loss: 0.979\n",
            "[2,    30] loss: 0.868\n",
            "[2,    40] loss: 0.679\n",
            "[2,    50] loss: 0.512\n",
            "[3,    10] loss: 0.386\n",
            "[3,    20] loss: 0.309\n",
            "[3,    30] loss: 0.272\n",
            "[3,    40] loss: 0.215\n",
            "[3,    50] loss: 0.179\n",
            "[4,    10] loss: 0.172\n",
            "[4,    20] loss: 0.137\n",
            "[4,    30] loss: 0.128\n",
            "[4,    40] loss: 0.117\n",
            "[4,    50] loss: 0.121\n",
            "[5,    10] loss: 0.118\n",
            "[5,    20] loss: 0.104\n",
            "[5,    30] loss: 0.148\n",
            "[5,    40] loss: 0.283\n",
            "[5,    50] loss: 0.315\n",
            "[6,    10] loss: 0.257\n",
            "[6,    20] loss: 0.147\n",
            "[6,    30] loss: 0.128\n",
            "[6,    40] loss: 0.105\n",
            "[6,    50] loss: 0.099\n",
            "[7,    10] loss: 0.094\n",
            "[7,    20] loss: 0.098\n",
            "[7,    30] loss: 0.081\n",
            "[7,    40] loss: 0.098\n",
            "[7,    50] loss: 0.090\n",
            "[8,    10] loss: 0.089\n",
            "[8,    20] loss: 0.084\n",
            "[8,    30] loss: 0.090\n",
            "[8,    40] loss: 0.087\n",
            "[8,    50] loss: 0.088\n",
            "[9,    10] loss: 0.086\n",
            "[9,    20] loss: 0.084\n",
            "[9,    30] loss: 0.078\n",
            "[9,    40] loss: 0.085\n",
            "[9,    50] loss: 0.079\n",
            "[10,    10] loss: 0.086\n",
            "[10,    20] loss: 0.081\n",
            "[10,    30] loss: 0.083\n",
            "[10,    40] loss: 0.079\n",
            "[10,    50] loss: 0.087\n",
            "[11,    10] loss: 0.088\n",
            "[11,    20] loss: 0.094\n",
            "[11,    30] loss: 0.100\n",
            "[11,    40] loss: 0.089\n",
            "[11,    50] loss: 0.086\n",
            "[12,    10] loss: 0.080\n",
            "[12,    20] loss: 0.079\n",
            "[12,    30] loss: 0.082\n",
            "[12,    40] loss: 0.078\n",
            "[12,    50] loss: 0.085\n",
            "[13,    10] loss: 0.080\n",
            "[13,    20] loss: 0.078\n",
            "[13,    30] loss: 0.085\n",
            "[13,    40] loss: 0.080\n",
            "[13,    50] loss: 0.089\n",
            "[14,    10] loss: 0.076\n",
            "[14,    20] loss: 0.077\n",
            "[14,    30] loss: 0.076\n",
            "[14,    40] loss: 0.081\n",
            "[14,    50] loss: 0.076\n",
            "[15,    10] loss: 0.080\n",
            "[15,    20] loss: 0.088\n",
            "[15,    30] loss: 0.090\n",
            "[15,    40] loss: 0.076\n",
            "[15,    50] loss: 0.092\n",
            "[16,    10] loss: 0.097\n",
            "[16,    20] loss: 0.093\n",
            "[16,    30] loss: 0.085\n",
            "[16,    40] loss: 0.077\n",
            "[16,    50] loss: 0.089\n",
            "[17,    10] loss: 0.109\n",
            "[17,    20] loss: 0.122\n",
            "[17,    30] loss: 0.102\n",
            "[17,    40] loss: 0.088\n",
            "[17,    50] loss: 0.080\n",
            "[18,    10] loss: 0.080\n",
            "[18,    20] loss: 0.093\n",
            "[18,    30] loss: 0.096\n",
            "[18,    40] loss: 0.081\n",
            "[18,    50] loss: 0.088\n",
            "[19,    10] loss: 0.081\n",
            "[19,    20] loss: 0.086\n",
            "[19,    30] loss: 0.078\n",
            "[19,    40] loss: 0.081\n",
            "[19,    50] loss: 0.075\n",
            "[20,    10] loss: 0.078\n",
            "[20,    20] loss: 0.082\n",
            "[20,    30] loss: 0.086\n",
            "[20,    40] loss: 0.088\n",
            "[20,    50] loss: 0.070\n",
            "[21,    10] loss: 0.081\n",
            "[21,    20] loss: 0.070\n",
            "[21,    30] loss: 0.074\n",
            "[21,    40] loss: 0.094\n",
            "[21,    50] loss: 0.098\n",
            "[22,    10] loss: 0.098\n",
            "[22,    20] loss: 0.101\n",
            "[22,    30] loss: 0.087\n",
            "[22,    40] loss: 0.074\n",
            "[22,    50] loss: 0.081\n",
            "[23,    10] loss: 0.080\n",
            "[23,    20] loss: 0.101\n",
            "[23,    30] loss: 0.081\n",
            "[23,    40] loss: 0.082\n",
            "[23,    50] loss: 0.096\n",
            "[24,    10] loss: 0.114\n",
            "[24,    20] loss: 0.082\n",
            "[24,    30] loss: 0.079\n",
            "[24,    40] loss: 0.072\n",
            "[24,    50] loss: 0.081\n",
            "[25,    10] loss: 0.081\n",
            "[25,    20] loss: 0.075\n",
            "[25,    30] loss: 0.081\n",
            "[25,    40] loss: 0.077\n",
            "[25,    50] loss: 0.084\n",
            "[26,    10] loss: 0.078\n",
            "[26,    20] loss: 0.081\n",
            "[26,    30] loss: 0.166\n",
            "[26,    40] loss: 0.104\n",
            "[26,    50] loss: 0.087\n",
            "[27,    10] loss: 0.087\n",
            "[27,    20] loss: 0.086\n",
            "[27,    30] loss: 0.106\n",
            "[27,    40] loss: 0.071\n",
            "[27,    50] loss: 0.076\n",
            "[28,    10] loss: 0.076\n",
            "[28,    20] loss: 0.084\n",
            "[28,    30] loss: 0.098\n",
            "[28,    40] loss: 0.092\n",
            "[28,    50] loss: 0.075\n",
            "[29,    10] loss: 0.085\n",
            "[29,    20] loss: 0.068\n",
            "[29,    30] loss: 0.081\n",
            "[29,    40] loss: 0.086\n",
            "[29,    50] loss: 0.086\n",
            "[30,    10] loss: 0.136\n",
            "[30,    20] loss: 0.154\n",
            "[30,    30] loss: 0.085\n",
            "[30,    40] loss: 0.074\n",
            "[30,    50] loss: 0.092\n",
            "[31,    10] loss: 0.090\n",
            "[31,    20] loss: 0.101\n",
            "[31,    30] loss: 0.095\n",
            "[31,    40] loss: 0.076\n",
            "[31,    50] loss: 0.076\n",
            "[32,    10] loss: 0.073\n",
            "[32,    20] loss: 0.081\n",
            "[32,    30] loss: 0.078\n",
            "[32,    40] loss: 0.086\n",
            "[32,    50] loss: 0.089\n",
            "[33,    10] loss: 0.093\n",
            "[33,    20] loss: 0.093\n",
            "[33,    30] loss: 0.102\n",
            "[33,    40] loss: 0.079\n",
            "[33,    50] loss: 0.068\n",
            "[34,    10] loss: 0.087\n",
            "[34,    20] loss: 0.079\n",
            "[34,    30] loss: 0.093\n",
            "[34,    40] loss: 0.074\n",
            "[34,    50] loss: 0.074\n",
            "[35,    10] loss: 0.076\n",
            "[35,    20] loss: 0.078\n",
            "[35,    30] loss: 0.071\n",
            "[35,    40] loss: 0.072\n",
            "[35,    50] loss: 0.094\n",
            "[36,    10] loss: 0.096\n",
            "[36,    20] loss: 0.082\n",
            "[36,    30] loss: 0.085\n",
            "[36,    40] loss: 0.078\n",
            "[36,    50] loss: 0.102\n",
            "[37,    10] loss: 0.089\n",
            "[37,    20] loss: 0.076\n",
            "[37,    30] loss: 0.092\n",
            "[37,    40] loss: 0.114\n",
            "[37,    50] loss: 0.108\n",
            "[38,    10] loss: 0.093\n",
            "[38,    20] loss: 0.082\n",
            "[38,    30] loss: 0.078\n",
            "[38,    40] loss: 0.088\n",
            "[38,    50] loss: 0.085\n",
            "[39,    10] loss: 0.094\n",
            "[39,    20] loss: 0.085\n",
            "[39,    30] loss: 0.085\n",
            "[39,    40] loss: 0.085\n",
            "[39,    50] loss: 0.105\n",
            "[40,    10] loss: 0.085\n",
            "[40,    20] loss: 0.082\n",
            "[40,    30] loss: 0.069\n",
            "[40,    40] loss: 0.076\n",
            "[40,    50] loss: 0.085\n",
            "[41,    10] loss: 0.107\n",
            "[41,    20] loss: 0.160\n",
            "[41,    30] loss: 0.089\n",
            "[41,    40] loss: 0.084\n",
            "[41,    50] loss: 0.074\n",
            "[42,    10] loss: 0.075\n",
            "[42,    20] loss: 0.080\n",
            "[42,    30] loss: 0.073\n",
            "[42,    40] loss: 0.070\n",
            "[42,    50] loss: 0.078\n",
            "[43,    10] loss: 0.070\n",
            "[43,    20] loss: 0.076\n",
            "[43,    30] loss: 0.095\n",
            "[43,    40] loss: 0.081\n",
            "[43,    50] loss: 0.072\n",
            "[44,    10] loss: 0.098\n",
            "[44,    20] loss: 0.145\n",
            "[44,    30] loss: 0.111\n",
            "[44,    40] loss: 0.095\n",
            "[44,    50] loss: 0.075\n",
            "[45,    10] loss: 0.070\n",
            "[45,    20] loss: 0.085\n",
            "[45,    30] loss: 0.087\n",
            "[45,    40] loss: 0.085\n",
            "[45,    50] loss: 0.081\n",
            "[46,    10] loss: 0.076\n",
            "[46,    20] loss: 0.076\n",
            "[46,    30] loss: 0.070\n",
            "[46,    40] loss: 0.082\n",
            "[46,    50] loss: 0.083\n",
            "[47,    10] loss: 0.066\n",
            "[47,    20] loss: 0.088\n",
            "[47,    30] loss: 0.103\n",
            "[47,    40] loss: 0.086\n",
            "[47,    50] loss: 0.079\n",
            "[48,    10] loss: 0.088\n",
            "[48,    20] loss: 0.073\n",
            "[48,    30] loss: 0.097\n",
            "[48,    40] loss: 0.095\n",
            "[48,    50] loss: 0.076\n",
            "[49,    10] loss: 0.077\n",
            "[49,    20] loss: 0.075\n",
            "[49,    30] loss: 0.078\n",
            "[49,    40] loss: 0.074\n",
            "[49,    50] loss: 0.084\n",
            "[50,    10] loss: 0.082\n",
            "[50,    20] loss: 0.090\n",
            "[50,    30] loss: 0.089\n",
            "[50,    40] loss: 0.087\n",
            "[50,    50] loss: 0.069\n",
            "[51,    10] loss: 0.071\n",
            "[51,    20] loss: 0.071\n",
            "[51,    30] loss: 0.085\n",
            "[51,    40] loss: 0.078\n",
            "[51,    50] loss: 0.087\n",
            "[52,    10] loss: 0.082\n",
            "[52,    20] loss: 0.082\n",
            "[52,    30] loss: 0.082\n",
            "[52,    40] loss: 0.071\n",
            "[52,    50] loss: 0.064\n",
            "[53,    10] loss: 0.070\n",
            "[53,    20] loss: 0.076\n",
            "[53,    30] loss: 0.079\n",
            "[53,    40] loss: 0.085\n",
            "[53,    50] loss: 0.082\n",
            "[54,    10] loss: 0.079\n",
            "[54,    20] loss: 0.076\n",
            "[54,    30] loss: 0.077\n",
            "[54,    40] loss: 0.069\n",
            "[54,    50] loss: 0.077\n",
            "[55,    10] loss: 0.072\n",
            "[55,    20] loss: 0.082\n",
            "[55,    30] loss: 0.098\n",
            "[55,    40] loss: 0.168\n",
            "[55,    50] loss: 0.090\n",
            "[56,    10] loss: 0.078\n",
            "[56,    20] loss: 0.074\n",
            "[56,    30] loss: 0.092\n",
            "[56,    40] loss: 0.127\n",
            "[56,    50] loss: 0.081\n",
            "[57,    10] loss: 0.074\n",
            "[57,    20] loss: 0.101\n",
            "[57,    30] loss: 0.095\n",
            "[57,    40] loss: 0.087\n",
            "[57,    50] loss: 0.077\n",
            "[58,    10] loss: 0.083\n",
            "[58,    20] loss: 0.078\n",
            "[58,    30] loss: 0.076\n",
            "[58,    40] loss: 0.075\n",
            "[58,    50] loss: 0.080\n",
            "[59,    10] loss: 0.121\n",
            "[59,    20] loss: 0.137\n",
            "[59,    30] loss: 0.098\n",
            "[59,    40] loss: 0.075\n",
            "[59,    50] loss: 0.092\n",
            "[60,    10] loss: 0.083\n",
            "[60,    20] loss: 0.072\n",
            "[60,    30] loss: 0.093\n",
            "[60,    40] loss: 0.092\n",
            "[60,    50] loss: 0.078\n",
            "[61,    10] loss: 0.070\n",
            "[61,    20] loss: 0.076\n",
            "[61,    30] loss: 0.079\n",
            "[61,    40] loss: 0.113\n",
            "[61,    50] loss: 0.087\n",
            "[62,    10] loss: 0.066\n",
            "[62,    20] loss: 0.082\n",
            "[62,    30] loss: 0.111\n",
            "[62,    40] loss: 0.088\n",
            "[62,    50] loss: 0.090\n",
            "[63,    10] loss: 0.071\n",
            "[63,    20] loss: 0.081\n",
            "[63,    30] loss: 0.083\n",
            "[63,    40] loss: 0.078\n",
            "[63,    50] loss: 0.086\n",
            "[64,    10] loss: 0.089\n",
            "[64,    20] loss: 0.092\n",
            "[64,    30] loss: 0.073\n",
            "[64,    40] loss: 0.072\n",
            "[64,    50] loss: 0.087\n",
            "[65,    10] loss: 0.109\n",
            "[65,    20] loss: 0.080\n",
            "[65,    30] loss: 0.071\n",
            "[65,    40] loss: 0.088\n",
            "[65,    50] loss: 0.091\n",
            "[66,    10] loss: 0.080\n",
            "[66,    20] loss: 0.079\n",
            "[66,    30] loss: 0.071\n",
            "[66,    40] loss: 0.089\n",
            "[66,    50] loss: 0.073\n",
            "[67,    10] loss: 0.066\n",
            "[67,    20] loss: 0.080\n",
            "[67,    30] loss: 0.083\n",
            "[67,    40] loss: 0.085\n",
            "[67,    50] loss: 0.116\n",
            "[68,    10] loss: 0.075\n",
            "[68,    20] loss: 0.072\n",
            "[68,    30] loss: 0.081\n",
            "[68,    40] loss: 0.068\n",
            "[68,    50] loss: 0.072\n",
            "[69,    10] loss: 0.077\n",
            "[69,    20] loss: 0.080\n",
            "[69,    30] loss: 0.076\n",
            "[69,    40] loss: 0.067\n",
            "[69,    50] loss: 0.075\n",
            "[70,    10] loss: 0.073\n",
            "[70,    20] loss: 0.082\n",
            "[70,    30] loss: 0.091\n",
            "[70,    40] loss: 0.095\n",
            "[70,    50] loss: 0.087\n",
            "[71,    10] loss: 0.069\n",
            "[71,    20] loss: 0.071\n",
            "[71,    30] loss: 0.070\n",
            "[71,    40] loss: 0.072\n",
            "[71,    50] loss: 0.079\n",
            "[72,    10] loss: 0.085\n",
            "[72,    20] loss: 0.076\n",
            "[72,    30] loss: 0.069\n",
            "[72,    40] loss: 0.095\n",
            "[72,    50] loss: 0.073\n",
            "[73,    10] loss: 0.073\n",
            "[73,    20] loss: 0.070\n",
            "[73,    30] loss: 0.078\n",
            "[73,    40] loss: 0.072\n",
            "[73,    50] loss: 0.081\n",
            "[74,    10] loss: 0.077\n",
            "[74,    20] loss: 0.069\n",
            "[74,    30] loss: 0.076\n",
            "[74,    40] loss: 0.080\n",
            "[74,    50] loss: 0.079\n",
            "[75,    10] loss: 0.083\n",
            "[75,    20] loss: 0.072\n",
            "[75,    30] loss: 0.074\n",
            "[75,    40] loss: 0.085\n",
            "[75,    50] loss: 0.094\n",
            "[76,    10] loss: 0.100\n",
            "[76,    20] loss: 0.074\n",
            "[76,    30] loss: 0.072\n",
            "[76,    40] loss: 0.085\n",
            "[76,    50] loss: 0.077\n",
            "[77,    10] loss: 0.084\n",
            "[77,    20] loss: 0.071\n",
            "[77,    30] loss: 0.070\n",
            "[77,    40] loss: 0.075\n",
            "[77,    50] loss: 0.088\n",
            "[78,    10] loss: 0.076\n",
            "[78,    20] loss: 0.084\n",
            "[78,    30] loss: 0.084\n",
            "[78,    40] loss: 0.079\n",
            "[78,    50] loss: 0.092\n",
            "[79,    10] loss: 0.090\n",
            "[79,    20] loss: 0.072\n",
            "[79,    30] loss: 0.087\n",
            "[79,    40] loss: 0.074\n",
            "[79,    50] loss: 0.072\n",
            "[80,    10] loss: 0.093\n",
            "[80,    20] loss: 0.082\n",
            "[80,    30] loss: 0.075\n",
            "[80,    40] loss: 0.083\n",
            "[80,    50] loss: 0.082\n",
            "[81,    10] loss: 0.085\n",
            "[81,    20] loss: 0.075\n",
            "[81,    30] loss: 0.075\n",
            "[81,    40] loss: 0.066\n",
            "[81,    50] loss: 0.098\n",
            "[82,    10] loss: 0.087\n",
            "[82,    20] loss: 0.074\n",
            "[82,    30] loss: 0.096\n",
            "[82,    40] loss: 0.093\n",
            "[82,    50] loss: 0.088\n",
            "[83,    10] loss: 0.081\n",
            "[83,    20] loss: 0.079\n",
            "[83,    30] loss: 0.101\n",
            "[83,    40] loss: 0.082\n",
            "[83,    50] loss: 0.090\n",
            "[84,    10] loss: 0.076\n",
            "[84,    20] loss: 0.081\n",
            "[84,    30] loss: 0.088\n",
            "[84,    40] loss: 0.068\n",
            "[84,    50] loss: 0.073\n",
            "[85,    10] loss: 0.076\n",
            "[85,    20] loss: 0.079\n",
            "[85,    30] loss: 0.063\n",
            "[85,    40] loss: 0.095\n",
            "[85,    50] loss: 0.074\n",
            "[86,    10] loss: 0.067\n",
            "[86,    20] loss: 0.076\n",
            "[86,    30] loss: 0.080\n",
            "[86,    40] loss: 0.072\n",
            "[86,    50] loss: 0.081\n",
            "[87,    10] loss: 0.076\n",
            "[87,    20] loss: 0.082\n",
            "[87,    30] loss: 0.075\n",
            "[87,    40] loss: 0.113\n",
            "[87,    50] loss: 0.167\n",
            "[88,    10] loss: 0.130\n",
            "[88,    20] loss: 0.100\n",
            "[88,    30] loss: 0.086\n",
            "[88,    40] loss: 0.080\n",
            "[88,    50] loss: 0.090\n",
            "[89,    10] loss: 0.101\n",
            "[89,    20] loss: 0.070\n",
            "[89,    30] loss: 0.101\n",
            "[89,    40] loss: 0.091\n",
            "[89,    50] loss: 0.075\n",
            "[90,    10] loss: 0.095\n",
            "[90,    20] loss: 0.094\n",
            "[90,    30] loss: 0.079\n",
            "[90,    40] loss: 0.072\n",
            "[90,    50] loss: 0.076\n",
            "[91,    10] loss: 0.071\n",
            "[91,    20] loss: 0.104\n",
            "[91,    30] loss: 0.082\n",
            "[91,    40] loss: 0.086\n",
            "[91,    50] loss: 0.080\n",
            "[92,    10] loss: 0.087\n",
            "[92,    20] loss: 0.086\n",
            "[92,    30] loss: 0.071\n",
            "[92,    40] loss: 0.080\n",
            "[92,    50] loss: 0.133\n",
            "[93,    10] loss: 0.084\n",
            "[93,    20] loss: 0.079\n",
            "[93,    30] loss: 0.077\n",
            "[93,    40] loss: 0.094\n",
            "[93,    50] loss: 0.096\n",
            "[94,    10] loss: 0.076\n",
            "[94,    20] loss: 0.075\n",
            "[94,    30] loss: 0.091\n",
            "[94,    40] loss: 0.091\n",
            "[94,    50] loss: 0.075\n",
            "[95,    10] loss: 0.070\n",
            "[95,    20] loss: 0.074\n",
            "[95,    30] loss: 0.070\n",
            "[95,    40] loss: 0.082\n",
            "[95,    50] loss: 0.070\n",
            "[96,    10] loss: 0.075\n",
            "[96,    20] loss: 0.076\n",
            "[96,    30] loss: 0.074\n",
            "[96,    40] loss: 0.093\n",
            "[96,    50] loss: 0.087\n",
            "[97,    10] loss: 0.074\n",
            "[97,    20] loss: 0.092\n",
            "[97,    30] loss: 0.092\n",
            "[97,    40] loss: 0.077\n",
            "[97,    50] loss: 0.074\n",
            "[98,    10] loss: 0.075\n",
            "[98,    20] loss: 0.077\n",
            "[98,    30] loss: 0.076\n",
            "[98,    40] loss: 0.071\n",
            "[98,    50] loss: 0.068\n",
            "[99,    10] loss: 0.066\n",
            "[99,    20] loss: 0.075\n",
            "[99,    30] loss: 0.086\n",
            "[99,    40] loss: 0.083\n",
            "[99,    50] loss: 0.074\n",
            "[100,    10] loss: 0.070\n",
            "[100,    20] loss: 0.085\n",
            "[100,    30] loss: 0.076\n",
            "[100,    40] loss: 0.074\n",
            "[100,    50] loss: 0.086\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 33 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 54 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 63 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 74 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 94 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 1.088\n",
            "[1,    20] loss: 1.085\n",
            "[1,    30] loss: 1.081\n",
            "[1,    40] loss: 1.069\n",
            "[1,    50] loss: 1.052\n",
            "[2,    10] loss: 0.998\n",
            "[2,    20] loss: 0.906\n",
            "[2,    30] loss: 0.761\n",
            "[2,    40] loss: 0.579\n",
            "[2,    50] loss: 0.404\n",
            "[3,    10] loss: 0.299\n",
            "[3,    20] loss: 0.223\n",
            "[3,    30] loss: 0.160\n",
            "[3,    40] loss: 0.124\n",
            "[3,    50] loss: 0.091\n",
            "[4,    10] loss: 0.081\n",
            "[4,    20] loss: 0.074\n",
            "[4,    30] loss: 0.064\n",
            "[4,    40] loss: 0.057\n",
            "[4,    50] loss: 0.071\n",
            "[5,    10] loss: 0.082\n",
            "[5,    20] loss: 0.057\n",
            "[5,    30] loss: 0.048\n",
            "[5,    40] loss: 0.047\n",
            "[5,    50] loss: 0.046\n",
            "[6,    10] loss: 0.044\n",
            "[6,    20] loss: 0.048\n",
            "[6,    30] loss: 0.049\n",
            "[6,    40] loss: 0.044\n",
            "[6,    50] loss: 0.053\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 32 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 37 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 41 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 51 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 58 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 66 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 75 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 98 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In76SYH_zZHV"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS4HtOHEzZ0E",
        "outputId": "4d5a2bd3-6ccd-4e7c-ef6c-340912fc3aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd3fa9adcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS5D_SpOzj9n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va4L2IZ1zj6o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}