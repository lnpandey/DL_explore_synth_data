# -*- coding: utf-8 -*-
"""MyRmsprop

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L18LuZ_AxCtmMzbyQspfGVMs7TtAXM4-
"""

import torch 
import torch.optim as optim
from typing import List, Optional
import numpy as np

def update_rmsprop(params: List[torch.Tensor],grads: List[torch.Tensor],square_avgs: List[torch.Tensor],grad_avgs: List[torch.Tensor],
            momentum_buffer_list: List[torch.Tensor],H:torch.Tensor,*,lr: float,alpha: float,eps: float,weight_decay: float,momentum: float):
    
    
    """Function that performs rmsprop algorithm computation.
    See :class:`~torch.optim.RMSProp` for details.
    
    
    """

    for i, param in enumerate(params):
      
        grad = grads[i]  
        r,c = grad.shape
        grd1 = torch.reshape(grad,(r*c,1))
        grad = torch.reshape(torch.matmul(H,grd1),(r,c))
        #print("1111",square_avgs,len(square_avgs))
        square_avg = square_avgs[i] 

        if weight_decay != 0:
            grad = grad.add(param, alpha=weight_decay)

        square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)     # grad_squared_avg = alpha*(grad_squared_avg) + (1-alpha)*(grad)**2
        avg = square_avg.sqrt().add_(eps)   # avg sqrt(grad_sqaured_avg) + eps

        if momentum > 0:
            buf = momentum_buffer_list[i]
            buf.mul_(momentum).addcdiv_(grad, avg)
            param.add_(buf, alpha=-lr)
        else:
            param.addcdiv_(grad, avg, value=-lr)

class MyRmsprop(optim.Optimizer):
  '''

  '''
  def __init__(self,params,H:torch.Tensor,lr =0.01,alpha=0.99,eps=1e-08, weight_decay=0, momentum=0):
    '''

    '''
    #print(params[0].shape)
    defaults = dict(H=H,lr = lr,alpha=alpha,eps=eps,weight_decay=weight_decay,momentum=momentum)
    super(MyRmsprop,self).__init__(params,defaults)
    #print(self.param_groups)


  def __setstate__(self, state):
    super(MyRmsprop, self).__setstate__(state)
    for group in self.param_groups:
      group.setdefault('momentum', 0)
  
  @torch.no_grad()
  def step(self,closure=None):
    '''

    '''
    loss = None
    for group in self.param_groups:
      params_with_grad = []
      grads = []
      square_avgs = []
      grad_avgs = []
      momentum_buffer_list = []
      
      for p in group['params']:
        if p.grad is None:
          continue
        params_with_grad.append(p)

        grads.append(p.grad)
        state = self.state[p]



        # state initialization
        if len(state) == 0:
          state['step'] = 0 
          state['square_avg'] = torch.zeros_like(p,memory_format=torch.preserve_format )
          if group['momentum'] >0:
            state['momentum_buffer'] = torch.zeros_like(p, memory_format=torch.preserve_format)
        square_avgs.append(state['square_avg'])

        if group['momentum'] > 0:
          momentum_buffer_list.append(state['momentum_buffer'])
        state['step'] += 1
      #print(group['H'],group['lr'])
      update_rmsprop(params_with_grad,grads,square_avgs,grad_avgs,momentum_buffer_list,H=group['H'],lr=group['lr'],alpha=group['alpha'],
                     eps=group['eps'],weight_decay=group['weight_decay'],momentum=group['momentum'])
    return loss