{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "What_4layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjG64ra4aFu",
        "outputId": "3b34ba18-e808-4ab3-a21b-3bc52fb504da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V8-7SARDZErK",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import copy\n",
        "import pickle\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwJv7Y8Rewez",
        "outputId": "5f08ab72-70cf-42b0-9275-d5738b53b7cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "foreground_classes = {'plane', 'car', 'bird'}\n",
        "\n",
        "background_classes = {'cat', 'deer', 'dog', 'frog', 'horse','ship', 'truck'}\n",
        "\n",
        "# print(type(foreground_classes))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "background_data=[]\n",
        "background_label=[]\n",
        "foreground_data=[]\n",
        "foreground_label=[]\n",
        "batch_size=10\n",
        "\n",
        "for i in range(5000):   #5000*batch_size = 50000 data points\n",
        "  images, labels = dataiter.next()\n",
        "  for j in range(batch_size):\n",
        "    if(classes[labels[j]] in background_classes):\n",
        "      img = images[j].tolist()\n",
        "      background_data.append(img)\n",
        "      background_label.append(labels[j])\n",
        "    else:\n",
        "      img = images[j].tolist()\n",
        "      foreground_data.append(img)\n",
        "      foreground_label.append(labels[j])\n",
        "            \n",
        "foreground_data = torch.tensor(foreground_data)\n",
        "foreground_label = torch.tensor(foreground_label)\n",
        "background_data = torch.tensor(background_data)\n",
        "background_label = torch.tensor(background_label)\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:02, 73561251.22it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nDYhjJse6Qq",
        "colab": {}
      },
      "source": [
        "def create_mosaic_img(bg_idx,fg_idx,fg): \n",
        "  \"\"\"\n",
        "  bg_idx : list of indexes of background_data[] to be used as background images in mosaic\n",
        "  fg_idx : index of image to be used as foreground image from foreground data\n",
        "  fg : at what position/index foreground image has to be stored out of 0-8\n",
        "  \"\"\"\n",
        "  image_list=[]\n",
        "  j=0\n",
        "  for i in range(9):\n",
        "    if i != fg:\n",
        "      image_list.append(background_data[bg_idx[j]].type(\"torch.DoubleTensor\"))\n",
        "      j+=1\n",
        "    else: \n",
        "      image_list.append(foreground_data[fg_idx].type(\"torch.DoubleTensor\"))\n",
        "      label = foreground_label[fg_idx]  #-7  # minus 7 because our fore ground classes are 7,8,9 but we have to store it as 0,1,2\n",
        "  #image_list = np.concatenate(image_list ,axis=0)\n",
        "  image_list = torch.stack(image_list) \n",
        "  return image_list,label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aivGVg14e9iZ",
        "colab": {}
      },
      "source": [
        "desired_num = 20000\n",
        "mosaic_list_of_images =[]      # list of mosaic images, each mosaic image is saved as list of 9 images\n",
        "fore_idx =[]                   # list of indexes at which foreground image is present in a mosaic image i.e from 0 to 9               \n",
        "mosaic_label=[]                # label of mosaic image = foreground class present in that mosaic\n",
        "for i in range(desired_num):\n",
        "  bg_idx = np.random.randint(0,35000,8)\n",
        "  fg_idx = np.random.randint(0,15000)\n",
        "  fg = np.random.randint(0,9)\n",
        "  fore_idx.append(fg)\n",
        "  image_list,label = create_mosaic_img(bg_idx,fg_idx,fg)\n",
        "  mosaic_list_of_images.append(image_list)\n",
        "  mosaic_label.append(label)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIuiboIUfViV"
      },
      "source": [
        "# Load data from saved file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cog5VUzGgE5L",
        "outputId": "1df71c8d-d9e9-4ccf-b355-f9725b7a2827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(len(mosaic_list_of_images) , len(mosaic_label), len(mosaic_list_of_images[0:10000]))\n",
        "print(len(fore_idx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 20000 10000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xX91RwMy-IP4",
        "colab": {}
      },
      "source": [
        "def create_avg_image_from_mosaic_dataset(mosaic_dataset,labels,foreground_index,dataset_number):\n",
        "  \"\"\"\n",
        "  mosaic_dataset : mosaic_dataset contains 9 images 32 x 32 each as 1 data point\n",
        "  labels : mosaic_dataset labels\n",
        "  foreground_index : contains list of indexes where foreground image is present so that using this we can take weighted average\n",
        "  dataset_number : will help us to tell what ratio of foreground image to be taken. for eg: if it is \"j\" then fg_image_ratio = j/9 , bg_image_ratio = (9-j)/8*9\n",
        "  \"\"\"\n",
        "  avg_image_dataset = []\n",
        "  for i in range(len(mosaic_dataset)):\n",
        "    img = torch.zeros([3, 32,32], dtype=torch.float64)\n",
        "    for j in range(9):\n",
        "      if j == foreground_index[i]:\n",
        "        img = img + mosaic_dataset[i][j]*dataset_number/9\n",
        "      else :\n",
        "        img = img + mosaic_dataset[i][j]*(9-dataset_number)/(8*9)\n",
        "    \n",
        "    avg_image_dataset.append(img)\n",
        "    \n",
        "  return avg_image_dataset , labels , foreground_index\n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGz8Y88vIZPT",
        "colab": {}
      },
      "source": [
        "avg_image_dataset_1 , labels_1,  fg_index_1 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 1)\n",
        "avg_image_dataset_2 , labels_2,  fg_index_2 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 2)\n",
        "avg_image_dataset_3 , labels_3,  fg_index_3 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 3)\n",
        "avg_image_dataset_4 , labels_4,  fg_index_4 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 4)\n",
        "avg_image_dataset_5 , labels_5,  fg_index_5 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 5)\n",
        "avg_image_dataset_6 , labels_6,  fg_index_6 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 6)\n",
        "avg_image_dataset_7 , labels_7,  fg_index_7 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 7)\n",
        "avg_image_dataset_8 , labels_8,  fg_index_8 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 8)\n",
        "avg_image_dataset_9 , labels_9,  fg_index_9 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[0:10000], mosaic_label[0:10000], fore_idx[0:10000] , 9)\n",
        "\n",
        "test_dataset_10 , labels_10 , fg_index_10 = create_avg_image_from_mosaic_dataset(mosaic_list_of_images[10000:20000], mosaic_label[10000:20000], fore_idx[10000:20000] , 9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nSO9SFE25Lrk",
        "colab": {}
      },
      "source": [
        "class MosaicDataset(Dataset):\n",
        "  \"\"\"MosaicDataset dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, mosaic_list_of_images, mosaic_label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        root_dir (string): Directory with all the images.\n",
        "        transform (callable, optional): Optional transform to be applied\n",
        "            on a sample.\n",
        "    \"\"\"\n",
        "    self.mosaic = mosaic_list_of_images\n",
        "    self.label = mosaic_label\n",
        "    #self.fore_idx = fore_idx\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.mosaic[idx] , self.label[idx] #, self.fore_idx[idx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obE1xeyRks1Q",
        "colab": {}
      },
      "source": [
        "batch = 256\n",
        "\n",
        "traindata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "trainloader_1 = DataLoader( traindata_1 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "trainloader_2 = DataLoader( traindata_2 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "trainloader_3 = DataLoader( traindata_3 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "trainloader_4 = DataLoader( traindata_4 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "trainloader_5 = DataLoader( traindata_5 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "trainloader_6 = DataLoader( traindata_6 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "trainloader_7 = DataLoader( traindata_7 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "trainloader_8 = DataLoader( traindata_8 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "traindata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "trainloader_9 = DataLoader( traindata_9 , batch_size= batch ,shuffle=True)\n",
        "\n",
        "testdata_1 = MosaicDataset(avg_image_dataset_1, labels_1 )\n",
        "testloader_1 = DataLoader( testdata_1 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_2 = MosaicDataset(avg_image_dataset_2, labels_2 )\n",
        "testloader_2 = DataLoader( testdata_2 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_3 = MosaicDataset(avg_image_dataset_3, labels_3 )\n",
        "testloader_3 = DataLoader( testdata_3 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_4 = MosaicDataset(avg_image_dataset_4, labels_4 )\n",
        "testloader_4 = DataLoader( testdata_4 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_5 = MosaicDataset(avg_image_dataset_5, labels_5 )\n",
        "testloader_5 = DataLoader( testdata_5 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_6 = MosaicDataset(avg_image_dataset_6, labels_6 )\n",
        "testloader_6 = DataLoader( testdata_6 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_7 = MosaicDataset(avg_image_dataset_7, labels_7 )\n",
        "testloader_7 = DataLoader( testdata_7 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_8 = MosaicDataset(avg_image_dataset_8, labels_8 )\n",
        "testloader_8 = DataLoader( testdata_8 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_9 = MosaicDataset(avg_image_dataset_9, labels_9 )\n",
        "testloader_9 = DataLoader( testdata_9 , batch_size= batch ,shuffle=False)\n",
        "\n",
        "testdata_10 = MosaicDataset(test_dataset_10, labels_10 )\n",
        "testloader_10 = DataLoader( testdata_10 , batch_size= batch ,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SadRzWBBZEsP",
        "colab": {}
      },
      "source": [
        "class Conv_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f,s,k,pad):\n",
        "        super(Conv_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.s = s \n",
        "        self.k = k \n",
        "        self.pad = pad\n",
        "        \n",
        "        \n",
        "        self.conv = nn.Conv2d(self.inp_ch,self.f,k,stride=s,padding=self.pad)\n",
        "        self.bn = nn.BatchNorm2d(self.f)\n",
        "        self.act = nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IgGYMG_ZZEsT",
        "colab": {}
      },
      "source": [
        "class inception_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f0,f1):\n",
        "        super(inception_module, self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f0 = f0\n",
        "        self.f1 = f1 \n",
        "        self.conv1 = Conv_module(self.inp_ch,self.f0,1,1,pad=0)\n",
        "        self.conv3 = Conv_module(self.inp_ch,self.f1,1,3,pad=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv1.forward(x)\n",
        "        x3 = self.conv3.forward(x)\n",
        "        #print(x1.shape,x3.shape)\n",
        "        \n",
        "        x = torch.cat((x1,x3),dim=1)\n",
        "        \n",
        "    \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "thkdqW91Hpju",
        "colab": {}
      },
      "source": [
        "class downsample_module(nn.Module):\n",
        "    def __init__(self,inp_ch,f):\n",
        "        super(downsample_module,self).__init__()\n",
        "        self.inp_ch = inp_ch\n",
        "        self.f = f\n",
        "        self.conv = Conv_module(self.inp_ch,self.f,1,3,pad=1)\n",
        "        self.pool = nn.MaxPool2d(3,stride=1,padding=1)\n",
        "    def forward(self,x):\n",
        "        x1 = self.conv(x)\n",
        "        #print(x1.shape)\n",
        "        x2 = self.pool(x)\n",
        "        #print(x2.shape)\n",
        "        x = torch.cat((x1,x2),dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u1yVWgR4vFhe",
        "colab": {}
      },
      "source": [
        "\n",
        "class classy_inception_net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classy_inception_net,self).__init__()\n",
        "        self.conv1 = Conv_module(3,64,1,3,0)\n",
        "        \n",
        "        self.incept1 = inception_module(64,16,16)\n",
        "        self.incept2 = inception_module(32,16,56)\n",
        "        \n",
        "        self.downsample1 = downsample_module(72,72)\n",
        "        \n",
        "        self.incept3 = inception_module(144,32,32)\n",
        "        self.incept4 = inception_module(64,48,32)\n",
        "        self.pool = nn.AvgPool2d(7)       \n",
        "        self.linear = nn.Linear(4*4*80,3)\n",
        "    def forward(self,x):\n",
        "        x = self.conv1.forward(x)\n",
        "        x = self.incept1.forward(x)\n",
        "        x = self.incept2.forward(x)\n",
        "        x = self.downsample1.forward(x)\n",
        "        \n",
        "        x = self.incept3.forward(x)\n",
        "        x = self.incept4.forward(x)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1,4*4*80)\n",
        "        x = self.linear(x) \n",
        "        #activatn = {\"act1\":act1,\"act2\":act2,\"act3\":act3,\"act4\":act4,\"act5\":act5,\"act6\":act6,\n",
        "         #           \"act7\":act7,\"act8\":act8,\"act9\":act9,\"act10\":act10,\"act11\":act11}\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cOWrnzv1fVjD",
        "colab": {}
      },
      "source": [
        "def test_all(number, testloader,inc):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    out = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            out.append(labels.cpu().numpy())\n",
        "            outputs= inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test dataset %d: %d %%' % (number , 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFfAJZkcZEsY",
        "colab": {}
      },
      "source": [
        "def train_all(trainloader, ds_number, testloader_list):\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    print(\"training on data set  \", ds_number)\n",
        "    \n",
        "    inc = classy_inception_net().double()\n",
        "    inc = inc.to(\"cuda\")\n",
        "    \n",
        "    criterion_inception = nn.CrossEntropyLoss()\n",
        "    optimizer_inception = optim.SGD(inc.parameters(), lr=0.01, momentum=0.9)\n",
        "    \n",
        "    acti = []\n",
        "    loss_curi = []\n",
        "    epochs = 70\n",
        "    \n",
        "    for epoch in range(epochs): # loop over the dataset multiple times\n",
        "        ep_lossi = []\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(\"cuda\"),labels.to(\"cuda\")\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_inception.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = inc(inputs)\n",
        "            loss = criterion_inception(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_inception.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 9:    # print every 10 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 10))\n",
        "                ep_lossi.append(running_loss/10) # loss per minibatch\n",
        "                running_loss = 0.0\n",
        "                \n",
        "        loss_curi.append(np.mean(ep_lossi))   #loss per epoch\n",
        "    #     if (epoch%5 == 0):\n",
        "    #         _,actis= inc(inputs)\n",
        "    #         acti.append(actis)\n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    torch.save(inc.state_dict(),\"/content/drive/My Drive/Research/checking_what_net/weights_12layers_inception/train_dataset_\"+str(ds_number)+\"_\"+str(epochs)+\".pt\")\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in trainloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
        "            outputs = inc(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 train images: %d %%' % (  100 * correct / total))\n",
        "    \n",
        "    for i, j in enumerate(testloader_list):\n",
        "        test_all(i+1, j,inc)\n",
        "    \n",
        "    print(\"--\"*40)\n",
        "    \n",
        "    return loss_curi\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mI-vqhB-fVjJ",
        "outputId": "f2c0030b-d869-42ce-8ffb-f3f36b21c8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss_all=[]\n",
        "\n",
        "testloader_list= [ testloader_1, testloader_2, testloader_3, testloader_4, testloader_5, testloader_6,\n",
        "                 testloader_7, testloader_8, testloader_9, testloader_10]\n",
        "\n",
        "train_loss_all.append(train_all(trainloader_1, 1, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_2, 2, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_3, 3, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_4, 4, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_5, 5, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_6, 6, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_7, 7, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_8, 8, testloader_list))\n",
        "train_loss_all.append(train_all(trainloader_9, 9, testloader_list))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "training on data set   1\n",
            "[1,    10] loss: 1.091\n",
            "[1,    20] loss: 1.073\n",
            "[1,    30] loss: 1.065\n",
            "[1,    40] loss: 1.050\n",
            "[2,    10] loss: 1.051\n",
            "[2,    20] loss: 1.055\n",
            "[2,    30] loss: 1.033\n",
            "[2,    40] loss: 1.019\n",
            "[3,    10] loss: 1.014\n",
            "[3,    20] loss: 1.014\n",
            "[3,    30] loss: 0.999\n",
            "[3,    40] loss: 1.032\n",
            "[4,    10] loss: 0.985\n",
            "[4,    20] loss: 1.002\n",
            "[4,    30] loss: 1.044\n",
            "[4,    40] loss: 1.028\n",
            "[5,    10] loss: 1.005\n",
            "[5,    20] loss: 0.974\n",
            "[5,    30] loss: 0.980\n",
            "[5,    40] loss: 0.966\n",
            "[6,    10] loss: 0.970\n",
            "[6,    20] loss: 0.979\n",
            "[6,    30] loss: 0.975\n",
            "[6,    40] loss: 0.973\n",
            "[7,    10] loss: 0.951\n",
            "[7,    20] loss: 0.955\n",
            "[7,    30] loss: 0.969\n",
            "[7,    40] loss: 0.964\n",
            "[8,    10] loss: 0.955\n",
            "[8,    20] loss: 0.943\n",
            "[8,    30] loss: 0.947\n",
            "[8,    40] loss: 0.944\n",
            "[9,    10] loss: 0.949\n",
            "[9,    20] loss: 0.932\n",
            "[9,    30] loss: 0.931\n",
            "[9,    40] loss: 0.928\n",
            "[10,    10] loss: 0.949\n",
            "[10,    20] loss: 0.935\n",
            "[10,    30] loss: 0.927\n",
            "[10,    40] loss: 0.898\n",
            "[11,    10] loss: 0.951\n",
            "[11,    20] loss: 0.899\n",
            "[11,    30] loss: 0.931\n",
            "[11,    40] loss: 0.944\n",
            "[12,    10] loss: 0.947\n",
            "[12,    20] loss: 0.953\n",
            "[12,    30] loss: 0.878\n",
            "[12,    40] loss: 0.877\n",
            "[13,    10] loss: 0.874\n",
            "[13,    20] loss: 0.847\n",
            "[13,    30] loss: 0.871\n",
            "[13,    40] loss: 0.886\n",
            "[14,    10] loss: 0.851\n",
            "[14,    20] loss: 0.867\n",
            "[14,    30] loss: 0.814\n",
            "[14,    40] loss: 0.825\n",
            "[15,    10] loss: 0.812\n",
            "[15,    20] loss: 0.842\n",
            "[15,    30] loss: 0.815\n",
            "[15,    40] loss: 0.833\n",
            "[16,    10] loss: 0.831\n",
            "[16,    20] loss: 0.801\n",
            "[16,    30] loss: 0.803\n",
            "[16,    40] loss: 0.818\n",
            "[17,    10] loss: 0.788\n",
            "[17,    20] loss: 0.758\n",
            "[17,    30] loss: 0.767\n",
            "[17,    40] loss: 0.804\n",
            "[18,    10] loss: 0.764\n",
            "[18,    20] loss: 0.761\n",
            "[18,    30] loss: 0.780\n",
            "[18,    40] loss: 0.765\n",
            "[19,    10] loss: 0.774\n",
            "[19,    20] loss: 0.756\n",
            "[19,    30] loss: 0.736\n",
            "[19,    40] loss: 0.751\n",
            "[20,    10] loss: 0.729\n",
            "[20,    20] loss: 0.729\n",
            "[20,    30] loss: 0.724\n",
            "[20,    40] loss: 0.712\n",
            "[21,    10] loss: 0.679\n",
            "[21,    20] loss: 0.696\n",
            "[21,    30] loss: 0.698\n",
            "[21,    40] loss: 0.675\n",
            "[22,    10] loss: 0.657\n",
            "[22,    20] loss: 0.658\n",
            "[22,    30] loss: 0.624\n",
            "[22,    40] loss: 0.643\n",
            "[23,    10] loss: 0.655\n",
            "[23,    20] loss: 0.638\n",
            "[23,    30] loss: 0.641\n",
            "[23,    40] loss: 0.646\n",
            "[24,    10] loss: 0.619\n",
            "[24,    20] loss: 0.604\n",
            "[24,    30] loss: 0.598\n",
            "[24,    40] loss: 0.621\n",
            "[25,    10] loss: 0.665\n",
            "[25,    20] loss: 0.625\n",
            "[25,    30] loss: 0.600\n",
            "[25,    40] loss: 0.631\n",
            "[26,    10] loss: 0.694\n",
            "[26,    20] loss: 0.627\n",
            "[26,    30] loss: 0.602\n",
            "[26,    40] loss: 0.546\n",
            "[27,    10] loss: 0.543\n",
            "[27,    20] loss: 0.513\n",
            "[27,    30] loss: 0.500\n",
            "[27,    40] loss: 0.535\n",
            "[28,    10] loss: 0.506\n",
            "[28,    20] loss: 0.501\n",
            "[28,    30] loss: 0.482\n",
            "[28,    40] loss: 0.508\n",
            "[29,    10] loss: 0.614\n",
            "[29,    20] loss: 0.616\n",
            "[29,    30] loss: 0.523\n",
            "[29,    40] loss: 0.511\n",
            "[30,    10] loss: 0.544\n",
            "[30,    20] loss: 0.521\n",
            "[30,    30] loss: 0.489\n",
            "[30,    40] loss: 0.492\n",
            "[31,    10] loss: 0.530\n",
            "[31,    20] loss: 0.543\n",
            "[31,    30] loss: 0.498\n",
            "[31,    40] loss: 0.442\n",
            "[32,    10] loss: 0.429\n",
            "[32,    20] loss: 0.395\n",
            "[32,    30] loss: 0.381\n",
            "[32,    40] loss: 0.382\n",
            "[33,    10] loss: 0.432\n",
            "[33,    20] loss: 0.373\n",
            "[33,    30] loss: 0.353\n",
            "[33,    40] loss: 0.352\n",
            "[34,    10] loss: 0.326\n",
            "[34,    20] loss: 0.314\n",
            "[34,    30] loss: 0.285\n",
            "[34,    40] loss: 0.307\n",
            "[35,    10] loss: 0.306\n",
            "[35,    20] loss: 0.297\n",
            "[35,    30] loss: 0.300\n",
            "[35,    40] loss: 0.297\n",
            "[36,    10] loss: 0.341\n",
            "[36,    20] loss: 0.313\n",
            "[36,    30] loss: 0.300\n",
            "[36,    40] loss: 0.260\n",
            "[37,    10] loss: 0.269\n",
            "[37,    20] loss: 0.247\n",
            "[37,    30] loss: 0.244\n",
            "[37,    40] loss: 0.234\n",
            "[38,    10] loss: 0.215\n",
            "[38,    20] loss: 0.200\n",
            "[38,    30] loss: 0.188\n",
            "[38,    40] loss: 0.183\n",
            "[39,    10] loss: 0.184\n",
            "[39,    20] loss: 0.179\n",
            "[39,    30] loss: 0.177\n",
            "[39,    40] loss: 0.171\n",
            "[40,    10] loss: 0.180\n",
            "[40,    20] loss: 0.166\n",
            "[40,    30] loss: 0.149\n",
            "[40,    40] loss: 0.160\n",
            "[41,    10] loss: 0.245\n",
            "[41,    20] loss: 0.255\n",
            "[41,    30] loss: 0.204\n",
            "[41,    40] loss: 0.194\n",
            "[42,    10] loss: 0.321\n",
            "[42,    20] loss: 0.270\n",
            "[42,    30] loss: 0.199\n",
            "[42,    40] loss: 0.198\n",
            "[43,    10] loss: 0.160\n",
            "[43,    20] loss: 0.151\n",
            "[43,    30] loss: 0.121\n",
            "[43,    40] loss: 0.117\n",
            "[44,    10] loss: 0.119\n",
            "[44,    20] loss: 0.108\n",
            "[44,    30] loss: 0.096\n",
            "[44,    40] loss: 0.097\n",
            "[45,    10] loss: 0.125\n",
            "[45,    20] loss: 0.115\n",
            "[45,    30] loss: 0.104\n",
            "[45,    40] loss: 0.106\n",
            "[46,    10] loss: 0.194\n",
            "[46,    20] loss: 0.158\n",
            "[46,    30] loss: 0.132\n",
            "[46,    40] loss: 0.121\n",
            "[47,    10] loss: 0.175\n",
            "[47,    20] loss: 0.154\n",
            "[47,    30] loss: 0.127\n",
            "[47,    40] loss: 0.118\n",
            "[48,    10] loss: 0.142\n",
            "[48,    20] loss: 0.115\n",
            "[48,    30] loss: 0.099\n",
            "[48,    40] loss: 0.105\n",
            "[49,    10] loss: 0.214\n",
            "[49,    20] loss: 0.165\n",
            "[49,    30] loss: 0.142\n",
            "[49,    40] loss: 0.134\n",
            "[50,    10] loss: 0.115\n",
            "[50,    20] loss: 0.101\n",
            "[50,    30] loss: 0.089\n",
            "[50,    40] loss: 0.084\n",
            "[51,    10] loss: 0.108\n",
            "[51,    20] loss: 0.100\n",
            "[51,    30] loss: 0.088\n",
            "[51,    40] loss: 0.069\n",
            "[52,    10] loss: 0.114\n",
            "[52,    20] loss: 0.090\n",
            "[52,    30] loss: 0.075\n",
            "[52,    40] loss: 0.055\n",
            "[53,    10] loss: 0.043\n",
            "[53,    20] loss: 0.038\n",
            "[53,    30] loss: 0.029\n",
            "[53,    40] loss: 0.028\n",
            "[54,    10] loss: 0.019\n",
            "[54,    20] loss: 0.018\n",
            "[54,    30] loss: 0.016\n",
            "[54,    40] loss: 0.016\n",
            "[55,    10] loss: 0.012\n",
            "[55,    20] loss: 0.012\n",
            "[55,    30] loss: 0.012\n",
            "[55,    40] loss: 0.012\n",
            "[56,    10] loss: 0.011\n",
            "[56,    20] loss: 0.011\n",
            "[56,    30] loss: 0.010\n",
            "[56,    40] loss: 0.013\n",
            "[57,    10] loss: 0.014\n",
            "[57,    20] loss: 0.015\n",
            "[57,    30] loss: 0.012\n",
            "[57,    40] loss: 0.015\n",
            "[58,    10] loss: 0.022\n",
            "[58,    20] loss: 0.020\n",
            "[58,    30] loss: 0.016\n",
            "[58,    40] loss: 0.015\n",
            "[59,    10] loss: 0.015\n",
            "[59,    20] loss: 0.013\n",
            "[59,    30] loss: 0.011\n",
            "[59,    40] loss: 0.010\n",
            "[60,    10] loss: 0.008\n",
            "[60,    20] loss: 0.008\n",
            "[60,    30] loss: 0.008\n",
            "[60,    40] loss: 0.008\n",
            "[61,    10] loss: 0.007\n",
            "[61,    20] loss: 0.007\n",
            "[61,    30] loss: 0.007\n",
            "[61,    40] loss: 0.007\n",
            "[62,    10] loss: 0.006\n",
            "[62,    20] loss: 0.006\n",
            "[62,    30] loss: 0.007\n",
            "[62,    40] loss: 0.011\n",
            "[63,    10] loss: 0.025\n",
            "[63,    20] loss: 0.020\n",
            "[63,    30] loss: 0.016\n",
            "[63,    40] loss: 0.015\n",
            "[64,    10] loss: 0.026\n",
            "[64,    20] loss: 0.021\n",
            "[64,    30] loss: 0.015\n",
            "[64,    40] loss: 0.014\n",
            "[65,    10] loss: 0.016\n",
            "[65,    20] loss: 0.014\n",
            "[65,    30] loss: 0.011\n",
            "[65,    40] loss: 0.011\n",
            "[66,    10] loss: 0.009\n",
            "[66,    20] loss: 0.009\n",
            "[66,    30] loss: 0.007\n",
            "[66,    40] loss: 0.007\n",
            "[67,    10] loss: 0.006\n",
            "[67,    20] loss: 0.006\n",
            "[67,    30] loss: 0.006\n",
            "[67,    40] loss: 0.007\n",
            "[68,    10] loss: 0.007\n",
            "[68,    20] loss: 0.006\n",
            "[68,    30] loss: 0.006\n",
            "[68,    40] loss: 0.005\n",
            "[69,    10] loss: 0.004\n",
            "[69,    20] loss: 0.004\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.005\n",
            "[70,    10] loss: 0.004\n",
            "[70,    20] loss: 0.004\n",
            "[70,    30] loss: 0.004\n",
            "[70,    40] loss: 0.004\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 81 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 76 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 73 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 71 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 69 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   2\n",
            "[1,    10] loss: 1.012\n",
            "[1,    20] loss: 0.866\n",
            "[1,    30] loss: 0.798\n",
            "[1,    40] loss: 0.784\n",
            "[2,    10] loss: 0.773\n",
            "[2,    20] loss: 0.746\n",
            "[2,    30] loss: 0.719\n",
            "[2,    40] loss: 0.705\n",
            "[3,    10] loss: 0.676\n",
            "[3,    20] loss: 0.686\n",
            "[3,    30] loss: 0.666\n",
            "[3,    40] loss: 0.678\n",
            "[4,    10] loss: 0.699\n",
            "[4,    20] loss: 0.658\n",
            "[4,    30] loss: 0.642\n",
            "[4,    40] loss: 0.611\n",
            "[5,    10] loss: 0.639\n",
            "[5,    20] loss: 0.609\n",
            "[5,    30] loss: 0.595\n",
            "[5,    40] loss: 0.627\n",
            "[6,    10] loss: 0.586\n",
            "[6,    20] loss: 0.585\n",
            "[6,    30] loss: 0.597\n",
            "[6,    40] loss: 0.601\n",
            "[7,    10] loss: 0.569\n",
            "[7,    20] loss: 0.563\n",
            "[7,    30] loss: 0.541\n",
            "[7,    40] loss: 0.576\n",
            "[8,    10] loss: 0.575\n",
            "[8,    20] loss: 0.532\n",
            "[8,    30] loss: 0.531\n",
            "[8,    40] loss: 0.569\n",
            "[9,    10] loss: 0.580\n",
            "[9,    20] loss: 0.533\n",
            "[9,    30] loss: 0.540\n",
            "[9,    40] loss: 0.537\n",
            "[10,    10] loss: 0.510\n",
            "[10,    20] loss: 0.506\n",
            "[10,    30] loss: 0.524\n",
            "[10,    40] loss: 0.482\n",
            "[11,    10] loss: 0.465\n",
            "[11,    20] loss: 0.475\n",
            "[11,    30] loss: 0.473\n",
            "[11,    40] loss: 0.485\n",
            "[12,    10] loss: 0.474\n",
            "[12,    20] loss: 0.464\n",
            "[12,    30] loss: 0.462\n",
            "[12,    40] loss: 0.444\n",
            "[13,    10] loss: 0.462\n",
            "[13,    20] loss: 0.454\n",
            "[13,    30] loss: 0.461\n",
            "[13,    40] loss: 0.405\n",
            "[14,    10] loss: 0.401\n",
            "[14,    20] loss: 0.411\n",
            "[14,    30] loss: 0.417\n",
            "[14,    40] loss: 0.420\n",
            "[15,    10] loss: 0.414\n",
            "[15,    20] loss: 0.430\n",
            "[15,    30] loss: 0.400\n",
            "[15,    40] loss: 0.417\n",
            "[16,    10] loss: 0.394\n",
            "[16,    20] loss: 0.400\n",
            "[16,    30] loss: 0.384\n",
            "[16,    40] loss: 0.373\n",
            "[17,    10] loss: 0.344\n",
            "[17,    20] loss: 0.365\n",
            "[17,    30] loss: 0.370\n",
            "[17,    40] loss: 0.367\n",
            "[18,    10] loss: 0.379\n",
            "[18,    20] loss: 0.333\n",
            "[18,    30] loss: 0.360\n",
            "[18,    40] loss: 0.388\n",
            "[19,    10] loss: 0.370\n",
            "[19,    20] loss: 0.353\n",
            "[19,    30] loss: 0.366\n",
            "[19,    40] loss: 0.346\n",
            "[20,    10] loss: 0.313\n",
            "[20,    20] loss: 0.295\n",
            "[20,    30] loss: 0.300\n",
            "[20,    40] loss: 0.341\n",
            "[21,    10] loss: 0.436\n",
            "[21,    20] loss: 0.344\n",
            "[21,    30] loss: 0.334\n",
            "[21,    40] loss: 0.320\n",
            "[22,    10] loss: 0.285\n",
            "[22,    20] loss: 0.266\n",
            "[22,    30] loss: 0.280\n",
            "[22,    40] loss: 0.256\n",
            "[23,    10] loss: 0.247\n",
            "[23,    20] loss: 0.253\n",
            "[23,    30] loss: 0.237\n",
            "[23,    40] loss: 0.235\n",
            "[24,    10] loss: 0.260\n",
            "[24,    20] loss: 0.248\n",
            "[24,    30] loss: 0.248\n",
            "[24,    40] loss: 0.242\n",
            "[25,    10] loss: 0.262\n",
            "[25,    20] loss: 0.237\n",
            "[25,    30] loss: 0.245\n",
            "[25,    40] loss: 0.217\n",
            "[26,    10] loss: 0.195\n",
            "[26,    20] loss: 0.208\n",
            "[26,    30] loss: 0.196\n",
            "[26,    40] loss: 0.185\n",
            "[27,    10] loss: 0.184\n",
            "[27,    20] loss: 0.195\n",
            "[27,    30] loss: 0.175\n",
            "[27,    40] loss: 0.179\n",
            "[28,    10] loss: 0.229\n",
            "[28,    20] loss: 0.197\n",
            "[28,    30] loss: 0.189\n",
            "[28,    40] loss: 0.195\n",
            "[29,    10] loss: 0.231\n",
            "[29,    20] loss: 0.225\n",
            "[29,    30] loss: 0.196\n",
            "[29,    40] loss: 0.166\n",
            "[30,    10] loss: 0.147\n",
            "[30,    20] loss: 0.148\n",
            "[30,    30] loss: 0.153\n",
            "[30,    40] loss: 0.157\n",
            "[31,    10] loss: 0.213\n",
            "[31,    20] loss: 0.166\n",
            "[31,    30] loss: 0.144\n",
            "[31,    40] loss: 0.143\n",
            "[32,    10] loss: 0.142\n",
            "[32,    20] loss: 0.120\n",
            "[32,    30] loss: 0.120\n",
            "[32,    40] loss: 0.109\n",
            "[33,    10] loss: 0.143\n",
            "[33,    20] loss: 0.136\n",
            "[33,    30] loss: 0.115\n",
            "[33,    40] loss: 0.100\n",
            "[34,    10] loss: 0.093\n",
            "[34,    20] loss: 0.082\n",
            "[34,    30] loss: 0.073\n",
            "[34,    40] loss: 0.071\n",
            "[35,    10] loss: 0.086\n",
            "[35,    20] loss: 0.076\n",
            "[35,    30] loss: 0.070\n",
            "[35,    40] loss: 0.073\n",
            "[36,    10] loss: 0.098\n",
            "[36,    20] loss: 0.096\n",
            "[36,    30] loss: 0.085\n",
            "[36,    40] loss: 0.079\n",
            "[37,    10] loss: 0.095\n",
            "[37,    20] loss: 0.087\n",
            "[37,    30] loss: 0.077\n",
            "[37,    40] loss: 0.074\n",
            "[38,    10] loss: 0.051\n",
            "[38,    20] loss: 0.052\n",
            "[38,    30] loss: 0.042\n",
            "[38,    40] loss: 0.042\n",
            "[39,    10] loss: 0.036\n",
            "[39,    20] loss: 0.034\n",
            "[39,    30] loss: 0.030\n",
            "[39,    40] loss: 0.029\n",
            "[40,    10] loss: 0.028\n",
            "[40,    20] loss: 0.024\n",
            "[40,    30] loss: 0.023\n",
            "[40,    40] loss: 0.021\n",
            "[41,    10] loss: 0.019\n",
            "[41,    20] loss: 0.018\n",
            "[41,    30] loss: 0.016\n",
            "[41,    40] loss: 0.018\n",
            "[42,    10] loss: 0.015\n",
            "[42,    20] loss: 0.015\n",
            "[42,    30] loss: 0.015\n",
            "[42,    40] loss: 0.021\n",
            "[43,    10] loss: 0.049\n",
            "[43,    20] loss: 0.046\n",
            "[43,    30] loss: 0.033\n",
            "[43,    40] loss: 0.034\n",
            "[44,    10] loss: 0.033\n",
            "[44,    20] loss: 0.030\n",
            "[44,    30] loss: 0.025\n",
            "[44,    40] loss: 0.021\n",
            "[45,    10] loss: 0.017\n",
            "[45,    20] loss: 0.015\n",
            "[45,    30] loss: 0.013\n",
            "[45,    40] loss: 0.016\n",
            "[46,    10] loss: 0.026\n",
            "[46,    20] loss: 0.026\n",
            "[46,    30] loss: 0.022\n",
            "[46,    40] loss: 0.017\n",
            "[47,    10] loss: 0.013\n",
            "[47,    20] loss: 0.011\n",
            "[47,    30] loss: 0.011\n",
            "[47,    40] loss: 0.014\n",
            "[48,    10] loss: 0.022\n",
            "[48,    20] loss: 0.021\n",
            "[48,    30] loss: 0.018\n",
            "[48,    40] loss: 0.040\n",
            "[49,    10] loss: 0.276\n",
            "[49,    20] loss: 0.268\n",
            "[49,    30] loss: 0.219\n",
            "[49,    40] loss: 0.180\n",
            "[50,    10] loss: 0.214\n",
            "[50,    20] loss: 0.190\n",
            "[50,    30] loss: 0.159\n",
            "[50,    40] loss: 0.154\n",
            "[51,    10] loss: 0.217\n",
            "[51,    20] loss: 0.193\n",
            "[51,    30] loss: 0.125\n",
            "[51,    40] loss: 0.135\n",
            "[52,    10] loss: 0.266\n",
            "[52,    20] loss: 0.236\n",
            "[52,    30] loss: 0.175\n",
            "[52,    40] loss: 0.147\n",
            "[53,    10] loss: 0.119\n",
            "[53,    20] loss: 0.100\n",
            "[53,    30] loss: 0.072\n",
            "[53,    40] loss: 0.052\n",
            "[54,    10] loss: 0.035\n",
            "[54,    20] loss: 0.032\n",
            "[54,    30] loss: 0.025\n",
            "[54,    40] loss: 0.023\n",
            "[55,    10] loss: 0.016\n",
            "[55,    20] loss: 0.016\n",
            "[55,    30] loss: 0.015\n",
            "[55,    40] loss: 0.017\n",
            "[56,    10] loss: 0.015\n",
            "[56,    20] loss: 0.013\n",
            "[56,    30] loss: 0.014\n",
            "[56,    40] loss: 0.012\n",
            "[57,    10] loss: 0.010\n",
            "[57,    20] loss: 0.011\n",
            "[57,    30] loss: 0.009\n",
            "[57,    40] loss: 0.010\n",
            "[58,    10] loss: 0.008\n",
            "[58,    20] loss: 0.009\n",
            "[58,    30] loss: 0.008\n",
            "[58,    40] loss: 0.009\n",
            "[59,    10] loss: 0.008\n",
            "[59,    20] loss: 0.008\n",
            "[59,    30] loss: 0.008\n",
            "[59,    40] loss: 0.008\n",
            "[60,    10] loss: 0.007\n",
            "[60,    20] loss: 0.007\n",
            "[60,    30] loss: 0.007\n",
            "[60,    40] loss: 0.008\n",
            "[61,    10] loss: 0.007\n",
            "[61,    20] loss: 0.006\n",
            "[61,    30] loss: 0.006\n",
            "[61,    40] loss: 0.007\n",
            "[62,    10] loss: 0.006\n",
            "[62,    20] loss: 0.006\n",
            "[62,    30] loss: 0.006\n",
            "[62,    40] loss: 0.008\n",
            "[63,    10] loss: 0.006\n",
            "[63,    20] loss: 0.007\n",
            "[63,    30] loss: 0.006\n",
            "[63,    40] loss: 0.006\n",
            "[64,    10] loss: 0.005\n",
            "[64,    20] loss: 0.005\n",
            "[64,    30] loss: 0.005\n",
            "[64,    40] loss: 0.006\n",
            "[65,    10] loss: 0.005\n",
            "[65,    20] loss: 0.005\n",
            "[65,    30] loss: 0.005\n",
            "[65,    40] loss: 0.005\n",
            "[66,    10] loss: 0.004\n",
            "[66,    20] loss: 0.004\n",
            "[66,    30] loss: 0.004\n",
            "[66,    40] loss: 0.006\n",
            "[67,    10] loss: 0.008\n",
            "[67,    20] loss: 0.007\n",
            "[67,    30] loss: 0.005\n",
            "[67,    40] loss: 0.004\n",
            "[68,    10] loss: 0.004\n",
            "[68,    20] loss: 0.004\n",
            "[68,    30] loss: 0.004\n",
            "[68,    40] loss: 0.004\n",
            "[69,    10] loss: 0.003\n",
            "[69,    20] loss: 0.004\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.004\n",
            "[70,    10] loss: 0.003\n",
            "[70,    20] loss: 0.003\n",
            "[70,    30] loss: 0.004\n",
            "[70,    40] loss: 0.003\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 92 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 88 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   3\n",
            "[1,    10] loss: 0.927\n",
            "[1,    20] loss: 0.708\n",
            "[1,    30] loss: 0.623\n",
            "[1,    40] loss: 0.611\n",
            "[2,    10] loss: 0.579\n",
            "[2,    20] loss: 0.554\n",
            "[2,    30] loss: 0.534\n",
            "[2,    40] loss: 0.491\n",
            "[3,    10] loss: 0.531\n",
            "[3,    20] loss: 0.524\n",
            "[3,    30] loss: 0.485\n",
            "[3,    40] loss: 0.465\n",
            "[4,    10] loss: 0.465\n",
            "[4,    20] loss: 0.443\n",
            "[4,    30] loss: 0.432\n",
            "[4,    40] loss: 0.436\n",
            "[5,    10] loss: 0.429\n",
            "[5,    20] loss: 0.419\n",
            "[5,    30] loss: 0.418\n",
            "[5,    40] loss: 0.432\n",
            "[6,    10] loss: 0.411\n",
            "[6,    20] loss: 0.428\n",
            "[6,    30] loss: 0.379\n",
            "[6,    40] loss: 0.363\n",
            "[7,    10] loss: 0.373\n",
            "[7,    20] loss: 0.378\n",
            "[7,    30] loss: 0.364\n",
            "[7,    40] loss: 0.359\n",
            "[8,    10] loss: 0.361\n",
            "[8,    20] loss: 0.343\n",
            "[8,    30] loss: 0.337\n",
            "[8,    40] loss: 0.327\n",
            "[9,    10] loss: 0.363\n",
            "[9,    20] loss: 0.350\n",
            "[9,    30] loss: 0.314\n",
            "[9,    40] loss: 0.304\n",
            "[10,    10] loss: 0.298\n",
            "[10,    20] loss: 0.285\n",
            "[10,    30] loss: 0.289\n",
            "[10,    40] loss: 0.301\n",
            "[11,    10] loss: 0.271\n",
            "[11,    20] loss: 0.298\n",
            "[11,    30] loss: 0.276\n",
            "[11,    40] loss: 0.281\n",
            "[12,    10] loss: 0.267\n",
            "[12,    20] loss: 0.280\n",
            "[12,    30] loss: 0.267\n",
            "[12,    40] loss: 0.275\n",
            "[13,    10] loss: 0.241\n",
            "[13,    20] loss: 0.246\n",
            "[13,    30] loss: 0.240\n",
            "[13,    40] loss: 0.230\n",
            "[14,    10] loss: 0.232\n",
            "[14,    20] loss: 0.226\n",
            "[14,    30] loss: 0.227\n",
            "[14,    40] loss: 0.274\n",
            "[15,    10] loss: 0.343\n",
            "[15,    20] loss: 0.275\n",
            "[15,    30] loss: 0.268\n",
            "[15,    40] loss: 0.249\n",
            "[16,    10] loss: 0.217\n",
            "[16,    20] loss: 0.210\n",
            "[16,    30] loss: 0.213\n",
            "[16,    40] loss: 0.210\n",
            "[17,    10] loss: 0.210\n",
            "[17,    20] loss: 0.190\n",
            "[17,    30] loss: 0.194\n",
            "[17,    40] loss: 0.181\n",
            "[18,    10] loss: 0.166\n",
            "[18,    20] loss: 0.172\n",
            "[18,    30] loss: 0.161\n",
            "[18,    40] loss: 0.169\n",
            "[19,    10] loss: 0.174\n",
            "[19,    20] loss: 0.180\n",
            "[19,    30] loss: 0.172\n",
            "[19,    40] loss: 0.156\n",
            "[20,    10] loss: 0.151\n",
            "[20,    20] loss: 0.147\n",
            "[20,    30] loss: 0.143\n",
            "[20,    40] loss: 0.135\n",
            "[21,    10] loss: 0.144\n",
            "[21,    20] loss: 0.127\n",
            "[21,    30] loss: 0.136\n",
            "[21,    40] loss: 0.152\n",
            "[22,    10] loss: 0.163\n",
            "[22,    20] loss: 0.148\n",
            "[22,    30] loss: 0.135\n",
            "[22,    40] loss: 0.138\n",
            "[23,    10] loss: 0.112\n",
            "[23,    20] loss: 0.107\n",
            "[23,    30] loss: 0.113\n",
            "[23,    40] loss: 0.120\n",
            "[24,    10] loss: 0.152\n",
            "[24,    20] loss: 0.136\n",
            "[24,    30] loss: 0.132\n",
            "[24,    40] loss: 0.132\n",
            "[25,    10] loss: 0.207\n",
            "[25,    20] loss: 0.170\n",
            "[25,    30] loss: 0.148\n",
            "[25,    40] loss: 0.130\n",
            "[26,    10] loss: 0.108\n",
            "[26,    20] loss: 0.095\n",
            "[26,    30] loss: 0.097\n",
            "[26,    40] loss: 0.106\n",
            "[27,    10] loss: 0.110\n",
            "[27,    20] loss: 0.096\n",
            "[27,    30] loss: 0.092\n",
            "[27,    40] loss: 0.090\n",
            "[28,    10] loss: 0.119\n",
            "[28,    20] loss: 0.094\n",
            "[28,    30] loss: 0.088\n",
            "[28,    40] loss: 0.080\n",
            "[29,    10] loss: 0.068\n",
            "[29,    20] loss: 0.061\n",
            "[29,    30] loss: 0.058\n",
            "[29,    40] loss: 0.060\n",
            "[30,    10] loss: 0.050\n",
            "[30,    20] loss: 0.057\n",
            "[30,    30] loss: 0.050\n",
            "[30,    40] loss: 0.048\n",
            "[31,    10] loss: 0.053\n",
            "[31,    20] loss: 0.045\n",
            "[31,    30] loss: 0.040\n",
            "[31,    40] loss: 0.041\n",
            "[32,    10] loss: 0.051\n",
            "[32,    20] loss: 0.039\n",
            "[32,    30] loss: 0.041\n",
            "[32,    40] loss: 0.040\n",
            "[33,    10] loss: 0.042\n",
            "[33,    20] loss: 0.035\n",
            "[33,    30] loss: 0.037\n",
            "[33,    40] loss: 0.032\n",
            "[34,    10] loss: 0.028\n",
            "[34,    20] loss: 0.030\n",
            "[34,    30] loss: 0.026\n",
            "[34,    40] loss: 0.025\n",
            "[35,    10] loss: 0.022\n",
            "[35,    20] loss: 0.023\n",
            "[35,    30] loss: 0.021\n",
            "[35,    40] loss: 0.020\n",
            "[36,    10] loss: 0.020\n",
            "[36,    20] loss: 0.019\n",
            "[36,    30] loss: 0.020\n",
            "[36,    40] loss: 0.018\n",
            "[37,    10] loss: 0.014\n",
            "[37,    20] loss: 0.014\n",
            "[37,    30] loss: 0.013\n",
            "[37,    40] loss: 0.014\n",
            "[38,    10] loss: 0.012\n",
            "[38,    20] loss: 0.012\n",
            "[38,    30] loss: 0.012\n",
            "[38,    40] loss: 0.017\n",
            "[39,    10] loss: 0.052\n",
            "[39,    20] loss: 0.043\n",
            "[39,    30] loss: 0.042\n",
            "[39,    40] loss: 0.031\n",
            "[40,    10] loss: 0.029\n",
            "[40,    20] loss: 0.020\n",
            "[40,    30] loss: 0.018\n",
            "[40,    40] loss: 0.020\n",
            "[41,    10] loss: 0.024\n",
            "[41,    20] loss: 0.019\n",
            "[41,    30] loss: 0.021\n",
            "[41,    40] loss: 0.029\n",
            "[42,    10] loss: 0.163\n",
            "[42,    20] loss: 0.155\n",
            "[42,    30] loss: 0.135\n",
            "[42,    40] loss: 0.140\n",
            "[43,    10] loss: 0.164\n",
            "[43,    20] loss: 0.128\n",
            "[43,    30] loss: 0.094\n",
            "[43,    40] loss: 0.077\n",
            "[44,    10] loss: 0.052\n",
            "[44,    20] loss: 0.043\n",
            "[44,    30] loss: 0.036\n",
            "[44,    40] loss: 0.031\n",
            "[45,    10] loss: 0.028\n",
            "[45,    20] loss: 0.023\n",
            "[45,    30] loss: 0.026\n",
            "[45,    40] loss: 0.020\n",
            "[46,    10] loss: 0.018\n",
            "[46,    20] loss: 0.018\n",
            "[46,    30] loss: 0.017\n",
            "[46,    40] loss: 0.014\n",
            "[47,    10] loss: 0.011\n",
            "[47,    20] loss: 0.010\n",
            "[47,    30] loss: 0.009\n",
            "[47,    40] loss: 0.010\n",
            "[48,    10] loss: 0.009\n",
            "[48,    20] loss: 0.009\n",
            "[48,    30] loss: 0.008\n",
            "[48,    40] loss: 0.009\n",
            "[49,    10] loss: 0.007\n",
            "[49,    20] loss: 0.007\n",
            "[49,    30] loss: 0.006\n",
            "[49,    40] loss: 0.011\n",
            "[50,    10] loss: 0.026\n",
            "[50,    20] loss: 0.022\n",
            "[50,    30] loss: 0.018\n",
            "[50,    40] loss: 0.037\n",
            "[51,    10] loss: 0.144\n",
            "[51,    20] loss: 0.105\n",
            "[51,    30] loss: 0.083\n",
            "[51,    40] loss: 0.061\n",
            "[52,    10] loss: 0.046\n",
            "[52,    20] loss: 0.032\n",
            "[52,    30] loss: 0.028\n",
            "[52,    40] loss: 0.025\n",
            "[53,    10] loss: 0.033\n",
            "[53,    20] loss: 0.021\n",
            "[53,    30] loss: 0.016\n",
            "[53,    40] loss: 0.012\n",
            "[54,    10] loss: 0.010\n",
            "[54,    20] loss: 0.008\n",
            "[54,    30] loss: 0.009\n",
            "[54,    40] loss: 0.008\n",
            "[55,    10] loss: 0.007\n",
            "[55,    20] loss: 0.007\n",
            "[55,    30] loss: 0.006\n",
            "[55,    40] loss: 0.006\n",
            "[56,    10] loss: 0.005\n",
            "[56,    20] loss: 0.005\n",
            "[56,    30] loss: 0.006\n",
            "[56,    40] loss: 0.005\n",
            "[57,    10] loss: 0.005\n",
            "[57,    20] loss: 0.005\n",
            "[57,    30] loss: 0.005\n",
            "[57,    40] loss: 0.006\n",
            "[58,    10] loss: 0.009\n",
            "[58,    20] loss: 0.008\n",
            "[58,    30] loss: 0.006\n",
            "[58,    40] loss: 0.006\n",
            "[59,    10] loss: 0.005\n",
            "[59,    20] loss: 0.004\n",
            "[59,    30] loss: 0.005\n",
            "[59,    40] loss: 0.005\n",
            "[60,    10] loss: 0.008\n",
            "[60,    20] loss: 0.006\n",
            "[60,    30] loss: 0.005\n",
            "[60,    40] loss: 0.006\n",
            "[61,    10] loss: 0.013\n",
            "[61,    20] loss: 0.011\n",
            "[61,    30] loss: 0.007\n",
            "[61,    40] loss: 0.010\n",
            "[62,    10] loss: 0.027\n",
            "[62,    20] loss: 0.023\n",
            "[62,    30] loss: 0.017\n",
            "[62,    40] loss: 0.053\n",
            "[63,    10] loss: 0.236\n",
            "[63,    20] loss: 0.164\n",
            "[63,    30] loss: 0.151\n",
            "[63,    40] loss: 0.107\n",
            "[64,    10] loss: 0.078\n",
            "[64,    20] loss: 0.056\n",
            "[64,    30] loss: 0.042\n",
            "[64,    40] loss: 0.034\n",
            "[65,    10] loss: 0.021\n",
            "[65,    20] loss: 0.018\n",
            "[65,    30] loss: 0.015\n",
            "[65,    40] loss: 0.012\n",
            "[66,    10] loss: 0.010\n",
            "[66,    20] loss: 0.008\n",
            "[66,    30] loss: 0.008\n",
            "[66,    40] loss: 0.009\n",
            "[67,    10] loss: 0.010\n",
            "[67,    20] loss: 0.011\n",
            "[67,    30] loss: 0.011\n",
            "[67,    40] loss: 0.009\n",
            "[68,    10] loss: 0.008\n",
            "[68,    20] loss: 0.007\n",
            "[68,    30] loss: 0.006\n",
            "[68,    40] loss: 0.006\n",
            "[69,    10] loss: 0.004\n",
            "[69,    20] loss: 0.005\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.004\n",
            "[70,    10] loss: 0.004\n",
            "[70,    20] loss: 0.004\n",
            "[70,    30] loss: 0.004\n",
            "[70,    40] loss: 0.004\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 52 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 91 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 93 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   4\n",
            "[1,    10] loss: 0.901\n",
            "[1,    20] loss: 0.647\n",
            "[1,    30] loss: 0.542\n",
            "[1,    40] loss: 0.566\n",
            "[2,    10] loss: 0.512\n",
            "[2,    20] loss: 0.472\n",
            "[2,    30] loss: 0.467\n",
            "[2,    40] loss: 0.463\n",
            "[3,    10] loss: 0.450\n",
            "[3,    20] loss: 0.395\n",
            "[3,    30] loss: 0.371\n",
            "[3,    40] loss: 0.361\n",
            "[4,    10] loss: 0.350\n",
            "[4,    20] loss: 0.341\n",
            "[4,    30] loss: 0.340\n",
            "[4,    40] loss: 0.335\n",
            "[5,    10] loss: 0.338\n",
            "[5,    20] loss: 0.293\n",
            "[5,    30] loss: 0.350\n",
            "[5,    40] loss: 0.286\n",
            "[6,    10] loss: 0.291\n",
            "[6,    20] loss: 0.279\n",
            "[6,    30] loss: 0.268\n",
            "[6,    40] loss: 0.285\n",
            "[7,    10] loss: 0.275\n",
            "[7,    20] loss: 0.283\n",
            "[7,    30] loss: 0.258\n",
            "[7,    40] loss: 0.245\n",
            "[8,    10] loss: 0.245\n",
            "[8,    20] loss: 0.251\n",
            "[8,    30] loss: 0.234\n",
            "[8,    40] loss: 0.236\n",
            "[9,    10] loss: 0.233\n",
            "[9,    20] loss: 0.214\n",
            "[9,    30] loss: 0.201\n",
            "[9,    40] loss: 0.208\n",
            "[10,    10] loss: 0.208\n",
            "[10,    20] loss: 0.206\n",
            "[10,    30] loss: 0.182\n",
            "[10,    40] loss: 0.182\n",
            "[11,    10] loss: 0.175\n",
            "[11,    20] loss: 0.167\n",
            "[11,    30] loss: 0.186\n",
            "[11,    40] loss: 0.168\n",
            "[12,    10] loss: 0.166\n",
            "[12,    20] loss: 0.159\n",
            "[12,    30] loss: 0.165\n",
            "[12,    40] loss: 0.170\n",
            "[13,    10] loss: 0.210\n",
            "[13,    20] loss: 0.183\n",
            "[13,    30] loss: 0.170\n",
            "[13,    40] loss: 0.166\n",
            "[14,    10] loss: 0.175\n",
            "[14,    20] loss: 0.173\n",
            "[14,    30] loss: 0.162\n",
            "[14,    40] loss: 0.149\n",
            "[15,    10] loss: 0.135\n",
            "[15,    20] loss: 0.127\n",
            "[15,    30] loss: 0.117\n",
            "[15,    40] loss: 0.128\n",
            "[16,    10] loss: 0.126\n",
            "[16,    20] loss: 0.125\n",
            "[16,    30] loss: 0.126\n",
            "[16,    40] loss: 0.173\n",
            "[17,    10] loss: 0.251\n",
            "[17,    20] loss: 0.190\n",
            "[17,    30] loss: 0.182\n",
            "[17,    40] loss: 0.179\n",
            "[18,    10] loss: 0.209\n",
            "[18,    20] loss: 0.172\n",
            "[18,    30] loss: 0.143\n",
            "[18,    40] loss: 0.133\n",
            "[19,    10] loss: 0.109\n",
            "[19,    20] loss: 0.112\n",
            "[19,    30] loss: 0.106\n",
            "[19,    40] loss: 0.090\n",
            "[20,    10] loss: 0.084\n",
            "[20,    20] loss: 0.092\n",
            "[20,    30] loss: 0.089\n",
            "[20,    40] loss: 0.102\n",
            "[21,    10] loss: 0.124\n",
            "[21,    20] loss: 0.105\n",
            "[21,    30] loss: 0.106\n",
            "[21,    40] loss: 0.112\n",
            "[22,    10] loss: 0.149\n",
            "[22,    20] loss: 0.106\n",
            "[22,    30] loss: 0.091\n",
            "[22,    40] loss: 0.078\n",
            "[23,    10] loss: 0.074\n",
            "[23,    20] loss: 0.068\n",
            "[23,    30] loss: 0.065\n",
            "[23,    40] loss: 0.076\n",
            "[24,    10] loss: 0.089\n",
            "[24,    20] loss: 0.064\n",
            "[24,    30] loss: 0.072\n",
            "[24,    40] loss: 0.066\n",
            "[25,    10] loss: 0.051\n",
            "[25,    20] loss: 0.044\n",
            "[25,    30] loss: 0.048\n",
            "[25,    40] loss: 0.074\n",
            "[26,    10] loss: 0.083\n",
            "[26,    20] loss: 0.081\n",
            "[26,    30] loss: 0.060\n",
            "[26,    40] loss: 0.055\n",
            "[27,    10] loss: 0.059\n",
            "[27,    20] loss: 0.046\n",
            "[27,    30] loss: 0.043\n",
            "[27,    40] loss: 0.050\n",
            "[28,    10] loss: 0.116\n",
            "[28,    20] loss: 0.075\n",
            "[28,    30] loss: 0.067\n",
            "[28,    40] loss: 0.051\n",
            "[29,    10] loss: 0.060\n",
            "[29,    20] loss: 0.052\n",
            "[29,    30] loss: 0.045\n",
            "[29,    40] loss: 0.046\n",
            "[30,    10] loss: 0.033\n",
            "[30,    20] loss: 0.033\n",
            "[30,    30] loss: 0.031\n",
            "[30,    40] loss: 0.034\n",
            "[31,    10] loss: 0.048\n",
            "[31,    20] loss: 0.042\n",
            "[31,    30] loss: 0.041\n",
            "[31,    40] loss: 0.038\n",
            "[32,    10] loss: 0.026\n",
            "[32,    20] loss: 0.024\n",
            "[32,    30] loss: 0.023\n",
            "[32,    40] loss: 0.026\n",
            "[33,    10] loss: 0.025\n",
            "[33,    20] loss: 0.020\n",
            "[33,    30] loss: 0.023\n",
            "[33,    40] loss: 0.019\n",
            "[34,    10] loss: 0.016\n",
            "[34,    20] loss: 0.013\n",
            "[34,    30] loss: 0.014\n",
            "[34,    40] loss: 0.020\n",
            "[35,    10] loss: 0.030\n",
            "[35,    20] loss: 0.025\n",
            "[35,    30] loss: 0.019\n",
            "[35,    40] loss: 0.019\n",
            "[36,    10] loss: 0.036\n",
            "[36,    20] loss: 0.028\n",
            "[36,    30] loss: 0.021\n",
            "[36,    40] loss: 0.018\n",
            "[37,    10] loss: 0.017\n",
            "[37,    20] loss: 0.014\n",
            "[37,    30] loss: 0.014\n",
            "[37,    40] loss: 0.012\n",
            "[38,    10] loss: 0.010\n",
            "[38,    20] loss: 0.010\n",
            "[38,    30] loss: 0.009\n",
            "[38,    40] loss: 0.010\n",
            "[39,    10] loss: 0.009\n",
            "[39,    20] loss: 0.008\n",
            "[39,    30] loss: 0.007\n",
            "[39,    40] loss: 0.010\n",
            "[40,    10] loss: 0.023\n",
            "[40,    20] loss: 0.021\n",
            "[40,    30] loss: 0.015\n",
            "[40,    40] loss: 0.022\n",
            "[41,    10] loss: 0.072\n",
            "[41,    20] loss: 0.054\n",
            "[41,    30] loss: 0.046\n",
            "[41,    40] loss: 0.033\n",
            "[42,    10] loss: 0.025\n",
            "[42,    20] loss: 0.019\n",
            "[42,    30] loss: 0.014\n",
            "[42,    40] loss: 0.011\n",
            "[43,    10] loss: 0.012\n",
            "[43,    20] loss: 0.011\n",
            "[43,    30] loss: 0.008\n",
            "[43,    40] loss: 0.009\n",
            "[44,    10] loss: 0.022\n",
            "[44,    20] loss: 0.018\n",
            "[44,    30] loss: 0.016\n",
            "[44,    40] loss: 0.016\n",
            "[45,    10] loss: 0.026\n",
            "[45,    20] loss: 0.021\n",
            "[45,    30] loss: 0.021\n",
            "[45,    40] loss: 0.017\n",
            "[46,    10] loss: 0.009\n",
            "[46,    20] loss: 0.009\n",
            "[46,    30] loss: 0.008\n",
            "[46,    40] loss: 0.009\n",
            "[47,    10] loss: 0.007\n",
            "[47,    20] loss: 0.007\n",
            "[47,    30] loss: 0.006\n",
            "[47,    40] loss: 0.006\n",
            "[48,    10] loss: 0.004\n",
            "[48,    20] loss: 0.004\n",
            "[48,    30] loss: 0.004\n",
            "[48,    40] loss: 0.004\n",
            "[49,    10] loss: 0.004\n",
            "[49,    20] loss: 0.004\n",
            "[49,    30] loss: 0.003\n",
            "[49,    40] loss: 0.003\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.003\n",
            "[50,    30] loss: 0.003\n",
            "[50,    40] loss: 0.005\n",
            "[51,    10] loss: 0.005\n",
            "[51,    20] loss: 0.005\n",
            "[51,    30] loss: 0.004\n",
            "[51,    40] loss: 0.004\n",
            "[52,    10] loss: 0.003\n",
            "[52,    20] loss: 0.003\n",
            "[52,    30] loss: 0.003\n",
            "[52,    40] loss: 0.011\n",
            "[53,    10] loss: 0.061\n",
            "[53,    20] loss: 0.050\n",
            "[53,    30] loss: 0.035\n",
            "[53,    40] loss: 0.027\n",
            "[54,    10] loss: 0.019\n",
            "[54,    20] loss: 0.014\n",
            "[54,    30] loss: 0.010\n",
            "[54,    40] loss: 0.009\n",
            "[55,    10] loss: 0.005\n",
            "[55,    20] loss: 0.005\n",
            "[55,    30] loss: 0.004\n",
            "[55,    40] loss: 0.005\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.008\n",
            "[57,    10] loss: 0.016\n",
            "[57,    20] loss: 0.012\n",
            "[57,    30] loss: 0.010\n",
            "[57,    40] loss: 0.008\n",
            "[58,    10] loss: 0.006\n",
            "[58,    20] loss: 0.005\n",
            "[58,    30] loss: 0.004\n",
            "[58,    40] loss: 0.010\n",
            "[59,    10] loss: 0.017\n",
            "[59,    20] loss: 0.010\n",
            "[59,    30] loss: 0.010\n",
            "[59,    40] loss: 0.010\n",
            "[60,    10] loss: 0.009\n",
            "[60,    20] loss: 0.007\n",
            "[60,    30] loss: 0.005\n",
            "[60,    40] loss: 0.004\n",
            "[61,    10] loss: 0.003\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.003\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.002\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.003\n",
            "[64,    10] loss: 0.003\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.002\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.005\n",
            "[66,    10] loss: 0.016\n",
            "[66,    20] loss: 0.010\n",
            "[66,    30] loss: 0.008\n",
            "[66,    40] loss: 0.006\n",
            "[67,    10] loss: 0.003\n",
            "[67,    20] loss: 0.003\n",
            "[67,    30] loss: 0.002\n",
            "[67,    40] loss: 0.004\n",
            "[68,    10] loss: 0.007\n",
            "[68,    20] loss: 0.004\n",
            "[68,    30] loss: 0.003\n",
            "[68,    40] loss: 0.003\n",
            "[69,    10] loss: 0.002\n",
            "[69,    20] loss: 0.002\n",
            "[69,    30] loss: 0.002\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 49 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 79 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   5\n",
            "[1,    10] loss: 0.936\n",
            "[1,    20] loss: 0.656\n",
            "[1,    30] loss: 0.558\n",
            "[1,    40] loss: 0.476\n",
            "[2,    10] loss: 0.513\n",
            "[2,    20] loss: 0.444\n",
            "[2,    30] loss: 0.407\n",
            "[2,    40] loss: 0.443\n",
            "[3,    10] loss: 0.392\n",
            "[3,    20] loss: 0.387\n",
            "[3,    30] loss: 0.351\n",
            "[3,    40] loss: 0.365\n",
            "[4,    10] loss: 0.334\n",
            "[4,    20] loss: 0.327\n",
            "[4,    30] loss: 0.301\n",
            "[4,    40] loss: 0.279\n",
            "[5,    10] loss: 0.284\n",
            "[5,    20] loss: 0.259\n",
            "[5,    30] loss: 0.267\n",
            "[5,    40] loss: 0.264\n",
            "[6,    10] loss: 0.251\n",
            "[6,    20] loss: 0.240\n",
            "[6,    30] loss: 0.243\n",
            "[6,    40] loss: 0.234\n",
            "[7,    10] loss: 0.279\n",
            "[7,    20] loss: 0.242\n",
            "[7,    30] loss: 0.248\n",
            "[7,    40] loss: 0.227\n",
            "[8,    10] loss: 0.234\n",
            "[8,    20] loss: 0.219\n",
            "[8,    30] loss: 0.222\n",
            "[8,    40] loss: 0.209\n",
            "[9,    10] loss: 0.186\n",
            "[9,    20] loss: 0.205\n",
            "[9,    30] loss: 0.185\n",
            "[9,    40] loss: 0.187\n",
            "[10,    10] loss: 0.159\n",
            "[10,    20] loss: 0.166\n",
            "[10,    30] loss: 0.178\n",
            "[10,    40] loss: 0.180\n",
            "[11,    10] loss: 0.159\n",
            "[11,    20] loss: 0.146\n",
            "[11,    30] loss: 0.153\n",
            "[11,    40] loss: 0.147\n",
            "[12,    10] loss: 0.162\n",
            "[12,    20] loss: 0.151\n",
            "[12,    30] loss: 0.155\n",
            "[12,    40] loss: 0.140\n",
            "[13,    10] loss: 0.119\n",
            "[13,    20] loss: 0.120\n",
            "[13,    30] loss: 0.118\n",
            "[13,    40] loss: 0.140\n",
            "[14,    10] loss: 0.213\n",
            "[14,    20] loss: 0.164\n",
            "[14,    30] loss: 0.137\n",
            "[14,    40] loss: 0.154\n",
            "[15,    10] loss: 0.164\n",
            "[15,    20] loss: 0.123\n",
            "[15,    30] loss: 0.120\n",
            "[15,    40] loss: 0.106\n",
            "[16,    10] loss: 0.129\n",
            "[16,    20] loss: 0.113\n",
            "[16,    30] loss: 0.108\n",
            "[16,    40] loss: 0.093\n",
            "[17,    10] loss: 0.085\n",
            "[17,    20] loss: 0.079\n",
            "[17,    30] loss: 0.078\n",
            "[17,    40] loss: 0.099\n",
            "[18,    10] loss: 0.128\n",
            "[18,    20] loss: 0.139\n",
            "[18,    30] loss: 0.120\n",
            "[18,    40] loss: 0.116\n",
            "[19,    10] loss: 0.106\n",
            "[19,    20] loss: 0.100\n",
            "[19,    30] loss: 0.082\n",
            "[19,    40] loss: 0.080\n",
            "[20,    10] loss: 0.068\n",
            "[20,    20] loss: 0.063\n",
            "[20,    30] loss: 0.060\n",
            "[20,    40] loss: 0.063\n",
            "[21,    10] loss: 0.056\n",
            "[21,    20] loss: 0.055\n",
            "[21,    30] loss: 0.053\n",
            "[21,    40] loss: 0.049\n",
            "[22,    10] loss: 0.052\n",
            "[22,    20] loss: 0.044\n",
            "[22,    30] loss: 0.044\n",
            "[22,    40] loss: 0.042\n",
            "[23,    10] loss: 0.043\n",
            "[23,    20] loss: 0.042\n",
            "[23,    30] loss: 0.037\n",
            "[23,    40] loss: 0.044\n",
            "[24,    10] loss: 0.049\n",
            "[24,    20] loss: 0.048\n",
            "[24,    30] loss: 0.043\n",
            "[24,    40] loss: 0.033\n",
            "[25,    10] loss: 0.031\n",
            "[25,    20] loss: 0.030\n",
            "[25,    30] loss: 0.029\n",
            "[25,    40] loss: 0.028\n",
            "[26,    10] loss: 0.030\n",
            "[26,    20] loss: 0.028\n",
            "[26,    30] loss: 0.034\n",
            "[26,    40] loss: 0.037\n",
            "[27,    10] loss: 0.042\n",
            "[27,    20] loss: 0.039\n",
            "[27,    30] loss: 0.032\n",
            "[27,    40] loss: 0.032\n",
            "[28,    10] loss: 0.022\n",
            "[28,    20] loss: 0.022\n",
            "[28,    30] loss: 0.019\n",
            "[28,    40] loss: 0.019\n",
            "[29,    10] loss: 0.016\n",
            "[29,    20] loss: 0.015\n",
            "[29,    30] loss: 0.015\n",
            "[29,    40] loss: 0.023\n",
            "[30,    10] loss: 0.102\n",
            "[30,    20] loss: 0.082\n",
            "[30,    30] loss: 0.067\n",
            "[30,    40] loss: 0.055\n",
            "[31,    10] loss: 0.071\n",
            "[31,    20] loss: 0.056\n",
            "[31,    30] loss: 0.049\n",
            "[31,    40] loss: 0.056\n",
            "[32,    10] loss: 0.122\n",
            "[32,    20] loss: 0.096\n",
            "[32,    30] loss: 0.074\n",
            "[32,    40] loss: 0.058\n",
            "[33,    10] loss: 0.056\n",
            "[33,    20] loss: 0.056\n",
            "[33,    30] loss: 0.042\n",
            "[33,    40] loss: 0.045\n",
            "[34,    10] loss: 0.089\n",
            "[34,    20] loss: 0.061\n",
            "[34,    30] loss: 0.051\n",
            "[34,    40] loss: 0.058\n",
            "[35,    10] loss: 0.071\n",
            "[35,    20] loss: 0.057\n",
            "[35,    30] loss: 0.043\n",
            "[35,    40] loss: 0.041\n",
            "[36,    10] loss: 0.029\n",
            "[36,    20] loss: 0.025\n",
            "[36,    30] loss: 0.022\n",
            "[36,    40] loss: 0.020\n",
            "[37,    10] loss: 0.014\n",
            "[37,    20] loss: 0.012\n",
            "[37,    30] loss: 0.013\n",
            "[37,    40] loss: 0.015\n",
            "[38,    10] loss: 0.016\n",
            "[38,    20] loss: 0.016\n",
            "[38,    30] loss: 0.017\n",
            "[38,    40] loss: 0.014\n",
            "[39,    10] loss: 0.021\n",
            "[39,    20] loss: 0.013\n",
            "[39,    30] loss: 0.012\n",
            "[39,    40] loss: 0.015\n",
            "[40,    10] loss: 0.018\n",
            "[40,    20] loss: 0.013\n",
            "[40,    30] loss: 0.011\n",
            "[40,    40] loss: 0.010\n",
            "[41,    10] loss: 0.007\n",
            "[41,    20] loss: 0.008\n",
            "[41,    30] loss: 0.006\n",
            "[41,    40] loss: 0.007\n",
            "[42,    10] loss: 0.009\n",
            "[42,    20] loss: 0.009\n",
            "[42,    30] loss: 0.007\n",
            "[42,    40] loss: 0.007\n",
            "[43,    10] loss: 0.009\n",
            "[43,    20] loss: 0.007\n",
            "[43,    30] loss: 0.006\n",
            "[43,    40] loss: 0.006\n",
            "[44,    10] loss: 0.005\n",
            "[44,    20] loss: 0.005\n",
            "[44,    30] loss: 0.004\n",
            "[44,    40] loss: 0.004\n",
            "[45,    10] loss: 0.004\n",
            "[45,    20] loss: 0.004\n",
            "[45,    30] loss: 0.004\n",
            "[45,    40] loss: 0.007\n",
            "[46,    10] loss: 0.011\n",
            "[46,    20] loss: 0.008\n",
            "[46,    30] loss: 0.006\n",
            "[46,    40] loss: 0.007\n",
            "[47,    10] loss: 0.005\n",
            "[47,    20] loss: 0.004\n",
            "[47,    30] loss: 0.004\n",
            "[47,    40] loss: 0.005\n",
            "[48,    10] loss: 0.005\n",
            "[48,    20] loss: 0.004\n",
            "[48,    30] loss: 0.004\n",
            "[48,    40] loss: 0.004\n",
            "[49,    10] loss: 0.003\n",
            "[49,    20] loss: 0.003\n",
            "[49,    30] loss: 0.003\n",
            "[49,    40] loss: 0.011\n",
            "[50,    10] loss: 0.068\n",
            "[50,    20] loss: 0.075\n",
            "[50,    30] loss: 0.047\n",
            "[50,    40] loss: 0.032\n",
            "[51,    10] loss: 0.021\n",
            "[51,    20] loss: 0.016\n",
            "[51,    30] loss: 0.013\n",
            "[51,    40] loss: 0.010\n",
            "[52,    10] loss: 0.007\n",
            "[52,    20] loss: 0.006\n",
            "[52,    30] loss: 0.005\n",
            "[52,    40] loss: 0.008\n",
            "[53,    10] loss: 0.009\n",
            "[53,    20] loss: 0.007\n",
            "[53,    30] loss: 0.007\n",
            "[53,    40] loss: 0.005\n",
            "[54,    10] loss: 0.004\n",
            "[54,    20] loss: 0.004\n",
            "[54,    30] loss: 0.003\n",
            "[54,    40] loss: 0.004\n",
            "[55,    10] loss: 0.003\n",
            "[55,    20] loss: 0.003\n",
            "[55,    30] loss: 0.003\n",
            "[55,    40] loss: 0.003\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.003\n",
            "[57,    10] loss: 0.002\n",
            "[57,    20] loss: 0.003\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.006\n",
            "[58,    10] loss: 0.025\n",
            "[58,    20] loss: 0.026\n",
            "[58,    30] loss: 0.016\n",
            "[58,    40] loss: 0.012\n",
            "[59,    10] loss: 0.007\n",
            "[59,    20] loss: 0.008\n",
            "[59,    30] loss: 0.004\n",
            "[59,    40] loss: 0.004\n",
            "[60,    10] loss: 0.003\n",
            "[60,    20] loss: 0.003\n",
            "[60,    30] loss: 0.003\n",
            "[60,    40] loss: 0.005\n",
            "[61,    10] loss: 0.007\n",
            "[61,    20] loss: 0.007\n",
            "[61,    30] loss: 0.005\n",
            "[61,    40] loss: 0.020\n",
            "[62,    10] loss: 0.100\n",
            "[62,    20] loss: 0.065\n",
            "[62,    30] loss: 0.053\n",
            "[62,    40] loss: 0.033\n",
            "[63,    10] loss: 0.014\n",
            "[63,    20] loss: 0.016\n",
            "[63,    30] loss: 0.010\n",
            "[63,    40] loss: 0.008\n",
            "[64,    10] loss: 0.006\n",
            "[64,    20] loss: 0.005\n",
            "[64,    30] loss: 0.005\n",
            "[64,    40] loss: 0.004\n",
            "[65,    10] loss: 0.003\n",
            "[65,    20] loss: 0.003\n",
            "[65,    30] loss: 0.003\n",
            "[65,    40] loss: 0.004\n",
            "[66,    10] loss: 0.003\n",
            "[66,    20] loss: 0.003\n",
            "[66,    30] loss: 0.003\n",
            "[66,    40] loss: 0.003\n",
            "[67,    10] loss: 0.002\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.003\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.002\n",
            "[68,    20] loss: 0.002\n",
            "[68,    30] loss: 0.002\n",
            "[68,    40] loss: 0.002\n",
            "[69,    10] loss: 0.002\n",
            "[69,    20] loss: 0.002\n",
            "[69,    30] loss: 0.002\n",
            "[69,    40] loss: 0.002\n",
            "[70,    10] loss: 0.002\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.002\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 74 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 93 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   6\n",
            "[1,    10] loss: 0.879\n",
            "[1,    20] loss: 0.604\n",
            "[1,    30] loss: 0.508\n",
            "[1,    40] loss: 0.470\n",
            "[2,    10] loss: 0.453\n",
            "[2,    20] loss: 0.419\n",
            "[2,    30] loss: 0.417\n",
            "[2,    40] loss: 0.430\n",
            "[3,    10] loss: 0.373\n",
            "[3,    20] loss: 0.375\n",
            "[3,    30] loss: 0.349\n",
            "[3,    40] loss: 0.331\n",
            "[4,    10] loss: 0.329\n",
            "[4,    20] loss: 0.301\n",
            "[4,    30] loss: 0.303\n",
            "[4,    40] loss: 0.300\n",
            "[5,    10] loss: 0.299\n",
            "[5,    20] loss: 0.269\n",
            "[5,    30] loss: 0.277\n",
            "[5,    40] loss: 0.250\n",
            "[6,    10] loss: 0.234\n",
            "[6,    20] loss: 0.217\n",
            "[6,    30] loss: 0.243\n",
            "[6,    40] loss: 0.261\n",
            "[7,    10] loss: 0.275\n",
            "[7,    20] loss: 0.233\n",
            "[7,    30] loss: 0.227\n",
            "[7,    40] loss: 0.214\n",
            "[8,    10] loss: 0.202\n",
            "[8,    20] loss: 0.194\n",
            "[8,    30] loss: 0.194\n",
            "[8,    40] loss: 0.200\n",
            "[9,    10] loss: 0.203\n",
            "[9,    20] loss: 0.169\n",
            "[9,    30] loss: 0.182\n",
            "[9,    40] loss: 0.166\n",
            "[10,    10] loss: 0.158\n",
            "[10,    20] loss: 0.154\n",
            "[10,    30] loss: 0.152\n",
            "[10,    40] loss: 0.139\n",
            "[11,    10] loss: 0.133\n",
            "[11,    20] loss: 0.138\n",
            "[11,    30] loss: 0.123\n",
            "[11,    40] loss: 0.126\n",
            "[12,    10] loss: 0.129\n",
            "[12,    20] loss: 0.122\n",
            "[12,    30] loss: 0.142\n",
            "[12,    40] loss: 0.107\n",
            "[13,    10] loss: 0.114\n",
            "[13,    20] loss: 0.109\n",
            "[13,    30] loss: 0.107\n",
            "[13,    40] loss: 0.109\n",
            "[14,    10] loss: 0.197\n",
            "[14,    20] loss: 0.167\n",
            "[14,    30] loss: 0.132\n",
            "[14,    40] loss: 0.138\n",
            "[15,    10] loss: 0.129\n",
            "[15,    20] loss: 0.125\n",
            "[15,    30] loss: 0.119\n",
            "[15,    40] loss: 0.099\n",
            "[16,    10] loss: 0.089\n",
            "[16,    20] loss: 0.080\n",
            "[16,    30] loss: 0.083\n",
            "[16,    40] loss: 0.079\n",
            "[17,    10] loss: 0.142\n",
            "[17,    20] loss: 0.122\n",
            "[17,    30] loss: 0.107\n",
            "[17,    40] loss: 0.087\n",
            "[18,    10] loss: 0.065\n",
            "[18,    20] loss: 0.062\n",
            "[18,    30] loss: 0.068\n",
            "[18,    40] loss: 0.070\n",
            "[19,    10] loss: 0.063\n",
            "[19,    20] loss: 0.052\n",
            "[19,    30] loss: 0.051\n",
            "[19,    40] loss: 0.061\n",
            "[20,    10] loss: 0.087\n",
            "[20,    20] loss: 0.081\n",
            "[20,    30] loss: 0.069\n",
            "[20,    40] loss: 0.059\n",
            "[21,    10] loss: 0.053\n",
            "[21,    20] loss: 0.047\n",
            "[21,    30] loss: 0.043\n",
            "[21,    40] loss: 0.053\n",
            "[22,    10] loss: 0.069\n",
            "[22,    20] loss: 0.071\n",
            "[22,    30] loss: 0.055\n",
            "[22,    40] loss: 0.053\n",
            "[23,    10] loss: 0.058\n",
            "[23,    20] loss: 0.052\n",
            "[23,    30] loss: 0.042\n",
            "[23,    40] loss: 0.045\n",
            "[24,    10] loss: 0.059\n",
            "[24,    20] loss: 0.053\n",
            "[24,    30] loss: 0.047\n",
            "[24,    40] loss: 0.044\n",
            "[25,    10] loss: 0.040\n",
            "[25,    20] loss: 0.035\n",
            "[25,    30] loss: 0.035\n",
            "[25,    40] loss: 0.028\n",
            "[26,    10] loss: 0.024\n",
            "[26,    20] loss: 0.022\n",
            "[26,    30] loss: 0.021\n",
            "[26,    40] loss: 0.020\n",
            "[27,    10] loss: 0.019\n",
            "[27,    20] loss: 0.017\n",
            "[27,    30] loss: 0.014\n",
            "[27,    40] loss: 0.020\n",
            "[28,    10] loss: 0.029\n",
            "[28,    20] loss: 0.029\n",
            "[28,    30] loss: 0.020\n",
            "[28,    40] loss: 0.027\n",
            "[29,    10] loss: 0.037\n",
            "[29,    20] loss: 0.051\n",
            "[29,    30] loss: 0.042\n",
            "[29,    40] loss: 0.029\n",
            "[30,    10] loss: 0.036\n",
            "[30,    20] loss: 0.034\n",
            "[30,    30] loss: 0.023\n",
            "[30,    40] loss: 0.025\n",
            "[31,    10] loss: 0.036\n",
            "[31,    20] loss: 0.034\n",
            "[31,    30] loss: 0.024\n",
            "[31,    40] loss: 0.020\n",
            "[32,    10] loss: 0.019\n",
            "[32,    20] loss: 0.018\n",
            "[32,    30] loss: 0.014\n",
            "[32,    40] loss: 0.012\n",
            "[33,    10] loss: 0.010\n",
            "[33,    20] loss: 0.009\n",
            "[33,    30] loss: 0.009\n",
            "[33,    40] loss: 0.010\n",
            "[34,    10] loss: 0.010\n",
            "[34,    20] loss: 0.009\n",
            "[34,    30] loss: 0.008\n",
            "[34,    40] loss: 0.008\n",
            "[35,    10] loss: 0.006\n",
            "[35,    20] loss: 0.006\n",
            "[35,    30] loss: 0.007\n",
            "[35,    40] loss: 0.010\n",
            "[36,    10] loss: 0.043\n",
            "[36,    20] loss: 0.031\n",
            "[36,    30] loss: 0.022\n",
            "[36,    40] loss: 0.019\n",
            "[37,    10] loss: 0.015\n",
            "[37,    20] loss: 0.012\n",
            "[37,    30] loss: 0.010\n",
            "[37,    40] loss: 0.008\n",
            "[38,    10] loss: 0.007\n",
            "[38,    20] loss: 0.006\n",
            "[38,    30] loss: 0.006\n",
            "[38,    40] loss: 0.008\n",
            "[39,    10] loss: 0.005\n",
            "[39,    20] loss: 0.007\n",
            "[39,    30] loss: 0.006\n",
            "[39,    40] loss: 0.005\n",
            "[40,    10] loss: 0.005\n",
            "[40,    20] loss: 0.004\n",
            "[40,    30] loss: 0.005\n",
            "[40,    40] loss: 0.004\n",
            "[41,    10] loss: 0.004\n",
            "[41,    20] loss: 0.003\n",
            "[41,    30] loss: 0.004\n",
            "[41,    40] loss: 0.021\n",
            "[42,    10] loss: 0.081\n",
            "[42,    20] loss: 0.081\n",
            "[42,    30] loss: 0.069\n",
            "[42,    40] loss: 0.041\n",
            "[43,    10] loss: 0.029\n",
            "[43,    20] loss: 0.024\n",
            "[43,    30] loss: 0.017\n",
            "[43,    40] loss: 0.013\n",
            "[44,    10] loss: 0.013\n",
            "[44,    20] loss: 0.009\n",
            "[44,    30] loss: 0.009\n",
            "[44,    40] loss: 0.011\n",
            "[45,    10] loss: 0.009\n",
            "[45,    20] loss: 0.009\n",
            "[45,    30] loss: 0.008\n",
            "[45,    40] loss: 0.006\n",
            "[46,    10] loss: 0.005\n",
            "[46,    20] loss: 0.005\n",
            "[46,    30] loss: 0.004\n",
            "[46,    40] loss: 0.007\n",
            "[47,    10] loss: 0.014\n",
            "[47,    20] loss: 0.015\n",
            "[47,    30] loss: 0.014\n",
            "[47,    40] loss: 0.015\n",
            "[48,    10] loss: 0.119\n",
            "[48,    20] loss: 0.102\n",
            "[48,    30] loss: 0.079\n",
            "[48,    40] loss: 0.055\n",
            "[49,    10] loss: 0.030\n",
            "[49,    20] loss: 0.023\n",
            "[49,    30] loss: 0.020\n",
            "[49,    40] loss: 0.014\n",
            "[50,    10] loss: 0.010\n",
            "[50,    20] loss: 0.008\n",
            "[50,    30] loss: 0.007\n",
            "[50,    40] loss: 0.008\n",
            "[51,    10] loss: 0.009\n",
            "[51,    20] loss: 0.007\n",
            "[51,    30] loss: 0.007\n",
            "[51,    40] loss: 0.007\n",
            "[52,    10] loss: 0.009\n",
            "[52,    20] loss: 0.009\n",
            "[52,    30] loss: 0.006\n",
            "[52,    40] loss: 0.007\n",
            "[53,    10] loss: 0.012\n",
            "[53,    20] loss: 0.012\n",
            "[53,    30] loss: 0.013\n",
            "[53,    40] loss: 0.008\n",
            "[54,    10] loss: 0.007\n",
            "[54,    20] loss: 0.006\n",
            "[54,    30] loss: 0.004\n",
            "[54,    40] loss: 0.006\n",
            "[55,    10] loss: 0.005\n",
            "[55,    20] loss: 0.006\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.004\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.005\n",
            "[57,    10] loss: 0.016\n",
            "[57,    20] loss: 0.013\n",
            "[57,    30] loss: 0.007\n",
            "[57,    40] loss: 0.006\n",
            "[58,    10] loss: 0.004\n",
            "[58,    20] loss: 0.004\n",
            "[58,    30] loss: 0.003\n",
            "[58,    40] loss: 0.003\n",
            "[59,    10] loss: 0.003\n",
            "[59,    20] loss: 0.003\n",
            "[59,    30] loss: 0.003\n",
            "[59,    40] loss: 0.003\n",
            "[60,    10] loss: 0.002\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.003\n",
            "[61,    10] loss: 0.003\n",
            "[61,    20] loss: 0.003\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.003\n",
            "[62,    10] loss: 0.002\n",
            "[62,    20] loss: 0.002\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.002\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.002\n",
            "[63,    40] loss: 0.001\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.002\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.002\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.002\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.002\n",
            "[66,    30] loss: 0.002\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.001\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.001\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.005\n",
            "[70,    10] loss: 0.018\n",
            "[70,    20] loss: 0.011\n",
            "[70,    30] loss: 0.011\n",
            "[70,    40] loss: 0.007\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 47 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 72 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 89 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 97 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   7\n",
            "[1,    10] loss: 0.867\n",
            "[1,    20] loss: 0.566\n",
            "[1,    30] loss: 0.516\n",
            "[1,    40] loss: 0.446\n",
            "[2,    10] loss: 0.433\n",
            "[2,    20] loss: 0.397\n",
            "[2,    30] loss: 0.376\n",
            "[2,    40] loss: 0.376\n",
            "[3,    10] loss: 0.423\n",
            "[3,    20] loss: 0.347\n",
            "[3,    30] loss: 0.335\n",
            "[3,    40] loss: 0.313\n",
            "[4,    10] loss: 0.282\n",
            "[4,    20] loss: 0.259\n",
            "[4,    30] loss: 0.263\n",
            "[4,    40] loss: 0.261\n",
            "[5,    10] loss: 0.256\n",
            "[5,    20] loss: 0.233\n",
            "[5,    30] loss: 0.243\n",
            "[5,    40] loss: 0.218\n",
            "[6,    10] loss: 0.222\n",
            "[6,    20] loss: 0.225\n",
            "[6,    30] loss: 0.198\n",
            "[6,    40] loss: 0.182\n",
            "[7,    10] loss: 0.183\n",
            "[7,    20] loss: 0.176\n",
            "[7,    30] loss: 0.174\n",
            "[7,    40] loss: 0.168\n",
            "[8,    10] loss: 0.180\n",
            "[8,    20] loss: 0.171\n",
            "[8,    30] loss: 0.163\n",
            "[8,    40] loss: 0.182\n",
            "[9,    10] loss: 0.207\n",
            "[9,    20] loss: 0.204\n",
            "[9,    30] loss: 0.182\n",
            "[9,    40] loss: 0.181\n",
            "[10,    10] loss: 0.179\n",
            "[10,    20] loss: 0.142\n",
            "[10,    30] loss: 0.127\n",
            "[10,    40] loss: 0.137\n",
            "[11,    10] loss: 0.119\n",
            "[11,    20] loss: 0.123\n",
            "[11,    30] loss: 0.138\n",
            "[11,    40] loss: 0.128\n",
            "[12,    10] loss: 0.161\n",
            "[12,    20] loss: 0.149\n",
            "[12,    30] loss: 0.146\n",
            "[12,    40] loss: 0.120\n",
            "[13,    10] loss: 0.108\n",
            "[13,    20] loss: 0.110\n",
            "[13,    30] loss: 0.107\n",
            "[13,    40] loss: 0.087\n",
            "[14,    10] loss: 0.094\n",
            "[14,    20] loss: 0.079\n",
            "[14,    30] loss: 0.085\n",
            "[14,    40] loss: 0.080\n",
            "[15,    10] loss: 0.079\n",
            "[15,    20] loss: 0.084\n",
            "[15,    30] loss: 0.078\n",
            "[15,    40] loss: 0.080\n",
            "[16,    10] loss: 0.100\n",
            "[16,    20] loss: 0.108\n",
            "[16,    30] loss: 0.101\n",
            "[16,    40] loss: 0.115\n",
            "[17,    10] loss: 0.157\n",
            "[17,    20] loss: 0.123\n",
            "[17,    30] loss: 0.115\n",
            "[17,    40] loss: 0.108\n",
            "[18,    10] loss: 0.087\n",
            "[18,    20] loss: 0.075\n",
            "[18,    30] loss: 0.073\n",
            "[18,    40] loss: 0.062\n",
            "[19,    10] loss: 0.057\n",
            "[19,    20] loss: 0.047\n",
            "[19,    30] loss: 0.050\n",
            "[19,    40] loss: 0.054\n",
            "[20,    10] loss: 0.051\n",
            "[20,    20] loss: 0.049\n",
            "[20,    30] loss: 0.054\n",
            "[20,    40] loss: 0.045\n",
            "[21,    10] loss: 0.034\n",
            "[21,    20] loss: 0.038\n",
            "[21,    30] loss: 0.033\n",
            "[21,    40] loss: 0.053\n",
            "[22,    10] loss: 0.151\n",
            "[22,    20] loss: 0.099\n",
            "[22,    30] loss: 0.090\n",
            "[22,    40] loss: 0.076\n",
            "[23,    10] loss: 0.059\n",
            "[23,    20] loss: 0.047\n",
            "[23,    30] loss: 0.045\n",
            "[23,    40] loss: 0.054\n",
            "[24,    10] loss: 0.070\n",
            "[24,    20] loss: 0.058\n",
            "[24,    30] loss: 0.047\n",
            "[24,    40] loss: 0.053\n",
            "[25,    10] loss: 0.090\n",
            "[25,    20] loss: 0.084\n",
            "[25,    30] loss: 0.060\n",
            "[25,    40] loss: 0.050\n",
            "[26,    10] loss: 0.038\n",
            "[26,    20] loss: 0.036\n",
            "[26,    30] loss: 0.038\n",
            "[26,    40] loss: 0.031\n",
            "[27,    10] loss: 0.032\n",
            "[27,    20] loss: 0.025\n",
            "[27,    30] loss: 0.028\n",
            "[27,    40] loss: 0.022\n",
            "[28,    10] loss: 0.019\n",
            "[28,    20] loss: 0.017\n",
            "[28,    30] loss: 0.016\n",
            "[28,    40] loss: 0.023\n",
            "[29,    10] loss: 0.029\n",
            "[29,    20] loss: 0.027\n",
            "[29,    30] loss: 0.027\n",
            "[29,    40] loss: 0.024\n",
            "[30,    10] loss: 0.016\n",
            "[30,    20] loss: 0.014\n",
            "[30,    30] loss: 0.015\n",
            "[30,    40] loss: 0.014\n",
            "[31,    10] loss: 0.012\n",
            "[31,    20] loss: 0.010\n",
            "[31,    30] loss: 0.010\n",
            "[31,    40] loss: 0.009\n",
            "[32,    10] loss: 0.009\n",
            "[32,    20] loss: 0.009\n",
            "[32,    30] loss: 0.009\n",
            "[32,    40] loss: 0.007\n",
            "[33,    10] loss: 0.008\n",
            "[33,    20] loss: 0.007\n",
            "[33,    30] loss: 0.008\n",
            "[33,    40] loss: 0.008\n",
            "[34,    10] loss: 0.007\n",
            "[34,    20] loss: 0.006\n",
            "[34,    30] loss: 0.007\n",
            "[34,    40] loss: 0.013\n",
            "[35,    10] loss: 0.032\n",
            "[35,    20] loss: 0.034\n",
            "[35,    30] loss: 0.023\n",
            "[35,    40] loss: 0.021\n",
            "[36,    10] loss: 0.026\n",
            "[36,    20] loss: 0.022\n",
            "[36,    30] loss: 0.015\n",
            "[36,    40] loss: 0.015\n",
            "[37,    10] loss: 0.021\n",
            "[37,    20] loss: 0.017\n",
            "[37,    30] loss: 0.014\n",
            "[37,    40] loss: 0.013\n",
            "[38,    10] loss: 0.014\n",
            "[38,    20] loss: 0.020\n",
            "[38,    30] loss: 0.014\n",
            "[38,    40] loss: 0.011\n",
            "[39,    10] loss: 0.014\n",
            "[39,    20] loss: 0.013\n",
            "[39,    30] loss: 0.010\n",
            "[39,    40] loss: 0.011\n",
            "[40,    10] loss: 0.010\n",
            "[40,    20] loss: 0.009\n",
            "[40,    30] loss: 0.007\n",
            "[40,    40] loss: 0.010\n",
            "[41,    10] loss: 0.020\n",
            "[41,    20] loss: 0.017\n",
            "[41,    30] loss: 0.014\n",
            "[41,    40] loss: 0.012\n",
            "[42,    10] loss: 0.017\n",
            "[42,    20] loss: 0.014\n",
            "[42,    30] loss: 0.009\n",
            "[42,    40] loss: 0.009\n",
            "[43,    10] loss: 0.006\n",
            "[43,    20] loss: 0.007\n",
            "[43,    30] loss: 0.005\n",
            "[43,    40] loss: 0.006\n",
            "[44,    10] loss: 0.004\n",
            "[44,    20] loss: 0.006\n",
            "[44,    30] loss: 0.004\n",
            "[44,    40] loss: 0.004\n",
            "[45,    10] loss: 0.003\n",
            "[45,    20] loss: 0.003\n",
            "[45,    30] loss: 0.003\n",
            "[45,    40] loss: 0.004\n",
            "[46,    10] loss: 0.003\n",
            "[46,    20] loss: 0.004\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.003\n",
            "[47,    10] loss: 0.003\n",
            "[47,    20] loss: 0.003\n",
            "[47,    30] loss: 0.003\n",
            "[47,    40] loss: 0.004\n",
            "[48,    10] loss: 0.005\n",
            "[48,    20] loss: 0.005\n",
            "[48,    30] loss: 0.004\n",
            "[48,    40] loss: 0.004\n",
            "[49,    10] loss: 0.003\n",
            "[49,    20] loss: 0.003\n",
            "[49,    30] loss: 0.002\n",
            "[49,    40] loss: 0.002\n",
            "[50,    10] loss: 0.002\n",
            "[50,    20] loss: 0.002\n",
            "[50,    30] loss: 0.002\n",
            "[50,    40] loss: 0.002\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.002\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.002\n",
            "[52,    10] loss: 0.002\n",
            "[52,    20] loss: 0.002\n",
            "[52,    30] loss: 0.002\n",
            "[52,    40] loss: 0.006\n",
            "[53,    10] loss: 0.015\n",
            "[53,    20] loss: 0.015\n",
            "[53,    30] loss: 0.009\n",
            "[53,    40] loss: 0.007\n",
            "[54,    10] loss: 0.004\n",
            "[54,    20] loss: 0.004\n",
            "[54,    30] loss: 0.003\n",
            "[54,    40] loss: 0.003\n",
            "[55,    10] loss: 0.002\n",
            "[55,    20] loss: 0.002\n",
            "[55,    30] loss: 0.002\n",
            "[55,    40] loss: 0.003\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.003\n",
            "[56,    40] loss: 0.003\n",
            "[57,    10] loss: 0.002\n",
            "[57,    20] loss: 0.002\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.003\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.003\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.002\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.001\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.001\n",
            "[61,    40] loss: 0.014\n",
            "[62,    10] loss: 0.149\n",
            "[62,    20] loss: 0.103\n",
            "[62,    30] loss: 0.081\n",
            "[62,    40] loss: 0.056\n",
            "[63,    10] loss: 0.031\n",
            "[63,    20] loss: 0.029\n",
            "[63,    30] loss: 0.024\n",
            "[63,    40] loss: 0.020\n",
            "[64,    10] loss: 0.032\n",
            "[64,    20] loss: 0.017\n",
            "[64,    30] loss: 0.017\n",
            "[64,    40] loss: 0.012\n",
            "[65,    10] loss: 0.007\n",
            "[65,    20] loss: 0.006\n",
            "[65,    30] loss: 0.005\n",
            "[65,    40] loss: 0.006\n",
            "[66,    10] loss: 0.005\n",
            "[66,    20] loss: 0.004\n",
            "[66,    30] loss: 0.004\n",
            "[66,    40] loss: 0.004\n",
            "[67,    10] loss: 0.003\n",
            "[67,    20] loss: 0.003\n",
            "[67,    30] loss: 0.003\n",
            "[67,    40] loss: 0.004\n",
            "[68,    10] loss: 0.010\n",
            "[68,    20] loss: 0.010\n",
            "[68,    30] loss: 0.004\n",
            "[68,    40] loss: 0.004\n",
            "[69,    10] loss: 0.003\n",
            "[69,    20] loss: 0.003\n",
            "[69,    30] loss: 0.003\n",
            "[69,    40] loss: 0.003\n",
            "[70,    10] loss: 0.002\n",
            "[70,    20] loss: 0.002\n",
            "[70,    30] loss: 0.002\n",
            "[70,    40] loss: 0.002\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 70 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 88 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 96 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   8\n",
            "[1,    10] loss: 0.829\n",
            "[1,    20] loss: 0.567\n",
            "[1,    30] loss: 0.486\n",
            "[1,    40] loss: 0.442\n",
            "[2,    10] loss: 0.442\n",
            "[2,    20] loss: 0.427\n",
            "[2,    30] loss: 0.364\n",
            "[2,    40] loss: 0.344\n",
            "[3,    10] loss: 0.391\n",
            "[3,    20] loss: 0.336\n",
            "[3,    30] loss: 0.318\n",
            "[3,    40] loss: 0.326\n",
            "[4,    10] loss: 0.307\n",
            "[4,    20] loss: 0.300\n",
            "[4,    30] loss: 0.261\n",
            "[4,    40] loss: 0.241\n",
            "[5,    10] loss: 0.293\n",
            "[5,    20] loss: 0.258\n",
            "[5,    30] loss: 0.232\n",
            "[5,    40] loss: 0.213\n",
            "[6,    10] loss: 0.228\n",
            "[6,    20] loss: 0.224\n",
            "[6,    30] loss: 0.214\n",
            "[6,    40] loss: 0.210\n",
            "[7,    10] loss: 0.187\n",
            "[7,    20] loss: 0.191\n",
            "[7,    30] loss: 0.195\n",
            "[7,    40] loss: 0.202\n",
            "[8,    10] loss: 0.193\n",
            "[8,    20] loss: 0.173\n",
            "[8,    30] loss: 0.192\n",
            "[8,    40] loss: 0.190\n",
            "[9,    10] loss: 0.171\n",
            "[9,    20] loss: 0.157\n",
            "[9,    30] loss: 0.158\n",
            "[9,    40] loss: 0.169\n",
            "[10,    10] loss: 0.194\n",
            "[10,    20] loss: 0.155\n",
            "[10,    30] loss: 0.155\n",
            "[10,    40] loss: 0.145\n",
            "[11,    10] loss: 0.138\n",
            "[11,    20] loss: 0.149\n",
            "[11,    30] loss: 0.137\n",
            "[11,    40] loss: 0.101\n",
            "[12,    10] loss: 0.107\n",
            "[12,    20] loss: 0.132\n",
            "[12,    30] loss: 0.119\n",
            "[12,    40] loss: 0.124\n",
            "[13,    10] loss: 0.157\n",
            "[13,    20] loss: 0.129\n",
            "[13,    30] loss: 0.120\n",
            "[13,    40] loss: 0.132\n",
            "[14,    10] loss: 0.111\n",
            "[14,    20] loss: 0.096\n",
            "[14,    30] loss: 0.101\n",
            "[14,    40] loss: 0.102\n",
            "[15,    10] loss: 0.120\n",
            "[15,    20] loss: 0.110\n",
            "[15,    30] loss: 0.090\n",
            "[15,    40] loss: 0.089\n",
            "[16,    10] loss: 0.080\n",
            "[16,    20] loss: 0.072\n",
            "[16,    30] loss: 0.066\n",
            "[16,    40] loss: 0.074\n",
            "[17,    10] loss: 0.067\n",
            "[17,    20] loss: 0.071\n",
            "[17,    30] loss: 0.064\n",
            "[17,    40] loss: 0.083\n",
            "[18,    10] loss: 0.094\n",
            "[18,    20] loss: 0.092\n",
            "[18,    30] loss: 0.079\n",
            "[18,    40] loss: 0.104\n",
            "[19,    10] loss: 0.155\n",
            "[19,    20] loss: 0.148\n",
            "[19,    30] loss: 0.107\n",
            "[19,    40] loss: 0.098\n",
            "[20,    10] loss: 0.114\n",
            "[20,    20] loss: 0.094\n",
            "[20,    30] loss: 0.070\n",
            "[20,    40] loss: 0.075\n",
            "[21,    10] loss: 0.060\n",
            "[21,    20] loss: 0.054\n",
            "[21,    30] loss: 0.046\n",
            "[21,    40] loss: 0.051\n",
            "[22,    10] loss: 0.045\n",
            "[22,    20] loss: 0.039\n",
            "[22,    30] loss: 0.042\n",
            "[22,    40] loss: 0.040\n",
            "[23,    10] loss: 0.042\n",
            "[23,    20] loss: 0.043\n",
            "[23,    30] loss: 0.039\n",
            "[23,    40] loss: 0.036\n",
            "[24,    10] loss: 0.035\n",
            "[24,    20] loss: 0.030\n",
            "[24,    30] loss: 0.029\n",
            "[24,    40] loss: 0.023\n",
            "[25,    10] loss: 0.021\n",
            "[25,    20] loss: 0.021\n",
            "[25,    30] loss: 0.020\n",
            "[25,    40] loss: 0.020\n",
            "[26,    10] loss: 0.017\n",
            "[26,    20] loss: 0.017\n",
            "[26,    30] loss: 0.018\n",
            "[26,    40] loss: 0.016\n",
            "[27,    10] loss: 0.015\n",
            "[27,    20] loss: 0.016\n",
            "[27,    30] loss: 0.017\n",
            "[27,    40] loss: 0.015\n",
            "[28,    10] loss: 0.014\n",
            "[28,    20] loss: 0.014\n",
            "[28,    30] loss: 0.012\n",
            "[28,    40] loss: 0.019\n",
            "[29,    10] loss: 0.076\n",
            "[29,    20] loss: 0.069\n",
            "[29,    30] loss: 0.059\n",
            "[29,    40] loss: 0.085\n",
            "[30,    10] loss: 0.183\n",
            "[30,    20] loss: 0.102\n",
            "[30,    30] loss: 0.073\n",
            "[30,    40] loss: 0.059\n",
            "[31,    10] loss: 0.077\n",
            "[31,    20] loss: 0.066\n",
            "[31,    30] loss: 0.056\n",
            "[31,    40] loss: 0.042\n",
            "[32,    10] loss: 0.034\n",
            "[32,    20] loss: 0.031\n",
            "[32,    30] loss: 0.028\n",
            "[32,    40] loss: 0.022\n",
            "[33,    10] loss: 0.016\n",
            "[33,    20] loss: 0.017\n",
            "[33,    30] loss: 0.015\n",
            "[33,    40] loss: 0.019\n",
            "[34,    10] loss: 0.036\n",
            "[34,    20] loss: 0.031\n",
            "[34,    30] loss: 0.028\n",
            "[34,    40] loss: 0.021\n",
            "[35,    10] loss: 0.016\n",
            "[35,    20] loss: 0.014\n",
            "[35,    30] loss: 0.015\n",
            "[35,    40] loss: 0.012\n",
            "[36,    10] loss: 0.010\n",
            "[36,    20] loss: 0.009\n",
            "[36,    30] loss: 0.009\n",
            "[36,    40] loss: 0.009\n",
            "[37,    10] loss: 0.008\n",
            "[37,    20] loss: 0.007\n",
            "[37,    30] loss: 0.006\n",
            "[37,    40] loss: 0.007\n",
            "[38,    10] loss: 0.006\n",
            "[38,    20] loss: 0.006\n",
            "[38,    30] loss: 0.007\n",
            "[38,    40] loss: 0.006\n",
            "[39,    10] loss: 0.006\n",
            "[39,    20] loss: 0.006\n",
            "[39,    30] loss: 0.005\n",
            "[39,    40] loss: 0.007\n",
            "[40,    10] loss: 0.013\n",
            "[40,    20] loss: 0.012\n",
            "[40,    30] loss: 0.010\n",
            "[40,    40] loss: 0.011\n",
            "[41,    10] loss: 0.011\n",
            "[41,    20] loss: 0.010\n",
            "[41,    30] loss: 0.008\n",
            "[41,    40] loss: 0.007\n",
            "[42,    10] loss: 0.006\n",
            "[42,    20] loss: 0.005\n",
            "[42,    30] loss: 0.005\n",
            "[42,    40] loss: 0.005\n",
            "[43,    10] loss: 0.005\n",
            "[43,    20] loss: 0.004\n",
            "[43,    30] loss: 0.005\n",
            "[43,    40] loss: 0.005\n",
            "[44,    10] loss: 0.004\n",
            "[44,    20] loss: 0.004\n",
            "[44,    30] loss: 0.003\n",
            "[44,    40] loss: 0.004\n",
            "[45,    10] loss: 0.003\n",
            "[45,    20] loss: 0.004\n",
            "[45,    30] loss: 0.003\n",
            "[45,    40] loss: 0.003\n",
            "[46,    10] loss: 0.003\n",
            "[46,    20] loss: 0.003\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.003\n",
            "[47,    10] loss: 0.003\n",
            "[47,    20] loss: 0.003\n",
            "[47,    30] loss: 0.003\n",
            "[47,    40] loss: 0.008\n",
            "[48,    10] loss: 0.039\n",
            "[48,    20] loss: 0.024\n",
            "[48,    30] loss: 0.024\n",
            "[48,    40] loss: 0.016\n",
            "[49,    10] loss: 0.010\n",
            "[49,    20] loss: 0.007\n",
            "[49,    30] loss: 0.006\n",
            "[49,    40] loss: 0.006\n",
            "[50,    10] loss: 0.007\n",
            "[50,    20] loss: 0.005\n",
            "[50,    30] loss: 0.005\n",
            "[50,    40] loss: 0.007\n",
            "[51,    10] loss: 0.020\n",
            "[51,    20] loss: 0.015\n",
            "[51,    30] loss: 0.012\n",
            "[51,    40] loss: 0.009\n",
            "[52,    10] loss: 0.007\n",
            "[52,    20] loss: 0.005\n",
            "[52,    30] loss: 0.004\n",
            "[52,    40] loss: 0.004\n",
            "[53,    10] loss: 0.003\n",
            "[53,    20] loss: 0.003\n",
            "[53,    30] loss: 0.003\n",
            "[53,    40] loss: 0.009\n",
            "[54,    10] loss: 0.028\n",
            "[54,    20] loss: 0.028\n",
            "[54,    30] loss: 0.018\n",
            "[54,    40] loss: 0.010\n",
            "[55,    10] loss: 0.008\n",
            "[55,    20] loss: 0.005\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.007\n",
            "[56,    10] loss: 0.012\n",
            "[56,    20] loss: 0.007\n",
            "[56,    30] loss: 0.005\n",
            "[56,    40] loss: 0.005\n",
            "[57,    10] loss: 0.003\n",
            "[57,    20] loss: 0.003\n",
            "[57,    30] loss: 0.003\n",
            "[57,    40] loss: 0.005\n",
            "[58,    10] loss: 0.006\n",
            "[58,    20] loss: 0.005\n",
            "[58,    30] loss: 0.005\n",
            "[58,    40] loss: 0.004\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.002\n",
            "[60,    10] loss: 0.002\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.002\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.002\n",
            "[61,    20] loss: 0.002\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.001\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.002\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.002\n",
            "[63,    20] loss: 0.001\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.003\n",
            "[64,    10] loss: 0.010\n",
            "[64,    20] loss: 0.007\n",
            "[64,    30] loss: 0.005\n",
            "[64,    40] loss: 0.003\n",
            "[65,    10] loss: 0.002\n",
            "[65,    20] loss: 0.002\n",
            "[65,    30] loss: 0.002\n",
            "[65,    40] loss: 0.002\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.002\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.002\n",
            "[67,    30] loss: 0.002\n",
            "[67,    40] loss: 0.002\n",
            "[68,    10] loss: 0.001\n",
            "[68,    20] loss: 0.001\n",
            "[68,    30] loss: 0.001\n",
            "[68,    40] loss: 0.002\n",
            "[69,    10] loss: 0.001\n",
            "[69,    20] loss: 0.001\n",
            "[69,    30] loss: 0.001\n",
            "[69,    40] loss: 0.001\n",
            "[70,    10] loss: 0.001\n",
            "[70,    20] loss: 0.001\n",
            "[70,    30] loss: 0.001\n",
            "[70,    40] loss: 0.001\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 46 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 69 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 86 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 95 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 96 %\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "training on data set   9\n",
            "[1,    10] loss: 0.908\n",
            "[1,    20] loss: 0.600\n",
            "[1,    30] loss: 0.484\n",
            "[1,    40] loss: 0.448\n",
            "[2,    10] loss: 0.485\n",
            "[2,    20] loss: 0.423\n",
            "[2,    30] loss: 0.382\n",
            "[2,    40] loss: 0.366\n",
            "[3,    10] loss: 0.336\n",
            "[3,    20] loss: 0.326\n",
            "[3,    30] loss: 0.316\n",
            "[3,    40] loss: 0.314\n",
            "[4,    10] loss: 0.303\n",
            "[4,    20] loss: 0.318\n",
            "[4,    30] loss: 0.273\n",
            "[4,    40] loss: 0.275\n",
            "[5,    10] loss: 0.286\n",
            "[5,    20] loss: 0.245\n",
            "[5,    30] loss: 0.242\n",
            "[5,    40] loss: 0.268\n",
            "[6,    10] loss: 0.252\n",
            "[6,    20] loss: 0.236\n",
            "[6,    30] loss: 0.219\n",
            "[6,    40] loss: 0.212\n",
            "[7,    10] loss: 0.222\n",
            "[7,    20] loss: 0.218\n",
            "[7,    30] loss: 0.203\n",
            "[7,    40] loss: 0.190\n",
            "[8,    10] loss: 0.169\n",
            "[8,    20] loss: 0.162\n",
            "[8,    30] loss: 0.174\n",
            "[8,    40] loss: 0.174\n",
            "[9,    10] loss: 0.192\n",
            "[9,    20] loss: 0.176\n",
            "[9,    30] loss: 0.152\n",
            "[9,    40] loss: 0.154\n",
            "[10,    10] loss: 0.140\n",
            "[10,    20] loss: 0.139\n",
            "[10,    30] loss: 0.141\n",
            "[10,    40] loss: 0.150\n",
            "[11,    10] loss: 0.177\n",
            "[11,    20] loss: 0.142\n",
            "[11,    30] loss: 0.143\n",
            "[11,    40] loss: 0.131\n",
            "[12,    10] loss: 0.136\n",
            "[12,    20] loss: 0.149\n",
            "[12,    30] loss: 0.125\n",
            "[12,    40] loss: 0.109\n",
            "[13,    10] loss: 0.113\n",
            "[13,    20] loss: 0.093\n",
            "[13,    30] loss: 0.107\n",
            "[13,    40] loss: 0.125\n",
            "[14,    10] loss: 0.120\n",
            "[14,    20] loss: 0.111\n",
            "[14,    30] loss: 0.107\n",
            "[14,    40] loss: 0.099\n",
            "[15,    10] loss: 0.092\n",
            "[15,    20] loss: 0.099\n",
            "[15,    30] loss: 0.075\n",
            "[15,    40] loss: 0.077\n",
            "[16,    10] loss: 0.067\n",
            "[16,    20] loss: 0.072\n",
            "[16,    30] loss: 0.066\n",
            "[16,    40] loss: 0.062\n",
            "[17,    10] loss: 0.051\n",
            "[17,    20] loss: 0.062\n",
            "[17,    30] loss: 0.056\n",
            "[17,    40] loss: 0.057\n",
            "[18,    10] loss: 0.056\n",
            "[18,    20] loss: 0.053\n",
            "[18,    30] loss: 0.050\n",
            "[18,    40] loss: 0.050\n",
            "[19,    10] loss: 0.055\n",
            "[19,    20] loss: 0.064\n",
            "[19,    30] loss: 0.055\n",
            "[19,    40] loss: 0.074\n",
            "[20,    10] loss: 0.103\n",
            "[20,    20] loss: 0.090\n",
            "[20,    30] loss: 0.073\n",
            "[20,    40] loss: 0.076\n",
            "[21,    10] loss: 0.063\n",
            "[21,    20] loss: 0.058\n",
            "[21,    30] loss: 0.048\n",
            "[21,    40] loss: 0.055\n",
            "[22,    10] loss: 0.082\n",
            "[22,    20] loss: 0.085\n",
            "[22,    30] loss: 0.065\n",
            "[22,    40] loss: 0.069\n",
            "[23,    10] loss: 0.100\n",
            "[23,    20] loss: 0.066\n",
            "[23,    30] loss: 0.062\n",
            "[23,    40] loss: 0.054\n",
            "[24,    10] loss: 0.041\n",
            "[24,    20] loss: 0.042\n",
            "[24,    30] loss: 0.037\n",
            "[24,    40] loss: 0.039\n",
            "[25,    10] loss: 0.037\n",
            "[25,    20] loss: 0.031\n",
            "[25,    30] loss: 0.033\n",
            "[25,    40] loss: 0.031\n",
            "[26,    10] loss: 0.030\n",
            "[26,    20] loss: 0.023\n",
            "[26,    30] loss: 0.020\n",
            "[26,    40] loss: 0.020\n",
            "[27,    10] loss: 0.017\n",
            "[27,    20] loss: 0.015\n",
            "[27,    30] loss: 0.016\n",
            "[27,    40] loss: 0.026\n",
            "[28,    10] loss: 0.052\n",
            "[28,    20] loss: 0.046\n",
            "[28,    30] loss: 0.035\n",
            "[28,    40] loss: 0.029\n",
            "[29,    10] loss: 0.025\n",
            "[29,    20] loss: 0.021\n",
            "[29,    30] loss: 0.019\n",
            "[29,    40] loss: 0.022\n",
            "[30,    10] loss: 0.021\n",
            "[30,    20] loss: 0.020\n",
            "[30,    30] loss: 0.019\n",
            "[30,    40] loss: 0.014\n",
            "[31,    10] loss: 0.012\n",
            "[31,    20] loss: 0.012\n",
            "[31,    30] loss: 0.013\n",
            "[31,    40] loss: 0.013\n",
            "[32,    10] loss: 0.012\n",
            "[32,    20] loss: 0.011\n",
            "[32,    30] loss: 0.010\n",
            "[32,    40] loss: 0.014\n",
            "[33,    10] loss: 0.030\n",
            "[33,    20] loss: 0.025\n",
            "[33,    30] loss: 0.020\n",
            "[33,    40] loss: 0.020\n",
            "[34,    10] loss: 0.023\n",
            "[34,    20] loss: 0.021\n",
            "[34,    30] loss: 0.016\n",
            "[34,    40] loss: 0.032\n",
            "[35,    10] loss: 0.146\n",
            "[35,    20] loss: 0.119\n",
            "[35,    30] loss: 0.086\n",
            "[35,    40] loss: 0.080\n",
            "[36,    10] loss: 0.093\n",
            "[36,    20] loss: 0.065\n",
            "[36,    30] loss: 0.045\n",
            "[36,    40] loss: 0.044\n",
            "[37,    10] loss: 0.068\n",
            "[37,    20] loss: 0.043\n",
            "[37,    30] loss: 0.031\n",
            "[37,    40] loss: 0.034\n",
            "[38,    10] loss: 0.048\n",
            "[38,    20] loss: 0.045\n",
            "[38,    30] loss: 0.035\n",
            "[38,    40] loss: 0.028\n",
            "[39,    10] loss: 0.019\n",
            "[39,    20] loss: 0.013\n",
            "[39,    30] loss: 0.013\n",
            "[39,    40] loss: 0.009\n",
            "[40,    10] loss: 0.008\n",
            "[40,    20] loss: 0.009\n",
            "[40,    30] loss: 0.008\n",
            "[40,    40] loss: 0.010\n",
            "[41,    10] loss: 0.020\n",
            "[41,    20] loss: 0.016\n",
            "[41,    30] loss: 0.013\n",
            "[41,    40] loss: 0.012\n",
            "[42,    10] loss: 0.013\n",
            "[42,    20] loss: 0.016\n",
            "[42,    30] loss: 0.009\n",
            "[42,    40] loss: 0.010\n",
            "[43,    10] loss: 0.007\n",
            "[43,    20] loss: 0.006\n",
            "[43,    30] loss: 0.006\n",
            "[43,    40] loss: 0.007\n",
            "[44,    10] loss: 0.009\n",
            "[44,    20] loss: 0.008\n",
            "[44,    30] loss: 0.007\n",
            "[44,    40] loss: 0.005\n",
            "[45,    10] loss: 0.004\n",
            "[45,    20] loss: 0.004\n",
            "[45,    30] loss: 0.004\n",
            "[45,    40] loss: 0.004\n",
            "[46,    10] loss: 0.003\n",
            "[46,    20] loss: 0.004\n",
            "[46,    30] loss: 0.003\n",
            "[46,    40] loss: 0.003\n",
            "[47,    10] loss: 0.003\n",
            "[47,    20] loss: 0.003\n",
            "[47,    30] loss: 0.003\n",
            "[47,    40] loss: 0.006\n",
            "[48,    10] loss: 0.010\n",
            "[48,    20] loss: 0.011\n",
            "[48,    30] loss: 0.007\n",
            "[48,    40] loss: 0.006\n",
            "[49,    10] loss: 0.006\n",
            "[49,    20] loss: 0.005\n",
            "[49,    30] loss: 0.004\n",
            "[49,    40] loss: 0.005\n",
            "[50,    10] loss: 0.003\n",
            "[50,    20] loss: 0.003\n",
            "[50,    30] loss: 0.003\n",
            "[50,    40] loss: 0.003\n",
            "[51,    10] loss: 0.002\n",
            "[51,    20] loss: 0.003\n",
            "[51,    30] loss: 0.002\n",
            "[51,    40] loss: 0.003\n",
            "[52,    10] loss: 0.005\n",
            "[52,    20] loss: 0.004\n",
            "[52,    30] loss: 0.003\n",
            "[52,    40] loss: 0.004\n",
            "[53,    10] loss: 0.003\n",
            "[53,    20] loss: 0.003\n",
            "[53,    30] loss: 0.002\n",
            "[53,    40] loss: 0.002\n",
            "[54,    10] loss: 0.002\n",
            "[54,    20] loss: 0.002\n",
            "[54,    30] loss: 0.002\n",
            "[54,    40] loss: 0.004\n",
            "[55,    10] loss: 0.011\n",
            "[55,    20] loss: 0.007\n",
            "[55,    30] loss: 0.005\n",
            "[55,    40] loss: 0.005\n",
            "[56,    10] loss: 0.003\n",
            "[56,    20] loss: 0.003\n",
            "[56,    30] loss: 0.002\n",
            "[56,    40] loss: 0.003\n",
            "[57,    10] loss: 0.003\n",
            "[57,    20] loss: 0.002\n",
            "[57,    30] loss: 0.002\n",
            "[57,    40] loss: 0.002\n",
            "[58,    10] loss: 0.002\n",
            "[58,    20] loss: 0.002\n",
            "[58,    30] loss: 0.002\n",
            "[58,    40] loss: 0.002\n",
            "[59,    10] loss: 0.002\n",
            "[59,    20] loss: 0.002\n",
            "[59,    30] loss: 0.002\n",
            "[59,    40] loss: 0.002\n",
            "[60,    10] loss: 0.001\n",
            "[60,    20] loss: 0.002\n",
            "[60,    30] loss: 0.001\n",
            "[60,    40] loss: 0.002\n",
            "[61,    10] loss: 0.001\n",
            "[61,    20] loss: 0.001\n",
            "[61,    30] loss: 0.002\n",
            "[61,    40] loss: 0.002\n",
            "[62,    10] loss: 0.001\n",
            "[62,    20] loss: 0.001\n",
            "[62,    30] loss: 0.001\n",
            "[62,    40] loss: 0.001\n",
            "[63,    10] loss: 0.001\n",
            "[63,    20] loss: 0.002\n",
            "[63,    30] loss: 0.001\n",
            "[63,    40] loss: 0.002\n",
            "[64,    10] loss: 0.001\n",
            "[64,    20] loss: 0.001\n",
            "[64,    30] loss: 0.001\n",
            "[64,    40] loss: 0.001\n",
            "[65,    10] loss: 0.001\n",
            "[65,    20] loss: 0.001\n",
            "[65,    30] loss: 0.001\n",
            "[65,    40] loss: 0.001\n",
            "[66,    10] loss: 0.001\n",
            "[66,    20] loss: 0.001\n",
            "[66,    30] loss: 0.001\n",
            "[66,    40] loss: 0.001\n",
            "[67,    10] loss: 0.001\n",
            "[67,    20] loss: 0.001\n",
            "[67,    30] loss: 0.001\n",
            "[67,    40] loss: 0.006\n",
            "[68,    10] loss: 0.038\n",
            "[68,    20] loss: 0.031\n",
            "[68,    30] loss: 0.020\n",
            "[68,    40] loss: 0.008\n",
            "[69,    10] loss: 0.009\n",
            "[69,    20] loss: 0.006\n",
            "[69,    30] loss: 0.004\n",
            "[69,    40] loss: 0.005\n",
            "[70,    10] loss: 0.005\n",
            "[70,    20] loss: 0.004\n",
            "[70,    30] loss: 0.004\n",
            "[70,    40] loss: 0.005\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 train images: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 1: 45 %\n",
            "Accuracy of the network on the 10000 test dataset 2: 67 %\n",
            "Accuracy of the network on the 10000 test dataset 3: 85 %\n",
            "Accuracy of the network on the 10000 test dataset 4: 94 %\n",
            "Accuracy of the network on the 10000 test dataset 5: 98 %\n",
            "Accuracy of the network on the 10000 test dataset 6: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 7: 99 %\n",
            "Accuracy of the network on the 10000 test dataset 8: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 9: 100 %\n",
            "Accuracy of the network on the 10000 test dataset 10: 95 %\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AbZaQekCfVjN",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ouBomi5DfVjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2465417a-2385-497e-92bc-823a8dcc3cc1"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i,j in enumerate(train_loss_all):\n",
        "    plt.plot(j,label =\"dataset \"+str(i+1))\n",
        "    \n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training_loss\")\n",
        "\n",
        "plt.legend()\n",
        "fig.savefig(\"Figure.pdf\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDeCPOAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlY1RknNfVjb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}